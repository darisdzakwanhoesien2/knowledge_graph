[
  {
    "entity": "QZ Algorithm",
    "type": "Algorithm",
    "domain": "Generalized Eigenvalue Problems",
    "definition": "An algorithm that reduces a matrix pencil (A, B) to generalized Schur form using unitary transformations.",
    "description": "The QZ algorithm generalizes the QR algorithm to solve A x = λ B x problems. It produces upper triangular matrices S and T such that Q* A Z = S and Q* B Z = T. It is the standard dense solver for generalized eigenvalues.",
    "properties": {
      "Goal": "Compute generalized eigenvalues reliably for matrix pencils.",
      "Applications": ["DAE systems", "Control theory", "Stability analysis"],
      "Methods": ["Generalized Hessenberg reduction", "Unitary similarity"],
      "Examples": ["MATLAB's eig(A, B)"]
    },
    "relations": [
      {"type": "related_to", "target": "Matrix Pencil"},
      {"type": "generalization_of", "target": "QR Algorithm"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_qz_algorithm.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Reflection",
    "type": "Concept",
    "domain": "Linear Algebra",
    "definition": "A linear transformation that flips vectors across a hyperplane while preserving norms.",
    "description": "Reflections form the basis for orthogonal transformations, including Householder reflections. They eliminate components in vectors or matrices while maintaining stability.",
    "properties": {
      "Goal": "Construct norm-preserving transformations used in matrix factorizations.",
      "Applications": ["Householder QR", "Orthogonalization", "Matrix reduction"],
      "Methods": ["Hyperplane reflections", "Orthogonal matrices"],
      "Examples": ["Householder transformation H = I - 2vvᵀ"]
    },
    "relations": [
      {"type": "generalization_of", "target": "Householder Reflection"},
      {"type": "related_to", "target": "Orthogonal Matrix"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_qr_factorization.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Row Reduction",
    "type": "Algorithm",
    "domain": "Linear Algebra",
    "definition": "A sequence of elementary row operations used to simplify matrices to row echelon or reduced row echelon form.",
    "description": "Row reduction is fundamental for solving linear systems, computing ranks, bases, and understanding matrix structure. It underpins Gaussian elimination and many algebraic manipulations.",
    "properties": {
      "Goal": "Transform matrices into simplified forms revealing rank and solution structure.",
      "Applications": ["Solving linear systems", "Rank computation", "Null space"],
      "Methods": ["Elementary row operations", "Pivoting"],
      "Examples": ["RREF calculation for Ax = b"]
    },
    "relations": [
      {"type": "foundation_for", "target": "Gaussian Elimination"},
      {"type": "related_to", "target": "Matrix Rank"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_matrix_simplification.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Schur Complement",
    "type": "Concept",
    "domain": "Matrix Theory",
    "definition": "For a block matrix, the Schur complement of A in M = [[A, B], [C, D]] is S = D - C A⁻¹ B.",
    "description": "The Schur complement appears in block Gaussian elimination, matrix inversion, optimization, and statistics. It captures conditional behavior and plays a crucial role in SPD testing.",
    "properties": {
      "Goal": "Reduce block-matrix operations to smaller components.",
      "Applications": ["Optimization", "Block LU", "Covariance matrices", "Linear systems"],
      "Methods": ["Block elimination", "Matrix inversion"],
      "Examples": ["Used in Kalman filter equations"]
    },
    "relations": [
      {"type": "related_to", "target": "Block Matrix"},
      {"type": "related_to", "target": "Positive Definite Matrix"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_schur_complement.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Search Directions (in CG)",
    "type": "Concept",
    "domain": "Iterative Methods",
    "definition": "The sequence of A-conjugate directions generated by the Conjugate Gradient method.",
    "description": "Search directions determine the efficiency and convergence of CG. Each direction is constructed to be A-orthogonal to all previous ones, ensuring optimality in SPD problems.",
    "properties": {
      "Goal": "Define the sequence of directions used to minimize the error over Krylov subspaces.",
      "Applications": ["CG iterations", "Krylov subspace minimization"],
      "Methods": ["A-orthogonalization", "Residual-based direction updates"],
      "Examples": ["pₖ = rₖ + βₖ pₖ₋₁"]
    },
    "relations": [
      {"type": "subcomponent_of", "target": "Conjugate Gradient Method"},
      {"type": "related_to", "target": "Krylov Subspace"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_cg_methods.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Similarity Transformation",
    "type": "Concept",
    "domain": "Linear Algebra",
    "definition": "A transformation of the form A → S⁻¹ A S with S invertible, preserving eigenvalues.",
    "description": "Similarity transformations classify matrices into equivalence classes and preserve spectral properties. They form the basis for diagonalization, Jordan forms, and spectral algorithms.",
    "properties": {
      "Goal": "Transform matrices while preserving eigenvalues and spectral characteristics.",
      "Applications": ["Diagonalization", "Jordan canonical form", "Invariant subspaces"],
      "Methods": ["Change of basis", "Matrix equivalence"],
      "Examples": ["S⁻¹ A S is diagonalizable"]
    },
    "relations": [
      {"type": "used_in", "target": "Jordan Canonical Form"},
      {"type": "used_in", "target": "Eigenvalue Decomposition"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_similarity.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Spectral Gap",
    "type": "Concept",
    "domain": "Spectral Theory",
    "definition": "The difference between two adjacent eigenvalues, often used to characterize convergence rates.",
    "description": "A larger spectral gap leads to faster convergence of iterative methods such as power iteration and CG. Spectral gap also governs mixing times in Markov chains.",
    "properties": {
      "Goal": "Measure separation between key eigenvalues.",
      "Applications": ["Power iteration", "Graph Laplacians", "Markov chains"],
      "Methods": ["Eigenvalue analysis"],
      "Examples": ["Gap = λ₂ - λ₁ for stochastic matrices"]
    },
    "relations": [
      {"type": "related_to", "target": "Spectrum Λ(A)"},
      {"type": "related_to", "target": "Power Iteration"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_spectral_gap.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Spectral Mapping Theorem",
    "type": "Concept",
    "domain": "Matrix Functions",
    "definition": "The theorem stating that for analytic functions f, the spectrum satisfies Λ(f(A)) = f(Λ(A)).",
    "description": "Spectral mapping allows computation of matrix functions by applying the scalar function to eigenvalues. It is fundamental to understanding behaviors of matrix exponentials and polynomials.",
    "properties": {
      "Goal": "Relate eigenvalues of functions of matrices to functions of eigenvalues.",
      "Applications": ["Matrix exponential", "Matrix square root", "Krylov methods"],
      "Methods": ["Analytic functions", "Contour integrals"],
      "Examples": ["Λ(e^A) = e^{Λ(A)}"]
    },
    "relations": [
      {"type": "foundation_for", "target": "Matrix Function"},
      {"type": "related_to", "target": "Spectrum Λ(A)"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_matrix_functions.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Spectral Radius",
    "type": "Concept",
    "domain": "Spectral Theory",
    "definition": "The quantity ρ(A) = max |λᵢ|, the maximum magnitude of eigenvalues of A.",
    "description": "Spectral radius determines convergence of matrix iterations, stability, and power method behavior. It is a core concept in numerical linear algebra and operator theory.",
    "properties": {
      "Goal": "Measure the largest eigenvalue magnitude.",
      "Applications": ["Stability analysis", "Power iteration", "Matrix norms"],
      "Methods": ["Eigenvalue computation"],
      "Examples": ["ρ(A) < 1 ensures convergence of Neumann series"]
    },
    "relations": [
      {"type": "related_to", "target": "Power Iteration"},
      {"type": "related_to", "target": "Numerical Radius"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_spectral_radius.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Strassen Algorithm",
    "type": "Algorithm",
    "domain": "Matrix Computations",
    "definition": "A subcubic algorithm for matrix multiplication with complexity O(n^{log₂7}) ≈ O(n^{2.807}).",
    "description": "Strassen’s method reduces the number of multiplications required for matrix multiplication. It forms the basis for fast algebraic algorithms and motivates subcubic research.",
    "properties": {
      "Goal": "Multiply matrices faster than classical O(n³) time.",
      "Applications": ["Large dense matrices", "Parallel computing"],
      "Methods": ["Divide-and-conquer", "Bilinear algorithms"],
      "Examples": ["7 multiplications instead of 8 for 2x2 blocks"]
    },
    "relations": [
      {"type": "subcategory_of", "target": "Subcubic Algorithms"},
      {"type": "related_to", "target": "Computational Complexity (Matrix Multiplication)"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_fast_multiplication.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Subcubic Algorithms",
    "type": "Concept",
    "domain": "Matrix Computations",
    "definition": "Algorithms for matrix multiplication with asymptotic complexity below O(n³).",
    "description": "Subcubic algorithms include Strassen, Coppersmith–Winograd, and subsequent improvements. They are foundational in algebraic complexity theory.",
    "properties": {
      "Goal": "Push theoretical limits of fast matrix multiplication.",
      "Applications": ["Computational complexity", "Large-scale algebraic computations"],
      "Methods": ["Tensor decomposition", "Bilinear complexity"],
      "Examples": ["Coppersmith–Winograd algorithm"]
    },
    "relations": [
      {"type": "generalization_of", "target": "Strassen Algorithm"},
      {"type": "related_to", "target": "Matrix Multiplication"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_fast_matrix_algorithms.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Subspace Decomposition",
    "type": "Concept",
    "domain": "Linear Algebra",
    "definition": "A decomposition of a vector or matrix space into multiple complementary subspaces.",
    "description": "Subspace decompositions underpin direct-sum representations, block structure in matrices, and multilevel numerical algorithms. They enable modularization of linear algebraic problems.",
    "properties": {
      "Goal": "Split spaces into lower-dimensional components with structured relationships.",
      "Applications": ["Multigrid", "Block matrices", "Direct sum"],
      "Methods": ["Orthogonal complement", "Projection operators"],
      "Examples": ["V = U1 ⊕ U2 ⊕ … ⊕ Uk"]
    },
    "relations": [
      {"type": "related_to", "target": "Direct Sum"},
      {"type": "related_to", "target": "Matrix Subspace"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_subspaces.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  },
  {
    "entity": "Subspace Iteration",
    "type": "Algorithm",
    "domain": "Eigenvalue Algorithms",
    "definition": "An iterative method that simultaneously computes multiple dominant eigenvectors by repeatedly applying A to a subspace.",
    "description": "Subspace iteration generalizes the power method to multiple vectors. It forms the basis of modern eigensolvers such as LOBPCG and block Krylov methods.",
    "properties": {
      "Goal": "Approximate several eigenpairs by iterating on subspaces.",
      "Applications": ["Large-scale eigenvalue problems", "PDE solvers", "Block Krylov methods"],
      "Methods": ["Orthogonalization", "Rayleigh–Ritz projection"],
      "Examples": ["Block power iteration"]
    },
    "relations": [
      {"type": "related_to", "target": "Power Iteration"},
      {"type": "related_to", "target": "Krylov Subspace"}
    ],
    "metadata": {
      "created_by": "system",
      "source": "lecture_notes_eigensolvers.pdf",
      "created_at": "2025-11-18",
      "version": "1.0",
      "synonyms": []
    }
  }
]
