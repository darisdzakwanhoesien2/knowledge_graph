[
  {
    "front": "üß© Singular Value Decomposition\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The Singular Value Decomposition (SVD) of a matrix A ‚àà ‚ÑÇ^{m√ón} is a factorization A = U Œ£ V^*, where U and V are unitary matrices with orthonormal columns, and Œ£ is a diagonal matrix with non-negative singular values œÉ‚ÇÅ ‚â• œÉ‚ÇÇ ‚â• ‚ãØ ‚â• œÉ_n ‚â• 0 on the diagonal.\n\n**Description:** SVD provides the best low-rank approximation of a matrix in both spectral and Frobenius norms, enabling optimal matrix compression, dimensionality reduction, and solving least squares problems. It decomposes any matrix into orthogonal bases that capture the principal directions of variation.\n\n**Goal:** Provide orthogonal factorization and low-rank approximations.\n**Applications:** Data compression, Principal component analysis, Pseudo-inverse\n**Methods:** Golub-Reinsch algorithm, Divide-and-conquer, Iterative methods\n**Examples:** A ‚âà U_k Œ£_k V_k^* for rank-k approximation"
  },
  {
    "front": "üß© Singular Value\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The singular values of a matrix A are the non-negative square roots of the eigenvalues of A^*A (or AA^*), ordered as œÉ‚ÇÅ ‚â• œÉ‚ÇÇ ‚â• ‚ãØ ‚â• œÉ_r > 0, where r is the rank of A.\n\n**Description:** Singular values measure the 'importance' or 'strength' of each principal direction in the matrix. The largest singular value œÉ‚ÇÅ equals the operator norm ||A||, and they uniquely determine the best low-rank approximations.\n\n**Goal:** Quantify the magnitude of principal components in matrix decomposition.\n**Applications:** Rank determination, Condition number computation (œÉ‚ÇÅ/œÉ_r), Numerical stability analysis\n**Methods:** Eigenvalue decomposition of A^*A, Square root of eigenvalues\n**Examples:** œÉ‚ÇÅ = ||A|| = max_{||x||=1} ||Ax|| represents maximum stretch"
  },
  {
    "front": "üß© Unitary Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A unitary matrix Q ‚àà ‚ÑÇ^{n√ón} satisfies Q^*Q = I, meaning its columns (and rows) form an orthonormal basis that preserves the Euclidean norm: ||Qx|| = ||x|| for all x ‚àà ‚ÑÇ^n.\n\n**Description:** Unitary matrices represent rotations and reflections in complex space. They appear in QR decomposition, SVD, and eigenvalue problems, preserving distances and angles during transformations.\n\n**Goal:** Preserve norms and inner products in linear transformations.\n**Applications:** QR decomposition, Singular Value Decomposition, Numerical linear algebra algorithms, Quantum computing gates\n**Methods:** Gram-Schmidt process, Householder reflections\n**Examples:** Q = [q‚ÇÅ ‚ãØ q‚Çô] where {q‚±º} are orthonormal vectors"
  },
  {
    "front": "üß© Low Rank Approximation\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The low-rank approximation problem finds a rank-k matrix F_k that minimizes ||A - F_k||_F among all rank-k matrices, solved uniquely by SVD: F_k = ‚àë_{j=1}^k œÉ‚±º u‚±º v‚±º^*.\n\n**Description:** SVD provides the optimal rank-k approximation in Frobenius norm, equivalent to keeping the k largest singular values. This compresses data while minimizing reconstruction error.\n\n**Goal:** Approximate high-dimensional matrix with lower-rank version minimizing ||A - F_k||_F.\n**Applications:** Data compression, Dimensionality reduction, Image denoising, Recommendation systems\n**Methods:** Truncated SVD, Eckart-Young theorem\n**Examples:** F_k = U_k Œ£_k V_k^* where error = ‚àë_{j=k+1}^r œÉ‚±º¬≤"
  },
  {
    "front": "üß© Frobenius Norm\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The Frobenius norm of A ‚àà ‚ÑÇ^{m√ón} is ||A||_F = ‚àö(‚àë_{j=1}^m ‚àë_{k=1}^n |a_{jk}|¬≤) = ‚àötrace(A^*A), measuring the Euclidean norm of the matrix treated as a vector in ‚ÑÇ^{mn}.\n\n**Description:** Widely used in matrix approximation problems due to computational convenience and equivalence to SVD error. It's unitarily invariant: ||Q‚ÇÅAQ‚ÇÇ||_F = ||A||_F for unitary Q‚ÇÅ, Q‚ÇÇ.\n\n**Goal:** Measure matrix 'size' as vector in high-dimensional space.\n**Applications:** Low-rank approximation error, Matrix compression, Least squares problems\n**Methods:** ‚àö(sum of squared entries), ‚àötrace(A^*A)\n**Examples:** ||A||_F¬≤ = ‚àë œÉ‚±º¬≤ where œÉ‚±º are singular values"
  },
  {
    "front": "üß© Operator Norm\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The operator norm ||A|| = max_{||x||=1} ||Ax|| measures the maximum stretch factor of the linear transformation A: ‚ÑÇ^n ‚Üí ‚ÑÇ^m.\n\n**Description:** For SVD, ||A|| = œÉ‚ÇÅ, the largest singular value. It quantifies how much A can amplify unit vectors and determines numerical stability.\n\n**Goal:** Measure maximum amplification by linear transformation.\n**Applications:** Condition number (||A|| ‚ãÖ ||A‚Åª¬π||), Stability analysis, Spectral radius bounds\n**Methods:** Largest singular value œÉ‚ÇÅ, max_{||x||=1} ||Ax||"
  },
  {
    "front": "üß© Matrix Function\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** A matrix function f(A) extends scalar functions to matrices A, defined via Jordan form, power series, or other representations, enabling operations like exponentials or square roots on matrices.\n\n**Description:** Matrix functions are computed iteratively for large matrices, used in differential equations, control theory, and eigenvalue problems.\n\n**Goal:** Extend scalar functions to matrices while preserving algebraic structure.\n**Applications:** Matrix exponential in ODEs, Matrix logarithm in geometry, Sign function in control\n**Methods:** Schur-Parlett, Scaling-and-squaring, Pad√© approximation, Contour integral\n**Examples:** exp(A), A^{1/2}, sign(A)"
  },
  {
    "front": "üß© Matrix Square Root\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The matrix square root X satisfies X^2 = A for a matrix A, with the principal square root being positive definite if A is.\n\n**Description:** It is computed iteratively, e.g., via Newton's method, for applications in statistics, control, and geometry.\n\n**Goal:** Find X such that X^2 = A.\n**Applications:** Covariance matrices, Riemannian metrics, Polar decomposition\n**Methods:** Newton iteration, Denman-Beavers, Schur method\n**Examples:** X = sqrt(A) for SPD A"
  },
  {
    "front": "üß© Newton's Iteration for Square Root\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Newton's iteration for the matrix square root updates X_{k+1} = (X_k + A X_k^{-1}) / 2, starting from X_0 = I or other, converging quadratically to sqrt(A).\n\n**Description:** It is stable for positive definite A, with safeguards for convergence, used in large-scale computations via iterative solvers.\n\n**Goal:** Compute sqrt(A) iteratively.\n**Applications:** Matrix sign function, Algebraic Riccati equations\n**Methods:** Matrix inversion at each step, Quadratic convergence\n**Examples:** X_{k+1} = (X_k + A X_k^{-1}) / 2"
  },
  {
    "front": "üß© Eigenvalue Problem\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The eigenvalue problem seeks scalars Œª and vectors x ‚â† 0 such that A x = Œª x for matrix A.\n\n**Description:** Iterative methods like power iteration or Lanczos are used for large sparse matrices, shifting spectrum for better conditioning.\n\n**Goal:** Compute spectrum and invariant subspaces for analysis and transformation of linear operators.\n**Applications:** Vibration analysis, Stability of dynamical systems, Google PageRank, Quantum mechanics\n**Methods:** Power iteration, QR algorithm, Arnoldi iteration, Jacobi-Davidson\n**Examples:** Ax = Œªx, det(A‚àíŒªI)=0 (characteristic equation)"
  },
  {
    "front": "üß© Spectral Shift\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Spectral shift transforms the eigenvalue problem to (A - Œº I) x = (Œª - Œº) x, shifting eigenvalues by Œº to target specific parts of the spectrum or improve conditioning.\n\n**Description:** Choosing Œº near a target eigenvalue accelerates convergence in iterative methods; for real Œº, it can make the matrix positive definite.\n\n**Goal:** Adjust spectrum for better numerical properties.\n**Applications:** Interior eigenvalues, Deflation, Preconditioning\n**Methods:** Œº close to target Œª, Œº = (Œª_min + Œª_max)/2\n**Examples:** Œõ(A - Œº I) = Œõ(A) - Œº"
  },
  {
    "front": "üß© Positive Definite Shift\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Positive definite shift chooses Œº such that A - Œº I is positive definite, e.g., Œº < Œª_min(A) for symmetric A, enabling use of CG or other SPD methods.\n\n**Description:** It stabilizes iterations and bounds condition number; for estimated ÀÜŒº ‚âà Œª_min, adjust with a > 0 to ensure positivity.\n\n**Goal:** Make shifted matrix SPD for efficient solving.\n**Applications:** Shift-and-invert, Preconditioned eigensolvers\n**Methods:** Œº = ÀÜŒº - a, a > 0, Rayleigh quotient estimate\n**Examples:** A - Œº I with Œº = ÀÜŒº - a"
  },
  {
    "front": "üß© Spectrum Œõ(A)\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The spectrum Œõ(A) is the set of all eigenvalues of matrix A.\n\n**Description:** Shifting modifies the spectrum as Œõ(A - Œº I) = Œõ(A) - Œº, used to isolate eigenvalues or improve numerical properties.\n\n**Goal:** Characterize matrix via its eigenvalues.\n**Applications:** Spectral radius, Conditioning, Stability\n**Methods:** Eigen decomposition, Characteristic polynomial\n**Examples:** Œõ(A) subset C for A in C^{n x n}"
  },
  {
    "front": "üß© Matrix Computations\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** The study and development of algorithms for performing operations on matrices, including addition, multiplication, inversion, decomposition, and solving linear systems.\n\n**Description:** Matrix computations are central to numerical analysis, scientific computing, and computational science. They underpin applications in physics simulations, data analysis, machine learning, and computer graphics. The computational complexity of classical algorithms for matrix multiplication is O(n¬≥), while storage is typically O(n¬≤).\n\n**Goal:** Solve matrix-related problems efficiently in numerical contexts.\n**Applications:** Scientific computing, Data analysis, PDE solving, Linear algebra problems\n**Methods:** Factorizations (LU, SVD), Eigenvalue computations, Iterative solvers, Subspace approximations\n**Examples:** Solving Ax = b, Approximating large matrices"
  },
  {
    "front": "üß© Inner Product\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A binary operation on two vectors in ‚ÑÇ‚Åø that produces a scalar, defined as ‚ü®x, y‚ü© = y·µÄxÃÑ = ‚àë‚±º x‚±º»≥‚±º, where x, y ‚àà ‚ÑÇ‚Åø.\n\n**Description:** The inner product (also known as dot product in real vector spaces) measures similarity between vectors and forms the foundation of orthogonality, norms, and projections. In complex spaces, it involves conjugation to ensure positive definiteness of the induced norm.\n\n**Goal:** Quantify angle and similarity between vectors; enable orthogonal decomposition\n**Applications:** Gram-Schmidt orthogonalization, Least squares, Signal processing, Quantum mechanics\n**Methods:** N/A\n**Examples:** ‚ü®x, y‚ü© = ‚àë x‚±º»≥‚±º over j=1 to n"
  },
  {
    "front": "üß© Computational Complexity (Matrix Multiplication)\nüìò Domain: Algorithm Analysis",
    "back": "**Definition:** The asymptotic resource requirement for matrix multiplication and related operations, classically O(n¬≥) time and O(n¬≤) space for n√ón matrices.\n\n**Description:** Standard matrix multiplication of two n√ón matrices requires O(n¬≥) arithmetic operations using the naive algorithm. While theoretical improvements exist (e.g., Strassen‚Äôs O(n¬≤.‚Å∏‚Å∞‚Å∑)), practical methods remain close to O(n¬≥). Storage scales quadratically as O(n¬≤).\n\n**Goal:** Assess efficiency and scalability of matrix algorithms\n**Applications:** Performance prediction, Algorithm selection, Hardware design\n**Methods:** Big-O notation, Arithmetic circuit complexity\n**Examples:** O(n¬≥) time, O(n¬≤) space"
  },
  {
    "front": "üß© LU Factorization\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** LU factorization decomposes a matrix A into a lower triangular matrix L and an upper triangular matrix U such that A = LU, or with pivoting PA = LU, enabling efficient solution of linear systems.\n\n**Description:** It is a fundamental algorithm with O(n^3) complexity, used for solving Ax = b by forward and backward substitution, and approximated for large matrices using subspace products.\n\n**Goal:** Decompose matrices for efficient linear system solving.\n**Applications:** Numerical linear algebra, PDE discretization, Data processing\n**Methods:** Gaussian elimination, Pivoting for stability, Partial pivoting\n**Examples:** A = LU for square matrices, PA = LU with permutation P"
  },
  {
    "front": "üß© Product of Matrix Subspaces\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The product of matrix subspaces V1 V2 is defined as {V1 V2 : V1 ‚àà V1, V2 ‚àà V2}, providing a framework for approximating matrices with fewer parameters than full rank.\n\n**Description:** It allows factoring matrices into low-complexity forms like sum of outer products, useful for large n with small k, achieving 2nk parameters and potential for lower computational costs.\n\n**Goal:** Approximate matrices efficiently using subspace products.\n**Applications:** Large matrix factorization, PDE discretization, Data storage\n**Methods:** Subspace multiplication, Rank-k approximation, Norm minimization\n**Examples:** A ‚âà sum_{j=1}^k u_j v_j^*, I + V1 V2 inversion"
  },
  {
    "front": "üß© Gram-Schmidt Orthogonalization\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Gram-Schmidt orthogonalization transforms a set of linearly independent vectors into an orthonormal set using projections and normalization.\n\n**Description:** It is used in QR factorization and subspace computations, with classical and modified variants for numerical stability.\n\n**Goal:** Produce orthonormal bases from vector sets.\n**Applications:** QR decomposition, Subspace orthogonalization, Least squares\n**Methods:** Classical Gram-Schmidt, Modified Gram-Schmidt, Householder reflections\n**Examples:** Orthogonalizing columns of A"
  },
  {
    "front": "üß© Low-Rank Approximation\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Low-rank approximation finds a matrix Fk of rank k that minimizes ||A - Fk|| for a given norm, often using SVD or subspace products for efficiency.\n\n**Description:** It reduces storage and computation for large matrices, with subspace products offering 2nk parameters for rank-k approximations.\n\n**Goal:** Approximate high-dimensional matrices with lower rank.\n**Applications:** Data compression, Noise reduction, Machine learning\n**Methods:** SVD truncation, Subspace product, Randomized algorithms\n**Examples:** Fk = sum u_j v_j^*, min_{rank(F)=k} ||A - F||"
  },
  {
    "front": "üß© Hermitian Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A Hermitian matrix M satisfies M^* = M, where * denotes conjugate transpose, with real eigenvalues and orthogonal eigenvectors.\n\n**Description:** It requires n^2 real parameters for storage, or fewer in subspace approximations, used in quantum mechanics and signal processing.\n\n**Goal:** Model self-adjoint operators in complex spaces.\n**Applications:** Quantum computing, Covariance matrices, Spectral analysis\n**Methods:** Eigen decomposition, Cholesky factorization (positive definite)\n**Examples:** M with real diagonal and conjugate symmetric off-diagonals"
  },
  {
    "front": "üß© Matrix Subspace\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A subset V ‚äÜ ‚ÑÇ^{n√ón} of matrices closed under addition and scalar multiplication, forming a vector space over ‚ÑÇ.\n\n**Description:** Matrix subspaces are used to model families of matrices sharing structural properties (e.g., invertibility, symmetry, triangularity). They enable low-rank approximations, invariant subspaces in eigenvalue problems, and structured matrix factorizations such as LU or SVD within constrained sets.\n\n**Goal:** Group matrices with common algebraic or analytic traits to simplify computations and preserve structure in factorizations.\n**Applications:** Structured matrix factorization, Low-rank approximation, Krylov methods, Invariant subspace computation\n**Methods:** Span construction, Closure under operations, Equivalence via similarity\n**Examples:** span{I, A}, span{A, B}, upper triangular matrices"
  },
  {
    "front": "üß© Nonsingular Matrix Subspace\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A matrix subspace V ‚äÜ ‚ÑÇ^{n√ón} that contains at least one invertible matrix (det V ‚â† 0 for some V ‚àà V).\n\n**Description:** Nonsingular subspaces guarantee the existence of invertible elements, enabling the definition of the set of inverses Inv(V) = {V‚Åª¬π : V ‚àà V, det V ‚â† 0}. Such subspaces are crucial for ensuring well-defined inverse-based factorizations (e.g., LU within V).\n\n**Goal:** Ensure invertibility within a structured family of matrices to support factorization algorithms.\n**Applications:** LU factorization, Generalized inverse, Structured preconditioning\n**Methods:** Perturbation arguments, Open-dense property in finite dimensions\n**Examples:** V = span{I, A} for nonsingular A, Upper triangular matrices with nonzero diagonals"
  },
  {
    "front": "üß© Inv(V)\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The set of inverses of all invertible matrices in a matrix subspace V: Inv(V) = {W : W = V‚Åª¬π, V ‚àà V, det V ‚â† 0}.\n\n**Description:** For nonsingular matrix subspaces, Inv(V) is itself a matrix subspace under mild conditions. This enables dual-space factorizations and ensures closure under inversion within structured sets, vital for algorithmic stability in LU-type methods.\n\n**Goal:** Construct a subspace of inverses to maintain structure across factorization steps.\n**Applications:** LU within subspace, Preconditioner design, Group-inverse problems\n**Methods:** Similarity transformation, Closure proof via nonsingularity\n**Examples:** Inv(upper triangular) = lower triangular, Inv(Hermitian) = Hermitian"
  },
  {
    "front": "üß© LU Factorization within Subspace\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Decomposition of a matrix A ‚àà V into A = LU where L is lower triangular and U is upper triangular, both belonging to predefined matrix subspaces derived from V.\n\n**Description:** By identifying nonsingular matrix subspaces V and W = Inv(V), LU factorization can be confined within V √ó W, ensuring all intermediate matrices remain structured. This supports specialized algorithms for banded, symmetric positive-definite, or approximate low-rank problems.\n\n**Goal:** Perform Gaussian elimination while preserving membership in designated matrix subspaces.\n**Applications:** Banded LU, SPD factorization, Low-rank updates, Krylov-based solvers\n**Methods:** Schur complement, Pivot-free elimination, Subspace projection\n**Examples:** A ‚àà upper triangular ‚Üí L = I, U = A, A ‚àà V, L ‚àà V, U ‚àà Inv(V)"
  },
  {
    "front": "üß© Krylov Subspace\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** The matrix subspace K_j(A; I) = span{I, A, A¬≤, ..., A^{j-1}} generated by powers of a matrix A starting from the identity.\n\n**Description:** Krylov subspaces lie at the core of iterative methods for large-scale eigenvalue and linear system problems (GMRES, Lanczos, Arnoldi). Their dimension grows linearly until saturation at the degree of the minimal polynomial of A.\n\n**Goal:** Generate subspaces for iterative numerical methods.\n**Applications:** Eigenvalue computation, Linear solvers, Matrix exponentiation\n**Methods:** Arnoldi iteration, Lanczos algorithm, GMRES\n**Examples:** K_1(A; I) = span{I}, K_2(A; I) = span{I, A}"
  },
  {
    "front": "üß© Matrix Polynomials\nüìò Domain: Linear Algebra",
    "back": "**Definition:** Functions p(z) = ‚àë_{k=0}^m c_k z^k evaluated at matrices: p(A) = ‚àë_{k=0}^m c_k A^k for A ‚àà ‚ÑÇ^{n√ón}.\n\n**Description:** Matrix polynomials map matrix subspaces to themselves if V is closed under powers of its members. They define minimal and characteristic polynomials, enable Cayley-Hamilton applications, and support function-based iterative methods.\n\n**Goal:** Extend scalar polynomial theory to matrices for spectral analysis and function approximation.\n**Applications:** Cayley-Hamilton theorem, Matrix exponential,  preconditioning, Spectral projectors\n**Methods:** Horner scheme, Paterson-Stockmeyer, Jordan form evaluation\n**Examples:** p(z) = z¬≤‚àí2z+1 ‚Üí p(A) = A¬≤‚àí2A+I = 0, q(z) = det(A‚àízI) ‚Üí q(A)=0"
  },
  {
    "front": "üß© Projection Operator\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A linear operator P on ‚ÑÇ^n such that P¬≤ = P, projecting vectors onto a subspace R(P) with kernel R(I‚àíP).\n\n**Description:** Projections decompose spaces into direct sums and are building blocks for oblique and orthogonal projections in iterative solvers. Orthogonal projections minimize least-squares error and appear in GMRES and conjugate gradients.\n\n**Goal:** Project vectors onto subspaces.\n**Applications:** Least squares, Subspace methods, Decomposition\n**Methods:** Orthogonal projection, Oblique projection\n**Examples:** P with P^2 = P, I - P as complement projection"
  },
  {
    "front": "üß© Preconditioning\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Preconditioning is the transformation of a linear system Ax = b into an equivalent system M^{-1}Ax = M^{-1}b (left preconditioning) or AM^{-1}y = b with x = M^{-1}y (right), where M ‚âà A is a nonsingular matrix that is inexpensive to invert or solve with, designed to improve the convergence rate of iterative methods.\n\n**Description:** The goal is to cluster the eigenvalues of the preconditioned matrix away from zero, making Krylov subspace methods like CG, GMRES, or MINRES converge in significantly fewer iterations. Effective preconditioners balance accuracy (M close to A) with computational cost (O(n) or O(n log n) per application).\n\n**Goal:** Accelerate iterative solver convergence.\n**Applications:** Large sparse systems, PDE solvers\n**Methods:** Incomplete LU, Jacobi, Multigrid, Domain decomposition\n**Examples:** Left preconditioning: solve M y = c then A x = M y"
  },
  {
    "front": "üß© Jacobi Preconditioner\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** The Jacobi preconditioner is M = diag(A) + œâI, where diag(A) contains the diagonal entries of A and œâ ‚àà ‚ÑÇ is an optional damping parameter (often œâ = 0). It is the simplest splitting M = D, N = A - D.\n\n**Description:** Extremely cheap to apply (O(n) scaling) and parallelizable. Effective when A is diagonally dominant. Corresponds to the classical Jacobi iterative method. Often used as a baseline or building block in more sophisticated preconditioners.\n\n**Goal:** Damp high-frequency error components using only diagonal information.\n**Applications:** Smooth initial guess for multigrid, Baseline for preconditioner comparison, Diagonally dominant systems\n**Methods:** Extract diagonal, Optional damping œâ, Inverse is element-wise division\n**Examples:** M_i = diag(A) + œâI with œâ = 0 for standard Jacobi"
  },
  {
    "front": "üß© Incomplete LU Factorization\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Incomplete LU (ILU) computes factors LÃÇ and UÃÇ such that LÃÇUÃÇ ‚âà A with the same sparsity pattern as A (or a prescribed superset), dropping fill-in during Gaussian elimination to control memory and cost.\n\n**Description:** Widely used for general sparse matrices. Variants include ILU(0) (no fill), ILU(k) (level-k fill), and ILUT (threshold dropping). Provides a trade-off between robustness and efficiency. Often combined with reordering (e.g., RCM, nested dissection).\n\n**Goal:** Approximate LU factorization while preserving sparsity for use as preconditioner.\n**Applications:** Finite element systems, CFD problems, Circuit simulation\n**Methods:** Modified Gaussian elimination with drop tolerance, Dual-threshold ILUT, Level-of-fill ILU(k)\n**Examples:** ILU(0): drop all fill-in outside original nonzero pattern"
  },
  {
    "front": "üß© Sparse Approximate Inverse\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A sparse approximate inverse (SAI) preconditioner computes a sparse matrix M such that ||I - MA||_F or ||I - AW||_F is minimized over all matrices W with a prescribed sparsity pattern, effectively approximating A^{-1} directly.\n\n**Description:** Application cost is O(nnz(M)) matrix-vector products. Excellent parallel performance due to explicit form. Often constructed via Frobenius norm minimization on independent columns or using QR factorizations of local submatrices.\n\n**Goal:** Construct explicit sparse approximation to A^{-1} for fast matrix-vector products\n**Applications:** Highly parallel architectures (GPU, many-core), Unstructured grids, High-performance computing\n**Methods:** Frobenius norm minimization per column, SPAID (sparse approximate inverse by distance), FSAI (factorized sparse approximate inverse)\n**Examples:** min_W ||AW - I||_F with W constrained to sparsity pattern of A^k"
  },
  {
    "front": "üß© Left Preconditioning\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Left preconditioning transforms Ax = b into M^{-1}Ax = M^{-1}b, preserving the solution x but altering the residual norm to ||M^{-1}(b - Ax)||.\n\n**Description:** Common in practice because it directly improves the convergence diagnostics used by Krylov methods (residual-based stopping criteria). Does not change the right-hand side in a way that affects eigenvalue clustering as strongly as right preconditioning.\n\n**Goal:** Improve convergence while keeping solution unchanged and monitoring preconditioned residuals.\n**Applications:** Standard choice in most libraries (PETSc, Trilinos), GMRES with ILU\n**Methods:** Apply M^{-1} to system matrix and RHS"
  },
  {
    "front": "üß© Right Preconditioning\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Right preconditioning transforms Ax = b into AM^{-1}y = b with x = M^{-1}y, preserving the residual norm ||b - Ax|| but solving for an intermediate variable y.\n\n**Description:** Often preferred when the preconditioner naturally approximates the inverse action from the right. Common in domain decomposition and multigrid methods. Requires extra step to recover x.\n\n**Goal:** Cluster eigenvalues of AM^{-1} while monitoring true residuals.\n**Applications:** Additive Schwarz, Algebraic multigrid (AMG), When M approximates A from the right\n**Methods:** Solve AM^{-1}y = b, then x = M^{-1}y"
  },
  {
    "front": "üß© Splitting Methods\nüìò Domain: Iterative Methods",
    "back": "**Definition:** A matrix splitting decomposes A = M - N where M is nonsingular and easy to invert. The iteration x^{k+1} = M^{-1}Nx^k + M^{-1}b converges if œÅ(M^{-1}N) < 1.\n\n**Description:** Foundation of classical iterative methods (Jacobi, Gauss-Seidel, SOR) and modern preconditioning. The spectral radius of the iteration matrix B = M^{-1}N determines convergence rate.\n\n**Goal:** Construct fixed-point iterations via A = M - N with œÅ(M^{-1}N) < 1.\n**Applications:** Stationary iterative methods, Preconditioner design, Convergence theory\n**Methods:** M = D (Jacobi), M = D+L (Gauss-Seidel), M = (D+œâL)/œâ (SOR)"
  },
  {
    "front": "üß© Cholesky Factorization\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A decomposition of a Hermitian positive definite matrix A into A = R*R, where R is an upper triangular matrix with positive diagonal entries.\n\n**Description:** Cholesky factorization exploits symmetry and positive definiteness to reduce computational cost from O(n¬≥) for general LU to approximately n¬≥/3 flops while requiring only n¬≤/2 storage. It is widely used in optimization, Monte Carlo simulations, and solving normal equations.\n\n**Goal:** Efficiently factorize Hermitian positive definite matrices with half the storage and one-third the operations of LU.\n**Applications:** Quadratic programming, Kalman filtering, Covariance decomposition, Monte Carlo methods\n**Methods:** Block elimination, Outer product form, Inner product form\n**Examples:** A = R*R with R upper triangular and diag(R) > 0"
  },
  {
    "front": "üß© Positive Definite Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A Hermitian matrix A ‚àà ‚ÑÇ^{n√ón} such that (Ax, x) > 0 for all nonzero x ‚àà ‚ÑÇ^n.\n\n**Description:** Positive definite matrices arise in energy minimization, covariance modeling, and elliptic PDEs. They guarantee unique Cholesky factors, stable inverses, and real positive eigenvalues. The property is preserved under congruence: M*A*M is positive definite if M is invertible.\n\n**Goal:** Model strictly convex quadratic forms and ensure numerical stability in factorizations.\n**Applications:** Optimization, Statistics, Physics simulations, Control theory\n**Methods:** Sylvester's criterion, Cholesky test, Eigenvalue analysis\n**Examples:** Covariance matrices, Hessians of convex functions, A = R*R"
  },
  {
    "front": "üß© Sylvester Equation\nüìò Domain: Control Theory",
    "back": "**Definition:** A matrix equation of the form AX ‚àí XB = C, where A, B, C are given matrices and X is unknown.\n\n**Description:** The Sylvester equation models linear system interconnections and appears in control design, model reduction, and eigenvalue assignment. When œÉ(A) ‚à© œÉ(B) = ‚àÖ, it has a unique solution solvable in O(n¬≥) via Schur triangulation or vectorization (kronecker form).\n\n**Goal:** Solve for coupling matrix X in interconnected linear systems or compute Lyapunov functions.\n**Applications:** Stability analysis, Model order reduction, Riccati equations, Pole placement\n**Methods:** Schur method, Hessenberg-Schur algorithm, Bartels-Stewart, Vectorization\n**Examples:** AX ‚àí XA* = ‚àíBB* (Lyapunov), AX ‚àí XB = C with diagonal A, B"
  },
  {
    "front": "üß© Discrete Fourier Transform (DFT)\nüìò Domain: Signal Processing",
    "back": "**Definition:** A linear transformation mapping a sequence x‚ÇÄ, ..., x_{n-1} to coefficients c‚±º = ‚àë‚Çñ x‚Çñ œâ^{jk}, where œâ = e^{-2œÄi/n}, represented by the Vandermonde matrix F‚Çô.\n\n**Description:** The DFT diagonalizes circulant matrices and enables fast convolution, filtering, and spectral analysis. The Fast Fourier Transform (FFT) computes it in O(n log n) using divide-and-conquer on power-of-two sizes.\n\n**Goal:** Decompose signals into frequency components and accelerate convolution/correlation\n**Applications:** Audio processing, Image compression, PDE solvers, Polynomial multiplication\n**Methods:** Cooley-Tukey FFT, Radix-2, Split-radix, Bluestein\n**Examples:** F‚ÇÑ = [[1,1,1,1], [1,-1,1,-1], ...], FFT of length 2^l"
  },
  {
    "front": "üß© Fast Fourier Transform (FFT)\nüìò Domain: Numerical Algorithms",
    "back": "**Definition:** An efficient algorithm for computing the DFT in O(n log n) operations by recursively splitting into even/odd indices when n is a power of two.\n\n**Description:** The Cooley-Tukey FFT reduces DFT complexity from O(n¬≤) to O(n log n) using butterfly operations and twiddle factors. It is foundational in digital signal processing and enables real-time spectral analysis.\n\n**Goal:** Compute DFT with minimal arithmetic operations using recursive decomposition\n**Applications:** Spectral methods, FFT-based convolution, MRI reconstruction, Audio synthesis\n**Methods:** Decimation-in-time, Decimation-in-frequency, Bit-reversal, In-place computation\n**Examples:** Radix-2 butterfly: y‚±º = x_{2j} + œâ^j x_{2j+1}"
  },
  {
    "front": "üß© Schur Triangulation\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A similarity transformation A = Q T Q* where T is upper triangular and Q is unitary, revealing eigenvalues on the diagonal of T.\n\n**Description:** Schur form is the foundation for robust eigenvalue computation and solving Sylvester equations. The QR-based Francis algorithm computes it in O(n¬≥) with high backward stability. Real Schur form handles complex conjugate pairs.\n\n**Goal:** Reduce matrix to triangular form under unitary similarity to expose eigenvalues and enable block algorithms.\n**Applications:** Eigenvalue problems, Sylvester solvers, Matrix functions, Control theory\n**Methods:** Francis QR algorithm, Hessenberg reduction, Double shift\n**Examples:** A = Q T Q* with T upper triangular, diag(T) = eigenvalues"
  },
  {
    "front": "üß© Iterative Methods\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Iterative methods are algorithms that generate a sequence of approximations x_j to the solution x of a linear system Ax = b, starting from an initial guess and refining it until convergence.\n\n**Description:** They are preferred for large sparse systems where direct methods like Gaussian elimination are O(n^3) and computationally expensive, with per-iteration costs often O(n^2) or less, such as O(n) for sparse or O(n log n) with structured matrices.\n\n**Goal:** Solve large linear systems efficiently without full factorization.\n**Applications:** PDE discretizations, Optimization, Eigenproblems\n**Methods:** Krylov subspace methods, Conjugate gradient, GMRES, Preconditioned iterations\n**Examples:** x_{j+1} = x_j + correction, Convergence when ||r_j|| small"
  },
  {
    "front": "üß© Linear System\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** A linear system is an equation of the form Ax = b where A is an n x n matrix, x is the unknown vector, and b is the right-hand side vector.\n\n**Description:** For large n (e.g., 10^4 to 10^8), iterative methods are used due to high O(n^3) cost of direct solvers, especially when A is sparse or structured.\n\n**Goal:** Find x such that Ax = b.\n**Applications:** Scientific simulations, Machine learning, Engineering\n**Methods:** Direct (LU, QR), Iterative (CG, GMRES)\n**Examples:** A in C^{n x n}, b in C^n"
  },
  {
    "front": "üß© Arnoldi Process\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The Arnoldi process builds an orthonormal basis Q‚±º for the Krylov subspace K‚±º(A; b) and a Hessenberg matrix H‚±º such that A Q‚±º = Q‚±º‚Çä‚ÇÅ HÃÇ‚±º.\n\n**Description:** It uses Gram‚ÄìSchmidt-like orthogonalisation to compute basis vectors q‚Çñ recursively, enabling reduced-order projections for solving systems or eigenvalues.\n\n**Goal:** Orthogonalise Krylov basis for stable computations.\n**Applications:** GMRES, Eigenvalue solvers, Matrix functions\n**Methods:** Recursive computation: h‚Çñ,‚Çñ‚Çã‚ÇÅ q‚Çñ = A q‚Çñ‚Çã‚ÇÅ ‚àí Œ£ h‚Çó,‚Çñ‚Çã‚ÇÅ q‚Çó\n**Examples:** Q‚±º‚Çä‚ÇÅ HÃÇ‚±º = A Q‚±º"
  },
  {
    "front": "üß© GMRES\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Generalised Minimal Residual (GMRES) is an iterative method that finds x‚±º in x‚ÇÄ + K‚±º(A; r‚ÇÄ) minimising ‚Äñb ‚àí A x‚±º‚Äñ‚ÇÇ for nonsymmetric systems.\n\n**Description:** It uses Arnoldi to build the basis and solves a least-squares problem with the Hessenberg matrix at each step, restarting when j is large.\n\n**Goal:** Minimise residual norm over Krylov subspace.\n**Applications:** Nonsymmetric linear systems, PDE solvers\n**Methods:** Arnoldi orthogonalisation, Least squares on HÃÇ‚±º y ‚àí Œ± e‚ÇÅ\n**Examples:** x‚±º = Q‚±º y‚±º where y‚±º minimises ‚ÄñHÃÇ‚±º y ‚àí Œ± e‚ÇÅ‚Äñ"
  },
  {
    "front": "üß© Conjugate Gradient Method\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The Conjugate Gradient (CG) method solves symmetric positive definite systems Ax = b by minimising the A-norm error ‚Äñx ‚àí x‚±º‚Äñ‚Çê over the Krylov subspace.\n\n**Description:** It generates A-conjugate search directions implicitly via a three-term recurrence, equivalent to Lanczos for symmetric matrices, with optimal polynomial approximation properties.\n\n**Goal:** Minimise quadratic form (x, A x)/2 ‚àí (b, x).\n**Applications:** SPD linear systems, Optimisation (Newton-CG)\n**Methods:** Conjugate directions, Residual orthogonalisation\n**Examples:** x‚±º‚Çä‚ÇÅ = x‚±º + Œ±‚±º p‚±º with p‚±º‚Çä‚ÇÅ = r‚±º‚Çä‚ÇÅ + Œ≤‚±º p‚±º"
  },
  {
    "front": "üß© Residual Norm\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The residual norm ‚Äñr‚±º‚Äñ = ‚Äñb ‚àí A x‚±º‚Äñ measures how well the approximate solution x‚±º satisfies the system Ax = b.\n\n**Description:** In iterative methods, residuals decrease monotonically in GMRES, and are used to test convergence.\n\n**Goal:** Quantify approximation error in equation satisfaction.\n**Applications:** Convergence testing, Stopping criteria\n**Methods:** Euclidean norm, Relative residual\n**Examples:** ‚Äñr‚±º‚Çä‚ÇÅ‚Äñ ‚â§ ‚Äñr‚±º‚Äñ in GMRES"
  },
  {
    "front": "üß© A-Norm Error\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The A-norm error ‚Äñx ‚àí x‚±º‚Äñ‚Çê = ( (x ‚àí x‚±º), A (x ‚àí x‚±º) )^{1/2} measures the error in the energy norm for SPD A.\n\n**Description:** CG minimises this norm over the Krylov subspace, relating to the quadratic form minimised in the system.\n\n**Goal:** Quantify solution error in energy sense.\n**Applications:** CG convergence analysis, Variational problems\n**Methods:** Defined via inner product (x, A y)\n**Examples:** Min ‚Äñx ‚àí x‚±º‚Äñ‚Çê in CG"
  },
  {
    "front": "üß© Polynomial Approximation\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Polynomial approximation in iterative methods views x‚±º = p‚±º‚Çã‚ÇÅ(A) b as a polynomial in A applied to b, minimising residuals or errors via min-max problems over polynomials.\n\n**Description:** Convergence bounds use Chebyshev or other polynomials to estimate rates based on eigenvalue distribution.\n\n**Goal:** Approximate A‚Åª¬π b via polynomials in A.\n**Applications:** Convergence analysis, Accelerated methods\n**Methods:** Min-max over deg ‚â§ j-1, Chebyshev acceleration\n**Examples:** min_{deg p ‚â§ j-1, p(0)=1} max_Œª |p(Œª)|"
  },
  {
    "front": "üß© Hessenberg Matrix\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** A Hessenberg matrix H is upper triangular except for the subdiagonal, arising in Arnoldi as the projection of A onto the Krylov basis.\n\n**Description:** It simplifies least-squares solves in GMRES and eigenvalue computations.\n\n**Goal:** Reduce matrix for efficient projections.\n**Applications:** GMRES minimisation, QR algorithm\n**Methods:** From Arnoldi: HÃÇ‚±º with subdiagonal\n**Examples:** H‚±º tridiagonal in Lanczos"
  },
  {
    "front": "üß© Orthogonal Complement\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The orthogonal complement of a subspace V is the set of vectors perpendicular to all elements in V, denoted V^‚ä•.\n\n**Description:** For projections, if P is orthogonal, then R(P) ‚ä• R(I - P), ensuring the decomposition is orthogonal.\n\n**Goal:** Decompose space into perpendicular subspaces.\n**Applications:** Gram-Schmidt, Least squares, Spectral methods\n**Methods:** Dot product zero condition, Null space of transpose\n**Examples:** R(I - P) as complement of R(P)"
  },
  {
    "front": "üß© Gershgorin Circle Theorem\nüìò Domain: Matrix Analysis",
    "back": "**Definition:** Every eigenvalue of A ‚àà ‚ÑÇ^{n√ón} lies in at least one disk D‚±º = {z : |z ‚àí a‚±º‚±º| ‚â§ R‚±º}, where R‚±º = Œ£_{l‚â†j} |a‚±º‚Çó|}.\n\n**Description:** Provides eigenvalue localization without computation. Disks are centered at diagonal entries with radii equal to off-diagonal row sums. Useful for bounding spectral radius and detecting diagonal dominance.\n\n**Goal:** Locate eigenvalues in complex plane using only matrix entries.\n**Applications:** Convergence analysis, Error bounding, Preconditioning design\n**Methods:** Row-sum computation, Union of disks, Refinement via similarity\n**Examples:** Strictly diagonally dominant ‚Üí eigenvalues in disjoint disks"
  },
  {
    "front": "üß© Power Iteration\nüìò Domain: Eigenvalue Algorithms",
    "back": "**Definition:** An iterative method that computes the dominant eigenvalue and eigenvector by repeatedly applying A to a vector and normalizing: q^{(k)} = A q^{(k-1)} / ||A q^{(k-1)}||, Œª^{(k)} = (A q^{(k)}, q^{(k)}).\n\n**Description:** Converges linearly to the eigenvector corresponding to the largest-magnitude eigenvalue if |Œª‚ÇÅ| > |Œª‚ÇÇ| ‚â• ... ‚â• |Œª‚Çô|. Convergence rate is |Œª‚ÇÇ/Œª‚ÇÅ|. Inverse iteration targets smallest or shifted eigenvalues.\n\n**Goal:** Find dominant eigenpair with minimal storage and simple operations.\n**Applications:** PageRank, Principal Component Analysis, Vibration modes\n**Methods:** Rayleigh quotient, Normalization, Deflation, Shift-and-invert\n**Examples:** q^{(k)} ‚âà x‚ÇÅ + O((Œª‚ÇÇ/Œª‚ÇÅ)^k)"
  },
  {
    "front": "üß© Householder Reflection\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A unitary matrix H = I ‚àí 2vv*/(v*v) that reflects a vector x across the hyperplane perpendicular to v, mapping x to œÉe‚ÇÅ.\n\n**Description:** Used in QR factorization and Hessenberg reduction to introduce zeros below the subdiagonal. Numerically stable and requires O(n) operations per reflection. Essential for implicit QR algorithm.\n\n**Goal:** Zero out selected entries via unitary similarity while preserving eigenvalues.\n**Applications:** QR decomposition, Hessenberg form, Tridiagonalization\n**Methods:** Sign choice for stability, WY representation, Blocked Householder\n**Examples:** H x = ‚àísign(x‚ÇÅ) ||x|| e‚ÇÅ"
  },
  {
    "front": "üß© QR Algorithm\nüìò Domain: Eigenvalue Computation",
    "back": "**Definition:** An iterative method that computes the Schur form via repeated QR decompositions: A_{k+1} = R_k Q_k, with A_0 = A. With shifts, converges cubically to upper triangular form.\n\n**Description:** The de facto standard for dense eigenvalue problems. Implicit version uses Householder/Bulge chasing to reduce cost from O(n¬≥) per iteration to O(n). Francis shift accelerates convergence.\n\n**Goal:** Compute all eigenvalues (and optionally eigenvectors) via unitary similarity to triangular form.\n**Applications:** MATLAB eig, LAPACK, Control theory, PDE solvers\n**Methods:** Hessenberg reduction, Francis double shift, Deflation, Balancing\n**Examples:** A_{k+1} = Q_k* A_k Q_k"
  },
  {
    "front": "üß© Generalized Eigenvalue Problem\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Find Œª ‚àà ‚ÑÇ and nonzero x ‚àà ‚ÑÇ‚Åø such that Ax = ŒªBx, with A, B ‚àà ‚ÑÇ^{n√ón}.\n\n**Description:** Arises in structural dynamics, control, and Markov chains. Reduced to standard form if B invertible (M = B‚Åª¬πA). QZ algorithm generalizes QR using unitary transformations to triangularize both matrices.\n\n**Goal:** Solve coupled systems or weighted eigenvalue problems.\n**Applications:** Vibration with constraints, Markov chain stationary distribution, Optimal control\n**Methods:** QZ algorithm, Cholesky + standard EVP, Shift-and-invert\n**Examples:** Ax = ŒªBx with B positive definite"
  },
  {
    "front": "üß© Field of Values\nüìò Domain: Matrix Analysis",
    "back": "**Definition:** The set F(A) = {x*Ax : x ‚àà ‚ÑÇ‚Åø, ||x||=1}, also known as the numerical range.\n\n**Description:** Convex, compact set containing all eigenvalues. Bounds spectral radius and condition number. For normal matrices, F(A) is the convex hull of eigenvalues.\n\n**Goal:** Characterize operator behavior beyond eigenvalues.\n**Applications:** Stability analysis, Convergence of iterations, Pseudospectrum approximation\n**Methods:** Rayleigh quotient, Hausdorff-Toeplitz theorem, Discretization\n**Examples:** F(A) = conv(œÉ(A)) if A normal"
  },
  {
    "front": "üß© LU Factorization with Partial Pivoting\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** LU factorization with partial pivoting decomposes a matrix A ‚àà ‚ÑÇ^{n√ón} into PA = LU, where P is a permutation matrix, L is unit lower triangular with |l_{ij}| ‚â§ 1 for i > j, and U is upper triangular. Partial pivoting selects the largest absolute entry in the current column as the pivot to minimize numerical instability.\n\n**Description:** This algorithm enhances the stability of Gaussian elimination by row permutations to avoid small pivots. It is the standard method for solving linear systems Ax = b in practice, balancing computational cost (O(n¬≥)) with robustness against round-off errors in floating-point arithmetic.\n\n**Goal:** Stable triangular factorization of a matrix for solving linear systems and computing inverses.\n**Applications:** Solving Ax = b, Matrix inversion, Determinant computation, Condition number estimation\n**Methods:** Gaussian elimination with row pivoting, In-place storage using single array, Compact WY representation for L\n**Examples:** 4√ó4 matrix example with pivots 8, 17/4, -6/7, 2 showing growth control"
  },
  {
    "front": "üß© Partial Pivoting\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A pivoting strategy in Gaussian elimination that, at step k, permutes rows so that the entry of largest magnitude in column k (from row k to n) becomes the pivot, ensuring |pivot| = max_{i‚â•k} |a_{ik}|.\n\n**Description:** Partial pivoting prevents division by small pivots, reducing amplification of round-off errors. It guarantees that all subdiagonal entries in L satisfy |l_{ij}| ‚â§ 1, bounding the growth factor œÅ ‚â§ 2^{n-1} in theory, though typically much smaller in practice.\n\n**Goal:** Minimize numerical error propagation during elimination by choosing largest available pivot.\n**Applications:** LU factorization, Linear system solving, Matrix decomposition in finite precision\n**Methods:** Row interchange before elimination step, Column scanning for max absolute value\n**Examples:** P1 swaps rows to bring largest entry to diagonal position"
  },
  {
    "front": "üß© Permutation Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A permutation matrix P is a square matrix with exactly one 1 in each row and column and 0s elsewhere. Multiplying PA permutes the rows of A; AP permutes columns.\n\n**Description:** In LU with partial pivoting, P represents the cumulative row interchanges. Since P^{-1} = P^T = P^*, it preserves norms: ||Px|| = ||x||. The final factorization is PA = LU.\n\n**Goal:** Represent row or column reordering in matrix factorizations.\n**Applications:** Pivoting in LU, Reordering for sparsity, Graph relabeling\n**Methods:** Identity with swapped rows, Product of elementary permutation matrices\n**Examples:** P = [0 1; 1 0] swaps rows 1 and 2"
  },
  {
    "front": "üß© Growth Factor\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** The growth factor rho in LU factorization with partial pivoting is defined as rho = max_{i,j} |u_{ij}| / max_{i,j} |a_{ij}|, measuring the largest entry in U relative to the original matrix A.\n\n**Description:** Controls backward stability: computed factors satisfy L_hat U_hat = P A + delta A with ||delta A|| / ||A|| = O(rho epsilon_machine). Partial pivoting keeps rho moderate in practice, though worst-case rho = 2^{n-1} is possible.\n\n**Goal:** Quantify element growth during Gaussian elimination to assess numerical stability.\n**Applications:** Backward error analysis, Condition estimation, Pivoting strategy evaluation\n**Methods:** Ratio of max |u_{ij}| to max |a_{ij}|, Monitored during factorization\n**Examples:** Wilkinson's matrix gives rho approx 2^{n-1}"
  },
  {
    "front": "üß© Condition Number\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** The condition number Œ∫(W) of a nonsingular matrix W is Œ∫(W) = ||W|| ‚ãÖ ||W^{-1}||, with respect to any consistent matrix norm. For the 1-norm or ‚àû-norm, Œ∫‚ÇÅ(W) = œÉ‚ÇÅ/œÉ‚Çô where œÉ are singular values.\n\n**Description:** Measures sensitivity of linear system solution to perturbations. Large Œ∫ implies ill-conditioned system: small changes in input cause large output changes. In LU context, related to pivot size and growth.\n\n**Goal:** Quantify sensitivity of Ax = b to perturbations in A or b.\n**Applications:** Error bounds in linear solvers, Preconditioning design, Numerical stability analysis\n**Methods:** SVD-based: Œ∫‚ÇÇ = œÉ_max / œÉ_min, 1-norm estimation via Hager's method\n**Examples:** Œ∫(A) ‚âà 10^k ‚áí lose k digits of accuracy"
  },
  {
    "front": "üß© Gaussian Elimination\nüìò Domain: Linear Algebra",
    "back": "**Definition:** Gaussian elimination transforms a linear system Ax = b into upper triangular form Ux = c via row operations: adding multiples of one row to another. With pivoting, it forms the basis of LU factorization.\n\n**Description:** Core algorithm for solving linear systems. Without pivoting, unstable for small pivots. With partial pivoting, becomes robust standard method. Can be expressed as sequence of rank-1 updates or multiplier storage in L.\n\n**Goal:** Reduce system to triangular form for back substitution.\n**Applications:** Linear system solving, Matrix factorization, Determinant via product of diagonals\n**Methods:** Forward elimination, Back substitution, Pivoting variants\n**Examples:** 4√ó4 system reduced step-by-step with P, L, U shown"
  },
  {
    "front": "üß© Matrix Product V1 V2\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The matrix product V1 V2 represents the set of all matrices formed by multiplying elements from subspaces V1 and V2, i.e., {V1 V2 : V1 ‚àà V1, V2 ‚àà V2}.\n\n**Description:** This construct is used to approximate or factor large matrices A by finding subspaces such that A lies in V1 V2, enabling low-parameter representations.\n\n**Goal:** Represent matrices as products of subspace elements for factorization.\n**Applications:** Matrix approximation, Low-rank factoring, Compression\n**Methods:** Subspace identification, Optimization over subspaces\n**Examples:** A ‚âà V1 V2 for V1, V2 low-dimensional"
  },
  {
    "front": "üß© Range of Projection\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The range R(P) of a projection P is the set {y : y = Px for some x}, representing the subspace onto which P projects.\n\n**Description:** For orthogonal projections, R(P) is perpendicular to R(I - P), forming an orthogonal decomposition of the space.\n\n**Goal:** Define the projected subspace.\n**Applications:** Subspace identification, Dimensionality reduction\n**Methods:** Column span of P, Fixed points of P\n**Examples:** R(P) = span{columns of P}"
  },
  {
    "front": "üß© Algorithmic Factoring\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Algorithmic factoring refers to computational methods for decomposing matrices into structured forms like products of subspaces or triangular factors.\n\n**Description:** It leverages Krylov subspaces and projections to factor matrices efficiently, especially for large-scale problems.\n\n**Goal:** Factor matrices using algorithmic techniques.\n**Applications:** Large linear systems, Eigenproblems, Approximations\n**Methods:** Subspace iteration, Projection-based factoring\n**Examples:** A = V1 W^{-1} using subspaces"
  },
  {
    "front": "üß© Invariant Subspace\nüìò Domain: Linear Algebra",
    "back": "**Definition:** An invariant subspace W for matrix A satisfies A W ‚äÜ W, meaning A maps W into itself.\n\n**Description:** Invariant subspaces are key in spectral theory and factoring, often identified via Krylov methods or projections.\n\n**Goal:** Find subspaces stable under matrix action.\n**Applications:** Eigen decomposition, Schur form, Model reduction\n**Methods:** Subspace iteration, Arnoldi process\n**Examples:** Eigenvector spans 1D invariant subspace"
  },
  {
    "front": "üß© Square Matrix A\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A square matrix A ‚àà ‚ÑÇ^{n√ón} is a matrix with equal rows and columns, central to linear transformations and eigenvalue problems.\n\n**Description:** In factoring contexts, A is decomposed using subspaces, projections, or iterations for computational efficiency.\n\n**Goal:** Represent linear operators on finite-dimensional spaces.\n**Applications:** Systems of equations, Transformations, Spectral analysis\n**Methods:** Factorization, Iteration, Diagonalization\n**Examples:** A with complex entries"
  }
]