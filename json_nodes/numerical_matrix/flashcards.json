[
  {
    "front": "üß© Singular Value Decomposition\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The Singular Value Decomposition (SVD) of a matrix A ‚àà ‚ÑÇ^{m√ón} is a factorization A = U Œ£ V^*, where U and V are unitary matrices with orthonormal columns, and Œ£ is a diagonal matrix with non-negative singular values œÉ‚ÇÅ ‚â• œÉ‚ÇÇ ‚â• ‚ãØ ‚â• œÉ_n ‚â• 0 on the diagonal.\n\n**Description:** SVD provides the best low-rank approximation of a matrix in both spectral and Frobenius norms, enabling optimal matrix compression, dimensionality reduction, and solving least squares problems. It decomposes any matrix into orthogonal bases that capture the principal directions of variation.\n\n**Goal:** Provide orthogonal factorization and low-rank approximations.\n**Applications:** Data compression, Principal component analysis, Pseudo-inverse\n**Methods:** Golub-Reinsch algorithm, Divide-and-conquer, Iterative methods\n**Examples:** A ‚âà U_k Œ£_k V_k^* for rank-k approximation"
  },
  {
    "front": "üß© Singular Value\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The singular values of a matrix A are the non-negative square roots of the eigenvalues of A^*A (or AA^*), ordered as œÉ‚ÇÅ ‚â• œÉ‚ÇÇ ‚â• ‚ãØ ‚â• œÉ_r > 0, where r is the rank of A.\n\n**Description:** Singular values measure the 'importance' or 'strength' of each principal direction in the matrix. The largest singular value œÉ‚ÇÅ equals the operator norm ||A||, and they uniquely determine the best low-rank approximations.\n\n**Goal:** Quantify the magnitude of principal components in matrix decomposition.\n**Applications:** Rank determination, Condition number computation (œÉ‚ÇÅ/œÉ_r), Numerical stability analysis\n**Methods:** Eigenvalue decomposition of A^*A, Square root of eigenvalues\n**Examples:** œÉ‚ÇÅ = ||A|| = max_{||x||=1} ||Ax|| represents maximum stretch"
  },
  {
    "front": "üß© Unitary Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A unitary matrix Q ‚àà ‚ÑÇ^{n√ón} satisfies Q^*Q = I, meaning its columns (and rows) form an orthonormal basis that preserves the Euclidean norm: ||Qx|| = ||x|| for all x ‚àà ‚ÑÇ^n.\n\n**Description:** Unitary matrices represent rotations and reflections in complex space. They appear in QR decomposition, SVD, and eigenvalue problems, preserving distances and angles during transformations.\n\n**Goal:** Preserve norms and inner products in linear transformations.\n**Applications:** QR decomposition, Singular Value Decomposition, Numerical linear algebra algorithms, Quantum computing gates\n**Methods:** Gram-Schmidt process, Householder reflections\n**Examples:** Q = [q‚ÇÅ ‚ãØ q‚Çô] where {q‚±º} are orthonormal vectors"
  },
  {
    "front": "üß© Low Rank Approximation\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The low-rank approximation problem finds a rank-k matrix F_k that minimizes ||A - F_k||_F among all rank-k matrices, solved uniquely by SVD: F_k = ‚àë_{j=1}^k œÉ‚±º u‚±º v‚±º^*.\n\n**Description:** SVD provides the optimal rank-k approximation in Frobenius norm, equivalent to keeping the k largest singular values. This compresses data while minimizing reconstruction error.\n\n**Goal:** Approximate high-dimensional matrix with lower-rank version minimizing ||A - F_k||_F.\n**Applications:** Data compression, Dimensionality reduction, Image denoising, Recommendation systems\n**Methods:** Truncated SVD, Eckart-Young theorem\n**Examples:** F_k = U_k Œ£_k V_k^* where error = ‚àë_{j=k+1}^r œÉ‚±º¬≤"
  },
  {
    "front": "üß© Frobenius Norm\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The Frobenius norm of A ‚àà ‚ÑÇ^{m√ón} is ||A||_F = ‚àö(‚àë_{j=1}^m ‚àë_{k=1}^n |a_{jk}|¬≤) = ‚àötrace(A^*A), measuring the Euclidean norm of the matrix treated as a vector in ‚ÑÇ^{mn}.\n\n**Description:** Widely used in matrix approximation problems due to computational convenience and equivalence to SVD error. It's unitarily invariant: ||Q‚ÇÅAQ‚ÇÇ||_F = ||A||_F for unitary Q‚ÇÅ, Q‚ÇÇ.\n\n**Goal:** Measure matrix 'size' as vector in high-dimensional space.\n**Applications:** Low-rank approximation error, Matrix compression, Least squares problems\n**Methods:** ‚àö(sum of squared entries), ‚àötrace(A^*A)\n**Examples:** ||A||_F¬≤ = ‚àë œÉ‚±º¬≤ where œÉ‚±º are singular values"
  },
  {
    "front": "üß© Operator Norm\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The operator norm ||A|| = max_{||x||=1} ||Ax|| measures the maximum stretch factor of the linear transformation A: ‚ÑÇ^n ‚Üí ‚ÑÇ^m.\n\n**Description:** For SVD, ||A|| = œÉ‚ÇÅ, the largest singular value. It quantifies how much A can amplify unit vectors and determines numerical stability.\n\n**Goal:** Measure maximum amplification by linear transformation.\n**Applications:** Condition number (||A|| ‚ãÖ ||A‚Åª¬π||), Stability analysis, Spectral radius bounds\n**Methods:** Largest singular value œÉ‚ÇÅ, max_{||x||=1} ||Ax||"
  },
  {
    "front": "üß© Matrix Function\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** A matrix function f(A) extends scalar functions to matrices A, defined via Jordan form, power series, or other representations, enabling operations like exponentials or square roots on matrices.\n\n**Description:** Matrix functions are computed iteratively for large matrices, used in differential equations, control theory, and eigenvalue problems.\n\n**Goal:** Extend scalar functions to matrices while preserving algebraic structure.\n**Applications:** Matrix exponential in ODEs, Matrix logarithm in geometry, Sign function in control\n**Methods:** Schur-Parlett, Scaling-and-squaring, Pad√© approximation, Contour integral\n**Examples:** exp(A), A^{1/2}, sign(A)"
  },
  {
    "front": "üß© Matrix Square Root\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The matrix square root X satisfies X^2 = A for a matrix A, with the principal square root being positive definite if A is.\n\n**Description:** It is computed iteratively, e.g., via Newton's method, for applications in statistics, control, and geometry.\n\n**Goal:** Find X such that X^2 = A.\n**Applications:** Covariance matrices, Riemannian metrics, Polar decomposition\n**Methods:** Newton iteration, Denman-Beavers, Schur method\n**Examples:** X = sqrt(A) for SPD A"
  },
  {
    "front": "üß© Newton's Iteration for Square Root\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Newton's iteration for the matrix square root updates X_{k+1} = (X_k + A X_k^{-1}) / 2, starting from X_0 = I or other, converging quadratically to sqrt(A).\n\n**Description:** It is stable for positive definite A, with safeguards for convergence, used in large-scale computations via iterative solvers.\n\n**Goal:** Compute sqrt(A) iteratively.\n**Applications:** Matrix sign function, Algebraic Riccati equations\n**Methods:** Matrix inversion at each step, Quadratic convergence\n**Examples:** X_{k+1} = (X_k + A X_k^{-1}) / 2"
  },
  {
    "front": "üß© Eigenvalue Problem\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The eigenvalue problem seeks scalars Œª and vectors x ‚â† 0 such that A x = Œª x for matrix A.\n\n**Description:** Iterative methods like power iteration or Lanczos are used for large sparse matrices, shifting spectrum for better conditioning.\n\n**Goal:** Compute spectrum and invariant subspaces for analysis and transformation of linear operators.\n**Applications:** Vibration analysis, Stability of dynamical systems, Google PageRank, Quantum mechanics\n**Methods:** Power iteration, QR algorithm, Arnoldi iteration, Jacobi-Davidson\n**Examples:** Ax = Œªx, det(A‚àíŒªI)=0 (characteristic equation)"
  },
  {
    "front": "üß© Spectral Shift\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Spectral shift transforms the eigenvalue problem to (A - Œº I) x = (Œª - Œº) x, shifting eigenvalues by Œº to target specific parts of the spectrum or improve conditioning.\n\n**Description:** Choosing Œº near a target eigenvalue accelerates convergence in iterative methods; for real Œº, it can make the matrix positive definite.\n\n**Goal:** Adjust spectrum for better numerical properties.\n**Applications:** Interior eigenvalues, Deflation, Preconditioning\n**Methods:** Œº close to target Œª, Œº = (Œª_min + Œª_max)/2\n**Examples:** Œõ(A - Œº I) = Œõ(A) - Œº"
  },
  {
    "front": "üß© Positive Definite Shift\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Positive definite shift chooses Œº such that A - Œº I is positive definite, e.g., Œº < Œª_min(A) for symmetric A, enabling use of CG or other SPD methods.\n\n**Description:** It stabilizes iterations and bounds condition number; for estimated ÀÜŒº ‚âà Œª_min, adjust with a > 0 to ensure positivity.\n\n**Goal:** Make shifted matrix SPD for efficient solving.\n**Applications:** Shift-and-invert, Preconditioned eigensolvers\n**Methods:** Œº = ÀÜŒº - a, a > 0, Rayleigh quotient estimate\n**Examples:** A - Œº I with Œº = ÀÜŒº - a"
  },
  {
    "front": "üß© Spectrum Œõ(A)\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The spectrum Œõ(A) is the set of all eigenvalues of matrix A.\n\n**Description:** Shifting modifies the spectrum as Œõ(A - Œº I) = Œõ(A) - Œº, used to isolate eigenvalues or improve numerical properties.\n\n**Goal:** Describe intrinsic behavior of linear operators.\n**Applications:** Eigenvalue problems, Stability analysis, Iterative method convergence, Spectral decomposition\n**Methods:** Characteristic polynomial, Schur decomposition, Jordan form\n**Examples:** Spectrum of a diagonal matrix, Spectrum of SPD matrices (real, positive)"
  },
  {
    "front": "üß© Matrix Computations\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** The study and development of algorithms for performing operations on matrices, including addition, multiplication, inversion, decomposition, and solving linear systems.\n\n**Description:** Matrix computations are central to numerical analysis, scientific computing, and computational science. They underpin applications in physics simulations, data analysis, machine learning, and computer graphics. The computational complexity of classical algorithms for matrix multiplication is O(n¬≥), while storage is typically O(n¬≤).\n\n**Goal:** Solve matrix-related problems efficiently in numerical contexts.\n**Applications:** Scientific computing, Data analysis, PDE solving, Linear algebra problems\n**Methods:** Factorizations (LU, SVD), Eigenvalue computations, Iterative solvers, Subspace approximations\n**Examples:** Solving Ax = b, Approximating large matrices"
  },
  {
    "front": "üß© Inner Product\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A binary operation on two vectors in ‚ÑÇ‚Åø that produces a scalar, defined as ‚ü®x, y‚ü© = y·µÄxÃÑ = ‚àë‚±º x‚±º»≥‚±º, where x, y ‚àà ‚ÑÇ‚Åø.\n\n**Description:** The inner product (also known as dot product in real vector spaces) measures similarity between vectors and forms the foundation of orthogonality, norms, and projections. In complex spaces, it involves conjugation to ensure positive definiteness of the induced norm.\n\n**Goal:** Quantify angle and similarity between vectors; enable orthogonal decomposition\n**Applications:** Gram-Schmidt orthogonalization, Least squares, Signal processing, Quantum mechanics\n**Methods:** N/A\n**Examples:** ‚ü®x, y‚ü© = ‚àë x‚±º»≥‚±º over j=1 to n"
  },
  {
    "front": "üß© Computational Complexity (Matrix Multiplication)\nüìò Domain: Algorithm Analysis",
    "back": "**Definition:** The asymptotic resource requirement for matrix multiplication and related operations, classically O(n¬≥) time and O(n¬≤) space for n√ón matrices.\n\n**Description:** Standard matrix multiplication of two n√ón matrices requires O(n¬≥) arithmetic operations using the naive algorithm. While theoretical improvements exist (e.g., Strassen‚Äôs O(n¬≤.‚Å∏‚Å∞‚Å∑)), practical methods remain close to O(n¬≥). Storage scales quadratically as O(n¬≤).\n\n**Goal:** Assess efficiency and scalability of matrix algorithms\n**Applications:** Performance prediction, Algorithm selection, Hardware design\n**Methods:** Big-O notation, Arithmetic circuit complexity\n**Examples:** O(n¬≥) time, O(n¬≤) space"
  },
  {
    "front": "üß© LU Factorization\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** LU factorization decomposes a matrix A into a lower triangular matrix L and an upper triangular matrix U such that A = LU, or with pivoting PA = LU, enabling efficient solution of linear systems.\n\n**Description:** It is a fundamental algorithm with O(n^3) complexity, used for solving Ax = b by forward and backward substitution, and approximated for large matrices using subspace products.\n\n**Goal:** Decompose matrices for efficient linear system solving.\n**Applications:** Numerical linear algebra, PDE discretization, Data processing\n**Methods:** Gaussian elimination, Pivoting for stability, Partial pivoting\n**Examples:** A = LU for square matrices, PA = LU with permutation P"
  },
  {
    "front": "üß© Product of Matrix Subspaces\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The product of matrix subspaces V1 V2 is defined as {V1 V2 : V1 ‚àà V1, V2 ‚àà V2}, providing a framework for approximating matrices with fewer parameters than full rank.\n\n**Description:** It allows factoring matrices into low-complexity forms like sum of outer products, useful for large n with small k, achieving 2nk parameters and potential for lower computational costs.\n\n**Goal:** Approximate matrices efficiently using subspace products.\n**Applications:** Large matrix factorization, PDE discretization, Data storage\n**Methods:** Subspace multiplication, Rank-k approximation, Norm minimization\n**Examples:** A ‚âà sum_{j=1}^k u_j v_j^*, I + V1 V2 inversion"
  },
  {
    "front": "üß© Gram-Schmidt Orthogonalization\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Gram-Schmidt orthogonalization transforms a set of linearly independent vectors into an orthonormal set using projections and normalization.\n\n**Description:** It is used in QR factorization and subspace computations, with classical and modified variants for numerical stability.\n\n**Goal:** Produce orthonormal bases from vector sets.\n**Applications:** QR decomposition, Subspace orthogonalization, Least squares\n**Methods:** Classical Gram-Schmidt, Modified Gram-Schmidt, Householder reflections\n**Examples:** Orthogonalizing columns of A"
  },
  {
    "front": "üß© Low-Rank Approximation\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Low-rank approximation finds a matrix Fk of rank k that minimizes ||A - Fk|| for a given norm, often using SVD or subspace products for efficiency.\n\n**Description:** It reduces storage and computation for large matrices, with subspace products offering 2nk parameters for rank-k approximations.\n\n**Goal:** Approximate high-dimensional matrices with lower rank.\n**Applications:** Data compression, Noise reduction, Machine learning\n**Methods:** SVD truncation, Subspace product, Randomized algorithms\n**Examples:** Fk = sum u_j v_j^*, min_{rank(F)=k} ||A - F||"
  },
  {
    "front": "üß© Hermitian Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A Hermitian matrix M satisfies M^* = M, where * denotes conjugate transpose, with real eigenvalues and orthogonal eigenvectors.\n\n**Description:** It requires n^2 real parameters for storage, or fewer in subspace approximations, used in quantum mechanics and signal processing.\n\n**Goal:** Model self-adjoint operators in complex spaces.\n**Applications:** Quantum computing, Covariance matrices, Spectral analysis\n**Methods:** Eigen decomposition, Cholesky factorization (positive definite)\n**Examples:** M with real diagonal and conjugate symmetric off-diagonals"
  },
  {
    "front": "üß© Conjugate Gradient Method\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** An iterative Krylov method for solving symmetric positive definite (SPD) linear systems Ax = b by minimizing the A-norm of the error over expanding Krylov subspaces.\n\n**Description:** CG constructs a sequence of search directions that are A-conjugate, guaranteeing error minimization properties and finite termination in exact arithmetic. It is widely used in PDE solvers, scientific computing, and large sparse systems where direct methods are too expensive.\n\n**Goal:** Minimise quadratic form (x, A x)/2 ‚àí (b, x).\n**Applications:** SPD linear systems, Optimisation (Newton-CG)\n**Methods:** Conjugate directions, Residual orthogonalisation\n**Examples:** x‚±º‚Çä‚ÇÅ = x‚±º + Œ±‚±º p‚±º with p‚±º‚Çä‚ÇÅ = r‚±º‚Çä‚ÇÅ + Œ≤‚±º p‚±º"
  },
  {
    "front": "üß© Preconditioned Conjugate Gradient\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A variant of the Conjugate Gradient method that applies a preconditioner M to cluster eigenvalues of A and accelerate convergence.\n\n**Description:** PCG solves M^{-1}Ax = M^{-1}b using CG on the transformed system. Proper preconditioning reduces iteration count and improves numerical stability.\n\n**Goal:** Improve convergence of CG via spectral conditioning.\n**Applications:** PDE solvers, Large-scale sparse systems, Domain decomposition\n**Methods:** Left/right preconditioning, Incomplete factorizations, Spectral clustering\n**Examples:** IC(0) preconditioning, AMG-preconditioned CG"
  },
  {
    "front": "üß© Krylov Subspace\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A subspace K_k(A, r0) = span{r0, Ar0, A¬≤r0, ‚Ä¶, A^{k‚àí1}r0} generated by repeated multiplication of the initial residual by A.\n\n**Description:** Krylov subspaces serve as the foundation for modern iterative methods for linear systems and eigenvalue problems. They enable low-memory, matrix-free solvers for very large sparse matrices.\n\n**Goal:** Generate subspaces for iterative numerical methods.\n**Applications:** Eigenvalue computation, Linear solvers, Matrix exponentiation\n**Methods:** Arnoldi iteration, Lanczos algorithm, GMRES\n**Examples:** K_1(A; I) = span{I}, K_2(A; I) = span{I, A}"
  },
  {
    "front": "üß© Krylov Subspace Methods\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A class of iterative algorithms that approximate solutions to Ax = b or eigenvalue problems by projecting them onto progressively larger Krylov subspaces.\n\n**Description:** Includes CG, GMRES, MINRES, BiCG, QMR, and others. These methods rely on polynomial approximations to A and require only matrix-vector products, making them ideal for large sparse systems.\n\n**Goal:** Solve large-scale problems using matrix-free operations.\n**Applications:** Sparse linear systems, Eigenvalue problems, PDE discretizations\n**Methods:** Arnoldi iteration, Lanczos iteration, Restarting, Preconditioning\n**Examples:** GMRES, CG, MINRES, BiCGSTAB"
  },
  {
    "front": "üß© Matrix Factorization\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** The process of decomposing a matrix into a product of structured factors such as triangular, orthogonal, or diagonal matrices.\n\n**Description:** Matrix factorizations enable efficient solving of linear systems, eigenvalue problems, and optimization tasks. Examples include LU, QR, Cholesky, Schur, and SVD.\n\n**Goal:** Reduce computational complexity by decomposing matrices into simpler components.\n**Applications:** Solving Ax = b, Eigenvalue algorithms, Least-squares problems, Preconditioning\n**Methods:** Pivoting, Orthogonal transformations, Triangularization\n**Examples:** LU, QR, SVD, Cholesky, Schur"
  },
  {
    "front": "üß© Incomplete LU Factorization\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** An approximate LU factorization where fill-in is restricted to preserve sparsity, producing LÃÇ and ≈® such that A ‚âà LÃÇ≈®.\n\n**Description:** ILU is widely used as a preconditioner in iterative methods, particularly PCG and GMRES. Variants include ILU(0), ILUT, and ILU(k).\n\n**Goal:** Approximate LU factorization while preserving sparsity for use as preconditioner.\n**Applications:** Finite element systems, CFD problems, Circuit simulation\n**Methods:** Modified Gaussian elimination with drop tolerance, Dual-threshold ILUT, Level-of-fill ILU(k)\n**Examples:** ILU(0): drop all fill-in outside original nonzero pattern"
  },
  {
    "front": "üß© Projection Operator\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A linear operator P satisfying P¬≤ = P, mapping a vector space onto a subspace while acting as identity on that subspace.\n\n**Description:** Projections decompose vector spaces into direct sums of a target subspace and its complement. They play central roles in numerical methods, approximations, and matrix factorizations.\n\n**Goal:** Project vectors onto subspaces.\n**Applications:** Least squares, Subspace methods, Decomposition\n**Methods:** Orthogonal projection, Oblique projection\n**Examples:** P with P^2 = P, I - P as complement projection"
  },
  {
    "front": "üß© Matrix Subspace\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A subset V ‚äÜ ‚ÑÇ^{n√ón} of matrices closed under addition and scalar multiplication, forming a vector space over ‚ÑÇ.\n\n**Description:** Matrix subspaces are used to model families of matrices sharing structural properties (e.g., invertibility, symmetry, triangularity). They enable low-rank approximations, invariant subspaces in eigenvalue problems, and structured matrix factorizations such as LU or SVD within constrained sets.\n\n**Goal:** Group matrices with common algebraic or analytic traits to simplify computations and preserve structure in factorizations.\n**Applications:** Structured matrix factorization, Low-rank approximation, Krylov methods, Invariant subspace computation\n**Methods:** Span construction, Closure under operations, Equivalence via similarity\n**Examples:** span{I, A}, span{A, B}, upper triangular matrices"
  },
  {
    "front": "üß© Nonsingular Matrix Subspace\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A matrix subspace V ‚äÜ ‚ÑÇ^{n√ón} that contains at least one invertible matrix (det V ‚â† 0 for some V ‚àà V).\n\n**Description:** Nonsingular subspaces guarantee the existence of invertible elements, enabling the definition of the set of inverses Inv(V) = {V‚Åª¬π : V ‚àà V, det V ‚â† 0}. Such subspaces are crucial for ensuring well-defined inverse-based factorizations (e.g., LU within V).\n\n**Goal:** Ensure invertibility within a structured family of matrices to support factorization algorithms.\n**Applications:** LU factorization, Generalized inverse, Structured preconditioning\n**Methods:** Perturbation arguments, Open-dense property in finite dimensions\n**Examples:** V = span{I, A} for nonsingular A, Upper triangular matrices with nonzero diagonals"
  },
  {
    "front": "üß© Inv(V)\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The set of inverses of all invertible matrices in a matrix subspace V: Inv(V) = {W : W = V‚Åª¬π, V ‚àà V, det V ‚â† 0}.\n\n**Description:** For nonsingular matrix subspaces, Inv(V) is itself a matrix subspace under mild conditions. This enables dual-space factorizations and ensures closure under inversion within structured sets, vital for algorithmic stability in LU-type methods.\n\n**Goal:** Construct a subspace of inverses to maintain structure across factorization steps.\n**Applications:** LU within subspace, Preconditioner design, Group-inverse problems\n**Methods:** Similarity transformation, Closure proof via nonsingularity\n**Examples:** Inv(upper triangular) = lower triangular, Inv(Hermitian) = Hermitian"
  },
  {
    "front": "üß© LU Factorization within Subspace\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Decomposition of a matrix A ‚àà V into A = LU where L is lower triangular and U is upper triangular, both belonging to predefined matrix subspaces derived from V.\n\n**Description:** By identifying nonsingular matrix subspaces V and W = Inv(V), LU factorization can be confined within V √ó W, ensuring all intermediate matrices remain structured. This supports specialized algorithms for banded, symmetric positive-definite, or approximate low-rank problems.\n\n**Goal:** Perform Gaussian elimination while preserving membership in designated matrix subspaces.\n**Applications:** Banded LU, SPD factorization, Low-rank updates, Krylov-based solvers\n**Methods:** Schur complement, Pivot-free elimination, Subspace projection\n**Examples:** A ‚àà upper triangular ‚Üí L = I, U = A, A ‚àà V, L ‚àà V, U ‚àà Inv(V)"
  },
  {
    "front": "üß© Matrix Polynomials\nüìò Domain: Linear Algebra",
    "back": "**Definition:** Functions p(z) = ‚àë_{k=0}^m c_k z^k evaluated at matrices: p(A) = ‚àë_{k=0}^m c_k A^k for A ‚àà ‚ÑÇ^{n√ón}.\n\n**Description:** Matrix polynomials map matrix subspaces to themselves if V is closed under powers of its members. They define minimal and characteristic polynomials, enable Cayley-Hamilton applications, and support function-based iterative methods.\n\n**Goal:** Extend scalar polynomial theory to matrices for spectral analysis and function approximation.\n**Applications:** Cayley-Hamilton theorem, Matrix exponential,  preconditioning, Spectral projectors\n**Methods:** Horner scheme, Paterson-Stockmeyer, Jordan form evaluation\n**Examples:** p(z) = z¬≤‚àí2z+1 ‚Üí p(A) = A¬≤‚àí2A+I = 0, q(z) = det(A‚àízI) ‚Üí q(A)=0"
  },
  {
    "front": "üß© QZ Algorithm\nüìò Domain: Generalized Eigenvalue Problems",
    "back": "**Definition:** An algorithm that reduces a matrix pencil (A, B) to generalized Schur form using unitary transformations.\n\n**Description:** The QZ algorithm generalizes the QR algorithm to solve A x = Œª B x problems. It produces upper triangular matrices S and T such that Q* A Z = S and Q* B Z = T. It is the standard dense solver for generalized eigenvalues.\n\n**Goal:** Compute generalized eigenvalues reliably for matrix pencils.\n**Applications:** DAE systems, Control theory, Stability analysis\n**Methods:** Generalized Hessenberg reduction, Unitary similarity\n**Examples:** MATLAB's eig(A, B)"
  },
  {
    "front": "üß© Reflection\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A linear transformation that flips vectors across a hyperplane while preserving norms.\n\n**Description:** Reflections form the basis for orthogonal transformations, including Householder reflections. They eliminate components in vectors or matrices while maintaining stability.\n\n**Goal:** Construct norm-preserving transformations used in matrix factorizations.\n**Applications:** Householder QR, Orthogonalization, Matrix reduction\n**Methods:** Hyperplane reflections, Orthogonal matrices\n**Examples:** Householder transformation H = I - 2vv·µÄ"
  },
  {
    "front": "üß© Row Reduction\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A sequence of elementary row operations used to simplify matrices to row echelon or reduced row echelon form.\n\n**Description:** Row reduction is fundamental for solving linear systems, computing ranks, bases, and understanding matrix structure. It underpins Gaussian elimination and many algebraic manipulations.\n\n**Goal:** Transform matrices into simplified forms revealing rank and solution structure.\n**Applications:** Solving linear systems, Rank computation, Null space\n**Methods:** Elementary row operations, Pivoting\n**Examples:** RREF calculation for Ax = b"
  },
  {
    "front": "üß© Schur Complement\nüìò Domain: Matrix Theory",
    "back": "**Definition:** For a block matrix, the Schur complement of A in M = [[A, B], [C, D]] is S = D - C A‚Åª¬π B.\n\n**Description:** The Schur complement appears in block Gaussian elimination, matrix inversion, optimization, and statistics. It captures conditional behavior and plays a crucial role in SPD testing.\n\n**Goal:** Reduce block-matrix operations to smaller components.\n**Applications:** Optimization, Block LU, Covariance matrices, Linear systems\n**Methods:** Block elimination, Matrix inversion\n**Examples:** Used in Kalman filter equations"
  },
  {
    "front": "üß© Search Directions (in CG)\nüìò Domain: Iterative Methods",
    "back": "**Definition:** The sequence of A-conjugate directions generated by the Conjugate Gradient method.\n\n**Description:** Search directions determine the efficiency and convergence of CG. Each direction is constructed to be A-orthogonal to all previous ones, ensuring optimality in SPD problems.\n\n**Goal:** Define the sequence of directions used to minimize the error over Krylov subspaces.\n**Applications:** CG iterations, Krylov subspace minimization\n**Methods:** A-orthogonalization, Residual-based direction updates\n**Examples:** p‚Çñ = r‚Çñ + Œ≤‚Çñ p‚Çñ‚Çã‚ÇÅ"
  },
  {
    "front": "üß© Similarity Transformation\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A transformation of the form A ‚Üí S‚Åª¬π A S with S invertible, preserving eigenvalues.\n\n**Description:** Similarity transformations classify matrices into equivalence classes and preserve spectral properties. They form the basis for diagonalization, Jordan forms, and spectral algorithms.\n\n**Goal:** Transform matrices while preserving eigenvalues and spectral characteristics.\n**Applications:** Diagonalization, Jordan canonical form, Invariant subspaces\n**Methods:** Change of basis, Matrix equivalence\n**Examples:** S‚Åª¬π A S is diagonalizable"
  },
  {
    "front": "üß© Spectral Gap\nüìò Domain: Spectral Theory",
    "back": "**Definition:** The difference between two adjacent eigenvalues, often used to characterize convergence rates.\n\n**Description:** A larger spectral gap leads to faster convergence of iterative methods such as power iteration and CG. Spectral gap also governs mixing times in Markov chains.\n\n**Goal:** Measure separation between key eigenvalues.\n**Applications:** Power iteration, Graph Laplacians, Markov chains\n**Methods:** Eigenvalue analysis\n**Examples:** Gap = Œª‚ÇÇ - Œª‚ÇÅ for stochastic matrices"
  },
  {
    "front": "üß© Spectral Mapping Theorem\nüìò Domain: Matrix Functions",
    "back": "**Definition:** The theorem stating that for analytic functions f, the spectrum satisfies Œõ(f(A)) = f(Œõ(A)).\n\n**Description:** Spectral mapping allows computation of matrix functions by applying the scalar function to eigenvalues. It is fundamental to understanding behaviors of matrix exponentials and polynomials.\n\n**Goal:** Relate eigenvalues of functions of matrices to functions of eigenvalues.\n**Applications:** Matrix exponential, Matrix square root, Krylov methods\n**Methods:** Analytic functions, Contour integrals\n**Examples:** Œõ(e^A) = e^{Œõ(A)}"
  },
  {
    "front": "üß© Spectral Radius\nüìò Domain: Spectral Theory",
    "back": "**Definition:** The quantity œÅ(A) = max |Œª·µ¢|, the maximum magnitude of eigenvalues of A.\n\n**Description:** Spectral radius determines convergence of matrix iterations, stability, and power method behavior. It is a core concept in numerical linear algebra and operator theory.\n\n**Goal:** Measure the largest eigenvalue magnitude.\n**Applications:** Stability analysis, Power iteration, Matrix norms\n**Methods:** Eigenvalue computation\n**Examples:** œÅ(A) < 1 ensures convergence of Neumann series"
  },
  {
    "front": "üß© Strassen Algorithm\nüìò Domain: Matrix Computations",
    "back": "**Definition:** A subcubic algorithm for matrix multiplication with complexity O(n^{log‚ÇÇ7}) ‚âà O(n^{2.807}).\n\n**Description:** Strassen‚Äôs method reduces the number of multiplications required for matrix multiplication. It forms the basis for fast algebraic algorithms and motivates subcubic research.\n\n**Goal:** Multiply matrices faster than classical O(n¬≥) time.\n**Applications:** Large dense matrices, Parallel computing\n**Methods:** Divide-and-conquer, Bilinear algorithms\n**Examples:** 7 multiplications instead of 8 for 2x2 blocks"
  },
  {
    "front": "üß© Subcubic Algorithms\nüìò Domain: Matrix Computations",
    "back": "**Definition:** Algorithms for matrix multiplication with asymptotic complexity below O(n¬≥).\n\n**Description:** Subcubic algorithms include Strassen, Coppersmith‚ÄìWinograd, and subsequent improvements. They are foundational in algebraic complexity theory.\n\n**Goal:** Push theoretical limits of fast matrix multiplication.\n**Applications:** Computational complexity, Large-scale algebraic computations\n**Methods:** Tensor decomposition, Bilinear complexity\n**Examples:** Coppersmith‚ÄìWinograd algorithm"
  },
  {
    "front": "üß© Subspace Decomposition\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A decomposition of a vector or matrix space into multiple complementary subspaces.\n\n**Description:** Subspace decompositions underpin direct-sum representations, block structure in matrices, and multilevel numerical algorithms. They enable modularization of linear algebraic problems.\n\n**Goal:** Split spaces into lower-dimensional components with structured relationships.\n**Applications:** Multigrid, Block matrices, Direct sum\n**Methods:** Orthogonal complement, Projection operators\n**Examples:** V = U1 ‚äï U2 ‚äï ‚Ä¶ ‚äï Uk"
  },
  {
    "front": "üß© Subspace Iteration\nüìò Domain: Eigenvalue Algorithms",
    "back": "**Definition:** An iterative method that simultaneously computes multiple dominant eigenvectors by repeatedly applying A to a subspace.\n\n**Description:** Subspace iteration generalizes the power method to multiple vectors. It forms the basis of modern eigensolvers such as LOBPCG and block Krylov methods.\n\n**Goal:** Approximate several eigenpairs by iterating on subspaces.\n**Applications:** Large-scale eigenvalue problems, PDE solvers, Block Krylov methods\n**Methods:** Orthogonalization, Rayleigh‚ÄìRitz projection\n**Examples:** Block power iteration"
  },
  {
    "front": "üß© Sparsity Structure\nüìò Domain: Matrix Computations",
    "back": "**Definition:** The sparsity structure of a matrix or matrix subspace specifies the pattern of entries that may be nonzero, independent of the actual numerical values.\n\n**Description:** Sparsity structures capture the combinatorial pattern of allowed nonzeros in matrices, enabling algorithms that exploit memory efficiency and reduce computational cost. They are essential in large-scale problems such as PDE discretizations, graph Laplacians, and structured matrix factorizations.\n\n**Goal:** Identify and exploit zero-patterns for efficient computation.\n**Applications:** Sparse LU factorization, Finite difference discretizations, Graph-based matrix algorithms\n**Methods:** Pattern analysis, Fill-in minimization, Graph reordering\n**Examples:** Tridiagonal pattern, Band matrix structure"
  },
  {
    "front": "üß© Standard Matrix Subspace\nüìò Domain: Matrix Computations",
    "back": "**Definition:** A standard matrix subspace is a subspace that admits a basis consisting of standard matrices, each with exactly one entry equal to 1 and all other entries equal to 0.\n\n**Description:** Standard matrix subspaces represent matrix sets defined purely by sparsity patterns, without structural constraints such as symmetry. Orthogonal projection onto these subspaces is straightforward‚Äîsimply zero out entries outside the allowed pattern.\n\n**Goal:** Model matrix sets defined solely by sparsity constraints.\n**Applications:** Sparse matrix approximations, Projection methods, Structured LU factorization\n**Methods:** Entrywise projection, Basis construction from standard matrices\n**Examples:** Diagonal matrices, Strictly lower triangular matrices"
  },
  {
    "front": "üß© Orthogonal Projector (Matrix Subspace)\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** An orthogonal projector onto a matrix subspace V is a linear operator P such that P¬≤ = P and the range of P is orthogonal to the nullspace of P.\n\n**Description:** Orthogonal projectors provide optimal approximations in least-squares and matrix approximation problems. Projection onto matrix subspaces is fundamental in algorithmic factoring, dimension reduction, and solving underdetermined systems.\n\n**Goal:** Project matrices onto a subspace with minimal error under the Frobenius inner product.\n**Applications:** Approximate factoring, Subspace splitting, Sylvester-type equations\n**Methods:** Construction via orthonormal basis, Symmetrization operators\n**Examples:** P(A) = (A + A*)/2 for Hermitian matrices"
  },
  {
    "front": "üß© Invertible Matrix Subspace\nüìò Domain: Matrix Theory",
    "back": "**Definition:** A matrix subspace V is invertible if the set of inverses of its nonsingular elements forms another matrix subspace V‚Åª¬π.\n\n**Description:** Invertible matrix subspaces allow algorithmic factorization because their inverses preserve linear structure. Classical examples include triangular matrices and Hermitian matrices, which maintain structure under inversion.\n\n**Goal:** Support structured factorization where both A and its factors belong to linear matrix families.\n**Applications:** LU factorization, Symmetric factorizations, Matrix subspace factoring\n**Methods:** Closure under inversion, Polynomial relations\n**Examples:** Lower triangular matrices, Hermitian matrices"
  },
  {
    "front": "üß© Singular Matrix Subspace\nüìò Domain: Matrix Theory",
    "back": "**Definition:** A matrix subspace is singular if every matrix within it is singular, i.e., no element has a nonzero determinant.\n\n**Description:** Singular matrix subspaces arise in low-rank approximations, special matrix pencils, and structured operator families. They often encode degenerate behavior and limit the applicability of classical factorizations.\n\n**Goal:** Characterize spaces where invertibility is impossible.\n**Applications:** Rank-k matrix sets, SVD truncation analysis, Generalized eigenvalue pencils\n**Methods:** Nullspace analysis, Subspace closure\n**Examples:** Rank-k matrices for k < n"
  },
  {
    "front": "üß© Polynomially Closed Matrix Subspace\nüìò Domain: Matrix Theory",
    "back": "**Definition:** A matrix subspace V is polynomially closed if p(A) ‚àà V for every A ‚àà V and every polynomial p with scalar coefficients.\n\n**Description:** Polynomial closure ensures that structural properties of matrices are preserved under algebraic operations such as exponentiation, inversion, or functional calculus. This property is essential when applying iterative or polynomial-based algorithms within the subspace.\n\n**Goal:** Guarantee closure under matrix polynomial transformations.\n**Applications:** Iterative methods, Matrix functions, Structured inversion\n**Methods:** Polynomial evaluation, Minimal polynomial arguments\n**Examples:** Hermitian matrices, Complex symmetric matrices"
  },
  {
    "front": "üß© Closure of V‚ÇÅV‚ÇÇ\nüìò Domain: Matrix Subspaces",
    "back": "**Definition:** The closure of the product set V‚ÇÅV‚ÇÇ consists of all matrices that can be approximated arbitrarily well by products of matrices from subspaces V‚ÇÅ and V‚ÇÇ.\n\n**Description:** The closure of V‚ÇÅV‚ÇÇ determines whether approximate factorizations exist even when exact ones do not. This concept plays a central role in understanding numerical stability, perturbation behavior, and feasibility of approximate matrix factorizations.\n\n**Goal:** Characterize attainable approximate factorizations.\n**Applications:** Approximate LU, Low-rank approximations, Structured matrix decompositions\n**Methods:** Topology of matrix spaces, Perturbation analysis\n**Examples:** Every matrix is arbitrarily close to one with an LU factorization"
  },
  {
    "front": "üß© Preconditioning\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Preconditioning is the transformation of a linear system Ax = b into an equivalent system M^{-1}Ax = M^{-1}b (left preconditioning) or AM^{-1}y = b with x = M^{-1}y (right), where M ‚âà A is a nonsingular matrix that is inexpensive to invert or solve with, designed to improve the convergence rate of iterative methods.\n\n**Description:** The goal is to cluster the eigenvalues of the preconditioned matrix away from zero, making Krylov subspace methods like CG, GMRES, or MINRES converge in significantly fewer iterations. Effective preconditioners balance accuracy (M close to A) with computational cost (O(n) or O(n log n) per application).\n\n**Goal:** Accelerate iterative solver convergence.\n**Applications:** Large sparse systems, PDE solvers\n**Methods:** Incomplete LU, Jacobi, Multigrid, Domain decomposition\n**Examples:** Left preconditioning: solve M y = c then A x = M y"
  },
  {
    "front": "üß© Jacobi Preconditioner\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** The Jacobi preconditioner is M = diag(A) + œâI, where diag(A) contains the diagonal entries of A and œâ ‚àà ‚ÑÇ is an optional damping parameter (often œâ = 0). It is the simplest splitting M = D, N = A - D.\n\n**Description:** Extremely cheap to apply (O(n) scaling) and parallelizable. Effective when A is diagonally dominant. Corresponds to the classical Jacobi iterative method. Often used as a baseline or building block in more sophisticated preconditioners.\n\n**Goal:** Damp high-frequency error components using only diagonal information.\n**Applications:** Smooth initial guess for multigrid, Baseline for preconditioner comparison, Diagonally dominant systems\n**Methods:** Extract diagonal, Optional damping œâ, Inverse is element-wise division\n**Examples:** M_i = diag(A) + œâI with œâ = 0 for standard Jacobi"
  },
  {
    "front": "üß© Sparse Approximate Inverse\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A sparse approximate inverse (SAI) preconditioner computes a sparse matrix M such that ||I - MA||_F or ||I - AW||_F is minimized over all matrices W with a prescribed sparsity pattern, effectively approximating A^{-1} directly.\n\n**Description:** Application cost is O(nnz(M)) matrix-vector products. Excellent parallel performance due to explicit form. Often constructed via Frobenius norm minimization on independent columns or using QR factorizations of local submatrices.\n\n**Goal:** Construct explicit sparse approximation to A^{-1} for fast matrix-vector products\n**Applications:** Highly parallel architectures (GPU, many-core), Unstructured grids, High-performance computing\n**Methods:** Frobenius norm minimization per column, SPAID (sparse approximate inverse by distance), FSAI (factorized sparse approximate inverse)\n**Examples:** min_W ||AW - I||_F with W constrained to sparsity pattern of A^k"
  },
  {
    "front": "üß© Left Preconditioning\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Left preconditioning transforms Ax = b into M^{-1}Ax = M^{-1}b, preserving the solution x but altering the residual norm to ||M^{-1}(b - Ax)||.\n\n**Description:** Common in practice because it directly improves the convergence diagnostics used by Krylov methods (residual-based stopping criteria). Does not change the right-hand side in a way that affects eigenvalue clustering as strongly as right preconditioning.\n\n**Goal:** Improve convergence while keeping solution unchanged and monitoring preconditioned residuals.\n**Applications:** Standard choice in most libraries (PETSc, Trilinos), GMRES with ILU\n**Methods:** Apply M^{-1} to system matrix and RHS"
  },
  {
    "front": "üß© Right Preconditioning\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Right preconditioning transforms Ax = b into AM^{-1}y = b with x = M^{-1}y, preserving the residual norm ||b - Ax|| but solving for an intermediate variable y.\n\n**Description:** Often preferred when the preconditioner naturally approximates the inverse action from the right. Common in domain decomposition and multigrid methods. Requires extra step to recover x.\n\n**Goal:** Cluster eigenvalues of AM^{-1} while monitoring true residuals.\n**Applications:** Additive Schwarz, Algebraic multigrid (AMG), When M approximates A from the right\n**Methods:** Solve AM^{-1}y = b, then x = M^{-1}y"
  },
  {
    "front": "üß© Splitting Methods\nüìò Domain: Iterative Methods",
    "back": "**Definition:** A matrix splitting decomposes A = M - N where M is nonsingular and easy to invert. The iteration x^{k+1} = M^{-1}Nx^k + M^{-1}b converges if œÅ(M^{-1}N) < 1.\n\n**Description:** Foundation of classical iterative methods (Jacobi, Gauss-Seidel, SOR) and modern preconditioning. The spectral radius of the iteration matrix B = M^{-1}N determines convergence rate.\n\n**Goal:** Construct fixed-point iterations via A = M - N with œÅ(M^{-1}N) < 1.\n**Applications:** Stationary iterative methods, Preconditioner design, Convergence theory\n**Methods:** M = D (Jacobi), M = D+L (Gauss-Seidel), M = (D+œâL)/œâ (SOR)"
  },
  {
    "front": "üß© Sylvester Equation Solution\nüìò Domain: Matrix Equations",
    "back": "**Definition:** The process of solving the matrix equation AX + XB = C for X, typically when spectra of A and B do not overlap.\n\n**Description:** Sylvester equation solvers rely on Schur decompositions and Bartels‚ÄìStewart algorithms. They play a central role in control theory, Lyapunov equations, and model reduction.\n\n**Goal:** Compute X efficiently when AX + XB = C.\n**Applications:** Control theory, Lyapunov equations, Model reduction\n**Methods:** Bartels‚ÄìStewart algorithm, Schur decomposition\n**Examples:** Solving AX + XB = Q for stability analysis"
  },
  {
    "front": "üß© Symmetric Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A real matrix satisfying A·µÄ = A.\n\n**Description:** Symmetric matrices enjoy real eigenvalues, orthogonal diagonalization, and strong numerical stability. They form the real counterpart of Hermitian matrices.\n\n**Goal:** Represent self-adjoint real operators.\n**Applications:** Optimization, Eigenvalue problems, PDE discretizations\n**Methods:** Orthogonal diagonalization\n**Examples:** Graph Laplacian matrices"
  },
  {
    "front": "üß© Symmetric Positive Definite Matrices\nüìò Domain: Linear Algebra",
    "back": "**Definition:** Real symmetric matrices A for which x·µÄAx > 0 for all nonzero x.\n\n**Description:** SPD matrices arise in PDEs, covariance analysis, optimization, and CG methods. They guarantee unique Cholesky factorizations and fast iterative convergence.\n\n**Goal:** Characterize strictly positive curvature of quadratic forms.\n**Applications:** Conjugate Gradient, Cholesky factorization, Machine learning\n**Methods:** Eigenvalue analysis, Quadratic forms\n**Examples:** Graph Laplacian + ŒµI"
  },
  {
    "front": "üß© Triangular Matrices\nüìò Domain: Linear Algebra",
    "back": "**Definition:** Matrices that are either upper or lower triangular.\n\n**Description:** Triangular matrices form the basis of LU factorization and back/forward substitution. They preserve structure in matrix decompositions and support fast algorithms.\n\n**Goal:** Define structured matrices enabling efficient solving.\n**Applications:** LU factorization, QR reduction, Matrix decomposition\n**Methods:** Back substitution, Forward substitution\n**Examples:** Upper and lower triangular matrices"
  },
  {
    "front": "üß© Triangular Solves\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Algorithms that solve triangular linear systems via forward or backward substitution.\n\n**Description:** Triangular solves are low-cost O(n¬≤) operations widely used inside LU, QR, Cholesky, and Krylov methods. They involve recursive elimination without pivoting.\n\n**Goal:** Solve triangular systems efficiently.\n**Applications:** LU solve, QR solve, Block triangular systems\n**Methods:** Forward substitution, Backward substitution\n**Examples:** Solving Lx = b in O(n¬≤) time"
  },
  {
    "front": "üß© Tridiagonal Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A matrix whose nonzero entries lie on the main diagonal and the two adjacent diagonals.\n\n**Description:** Tridiagonal matrices arise in 1D PDE discretizations, eigenvalue algorithms, and Lanczos methods. They enable fast O(n) algorithms for solving linear systems.\n\n**Goal:** Represent sparse structured operators with minimal bandwidth.\n**Applications:** Lanczos algorithm, Thomas algorithm, Finite differences\n**Methods:** Specialized LU, Orthogonal reduction\n**Examples:** 1D Poisson matrix"
  },
  {
    "front": "üß© Unitary Similarity\nüìò Domain: Linear Algebra / Spectral Theory",
    "back": "**Definition:** A similarity transformation of the form A ‚Üí U*AU where U is unitary.\n\n**Description:** Unitary similarity preserves norms, eigenvalues, and stability, making it essential in Schur decomposition, unitarily diagonalizable matrices, and spectral algorithms.\n\n**Goal:** Transform matrices while preserving numerical stability and eigenvalues.\n**Applications:** Schur decomposition, Matrix diagonalization\n**Methods:** Unitary transformations\n**Examples:** Hessenberg reduction via unitary similarity"
  },
  {
    "front": "üß© Unitary Transformation\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A transformation represented by a unitary matrix U such that U*U = I.\n\n**Description:** Unitary transformations preserve norms, orthogonality, and conditioning. They are essential for stable QR factorization, Hessenberg reduction, and SVD.\n\n**Goal:** Apply stable norm-preserving transformations.\n**Applications:** QR factorization, Eigenvalue methods, Matrix reductions\n**Methods:** Givens rotations, Householder reflections\n**Examples:** Q in QR decomposition"
  },
  {
    "front": "üß© Upper Triangular Factor\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** The U matrix in LU decomposition, where all entries below the diagonal are zero.\n\n**Description:** The upper triangular factor U stores the result of Gaussian elimination. It is used in backward substitution and forms half of the LU factorization.\n\n**Goal:** Store eliminated system structure for solving Ax = b.\n**Applications:** LU factorization, Backward substitution\n**Methods:** Gaussian elimination\n**Examples:** LU = L U with U triangular"
  },
  {
    "front": "üß© Upper Triangular Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A matrix whose entries below the diagonal are all zero.\n\n**Description:** Upper triangular matrices naturally arise in QR and LU factorizations. They support fast backward substitution and reveal eigenvalues directly on the diagonal.\n\n**Goal:** Provide structure for efficient solves and spectral access.\n**Applications:** Schur decomposition, LU factorization\n**Methods:** Back substitution\n**Examples:** R in QR factorization"
  },
  {
    "front": "üß© Vandermonde Matrix\nüìò Domain: Linear Algebra / Approximation Theory",
    "back": "**Definition:** A matrix whose rows or columns follow geometric progressions based on input nodes.\n\n**Description:** Vandermonde matrices appear in polynomial interpolation, Prony methods, and spectral approximations. They are often ill-conditioned, especially for large degrees.\n\n**Goal:** Represent polynomial basis evaluations compactly.\n**Applications:** Interpolation, Polynomial fitting, Signal processing\n**Methods:** Lagrange interpolation, Basis evaluation\n**Examples:** V(i,j) = x_i^{j-1}"
  },
  {
    "front": "üß© Vector Subspace\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A subset of a vector space closed under addition and scalar multiplication.\n\n**Description:** Vector subspaces are foundational structures of linear algebra. They define solution sets, null spaces, ranges, invariant subspaces, and decomposition structures.\n\n**Goal:** Define linear structure within a larger vector space.\n**Applications:** Null space, Range, Direct sum decompositions\n**Methods:** Linear span, Projection operators\n**Examples:** Null(A), Range(A)"
  },
  {
    "front": "üß© Residual r = b - Ax\nüìò Domain: Iterative Methods",
    "back": "**Definition:** The difference between the right-hand side and the current approximation‚Äôs predicted value in a linear system.\n\n**Description:** Residuals determine convergence, stopping criteria, and direction generation in iterative methods such as CG, GMRES, and Richardson iteration.\n\n**Goal:** Measure progress of iterative solvers.\n**Applications:** CG, GMRES, Jacobi, Richardson iteration\n**Methods:** Residual norm minimization\n**Examples:** ||r_k|| used as stopping criterion"
  },
  {
    "front": "üß© Matrix Square Root (Alternative Notation)\nüìò Domain: Matrix Functions",
    "back": "**Definition:** A matrix X satisfying X¬≤ = A. This entry covers the symbol-level term sqrt(A).\n\n**Description:** Matrix square roots arise in diffusion processes, covariance models, and analytic matrix functions. Many algorithms compute ‚àöA using Newton iterations or spectral decompositions.\n\n**Goal:** Compute a matrix whose square equals A.\n**Applications:** Diffusion equations, Covariance analysis\n**Methods:** Newton‚Äôs iteration, Schur decomposition\n**Examples:** Principal square root from spectral decomposition"
  },
  {
    "front": "üß© Cholesky Factorization\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A decomposition of a Hermitian positive definite matrix A into A = R*R, where R is an upper triangular matrix with positive diagonal entries.\n\n**Description:** Cholesky factorization exploits symmetry and positive definiteness to reduce computational cost from O(n¬≥) for general LU to approximately n¬≥/3 flops while requiring only n¬≤/2 storage. It is widely used in optimization, Monte Carlo simulations, and solving normal equations.\n\n**Goal:** Efficiently factorize Hermitian positive definite matrices with half the storage and one-third the operations of LU.\n**Applications:** Quadratic programming, Kalman filtering, Covariance decomposition, Monte Carlo methods\n**Methods:** Block elimination, Outer product form, Inner product form\n**Examples:** A = R*R with R upper triangular and diag(R) > 0"
  },
  {
    "front": "üß© Positive Definite Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A Hermitian matrix A ‚àà ‚ÑÇ^{n√ón} such that (Ax, x) > 0 for all nonzero x ‚àà ‚ÑÇ^n.\n\n**Description:** Positive definite matrices arise in energy minimization, covariance modeling, and elliptic PDEs. They guarantee unique Cholesky factors, stable inverses, and real positive eigenvalues. The property is preserved under congruence: M*A*M is positive definite if M is invertible.\n\n**Goal:** Model strictly convex quadratic forms and ensure numerical stability in factorizations.\n**Applications:** Optimization, Statistics, Physics simulations, Control theory\n**Methods:** Sylvester's criterion, Cholesky test, Eigenvalue analysis\n**Examples:** Covariance matrices, Hessians of convex functions, A = R*R"
  },
  {
    "front": "üß© Sylvester Equation\nüìò Domain: Control Theory",
    "back": "**Definition:** A matrix equation of the form AX ‚àí XB = C, where A, B, C are given matrices and X is unknown.\n\n**Description:** The Sylvester equation models linear system interconnections and appears in control design, model reduction, and eigenvalue assignment. When œÉ(A) ‚à© œÉ(B) = ‚àÖ, it has a unique solution solvable in O(n¬≥) via Schur triangulation or vectorization (kronecker form).\n\n**Goal:** Solve for coupling matrix X in interconnected linear systems or compute Lyapunov functions.\n**Applications:** Stability analysis, Model order reduction, Riccati equations, Pole placement\n**Methods:** Schur method, Hessenberg-Schur algorithm, Bartels-Stewart, Vectorization\n**Examples:** AX ‚àí XA* = ‚àíBB* (Lyapunov), AX ‚àí XB = C with diagonal A, B"
  },
  {
    "front": "üß© Discrete Fourier Transform (DFT)\nüìò Domain: Signal Processing",
    "back": "**Definition:** A linear transformation mapping a sequence x‚ÇÄ, ..., x_{n-1} to coefficients c‚±º = ‚àë‚Çñ x‚Çñ œâ^{jk}, where œâ = e^{-2œÄi/n}, represented by the Vandermonde matrix F‚Çô.\n\n**Description:** The DFT diagonalizes circulant matrices and enables fast convolution, filtering, and spectral analysis. The Fast Fourier Transform (FFT) computes it in O(n log n) using divide-and-conquer on power-of-two sizes.\n\n**Goal:** Decompose signals into frequency components and accelerate convolution/correlation\n**Applications:** Audio processing, Image compression, PDE solvers, Polynomial multiplication\n**Methods:** Cooley-Tukey FFT, Radix-2, Split-radix, Bluestein\n**Examples:** F‚ÇÑ = [[1,1,1,1], [1,-1,1,-1], ...], FFT of length 2^l"
  },
  {
    "front": "üß© Fast Fourier Transform (FFT)\nüìò Domain: Numerical Algorithms",
    "back": "**Definition:** An efficient algorithm for computing the DFT in O(n log n) operations by recursively splitting into even/odd indices when n is a power of two.\n\n**Description:** The Cooley-Tukey FFT reduces DFT complexity from O(n¬≤) to O(n log n) using butterfly operations and twiddle factors. It is foundational in digital signal processing and enables real-time spectral analysis.\n\n**Goal:** Compute DFT with minimal arithmetic operations using recursive decomposition\n**Applications:** Spectral methods, FFT-based convolution, MRI reconstruction, Audio synthesis\n**Methods:** Decimation-in-time, Decimation-in-frequency, Bit-reversal, In-place computation\n**Examples:** Radix-2 butterfly: y‚±º = x_{2j} + œâ^j x_{2j+1}"
  },
  {
    "front": "üß© Schur Triangulation\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A similarity transformation A = Q T Q* where T is upper triangular and Q is unitary, revealing eigenvalues on the diagonal of T.\n\n**Description:** Schur form is the foundation for robust eigenvalue computation and solving Sylvester equations. The QR-based Francis algorithm computes it in O(n¬≥) with high backward stability. Real Schur form handles complex conjugate pairs.\n\n**Goal:** Reduce matrix to triangular form under unitary similarity to expose eigenvalues and enable block algorithms.\n**Applications:** Eigenvalue problems, Sylvester solvers, Matrix functions, Control theory\n**Methods:** Francis QR algorithm, Hessenberg reduction, Double shift\n**Examples:** A = Q T Q* with T upper triangular, diag(T) = eigenvalues"
  },
  {
    "front": "üß© Fill-in\nüìò Domain: Sparse Linear Algebra",
    "back": "**Definition:** New nonzero entries created during matrix factorizations such as LU, Cholesky, or QR.\n\n**Description:** Fill-in affects storage requirements and computational cost. Minimizing fill-in is essential for sparse solvers and preconditioners. Ordering strategies such as minimum degree or nested dissection aim to reduce fill-in.\n\n**Goal:** Quantify and manage new nonzeros generated during factorization.\n**Applications:** Sparse LU factorization, Cholesky factorization, Preconditioner construction\n**Methods:** Graph-based ordering, Approximate minimum degree, Nested dissection\n**Examples:** Cholesky of 2D Laplacian grid matrices"
  },
  {
    "front": "üß© Francis Shift\nüìò Domain: Eigenvalue Algorithms",
    "back": "**Definition:** A shift strategy used in the QR algorithm to accelerate convergence to eigenvalues.\n\n**Description:** The implicit double-shift QR method introduced by Francis uses complex conjugate shifts to target eigenvalues more rapidly. This is a key technique in modern dense eigenvalue solvers.\n\n**Goal:** Accelerate QR eigenvalue algorithm convergence.\n**Applications:** QR algorithm, Hessenberg QR iteration\n**Methods:** Implicit double shift, Bulge-chasing\n**Examples:** Francis double-shift QR iteration"
  },
  {
    "front": "üß© Givens Rotation\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A plane rotation used to introduce zeros in matrices, especially in QR factorization.\n\n**Description:** Givens rotations apply a rotation in a 2D coordinate plane to eliminate specific matrix elements. They are ideal for sparse matrices because they introduce minimal fill-in.\n\n**Goal:** Eliminate matrix entries while preserving orthogonality.\n**Applications:** QR factorization, Least squares, Hessenberg reduction\n**Methods:** Orthogonal transformations, Sparse QR algorithms\n**Examples:** Givens QR for sparse least-squares problems"
  },
  {
    "front": "üß© GL(n, ‚ÑÇ)\nüìò Domain: Linear Algebra / Group Theory",
    "back": "**Definition:** The group of all invertible n√ón matrices over the complex numbers.\n\n**Description:** GL(n,‚ÑÇ) is the fundamental group representing all linear automorphisms of ‚ÑÇ‚Åø. Many matrix algorithms rely on structure and properties preserved under GL(n,‚ÑÇ) transformations.\n\n**Goal:** Describe the set of all invertible linear transformations.\n**Applications:** Matrix decomposition theory, Similarity transformations, Lie groups\n**Methods:** Determinant analysis, Group operations\n**Examples:** A ‚àà GL(n,‚ÑÇ) iff det(A) ‚â† 0"
  },
  {
    "front": "üß© Hessenberg Form\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A nearly triangular matrix where all entries below the first subdiagonal are zero.\n\n**Description:** Hessenberg form is the target of reduction prior to running the QR algorithm. Transforming a matrix into Hessenberg form preserves eigenvalues and reduces computational cost.\n\n**Goal:** Reduce general matrices to a structured form suitable for eigenvalue algorithms.\n**Applications:** QR algorithm, Iterative eigenvalue methods\n**Methods:** Householder transformations\n**Examples:** Upper Hessenberg reduction of dense matrices"
  },
  {
    "front": "üß© Hessenberg Reduction\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** An algorithm that transforms a matrix into Hessenberg form using orthogonal transformations.\n\n**Description:** Reduction to Hessenberg form is a standard preprocessing step in dense eigenvalue computations. It reduces complexity in subsequent QR iterations while preserving eigenvalues.\n\n**Goal:** Efficiently produce a Hessenberg matrix from a general matrix.\n**Applications:** Eigenvalue computations, QR algorithm initialization\n**Methods:** Householder reflections\n**Examples:** MATLAB's hess(A)"
  },
  {
    "front": "üß© High Parallelism\nüìò Domain: High-Performance Computing",
    "back": "**Definition:** The property of an algorithm being able to exploit many processors simultaneously with minimal communication.\n\n**Description:** High parallelism is crucial for modern numerical algorithms on GPUs, clusters, and distributed systems. FFT, matrix multiplication, and multigrid methods often exhibit strong parallel characteristics.\n\n**Goal:** Increase performance by reducing sequential bottlenecks.\n**Applications:** FFT, Multigrid, Parallel linear solvers\n**Methods:** Task decomposition, Domain decomposition\n**Examples:** FFT butterfly operations executed in parallel"
  },
  {
    "front": "üß© Idempotency\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A property of an operator P satisfying P¬≤ = P.\n\n**Description:** Idempotent operators correspond to projection operators onto subspaces. They play a central role in matrix subspace theory, direct sum decompositions, and oblique projections.\n\n**Goal:** Characterize projection-like operators with stable repeated action.\n**Applications:** Projection operators, Invariant subspaces\n**Methods:** Spectral decomposition, Subspace projections\n**Examples:** Orthogonal projection matrix"
  },
  {
    "front": "üß© Imaginary Unit j\nüìò Domain: Complex Analysis",
    "back": "**Definition:** The imaginary unit j satisfying j¬≤ = ‚àí1, commonly used in electrical engineering notation.\n\n**Description:** In many numerical algorithms involving complex arithmetic, j corresponds to the imaginary axis. It is mathematically equivalent to the complex unit i in mathematics.\n\n**Goal:** Represent the imaginary axis in complex numbers.\n**Applications:** Fourier analysis, Complex matrix computations\n**Methods:** Complex arithmetic\n**Examples:** e^{jŒ∏} representation in FFT"
  },
  {
    "front": "üß© Inverse Iteration\nüìò Domain: Eigenvalue Algorithms",
    "back": "**Definition:** An iterative method for approximating eigenvectors by repeatedly solving shifted linear systems.\n\n**Description:** Inverse iteration computes eigenvectors associated with a selected eigenvalue by repeatedly solving (A - ŒºI)x_{k+1} = x_k. When Œº is close to an eigenvalue, convergence is rapid.\n\n**Goal:** Compute eigenvectors accurately for a given eigenvalue shift.\n**Applications:** Eigenvalue refinement, Rayleigh Quotient Iteration\n**Methods:** Shifted linear solves, Spectral shift\n**Examples:** Inverse iteration with Rayleigh quotient shift"
  },
  {
    "front": "üß© Iterative Methods\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Iterative methods are algorithms that generate a sequence of approximations x_j to the solution x of a linear system Ax = b, starting from an initial guess and refining it until convergence.\n\n**Description:** They are preferred for large sparse systems where direct methods like Gaussian elimination are O(n^3) and computationally expensive, with per-iteration costs often O(n^2) or less, such as O(n) for sparse or O(n log n) with structured matrices.\n\n**Goal:** Solve large linear systems efficiently without full factorization.\n**Applications:** PDE discretizations, Optimization, Eigenproblems\n**Methods:** Krylov subspace methods, Conjugate gradient, GMRES, Preconditioned iterations\n**Examples:** x_{j+1} = x_j + correction, Convergence when ||r_j|| small"
  },
  {
    "front": "üß© Linear System\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** A linear system is an equation of the form Ax = b where A is an n x n matrix, x is the unknown vector, and b is the right-hand side vector.\n\n**Description:** For large n (e.g., 10^4 to 10^8), iterative methods are used due to high O(n^3) cost of direct solvers, especially when A is sparse or structured.\n\n**Goal:** Find x such that Ax = b.\n**Applications:** Scientific simulations, Machine learning, Engineering\n**Methods:** Direct (LU, QR), Iterative (CG, GMRES)\n**Examples:** A in C^{n x n}, b in C^n"
  },
  {
    "front": "üß© Arnoldi Process\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The Arnoldi process builds an orthonormal basis Q‚±º for the Krylov subspace K‚±º(A; b) and a Hessenberg matrix H‚±º such that A Q‚±º = Q‚±º‚Çä‚ÇÅ HÃÇ‚±º.\n\n**Description:** It uses Gram‚ÄìSchmidt-like orthogonalisation to compute basis vectors q‚Çñ recursively, enabling reduced-order projections for solving systems or eigenvalues.\n\n**Goal:** Orthogonalise Krylov basis for stable computations.\n**Applications:** GMRES, Eigenvalue solvers, Matrix functions\n**Methods:** Recursive computation: h‚Çñ,‚Çñ‚Çã‚ÇÅ q‚Çñ = A q‚Çñ‚Çã‚ÇÅ ‚àí Œ£ h‚Çó,‚Çñ‚Çã‚ÇÅ q‚Çó\n**Examples:** Q‚±º‚Çä‚ÇÅ HÃÇ‚±º = A Q‚±º"
  },
  {
    "front": "üß© GMRES\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Generalised Minimal Residual (GMRES) is an iterative method that finds x‚±º in x‚ÇÄ + K‚±º(A; r‚ÇÄ) minimising ‚Äñb ‚àí A x‚±º‚Äñ‚ÇÇ for nonsymmetric systems.\n\n**Description:** It uses Arnoldi to build the basis and solves a least-squares problem with the Hessenberg matrix at each step, restarting when j is large.\n\n**Goal:** Minimise residual norm over Krylov subspace.\n**Applications:** Nonsymmetric linear systems, PDE solvers\n**Methods:** Arnoldi orthogonalisation, Least squares on HÃÇ‚±º y ‚àí Œ± e‚ÇÅ\n**Examples:** x‚±º = Q‚±º y‚±º where y‚±º minimises ‚ÄñHÃÇ‚±º y ‚àí Œ± e‚ÇÅ‚Äñ"
  },
  {
    "front": "üß© Residual Norm\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The residual norm ‚Äñr‚±º‚Äñ = ‚Äñb ‚àí A x‚±º‚Äñ measures how well the approximate solution x‚±º satisfies the system Ax = b.\n\n**Description:** In iterative methods, residuals decrease monotonically in GMRES, and are used to test convergence.\n\n**Goal:** Quantify approximation error in equation satisfaction.\n**Applications:** Convergence testing, Stopping criteria\n**Methods:** Euclidean norm, Relative residual\n**Examples:** ‚Äñr‚±º‚Çä‚ÇÅ‚Äñ ‚â§ ‚Äñr‚±º‚Äñ in GMRES"
  },
  {
    "front": "üß© A-Norm Error\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** The A-norm error ‚Äñx ‚àí x‚±º‚Äñ‚Çê = ( (x ‚àí x‚±º), A (x ‚àí x‚±º) )^{1/2} measures the error in the energy norm for SPD A.\n\n**Description:** CG minimises this norm over the Krylov subspace, relating to the quadratic form minimised in the system.\n\n**Goal:** Quantify solution error in energy sense.\n**Applications:** CG convergence analysis, Variational problems\n**Methods:** Defined via inner product (x, A y)\n**Examples:** Min ‚Äñx ‚àí x‚±º‚Äñ‚Çê in CG"
  },
  {
    "front": "üß© Polynomial Approximation\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Polynomial approximation in iterative methods views x‚±º = p‚±º‚Çã‚ÇÅ(A) b as a polynomial in A applied to b, minimising residuals or errors via min-max problems over polynomials.\n\n**Description:** Convergence bounds use Chebyshev or other polynomials to estimate rates based on eigenvalue distribution.\n\n**Goal:** Approximate A‚Åª¬π b via polynomials in A.\n**Applications:** Convergence analysis, Accelerated methods\n**Methods:** Min-max over deg ‚â§ j-1, Chebyshev acceleration\n**Examples:** min_{deg p ‚â§ j-1, p(0)=1} max_Œª |p(Œª)|"
  },
  {
    "front": "üß© Hessenberg Matrix\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** A Hessenberg matrix H is upper triangular except for the subdiagonal, arising in Arnoldi as the projection of A onto the Krylov basis.\n\n**Description:** It simplifies least-squares solves in GMRES and eigenvalue computations.\n\n**Goal:** Reduce matrix for efficient projections.\n**Applications:** GMRES minimisation, QR algorithm\n**Methods:** From Arnoldi: HÃÇ‚±º with subdiagonal\n**Examples:** H‚±º tridiagonal in Lanczos"
  },
  {
    "front": "üß© Jordan Canonical Form\nüìò Domain: Linear Algebra / Spectral Theory",
    "back": "**Definition:** A block-diagonal matrix consisting of Jordan blocks that represent the structure of a linear operator up to similarity transformations.\n\n**Description:** The Jordan Canonical Form reveals eigenvalues, geometric multiplicities, and algebraic multiplicities of a matrix. It classifies matrices up to similarity and provides insight into non-diagonalizable operators.\n\n**Goal:** Classify matrices under similarity and reveal spectral structure.\n**Applications:** Differential equations, Matrix functions, Control theory\n**Methods:** Similarity transformations, Generalized eigenvectors\n**Examples:** Jordan blocks for repeated eigenvalues"
  },
  {
    "front": "üß© Kronecker Product\nüìò Domain: Linear Algebra",
    "back": "**Definition:** An operation on two matrices that produces a block matrix, defined as A ‚äó B = [a_ij B].\n\n**Description:** Kronecker products encode tensor product structures and appear in matrix equations, vectorization identities, and discretizations of PDEs. They are a foundation of fast algorithms for large structured systems.\n\n**Goal:** Represent tensor products and structured matrix operations.\n**Applications:** Sylvester equations, Quantum computing, Tensor calculus\n**Methods:** Block matrix construction, Vectorization identities\n**Examples:** vec(AXB) = (B·µÄ ‚äó A) vec(X)"
  },
  {
    "front": "üß© Lower Triangular Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A matrix whose entries above the main diagonal are all zero.\n\n**Description:** Lower triangular matrices appear naturally in LU factorization and recursive matrix algorithms. They support fast forward substitution for solving linear systems.\n\n**Goal:** Provide a structured matrix enabling efficient solves.\n**Applications:** LU factorization, Forward substitution\n**Methods:** Matrix decomposition\n**Examples:** L in LU = L U"
  },
  {
    "front": "üß© Matrix Pencil\nüìò Domain: Generalized Eigenvalue Theory",
    "back": "**Definition:** A parametric family of matrices of the form A - ŒªB representing a generalized eigenvalue problem.\n\n**Description:** Matrix pencils are central to generalized eigenvalue problems, QZ algorithms, and control theory. They allow describing systems with singular B or differential-algebraic structure.\n\n**Goal:** Represent generalized eigenvalue problems.\n**Applications:** QZ algorithm, Control synthesis, DAEs\n**Methods:** Generalized Schur decomposition\n**Examples:** det(A - ŒªB) = 0 gives generalized eigenvalues"
  },
  {
    "front": "üß© Matrix Rank\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The number of linearly independent rows or columns of a matrix.\n\n**Description:** Matrix rank reveals fundamental properties of a matrix such as invertibility, nullity, and the dimension of its image. Rank plays a core role in solving linear systems, low-rank approximation, and SVD.\n\n**Goal:** Quantify the dimension of the range of a matrix.\n**Applications:** Linear systems, Low-rank approximation, SVD\n**Methods:** Row reduction, Singular value decomposition\n**Examples:** Rank-deficient least squares problems"
  },
  {
    "front": "üß© Minimal Polynomial\nüìò Domain: Spectral Theory",
    "back": "**Definition:** The monic polynomial of least degree such that p(A) = 0.\n\n**Description:** The minimal polynomial characterizes the algebraic structure of a matrix. Its degree determines the size of Jordan blocks and governs convergence of polynomial iterative methods.\n\n**Goal:** Capture the smallest polynomial annihilating a matrix.\n**Applications:** Jordan form, Krylov methods, Matrix functions\n**Methods:** Cayley-Hamilton theorem, Companion matrices\n**Examples:** Diagonalizable matrices have minimal polynomial with simple roots"
  },
  {
    "front": "üß© Normal Matrix\nüìò Domain: Linear Algebra / Spectral Theory",
    "back": "**Definition:** A matrix that commutes with its conjugate transpose, satisfying AA* = A*A.\n\n**Description:** Normal matrices include Hermitian, unitary, and orthogonal matrices as special cases. They are diagonalizable by a unitary matrix and enjoy well-conditioned eigenvalue problems.\n\n**Goal:** Generalize diagonalizable and orthogonally diagonalizable matrices.\n**Applications:** Quantum mechanics, Spectral analysis, Matrix functions\n**Methods:** Unitary diagonalization\n**Examples:** Unitary matrices, Hermitian matrices"
  },
  {
    "front": "üß© Numerical Radius\nüìò Domain: Spectral Theory",
    "back": "**Definition:** The quantity w(A) = max_{‚Äñx‚Äñ=1} |x*Ax|, representing the radius of the smallest disk containing the field of values.\n\n**Description:** The numerical radius provides a tighter bound than the spectral radius for non-normal matrices. It is related to operator norms and is used in stability analysis.\n\n**Goal:** Bound eigenvalues and assess stability.\n**Applications:** Operator theory, Stability, Non-normal matrices\n**Methods:** Field of values analysis\n**Examples:** w(A) ‚â§ ‚ÄñA‚Äñ ‚â§ 2 w(A)"
  },
  {
    "front": "üß© Pseudospectrum\nüìò Domain: Spectral Theory",
    "back": "**Definition:** The Œµ-pseudospectrum of A is the set of complex numbers z for which ‚Äñ(A - zI)‚Åª¬π‚Äñ > 1/Œµ or z is an eigenvalue of a nearby matrix.\n\n**Description:** Pseudospectra describe sensitivity of eigenvalues to perturbations and are crucial in understanding non-normal behavior, transient growth, and numerical stability.\n\n**Goal:** Assess robustness of eigenvalues under perturbations.\n**Applications:** Non-normal matrices, Stability analysis, Control theory\n**Methods:** Resolvent norm computation\n**Examples:** Highly non-normal matrices have large pseudospectra"
  },
  {
    "front": "üß© Positive Semidefinite Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A Hermitian/Symmetric matrix A satisfying x·µÄAx ‚â• 0 for all x.\n\n**Description:** Positive semidefinite (PSD) matrices appear in optimization, covariance matrices, kernel methods, and PDE discretizations. They generalize positive definite matrices.\n\n**Goal:** Characterize matrices inducing non-negative quadratic forms.\n**Applications:** Optimization, Machine learning, Statistics\n**Methods:** Cholesky-like factorizations, Eigenvalue analysis\n**Examples:** Covariance matrices in statistics"
  },
  {
    "front": "üß© Quadratic Form\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A function of the form q(x) = x·µÄAx for a symmetric matrix A.\n\n**Description:** Quadratic forms relate directly to eigenvalues, definiteness, and optimization landscapes. They appear in stability theory, energy minimization, and classification of matrices.\n\n**Goal:** Model energy-like quantities through symmetric matrices.\n**Applications:** Optimization, Stability, Statistics\n**Methods:** Spectral decomposition\n**Examples:** Second-order Taylor approximations"
  },
  {
    "front": "üß© Orthogonal Complement\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The orthogonal complement of a subspace V is the set of vectors perpendicular to all elements in V, denoted V^‚ä•.\n\n**Description:** For projections, if P is orthogonal, then R(P) ‚ä• R(I - P), ensuring the decomposition is orthogonal.\n\n**Goal:** Decompose space into perpendicular subspaces.\n**Applications:** Gram-Schmidt, Least squares, Spectral methods\n**Methods:** Dot product zero condition, Null space of transpose\n**Examples:** R(I - P) as complement of R(P)"
  },
  {
    "front": "üß© Gershgorin Circle Theorem\nüìò Domain: Matrix Analysis",
    "back": "**Definition:** Every eigenvalue of A ‚àà ‚ÑÇ^{n√ón} lies in at least one disk D‚±º = {z : |z ‚àí a‚±º‚±º| ‚â§ R‚±º}, where R‚±º = Œ£_{l‚â†j} |a‚±º‚Çó|}.\n\n**Description:** Provides eigenvalue localization without computation. Disks are centered at diagonal entries with radii equal to off-diagonal row sums. Useful for bounding spectral radius and detecting diagonal dominance.\n\n**Goal:** Locate eigenvalues in complex plane using only matrix entries.\n**Applications:** Convergence analysis, Error bounding, Preconditioning design\n**Methods:** Row-sum computation, Union of disks, Refinement via similarity\n**Examples:** Strictly diagonally dominant ‚Üí eigenvalues in disjoint disks"
  },
  {
    "front": "üß© Power Iteration\nüìò Domain: Eigenvalue Algorithms",
    "back": "**Definition:** An iterative method that computes the dominant eigenvalue and eigenvector by repeatedly applying A to a vector and normalizing: q^{(k)} = A q^{(k-1)} / ||A q^{(k-1)}||, Œª^{(k)} = (A q^{(k)}, q^{(k)}).\n\n**Description:** Converges linearly to the eigenvector corresponding to the largest-magnitude eigenvalue if |Œª‚ÇÅ| > |Œª‚ÇÇ| ‚â• ... ‚â• |Œª‚Çô|. Convergence rate is |Œª‚ÇÇ/Œª‚ÇÅ|. Inverse iteration targets smallest or shifted eigenvalues.\n\n**Goal:** Find dominant eigenpair with minimal storage and simple operations.\n**Applications:** PageRank, Principal Component Analysis, Vibration modes\n**Methods:** Rayleigh quotient, Normalization, Deflation, Shift-and-invert\n**Examples:** q^{(k)} ‚âà x‚ÇÅ + O((Œª‚ÇÇ/Œª‚ÇÅ)^k)"
  },
  {
    "front": "üß© Householder Reflection\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A unitary matrix H = I ‚àí 2vv*/(v*v) that reflects a vector x across the hyperplane perpendicular to v, mapping x to œÉe‚ÇÅ.\n\n**Description:** Used in QR factorization and Hessenberg reduction to introduce zeros below the subdiagonal. Numerically stable and requires O(n) operations per reflection. Essential for implicit QR algorithm.\n\n**Goal:** Zero out selected entries via unitary similarity while preserving eigenvalues.\n**Applications:** QR decomposition, Hessenberg form, Tridiagonalization\n**Methods:** Sign choice for stability, WY representation, Blocked Householder\n**Examples:** H x = ‚àísign(x‚ÇÅ) ||x|| e‚ÇÅ"
  },
  {
    "front": "üß© QR Algorithm\nüìò Domain: Eigenvalue Computation",
    "back": "**Definition:** An iterative method that computes the Schur form via repeated QR decompositions: A_{k+1} = R_k Q_k, with A_0 = A. With shifts, converges cubically to upper triangular form.\n\n**Description:** The de facto standard for dense eigenvalue problems. Implicit version uses Householder/Bulge chasing to reduce cost from O(n¬≥) per iteration to O(n). Francis shift accelerates convergence.\n\n**Goal:** Compute all eigenvalues (and optionally eigenvectors) via unitary similarity to triangular form.\n**Applications:** MATLAB eig, LAPACK, Control theory, PDE solvers\n**Methods:** Hessenberg reduction, Francis double shift, Deflation, Balancing\n**Examples:** A_{k+1} = Q_k* A_k Q_k"
  },
  {
    "front": "üß© Generalized Eigenvalue Problem\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** Find Œª ‚àà ‚ÑÇ and nonzero x ‚àà ‚ÑÇ‚Åø such that Ax = ŒªBx, with A, B ‚àà ‚ÑÇ^{n√ón}.\n\n**Description:** Arises in structural dynamics, control, and Markov chains. Reduced to standard form if B invertible (M = B‚Åª¬πA). QZ algorithm generalizes QR using unitary transformations to triangularize both matrices.\n\n**Goal:** Solve coupled systems or weighted eigenvalue problems.\n**Applications:** Vibration with constraints, Markov chain stationary distribution, Optimal control\n**Methods:** QZ algorithm, Cholesky + standard EVP, Shift-and-invert\n**Examples:** Ax = ŒªBx with B positive definite"
  },
  {
    "front": "üß© Field of Values\nüìò Domain: Matrix Analysis",
    "back": "**Definition:** The set F(A) = {x*Ax : x ‚àà ‚ÑÇ‚Åø, ||x||=1}, also known as the numerical range.\n\n**Description:** Convex, compact set containing all eigenvalues. Bounds spectral radius and condition number. For normal matrices, F(A) is the convex hull of eigenvalues.\n\n**Goal:** Characterize operator behavior beyond eigenvalues.\n**Applications:** Stability analysis, Convergence of iterations, Pseudospectrum approximation\n**Methods:** Rayleigh quotient, Hausdorff-Toeplitz theorem, Discretization\n**Examples:** F(A) = conv(œÉ(A)) if A normal"
  },
  {
    "front": "üß© LU Factorization with Partial Pivoting\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** LU factorization with partial pivoting decomposes a matrix A ‚àà ‚ÑÇ^{n√ón} into PA = LU, where P is a permutation matrix, L is unit lower triangular with |l_{ij}| ‚â§ 1 for i > j, and U is upper triangular. Partial pivoting selects the largest absolute entry in the current column as the pivot to minimize numerical instability.\n\n**Description:** This algorithm enhances the stability of Gaussian elimination by row permutations to avoid small pivots. It is the standard method for solving linear systems Ax = b in practice, balancing computational cost (O(n¬≥)) with robustness against round-off errors in floating-point arithmetic.\n\n**Goal:** Stable triangular factorization of a matrix for solving linear systems and computing inverses.\n**Applications:** Solving Ax = b, Matrix inversion, Determinant computation, Condition number estimation\n**Methods:** Gaussian elimination with row pivoting, In-place storage using single array, Compact WY representation for L\n**Examples:** 4√ó4 matrix example with pivots 8, 17/4, -6/7, 2 showing growth control"
  },
  {
    "front": "üß© Partial Pivoting\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A pivoting strategy in Gaussian elimination that, at step k, permutes rows so that the entry of largest magnitude in column k (from row k to n) becomes the pivot, ensuring |pivot| = max_{i‚â•k} |a_{ik}|.\n\n**Description:** Partial pivoting prevents division by small pivots, reducing amplification of round-off errors. It guarantees that all subdiagonal entries in L satisfy |l_{ij}| ‚â§ 1, bounding the growth factor œÅ ‚â§ 2^{n-1} in theory, though typically much smaller in practice.\n\n**Goal:** Minimize numerical error propagation during elimination by choosing largest available pivot.\n**Applications:** LU factorization, Linear system solving, Matrix decomposition in finite precision\n**Methods:** Row interchange before elimination step, Column scanning for max absolute value\n**Examples:** P1 swaps rows to bring largest entry to diagonal position"
  },
  {
    "front": "üß© Permutation Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A permutation matrix P is a square matrix with exactly one 1 in each row and column and 0s elsewhere. Multiplying PA permutes the rows of A; AP permutes columns.\n\n**Description:** In LU with partial pivoting, P represents the cumulative row interchanges. Since P^{-1} = P^T = P^*, it preserves norms: ||Px|| = ||x||. The final factorization is PA = LU.\n\n**Goal:** Represent row or column reordering in matrix factorizations.\n**Applications:** Pivoting in LU, Reordering for sparsity, Graph relabeling\n**Methods:** Identity with swapped rows, Product of elementary permutation matrices\n**Examples:** P = [0 1; 1 0] swaps rows 1 and 2"
  },
  {
    "front": "üß© Growth Factor\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** The growth factor rho in LU factorization with partial pivoting is defined as rho = max_{i,j} |u_{ij}| / max_{i,j} |a_{ij}|, measuring the largest entry in U relative to the original matrix A.\n\n**Description:** Controls backward stability: computed factors satisfy L_hat U_hat = P A + delta A with ||delta A|| / ||A|| = O(rho epsilon_machine). Partial pivoting keeps rho moderate in practice, though worst-case rho = 2^{n-1} is possible.\n\n**Goal:** Quantify element growth during Gaussian elimination to assess numerical stability.\n**Applications:** Backward error analysis, Condition estimation, Pivoting strategy evaluation\n**Methods:** Ratio of max |u_{ij}| to max |a_{ij}|, Monitored during factorization\n**Examples:** Wilkinson's matrix gives rho approx 2^{n-1}"
  },
  {
    "front": "üß© Condition Number\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** The condition number Œ∫(W) of a nonsingular matrix W is Œ∫(W) = ||W|| ‚ãÖ ||W^{-1}||, with respect to any consistent matrix norm. For the 1-norm or ‚àû-norm, Œ∫‚ÇÅ(W) = œÉ‚ÇÅ/œÉ‚Çô where œÉ are singular values.\n\n**Description:** Measures sensitivity of linear system solution to perturbations. Large Œ∫ implies ill-conditioned system: small changes in input cause large output changes. In LU context, related to pivot size and growth.\n\n**Goal:** Quantify sensitivity of Ax = b to perturbations in A or b.\n**Applications:** Error bounds in linear solvers, Preconditioning design, Numerical stability analysis\n**Methods:** SVD-based: Œ∫‚ÇÇ = œÉ_max / œÉ_min, 1-norm estimation via Hager's method\n**Examples:** Œ∫(A) ‚âà 10^k ‚áí lose k digits of accuracy"
  },
  {
    "front": "üß© Gaussian Elimination\nüìò Domain: Linear Algebra",
    "back": "**Definition:** Gaussian elimination transforms a linear system Ax = b into upper triangular form Ux = c via row operations: adding multiples of one row to another. With pivoting, it forms the basis of LU factorization.\n\n**Description:** Core algorithm for solving linear systems. Without pivoting, unstable for small pivots. With partial pivoting, becomes robust standard method. Can be expressed as sequence of rank-1 updates or multiplier storage in L.\n\n**Goal:** Reduce system to triangular form for back substitution.\n**Applications:** Linear system solving, Matrix factorization, Determinant via product of diagonals\n**Methods:** Forward elimination, Back substitution, Pivoting variants\n**Examples:** 4√ó4 system reduced step-by-step with P, L, U shown"
  },
  {
    "front": "üß© A - Œº I\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A shifted matrix operator formed by subtracting a scalar multiple of the identity matrix from A.\n\n**Description:** The matrix A - ŒºI is central to spectral shift techniques, inverse iteration, Rayleigh Quotient Iteration, and many eigenvalue algorithms. Shifting by Œº changes the eigenvalues to Œª_i - Œº, allowing selective amplification of eigen-components.\n\n**Goal:** Shift the spectrum of A so selected eigenvalues become easier to isolate or invert.\n**Applications:** Inverse Iteration, Rayleigh Quotient Iteration, Spectral Shift, Shift-Invert Methods\n**Methods:** Shift-and-invert transformation, Spectral manipulation\n**Examples:** (A - Œª_min I) is used to emphasize smallest eigenvalues"
  },
  {
    "front": "üß© Backward Stability\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** A property of an algorithm where the computed solution is the exact solution to a slightly perturbed input problem.\n\n**Description:** Backward stability ensures that numerical errors arise only from small perturbations in the input, making the algorithm robust. LU with partial pivoting, QR factorization, and SVD-based algorithms are typical examples.\n\n**Goal:** Guarantee high numerical reliability even under rounding error.\n**Applications:** Linear system solving, Eigenvalue computations, Least squares\n**Methods:** Perturbation analysis, Stability theory\n**Examples:** LU with partial pivoting is backward stable for many matrices"
  },
  {
    "front": "üß© Brauer Cassini Ovals\nüìò Domain: Spectral Theory",
    "back": "**Definition:** A set of regions in the complex plane providing eigenvalue inclusion bounds based on a partition of the matrix.\n\n**Description:** Brauer‚Äôs theorem partitions the matrix into blocks and constructs Cassini ovals that enclose all eigenvalues. They generalize Gershgorin‚Äôs disks and often produce tighter bounds.\n\n**Goal:** Provide eigenvalue inclusion regions sharper than Gershgorin circles.\n**Applications:** Eigenvalue estimation, Spectral bounds, Iterative method convergence analysis\n**Methods:** Block partitioning, Cassini oval construction\n**Examples:** Used to bound eigenvalues of non-normal matrices"
  },
  {
    "front": "üß© Cauchy Integral Formula\nüìò Domain: Complex Analysis / Matrix Functions",
    "back": "**Definition:** A contour integral formula that represents analytic functions inside a closed curve using boundary values.\n\n**Description:** In matrix computations, the Cauchy integral formula is used to define analytic matrix functions such as f(A). It allows representing functions of matrices using contour integrals around the spectrum.\n\n**Goal:** Define analytic functions of matrices using contour integration.\n**Applications:** Matrix exponential, Matrix sign function, Matrix square root\n**Methods:** Contour integration, Residue calculus\n**Examples:** f(A) = (1/(2œÄi)) ‚àÆ f(z)(zI - A)‚Åª¬π dz"
  },
  {
    "front": "üß© Chebyshev Polynomials\nüìò Domain: Approximation Theory",
    "back": "**Definition:** A family of orthogonal polynomials minimizing the maximum error over [-1,1].\n\n**Description:** Chebyshev polynomials play a key role in iterative methods, eigenvalue solvers, spectral methods, and approximation theory. They provide optimal polynomial approximants and reduce oscillations.\n\n**Goal:** Achieve near-minimax polynomial approximation.\n**Applications:** Polynomial approximation, Iterative methods acceleration, Spectral discretization\n**Methods:** Orthogonal polynomials, Clenshaw recurrence\n**Examples:** Chebyshev semi-iterative method"
  },
  {
    "front": "üß© Circulant Matrix\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A structured matrix where each row is a cyclic shift of the previous row.\n\n**Description:** Circulant matrices are diagonalizable by the discrete Fourier transform (DFT), making them computationally efficient for convolution, filtering, and spectral analysis. Fast multiplication using FFT leads to O(n log n) operations.\n\n**Goal:** Exploit cyclic structure for fast computations.\n**Applications:** Signal processing, Fast convolution, Spectral graph theory\n**Methods:** DFT diagonalization, FFT-based multiplication\n**Examples:** Toeplitz-circulant approximation, Convolution as circulant matrix-vector product"
  },
  {
    "front": "üß© Complete Pivoting\nüìò Domain: Numerical Linear Algebra",
    "back": "**Definition:** A pivoting strategy for Gaussian elimination where the largest element in the remaining submatrix is chosen as the pivot.\n\n**Description:** Complete pivoting reduces numerical instability by selecting globally maximal pivots. Although more stable than partial pivoting, it is computationally more expensive and therefore rarely used in large-scale problems.\n\n**Goal:** Improve numerical stability of Gaussian elimination.\n**Applications:** Gaussian elimination, LU factorization\n**Methods:** Global pivot search, Row and column permutations\n**Examples:** Used when numerical precision is critical in small matrices"
  },
  {
    "front": "üß© Congruence Transformation\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A transformation of the form A ‚Ü¶ P·µÄAP where P is nonsingular, often used to analyze symmetric matrices.\n\n**Description:** Congruence transformations preserve definiteness properties of matrices. They are commonly used in quadratic forms, inertia theory, and Lyapunov stability analysis.\n\n**Goal:** Preserve quadratic form properties while transforming matrices.\n**Applications:** Quadratic forms, Lyapunov equations, Symmetric matrix analysis\n**Methods:** Matrix decomposition, Equivalence transformations\n**Examples:** Sylvester‚Äôs law of inertia"
  },
  {
    "front": "üß© Control Synthesis\nüìò Domain: Control Theory",
    "back": "**Definition:** The process of designing controllers that achieve desired system performance using mathematical models.\n\n**Description:** Control synthesis includes optimal control, state feedback, LQR design, and H‚àû control. Many linear algebraic problems such as Lyapunov and Riccati equations appear in control synthesis.\n\n**Goal:** Design controllers that stabilize and optimize dynamic systems.\n**Applications:** Robotics, Aerospace, Feedback systems\n**Methods:** State-space design, Linear algebraic equations, Optimization\n**Examples:** LQR controller synthesis, Pole placement"
  },
  {
    "front": "üß© Direct Sum\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A decomposition V = U ‚äï W where every vector in V has a unique representation as u + w with u ‚àà U and w ‚àà W.\n\n**Description:** Direct sum decompositions are fundamental in structure theory, invariant subspaces, and block matrix representations. They provide a clean separation of components across orthogonal or complementary subspaces.\n\n**Goal:** Split a vector space into independent, non-overlapping components.\n**Applications:** Invariant subspaces, Block diagonalization, Decomposing matrix subspaces\n**Methods:** Projection operators, Complementary subspaces\n**Examples:** V = span(e1) ‚äï span(e2,e3)"
  },
  {
    "front": "üß© Domain Decomposition\nüìò Domain: Numerical PDEs / Scientific Computing",
    "back": "**Definition:** A method for solving PDEs by dividing the computational domain into smaller subdomains and solving local problems.\n\n**Description:** Domain decomposition improves parallelism, reduces communication, and allows localized problem-solving. It forms the basis for Schwarz methods, additive and multiplicative preconditioners.\n\n**Goal:** Solve large PDE systems efficiently using parallel local solves.\n**Applications:** Elliptic PDEs, Finite element methods, Parallel solvers\n**Methods:** Schwarz iteration, Overlapping and non-overlapping decompositions\n**Examples:** Additive Schwarz method"
  },
  {
    "front": "üß© Elliptic PDE\nüìò Domain: Partial Differential Equations",
    "back": "**Definition:** A class of PDEs characterized by positive-definite differential operators, often requiring solution of large sparse systems.\n\n**Description:** Elliptic PDEs frequently arise in steady-state physical systems. Their discretizations lead to SPD linear systems that are ideal for conjugate gradients and multigrid solvers.\n\n**Goal:** Model steady-state physical processes such as diffusion, elasticity, and electrostatics.\n**Applications:** Finite element analysis, Heat conduction, Electrostatics\n**Methods:** Discretization, Iterative solvers\n**Examples:** Poisson equation: ‚àíŒîu = f"
  },
  {
    "front": "üß© Eigenvalue Decomposition\nüìò Domain: Spectral Theory",
    "back": "**Definition:** A decomposition A = VŒõV‚Åª¬π where Œõ is diagonal and V contains eigenvectors of A.\n\n**Description:** Eigenvalue decomposition exists for diagonalizable matrices and plays a central role in spectral analysis, diagonalization, and understanding matrix dynamics.\n\n**Goal:** Represent a matrix in terms of its eigenvalues and eigenvectors.\n**Applications:** Diagonalization, Matrix functions, Stability analysis\n**Methods:** Similarity transformation\n**Examples:** A diagonalizable matrix with distinct eigenvalues"
  },
  {
    "front": "üß© Exponential Integrators\nüìò Domain: Numerical Differential Equations",
    "back": "**Definition:** A class of time-stepping methods that use the matrix exponential to solve stiff differential equations.\n\n**Description:** By explicitly integrating the linear part of the system, exponential integrators reduce stiffness and enable stable large time steps. They use matrix exponential actions such as exp(A)v.\n\n**Goal:** Solve stiff ODEs efficiently with large stable time steps.\n**Applications:** Stiff ODEs, Schr√∂dinger equation, Fluid dynamics\n**Methods:** Krylov subspace exponential actions, œÜ-functions\n**Examples:** Exponential Euler method, ETD Runge‚ÄìKutta"
  },
  {
    "front": "üß© Matrix Product V1 V2\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The matrix product V1 V2 represents the set of all matrices formed by multiplying elements from subspaces V1 and V2, i.e., {V1 V2 : V1 ‚àà V1, V2 ‚àà V2}.\n\n**Description:** This construct is used to approximate or factor large matrices A by finding subspaces such that A lies in V1 V2, enabling low-parameter representations.\n\n**Goal:** Represent matrices as products of subspace elements for factorization.\n**Applications:** Matrix approximation, Low-rank factoring, Compression\n**Methods:** Subspace identification, Optimization over subspaces\n**Examples:** A ‚âà V1 V2 for V1, V2 low-dimensional"
  },
  {
    "front": "üß© Range of Projection\nüìò Domain: Linear Algebra",
    "back": "**Definition:** The range R(P) of a projection P is the set {y : y = Px for some x}, representing the subspace onto which P projects.\n\n**Description:** For orthogonal projections, R(P) is perpendicular to R(I - P), forming an orthogonal decomposition of the space.\n\n**Goal:** Define the projected subspace.\n**Applications:** Subspace identification, Dimensionality reduction\n**Methods:** Column span of P, Fixed points of P\n**Examples:** R(P) = span{columns of P}"
  },
  {
    "front": "üß© Algorithmic Factoring\nüìò Domain: Numerical Analysis",
    "back": "**Definition:** Algorithmic factoring refers to computational methods for decomposing matrices into structured forms like products of subspaces or triangular factors.\n\n**Description:** It leverages Krylov subspaces and projections to factor matrices efficiently, especially for large-scale problems.\n\n**Goal:** Factor matrices using algorithmic techniques.\n**Applications:** Large linear systems, Eigenproblems, Approximations\n**Methods:** Subspace iteration, Projection-based factoring\n**Examples:** A = V1 W^{-1} using subspaces"
  },
  {
    "front": "üß© Invariant Subspace\nüìò Domain: Linear Algebra",
    "back": "**Definition:** An invariant subspace W for matrix A satisfies A W ‚äÜ W, meaning A maps W into itself.\n\n**Description:** Invariant subspaces are key in spectral theory and factoring, often identified via Krylov methods or projections.\n\n**Goal:** Find subspaces stable under matrix action.\n**Applications:** Eigen decomposition, Schur form, Model reduction\n**Methods:** Subspace iteration, Arnoldi process\n**Examples:** Eigenvector spans 1D invariant subspace"
  },
  {
    "front": "üß© Square Matrix A\nüìò Domain: Linear Algebra",
    "back": "**Definition:** A square matrix A ‚àà ‚ÑÇ^{n√ón} is a matrix with equal rows and columns, central to linear transformations and eigenvalue problems.\n\n**Description:** In factoring contexts, A is decomposed using subspaces, projections, or iterations for computational efficiency.\n\n**Goal:** Represent linear operators on finite-dimensional spaces.\n**Applications:** Systems of equations, Transformations, Spectral analysis\n**Methods:** Factorization, Iteration, Diagonalization\n**Examples:** A with complex entries"
  }
]