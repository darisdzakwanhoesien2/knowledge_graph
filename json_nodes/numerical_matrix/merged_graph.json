{
  "nodes": {
    "Singular Value Decomposition": {
      "type": "Method",
      "domain": "Linear Algebra",
      "definition": "The Singular Value Decomposition (SVD) of a matrix A ∈ ℂ^{m×n} is a factorization A = U Σ V^*, where U and V are unitary matrices with orthonormal columns, and Σ is a diagonal matrix with non-negative singular values σ₁ ≥ σ₂ ≥ ⋯ ≥ σ_n ≥ 0 on the diagonal.",
      "description": "SVD provides the best low-rank approximation of a matrix in both spectral and Frobenius norms, enabling optimal matrix compression, dimensionality reduction, and solving least squares problems. It decomposes any matrix into orthogonal bases that capture the principal directions of variation.",
      "properties": {
        "Goal": "Provide orthogonal factorization and low-rank approximations.",
        "Applications": [
          "Data compression",
          "Principal component analysis",
          "Pseudo-inverse"
        ],
        "Methods": [
          "Golub-Reinsch algorithm",
          "Divide-and-conquer",
          "Iterative methods"
        ],
        "Examples": [
          "A ≈ U_k Σ_k V_k^* for rank-k approximation"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Singular Value": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The singular values of a matrix A are the non-negative square roots of the eigenvalues of A^*A (or AA^*), ordered as σ₁ ≥ σ₂ ≥ ⋯ ≥ σ_r > 0, where r is the rank of A.",
      "description": "Singular values measure the 'importance' or 'strength' of each principal direction in the matrix. The largest singular value σ₁ equals the operator norm ||A||, and they uniquely determine the best low-rank approximations.",
      "properties": {
        "Goal": "Quantify the magnitude of principal components in matrix decomposition.",
        "Applications": [
          "Rank determination",
          "Condition number computation (σ₁/σ_r)",
          "Numerical stability analysis"
        ],
        "Methods": [
          "Eigenvalue decomposition of A^*A",
          "Square root of eigenvalues"
        ],
        "Examples": [
          "σ₁ = ||A|| = max_{||x||=1} ||Ax|| represents maximum stretch"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Unitary Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A unitary matrix Q ∈ ℂ^{n×n} satisfies Q^*Q = I, meaning its columns (and rows) form an orthonormal basis that preserves the Euclidean norm: ||Qx|| = ||x|| for all x ∈ ℂ^n.",
      "description": "Unitary matrices represent rotations and reflections in complex space. They appear in QR decomposition, SVD, and eigenvalue problems, preserving distances and angles during transformations.",
      "properties": {
        "Goal": "Preserve norms and inner products in linear transformations.",
        "Applications": [
          "QR decomposition",
          "Singular Value Decomposition",
          "Numerical linear algebra algorithms",
          "Quantum computing gates"
        ],
        "Methods": [
          "Gram-Schmidt process",
          "Householder reflections"
        ],
        "Examples": [
          "Q = [q₁ ⋯ qₙ] where {qⱼ} are orthonormal vectors"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Low Rank Approximation": {
      "type": "Method",
      "domain": "Linear Algebra",
      "definition": "The low-rank approximation problem finds a rank-k matrix F_k that minimizes ||A - F_k||_F among all rank-k matrices, solved uniquely by SVD: F_k = ∑_{j=1}^k σⱼ uⱼ vⱼ^*.",
      "description": "SVD provides the optimal rank-k approximation in Frobenius norm, equivalent to keeping the k largest singular values. This compresses data while minimizing reconstruction error.",
      "properties": {
        "Goal": "Approximate high-dimensional matrix with lower-rank version minimizing ||A - F_k||_F.",
        "Applications": [
          "Data compression",
          "Dimensionality reduction",
          "Image denoising",
          "Recommendation systems"
        ],
        "Methods": [
          "Truncated SVD",
          "Eckart-Young theorem"
        ],
        "Examples": [
          "F_k = U_k Σ_k V_k^* where error = ∑_{j=k+1}^r σⱼ²"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Frobenius Norm": {
      "type": "Metric",
      "domain": "Linear Algebra",
      "definition": "The Frobenius norm of A ∈ ℂ^{m×n} is ||A||_F = √(∑_{j=1}^m ∑_{k=1}^n |a_{jk}|²) = √trace(A^*A), measuring the Euclidean norm of the matrix treated as a vector in ℂ^{mn}.",
      "description": "Widely used in matrix approximation problems due to computational convenience and equivalence to SVD error. It's unitarily invariant: ||Q₁AQ₂||_F = ||A||_F for unitary Q₁, Q₂.",
      "properties": {
        "Goal": "Measure matrix 'size' as vector in high-dimensional space.",
        "Applications": [
          "Low-rank approximation error",
          "Matrix compression",
          "Least squares problems"
        ],
        "Methods": [
          "√(sum of squared entries)",
          "√trace(A^*A)"
        ],
        "Examples": [
          "||A||_F² = ∑ σⱼ² where σⱼ are singular values"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Operator Norm": {
      "type": "Metric",
      "domain": "Linear Algebra",
      "definition": "The operator norm ||A|| = max_{||x||=1} ||Ax|| measures the maximum stretch factor of the linear transformation A: ℂ^n → ℂ^m.",
      "description": "For SVD, ||A|| = σ₁, the largest singular value. It quantifies how much A can amplify unit vectors and determines numerical stability.",
      "properties": {
        "Goal": "Measure maximum amplification by linear transformation.",
        "Applications": [
          "Condition number (||A|| ⋅ ||A⁻¹||)",
          "Stability analysis",
          "Spectral radius bounds"
        ],
        "Methods": [
          "Largest singular value σ₁",
          "max_{||x||=1} ||Ax||"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Matrix Function": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "A matrix function f(A) extends scalar functions to matrices A, defined via Jordan form, power series, or other representations, enabling operations like exponentials or square roots on matrices.",
      "description": "Matrix functions are computed iteratively for large matrices, used in differential equations, control theory, and eigenvalue problems.",
      "properties": {
        "Goal": "Extend scalar functions to matrices while preserving algebraic structure.",
        "Applications": [
          "Matrix exponential in ODEs",
          "Matrix logarithm in geometry",
          "Sign function in control"
        ],
        "Methods": [
          "Schur-Parlett",
          "Scaling-and-squaring",
          "Padé approximation",
          "Contour integral"
        ],
        "Examples": [
          "exp(A)",
          "A^{1/2}",
          "sign(A)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Matrix Square Root": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "The matrix square root X satisfies X^2 = A for a matrix A, with the principal square root being positive definite if A is.",
      "description": "It is computed iteratively, e.g., via Newton's method, for applications in statistics, control, and geometry.",
      "properties": {
        "Goal": "Find X such that X^2 = A.",
        "Applications": [
          "Covariance matrices",
          "Riemannian metrics",
          "Polar decomposition"
        ],
        "Methods": [
          "Newton iteration",
          "Denman-Beavers",
          "Schur method"
        ],
        "Examples": [
          "X = sqrt(A) for SPD A"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Newton's Iteration for Square Root": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Newton's iteration for the matrix square root updates X_{k+1} = (X_k + A X_k^{-1}) / 2, starting from X_0 = I or other, converging quadratically to sqrt(A).",
      "description": "It is stable for positive definite A, with safeguards for convergence, used in large-scale computations via iterative solvers.",
      "properties": {
        "Goal": "Compute sqrt(A) iteratively.",
        "Applications": [
          "Matrix sign function",
          "Algebraic Riccati equations"
        ],
        "Methods": [
          "Matrix inversion at each step",
          "Quadratic convergence"
        ],
        "Examples": [
          "X_{k+1} = (X_k + A X_k^{-1}) / 2"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Eigenvalue Problem": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "The eigenvalue problem seeks scalars λ and vectors x ≠ 0 such that A x = λ x for matrix A.",
      "description": "Iterative methods like power iteration or Lanczos are used for large sparse matrices, shifting spectrum for better conditioning.",
      "properties": {
        "Goal": "Compute spectrum and invariant subspaces for analysis and transformation of linear operators.",
        "Applications": [
          "Vibration analysis",
          "Stability of dynamical systems",
          "Google PageRank",
          "Quantum mechanics"
        ],
        "Methods": [
          "Power iteration",
          "QR algorithm",
          "Arnoldi iteration",
          "Jacobi-Davidson"
        ],
        "Examples": [
          "Ax = λx",
          "det(A−λI)=0 (characteristic equation)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Spectral Shift": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Spectral shift transforms the eigenvalue problem to (A - μ I) x = (λ - μ) x, shifting eigenvalues by μ to target specific parts of the spectrum or improve conditioning.",
      "description": "Choosing μ near a target eigenvalue accelerates convergence in iterative methods; for real μ, it can make the matrix positive definite.",
      "properties": {
        "Goal": "Adjust spectrum for better numerical properties.",
        "Applications": [
          "Interior eigenvalues",
          "Deflation",
          "Preconditioning"
        ],
        "Methods": [
          "μ close to target λ",
          "μ = (λ_min + λ_max)/2"
        ],
        "Examples": [
          "Λ(A - μ I) = Λ(A) - μ"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Positive Definite Shift": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Positive definite shift chooses μ such that A - μ I is positive definite, e.g., μ < λ_min(A) for symmetric A, enabling use of CG or other SPD methods.",
      "description": "It stabilizes iterations and bounds condition number; for estimated ˆμ ≈ λ_min, adjust with a > 0 to ensure positivity.",
      "properties": {
        "Goal": "Make shifted matrix SPD for efficient solving.",
        "Applications": [
          "Shift-and-invert",
          "Preconditioned eigensolvers"
        ],
        "Methods": [
          "μ = ˆμ - a, a > 0",
          "Rayleigh quotient estimate"
        ],
        "Examples": [
          "A - μ I with μ = ˆμ - a"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Spectrum Λ(A)": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The spectrum Λ(A) is the set of all eigenvalues of matrix A.",
      "description": "Shifting modifies the spectrum as Λ(A - μ I) = Λ(A) - μ, used to isolate eigenvalues or improve numerical properties.",
      "properties": {
        "Goal": "Describe intrinsic behavior of linear operators.",
        "Applications": [
          "Eigenvalue problems",
          "Stability analysis",
          "Iterative method convergence",
          "Spectral decomposition"
        ],
        "Methods": [
          "Characteristic polynomial",
          "Schur decomposition",
          "Jordan form"
        ],
        "Examples": [
          "Spectrum of a diagonal matrix",
          "Spectrum of SPD matrices (real, positive)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Matrix Computations": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "The study and development of algorithms for performing operations on matrices, including addition, multiplication, inversion, decomposition, and solving linear systems.",
      "description": "Matrix computations are central to numerical analysis, scientific computing, and computational science. They underpin applications in physics simulations, data analysis, machine learning, and computer graphics. The computational complexity of classical algorithms for matrix multiplication is O(n³), while storage is typically O(n²).",
      "properties": {
        "Goal": "Solve matrix-related problems efficiently in numerical contexts.",
        "Applications": [
          "Scientific computing",
          "Data analysis",
          "PDE solving",
          "Linear algebra problems"
        ],
        "Methods": [
          "Factorizations (LU, SVD)",
          "Eigenvalue computations",
          "Iterative solvers",
          "Subspace approximations"
        ],
        "Examples": [
          "Solving Ax = b",
          "Approximating large matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01_introduction.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Inner Product": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A binary operation on two vectors in ℂⁿ that produces a scalar, defined as ⟨x, y⟩ = yᵀx̄ = ∑ⱼ xⱼȳⱼ, where x, y ∈ ℂⁿ.",
      "description": "The inner product (also known as dot product in real vector spaces) measures similarity between vectors and forms the foundation of orthogonality, norms, and projections. In complex spaces, it involves conjugation to ensure positive definiteness of the induced norm.",
      "properties": {
        "Goal": "Quantify angle and similarity between vectors; enable orthogonal decomposition",
        "Applications": [
          "Gram-Schmidt orthogonalization",
          "Least squares",
          "Signal processing",
          "Quantum mechanics"
        ],
        "Methods": [
          "N/A"
        ],
        "Examples": [
          "⟨x, y⟩ = ∑ xⱼȳⱼ over j=1 to n"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01_introduction.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Computational Complexity (Matrix Multiplication)": {
      "type": "Metric",
      "domain": "Algorithm Analysis",
      "definition": "The asymptotic resource requirement for matrix multiplication and related operations, classically O(n³) time and O(n²) space for n×n matrices.",
      "description": "Standard matrix multiplication of two n×n matrices requires O(n³) arithmetic operations using the naive algorithm. While theoretical improvements exist (e.g., Strassen’s O(n².⁸⁰⁷)), practical methods remain close to O(n³). Storage scales quadratically as O(n²).",
      "properties": {
        "Goal": "Assess efficiency and scalability of matrix algorithms",
        "Applications": [
          "Performance prediction",
          "Algorithm selection",
          "Hardware design"
        ],
        "Methods": [
          "Big-O notation",
          "Arithmetic circuit complexity"
        ],
        "Examples": [
          "O(n³) time",
          "O(n²) space"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01_introduction.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "LU Factorization": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "LU factorization decomposes a matrix A into a lower triangular matrix L and an upper triangular matrix U such that A = LU, or with pivoting PA = LU, enabling efficient solution of linear systems.",
      "description": "It is a fundamental algorithm with O(n^3) complexity, used for solving Ax = b by forward and backward substitution, and approximated for large matrices using subspace products.",
      "properties": {
        "Goal": "Decompose matrices for efficient linear system solving.",
        "Applications": [
          "Numerical linear algebra",
          "PDE discretization",
          "Data processing"
        ],
        "Methods": [
          "Gaussian elimination",
          "Pivoting for stability",
          "Partial pivoting"
        ],
        "Examples": [
          "A = LU for square matrices",
          "PA = LU with permutation P"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02_product_of_matrix_subspaces_in_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Product of Matrix Subspaces": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "The product of matrix subspaces V1 V2 is defined as {V1 V2 : V1 ∈ V1, V2 ∈ V2}, providing a framework for approximating matrices with fewer parameters than full rank.",
      "description": "It allows factoring matrices into low-complexity forms like sum of outer products, useful for large n with small k, achieving 2nk parameters and potential for lower computational costs.",
      "properties": {
        "Goal": "Approximate matrices efficiently using subspace products.",
        "Applications": [
          "Large matrix factorization",
          "PDE discretization",
          "Data storage"
        ],
        "Methods": [
          "Subspace multiplication",
          "Rank-k approximation",
          "Norm minimization"
        ],
        "Examples": [
          "A ≈ sum_{j=1}^k u_j v_j^*",
          "I + V1 V2 inversion"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02_product_of_matrix_subspaces_in_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Gram-Schmidt Orthogonalization": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Gram-Schmidt orthogonalization transforms a set of linearly independent vectors into an orthonormal set using projections and normalization.",
      "description": "It is used in QR factorization and subspace computations, with classical and modified variants for numerical stability.",
      "properties": {
        "Goal": "Produce orthonormal bases from vector sets.",
        "Applications": [
          "QR decomposition",
          "Subspace orthogonalization",
          "Least squares"
        ],
        "Methods": [
          "Classical Gram-Schmidt",
          "Modified Gram-Schmidt",
          "Householder reflections"
        ],
        "Examples": [
          "Orthogonalizing columns of A"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02_product_of_matrix_subspaces_in_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Low-Rank Approximation": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Low-rank approximation finds a matrix Fk of rank k that minimizes ||A - Fk|| for a given norm, often using SVD or subspace products for efficiency.",
      "description": "It reduces storage and computation for large matrices, with subspace products offering 2nk parameters for rank-k approximations.",
      "properties": {
        "Goal": "Approximate high-dimensional matrices with lower rank.",
        "Applications": [
          "Data compression",
          "Noise reduction",
          "Machine learning"
        ],
        "Methods": [
          "SVD truncation",
          "Subspace product",
          "Randomized algorithms"
        ],
        "Examples": [
          "Fk = sum u_j v_j^*",
          "min_{rank(F)=k} ||A - F||"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02_product_of_matrix_subspaces_in_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Hermitian Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A Hermitian matrix M satisfies M^* = M, where * denotes conjugate transpose, with real eigenvalues and orthogonal eigenvectors.",
      "description": "It requires n^2 real parameters for storage, or fewer in subspace approximations, used in quantum mechanics and signal processing.",
      "properties": {
        "Goal": "Model self-adjoint operators in complex spaces.",
        "Applications": [
          "Quantum computing",
          "Covariance matrices",
          "Spectral analysis"
        ],
        "Methods": [
          "Eigen decomposition",
          "Cholesky factorization (positive definite)"
        ],
        "Examples": [
          "M with real diagonal and conjugate symmetric off-diagonals"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02_product_of_matrix_subspaces_in_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Conjugate Gradient Method": {
      "type": "Algorithm",
      "domain": "Numerical Linear Algebra",
      "definition": "An iterative Krylov method for solving symmetric positive definite (SPD) linear systems Ax = b by minimizing the A-norm of the error over expanding Krylov subspaces.",
      "description": "CG constructs a sequence of search directions that are A-conjugate, guaranteeing error minimization properties and finite termination in exact arithmetic. It is widely used in PDE solvers, scientific computing, and large sparse systems where direct methods are too expensive.",
      "properties": {
        "Goal": "Minimise quadratic form (x, A x)/2 − (b, x).",
        "Applications": [
          "SPD linear systems",
          "Optimisation (Newton-CG)"
        ],
        "Methods": [
          "Conjugate directions",
          "Residual orthogonalisation"
        ],
        "Examples": [
          "xⱼ₊₁ = xⱼ + αⱼ pⱼ with pⱼ₊₁ = rⱼ₊₁ + βⱼ pⱼ"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_krylov_methods.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "Conjugate Gradient",
          "CG"
        ]
      }
    },
    "Preconditioned Conjugate Gradient": {
      "type": "Algorithm",
      "domain": "Numerical Linear Algebra",
      "definition": "A variant of the Conjugate Gradient method that applies a preconditioner M to cluster eigenvalues of A and accelerate convergence.",
      "description": "PCG solves M^{-1}Ax = M^{-1}b using CG on the transformed system. Proper preconditioning reduces iteration count and improves numerical stability.",
      "properties": {
        "Goal": "Improve convergence of CG via spectral conditioning.",
        "Applications": [
          "PDE solvers",
          "Large-scale sparse systems",
          "Domain decomposition"
        ],
        "Methods": [
          "Left/right preconditioning",
          "Incomplete factorizations",
          "Spectral clustering"
        ],
        "Examples": [
          "IC(0) preconditioning",
          "AMG-preconditioned CG"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_krylov_methods.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "PCG"
        ]
      }
    },
    "Krylov Subspace": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "A subspace K_k(A, r0) = span{r0, Ar0, A²r0, …, A^{k−1}r0} generated by repeated multiplication of the initial residual by A.",
      "description": "Krylov subspaces serve as the foundation for modern iterative methods for linear systems and eigenvalue problems. They enable low-memory, matrix-free solvers for very large sparse matrices.",
      "properties": {
        "Goal": "Generate subspaces for iterative numerical methods.",
        "Applications": [
          "Eigenvalue computation",
          "Linear solvers",
          "Matrix exponentiation"
        ],
        "Methods": [
          "Arnoldi iteration",
          "Lanczos algorithm",
          "GMRES"
        ],
        "Examples": [
          "K_1(A; I) = span{I}",
          "K_2(A; I) = span{I, A}"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_krylov_methods.pdf",
        "created_at": "2025-11-18",
        "version": "1.0"
      }
    },
    "Krylov Subspace Methods": {
      "type": "Algorithm Family",
      "domain": "Numerical Linear Algebra",
      "definition": "A class of iterative algorithms that approximate solutions to Ax = b or eigenvalue problems by projecting them onto progressively larger Krylov subspaces.",
      "description": "Includes CG, GMRES, MINRES, BiCG, QMR, and others. These methods rely on polynomial approximations to A and require only matrix-vector products, making them ideal for large sparse systems.",
      "properties": {
        "Goal": "Solve large-scale problems using matrix-free operations.",
        "Applications": [
          "Sparse linear systems",
          "Eigenvalue problems",
          "PDE discretizations"
        ],
        "Methods": [
          "Arnoldi iteration",
          "Lanczos iteration",
          "Restarting",
          "Preconditioning"
        ],
        "Examples": [
          "GMRES",
          "CG",
          "MINRES",
          "BiCGSTAB"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_krylov_methods.pdf",
        "created_at": "2025-11-18",
        "version": "1.0"
      }
    },
    "Matrix Factorization": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "The process of decomposing a matrix into a product of structured factors such as triangular, orthogonal, or diagonal matrices.",
      "description": "Matrix factorizations enable efficient solving of linear systems, eigenvalue problems, and optimization tasks. Examples include LU, QR, Cholesky, Schur, and SVD.",
      "properties": {
        "Goal": "Reduce computational complexity by decomposing matrices into simpler components.",
        "Applications": [
          "Solving Ax = b",
          "Eigenvalue algorithms",
          "Least-squares problems",
          "Preconditioning"
        ],
        "Methods": [
          "Pivoting",
          "Orthogonal transformations",
          "Triangularization"
        ],
        "Examples": [
          "LU",
          "QR",
          "SVD",
          "Cholesky",
          "Schur"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_factorizations.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "Matrix Factoring",
          "Direct Factorization"
        ]
      }
    },
    "Incomplete LU Factorization": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "An approximate LU factorization where fill-in is restricted to preserve sparsity, producing L̂ and Ũ such that A ≈ L̂Ũ.",
      "description": "ILU is widely used as a preconditioner in iterative methods, particularly PCG and GMRES. Variants include ILU(0), ILUT, and ILU(k).",
      "properties": {
        "Goal": "Approximate LU factorization while preserving sparsity for use as preconditioner.",
        "Applications": [
          "Finite element systems",
          "CFD problems",
          "Circuit simulation"
        ],
        "Methods": [
          "Modified Gaussian elimination with drop tolerance",
          "Dual-threshold ILUT",
          "Level-of-fill ILU(k)"
        ],
        "Examples": [
          "ILU(0): drop all fill-in outside original nonzero pattern"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_factorizations.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "Incomplete LU"
        ]
      }
    },
    "Projection Operator": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A linear operator P satisfying P² = P, mapping a vector space onto a subspace while acting as identity on that subspace.",
      "description": "Projections decompose vector spaces into direct sums of a target subspace and its complement. They play central roles in numerical methods, approximations, and matrix factorizations.",
      "properties": {
        "Goal": "Project vectors onto subspaces.",
        "Applications": [
          "Least squares",
          "Subspace methods",
          "Decomposition"
        ],
        "Methods": [
          "Orthogonal projection",
          "Oblique projection"
        ],
        "Examples": [
          "P with P^2 = P",
          "I - P as complement projection"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_projection.pdf",
        "created_at": "2025-11-18",
        "version": "1.0"
      }
    },
    "Matrix Subspace": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A subset V ⊆ ℂ^{n×n} of matrices closed under addition and scalar multiplication, forming a vector space over ℂ.",
      "description": "Matrix subspaces are used to model families of matrices sharing structural properties (e.g., invertibility, symmetry, triangularity). They enable low-rank approximations, invariant subspaces in eigenvalue problems, and structured matrix factorizations such as LU or SVD within constrained sets.",
      "properties": {
        "Goal": "Group matrices with common algebraic or analytic traits to simplify computations and preserve structure in factorizations.",
        "Applications": [
          "Structured matrix factorization",
          "Low-rank approximation",
          "Krylov methods",
          "Invariant subspace computation"
        ],
        "Methods": [
          "Span construction",
          "Closure under operations",
          "Equivalence via similarity"
        ],
        "Examples": [
          "span{I, A}",
          "span{A, B}",
          "upper triangular matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Nonsingular Matrix Subspace": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A matrix subspace V ⊆ ℂ^{n×n} that contains at least one invertible matrix (det V ≠ 0 for some V ∈ V).",
      "description": "Nonsingular subspaces guarantee the existence of invertible elements, enabling the definition of the set of inverses Inv(V) = {V⁻¹ : V ∈ V, det V ≠ 0}. Such subspaces are crucial for ensuring well-defined inverse-based factorizations (e.g., LU within V).",
      "properties": {
        "Goal": "Ensure invertibility within a structured family of matrices to support factorization algorithms.",
        "Applications": [
          "LU factorization",
          "Generalized inverse",
          "Structured preconditioning"
        ],
        "Methods": [
          "Perturbation arguments",
          "Open-dense property in finite dimensions"
        ],
        "Examples": [
          "V = span{I, A} for nonsingular A",
          "Upper triangular matrices with nonzero diagonals"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Inv(V)": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The set of inverses of all invertible matrices in a matrix subspace V: Inv(V) = {W : W = V⁻¹, V ∈ V, det V ≠ 0}.",
      "description": "For nonsingular matrix subspaces, Inv(V) is itself a matrix subspace under mild conditions. This enables dual-space factorizations and ensures closure under inversion within structured sets, vital for algorithmic stability in LU-type methods.",
      "properties": {
        "Goal": "Construct a subspace of inverses to maintain structure across factorization steps.",
        "Applications": [
          "LU within subspace",
          "Preconditioner design",
          "Group-inverse problems"
        ],
        "Methods": [
          "Similarity transformation",
          "Closure proof via nonsingularity"
        ],
        "Examples": [
          "Inv(upper triangular) = lower triangular",
          "Inv(Hermitian) = Hermitian"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "LU Factorization within Subspace": {
      "type": "Method Gaussian elimination",
      "domain": "Numerical Linear Algebra",
      "definition": "Decomposition of a matrix A ∈ V into A = LU where L is lower triangular and U is upper triangular, both belonging to predefined matrix subspaces derived from V.",
      "description": "By identifying nonsingular matrix subspaces V and W = Inv(V), LU factorization can be confined within V × W, ensuring all intermediate matrices remain structured. This supports specialized algorithms for banded, symmetric positive-definite, or approximate low-rank problems.",
      "properties": {
        "Goal": "Perform Gaussian elimination while preserving membership in designated matrix subspaces.",
        "Applications": [
          "Banded LU",
          "SPD factorization",
          "Low-rank updates",
          "Krylov-based solvers"
        ],
        "Methods": [
          "Schur complement",
          "Pivot-free elimination",
          "Subspace projection"
        ],
        "Examples": [
          "A ∈ upper triangular → L = I, U = A",
          "A ∈ V, L ∈ V, U ∈ Inv(V)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Matrix Polynomials": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "Functions p(z) = ∑_{k=0}^m c_k z^k evaluated at matrices: p(A) = ∑_{k=0}^m c_k A^k for A ∈ ℂ^{n×n}.",
      "description": "Matrix polynomials map matrix subspaces to themselves if V is closed under powers of its members. They define minimal and characteristic polynomials, enable Cayley-Hamilton applications, and support function-based iterative methods.",
      "properties": {
        "Goal": "Extend scalar polynomial theory to matrices for spectral analysis and function approximation.",
        "Applications": [
          "Cayley-Hamilton theorem",
          "Matrix exponential",
          " preconditioning",
          "Spectral projectors"
        ],
        "Methods": [
          "Horner scheme",
          "Paterson-Stockmeyer",
          "Jordan form evaluation"
        ],
        "Examples": [
          "p(z) = z²−2z+1 → p(A) = A²−2A+I = 0",
          "q(z) = det(A−zI) → q(A)=0"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "QZ Algorithm": {
      "type": "Algorithm",
      "domain": "Generalized Eigenvalue Problems",
      "definition": "An algorithm that reduces a matrix pencil (A, B) to generalized Schur form using unitary transformations.",
      "description": "The QZ algorithm generalizes the QR algorithm to solve A x = λ B x problems. It produces upper triangular matrices S and T such that Q* A Z = S and Q* B Z = T. It is the standard dense solver for generalized eigenvalues.",
      "properties": {
        "Goal": "Compute generalized eigenvalues reliably for matrix pencils.",
        "Applications": [
          "DAE systems",
          "Control theory",
          "Stability analysis"
        ],
        "Methods": [
          "Generalized Hessenberg reduction",
          "Unitary similarity"
        ],
        "Examples": [
          "MATLAB's eig(A, B)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_qz_algorithm.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Reflection": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A linear transformation that flips vectors across a hyperplane while preserving norms.",
      "description": "Reflections form the basis for orthogonal transformations, including Householder reflections. They eliminate components in vectors or matrices while maintaining stability.",
      "properties": {
        "Goal": "Construct norm-preserving transformations used in matrix factorizations.",
        "Applications": [
          "Householder QR",
          "Orthogonalization",
          "Matrix reduction"
        ],
        "Methods": [
          "Hyperplane reflections",
          "Orthogonal matrices"
        ],
        "Examples": [
          "Householder transformation H = I - 2vvᵀ"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_qr_factorization.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Row Reduction": {
      "type": "Algorithm",
      "domain": "Linear Algebra",
      "definition": "A sequence of elementary row operations used to simplify matrices to row echelon or reduced row echelon form.",
      "description": "Row reduction is fundamental for solving linear systems, computing ranks, bases, and understanding matrix structure. It underpins Gaussian elimination and many algebraic manipulations.",
      "properties": {
        "Goal": "Transform matrices into simplified forms revealing rank and solution structure.",
        "Applications": [
          "Solving linear systems",
          "Rank computation",
          "Null space"
        ],
        "Methods": [
          "Elementary row operations",
          "Pivoting"
        ],
        "Examples": [
          "RREF calculation for Ax = b"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_matrix_simplification.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Schur Complement": {
      "type": "Concept",
      "domain": "Matrix Theory",
      "definition": "For a block matrix, the Schur complement of A in M = [[A, B], [C, D]] is S = D - C A⁻¹ B.",
      "description": "The Schur complement appears in block Gaussian elimination, matrix inversion, optimization, and statistics. It captures conditional behavior and plays a crucial role in SPD testing.",
      "properties": {
        "Goal": "Reduce block-matrix operations to smaller components.",
        "Applications": [
          "Optimization",
          "Block LU",
          "Covariance matrices",
          "Linear systems"
        ],
        "Methods": [
          "Block elimination",
          "Matrix inversion"
        ],
        "Examples": [
          "Used in Kalman filter equations"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_schur_complement.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Search Directions (in CG)": {
      "type": "Concept",
      "domain": "Iterative Methods",
      "definition": "The sequence of A-conjugate directions generated by the Conjugate Gradient method.",
      "description": "Search directions determine the efficiency and convergence of CG. Each direction is constructed to be A-orthogonal to all previous ones, ensuring optimality in SPD problems.",
      "properties": {
        "Goal": "Define the sequence of directions used to minimize the error over Krylov subspaces.",
        "Applications": [
          "CG iterations",
          "Krylov subspace minimization"
        ],
        "Methods": [
          "A-orthogonalization",
          "Residual-based direction updates"
        ],
        "Examples": [
          "pₖ = rₖ + βₖ pₖ₋₁"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_cg_methods.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Similarity Transformation": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A transformation of the form A → S⁻¹ A S with S invertible, preserving eigenvalues.",
      "description": "Similarity transformations classify matrices into equivalence classes and preserve spectral properties. They form the basis for diagonalization, Jordan forms, and spectral algorithms.",
      "properties": {
        "Goal": "Transform matrices while preserving eigenvalues and spectral characteristics.",
        "Applications": [
          "Diagonalization",
          "Jordan canonical form",
          "Invariant subspaces"
        ],
        "Methods": [
          "Change of basis",
          "Matrix equivalence"
        ],
        "Examples": [
          "S⁻¹ A S is diagonalizable"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_similarity.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Spectral Gap": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "The difference between two adjacent eigenvalues, often used to characterize convergence rates.",
      "description": "A larger spectral gap leads to faster convergence of iterative methods such as power iteration and CG. Spectral gap also governs mixing times in Markov chains.",
      "properties": {
        "Goal": "Measure separation between key eigenvalues.",
        "Applications": [
          "Power iteration",
          "Graph Laplacians",
          "Markov chains"
        ],
        "Methods": [
          "Eigenvalue analysis"
        ],
        "Examples": [
          "Gap = λ₂ - λ₁ for stochastic matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_spectral_gap.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Spectral Mapping Theorem": {
      "type": "Concept",
      "domain": "Matrix Functions",
      "definition": "The theorem stating that for analytic functions f, the spectrum satisfies Λ(f(A)) = f(Λ(A)).",
      "description": "Spectral mapping allows computation of matrix functions by applying the scalar function to eigenvalues. It is fundamental to understanding behaviors of matrix exponentials and polynomials.",
      "properties": {
        "Goal": "Relate eigenvalues of functions of matrices to functions of eigenvalues.",
        "Applications": [
          "Matrix exponential",
          "Matrix square root",
          "Krylov methods"
        ],
        "Methods": [
          "Analytic functions",
          "Contour integrals"
        ],
        "Examples": [
          "Λ(e^A) = e^{Λ(A)}"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_matrix_functions.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Spectral Radius": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "The quantity ρ(A) = max |λᵢ|, the maximum magnitude of eigenvalues of A.",
      "description": "Spectral radius determines convergence of matrix iterations, stability, and power method behavior. It is a core concept in numerical linear algebra and operator theory.",
      "properties": {
        "Goal": "Measure the largest eigenvalue magnitude.",
        "Applications": [
          "Stability analysis",
          "Power iteration",
          "Matrix norms"
        ],
        "Methods": [
          "Eigenvalue computation"
        ],
        "Examples": [
          "ρ(A) < 1 ensures convergence of Neumann series"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_spectral_radius.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Strassen Algorithm": {
      "type": "Algorithm",
      "domain": "Matrix Computations",
      "definition": "A subcubic algorithm for matrix multiplication with complexity O(n^{log₂7}) ≈ O(n^{2.807}).",
      "description": "Strassen’s method reduces the number of multiplications required for matrix multiplication. It forms the basis for fast algebraic algorithms and motivates subcubic research.",
      "properties": {
        "Goal": "Multiply matrices faster than classical O(n³) time.",
        "Applications": [
          "Large dense matrices",
          "Parallel computing"
        ],
        "Methods": [
          "Divide-and-conquer",
          "Bilinear algorithms"
        ],
        "Examples": [
          "7 multiplications instead of 8 for 2x2 blocks"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_fast_multiplication.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Subcubic Algorithms": {
      "type": "Concept",
      "domain": "Matrix Computations",
      "definition": "Algorithms for matrix multiplication with asymptotic complexity below O(n³).",
      "description": "Subcubic algorithms include Strassen, Coppersmith–Winograd, and subsequent improvements. They are foundational in algebraic complexity theory.",
      "properties": {
        "Goal": "Push theoretical limits of fast matrix multiplication.",
        "Applications": [
          "Computational complexity",
          "Large-scale algebraic computations"
        ],
        "Methods": [
          "Tensor decomposition",
          "Bilinear complexity"
        ],
        "Examples": [
          "Coppersmith–Winograd algorithm"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_fast_matrix_algorithms.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Subspace Decomposition": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A decomposition of a vector or matrix space into multiple complementary subspaces.",
      "description": "Subspace decompositions underpin direct-sum representations, block structure in matrices, and multilevel numerical algorithms. They enable modularization of linear algebraic problems.",
      "properties": {
        "Goal": "Split spaces into lower-dimensional components with structured relationships.",
        "Applications": [
          "Multigrid",
          "Block matrices",
          "Direct sum"
        ],
        "Methods": [
          "Orthogonal complement",
          "Projection operators"
        ],
        "Examples": [
          "V = U1 ⊕ U2 ⊕ … ⊕ Uk"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_subspaces.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Subspace Iteration": {
      "type": "Algorithm",
      "domain": "Eigenvalue Algorithms",
      "definition": "An iterative method that simultaneously computes multiple dominant eigenvectors by repeatedly applying A to a subspace.",
      "description": "Subspace iteration generalizes the power method to multiple vectors. It forms the basis of modern eigensolvers such as LOBPCG and block Krylov methods.",
      "properties": {
        "Goal": "Approximate several eigenpairs by iterating on subspaces.",
        "Applications": [
          "Large-scale eigenvalue problems",
          "PDE solvers",
          "Block Krylov methods"
        ],
        "Methods": [
          "Orthogonalization",
          "Rayleigh–Ritz projection"
        ],
        "Examples": [
          "Block power iteration"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_eigensolvers.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Sparsity Structure": {
      "type": "Concept",
      "domain": "Matrix Computations",
      "definition": "The sparsity structure of a matrix or matrix subspace specifies the pattern of entries that may be nonzero, independent of the actual numerical values.",
      "description": "Sparsity structures capture the combinatorial pattern of allowed nonzeros in matrices, enabling algorithms that exploit memory efficiency and reduce computational cost. They are essential in large-scale problems such as PDE discretizations, graph Laplacians, and structured matrix factorizations.",
      "properties": {
        "Goal": "Identify and exploit zero-patterns for efficient computation.",
        "Applications": [
          "Sparse LU factorization",
          "Finite difference discretizations",
          "Graph-based matrix algorithms"
        ],
        "Methods": [
          "Pattern analysis",
          "Fill-in minimization",
          "Graph reordering"
        ],
        "Examples": [
          "Tridiagonal pattern",
          "Band matrix structure"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_Section2.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Standard Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Computations",
      "definition": "A standard matrix subspace is a subspace that admits a basis consisting of standard matrices, each with exactly one entry equal to 1 and all other entries equal to 0.",
      "description": "Standard matrix subspaces represent matrix sets defined purely by sparsity patterns, without structural constraints such as symmetry. Orthogonal projection onto these subspaces is straightforward—simply zero out entries outside the allowed pattern.",
      "properties": {
        "Goal": "Model matrix sets defined solely by sparsity constraints.",
        "Applications": [
          "Sparse matrix approximations",
          "Projection methods",
          "Structured LU factorization"
        ],
        "Methods": [
          "Entrywise projection",
          "Basis construction from standard matrices"
        ],
        "Examples": [
          "Diagonal matrices",
          "Strictly lower triangular matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_Section2.3.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Orthogonal Projector (Matrix Subspace)": {
      "type": "Operator",
      "domain": "Numerical Analysis",
      "definition": "An orthogonal projector onto a matrix subspace V is a linear operator P such that P² = P and the range of P is orthogonal to the nullspace of P.",
      "description": "Orthogonal projectors provide optimal approximations in least-squares and matrix approximation problems. Projection onto matrix subspaces is fundamental in algorithmic factoring, dimension reduction, and solving underdetermined systems.",
      "properties": {
        "Goal": "Project matrices onto a subspace with minimal error under the Frobenius inner product.",
        "Applications": [
          "Approximate factoring",
          "Subspace splitting",
          "Sylvester-type equations"
        ],
        "Methods": [
          "Construction via orthonormal basis",
          "Symmetrization operators"
        ],
        "Examples": [
          "P(A) = (A + A*)/2 for Hermitian matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_ProjectionTheory.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Invertible Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Theory",
      "definition": "A matrix subspace V is invertible if the set of inverses of its nonsingular elements forms another matrix subspace V⁻¹.",
      "description": "Invertible matrix subspaces allow algorithmic factorization because their inverses preserve linear structure. Classical examples include triangular matrices and Hermitian matrices, which maintain structure under inversion.",
      "properties": {
        "Goal": "Support structured factorization where both A and its factors belong to linear matrix families.",
        "Applications": [
          "LU factorization",
          "Symmetric factorizations",
          "Matrix subspace factoring"
        ],
        "Methods": [
          "Closure under inversion",
          "Polynomial relations"
        ],
        "Examples": [
          "Lower triangular matrices",
          "Hermitian matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_MatrixSubspaces.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Singular Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Theory",
      "definition": "A matrix subspace is singular if every matrix within it is singular, i.e., no element has a nonzero determinant.",
      "description": "Singular matrix subspaces arise in low-rank approximations, special matrix pencils, and structured operator families. They often encode degenerate behavior and limit the applicability of classical factorizations.",
      "properties": {
        "Goal": "Characterize spaces where invertibility is impossible.",
        "Applications": [
          "Rank-k matrix sets",
          "SVD truncation analysis",
          "Generalized eigenvalue pencils"
        ],
        "Methods": [
          "Nullspace analysis",
          "Subspace closure"
        ],
        "Examples": [
          "Rank-k matrices for k < n"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_Section2.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Polynomially Closed Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Theory",
      "definition": "A matrix subspace V is polynomially closed if p(A) ∈ V for every A ∈ V and every polynomial p with scalar coefficients.",
      "description": "Polynomial closure ensures that structural properties of matrices are preserved under algebraic operations such as exponentiation, inversion, or functional calculus. This property is essential when applying iterative or polynomial-based algorithms within the subspace.",
      "properties": {
        "Goal": "Guarantee closure under matrix polynomial transformations.",
        "Applications": [
          "Iterative methods",
          "Matrix functions",
          "Structured inversion"
        ],
        "Methods": [
          "Polynomial evaluation",
          "Minimal polynomial arguments"
        ],
        "Examples": [
          "Hermitian matrices",
          "Complex symmetric matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_MinimalPolynomials.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Closure of V₁V₂": {
      "type": "Concept",
      "domain": "Matrix Subspaces",
      "definition": "The closure of the product set V₁V₂ consists of all matrices that can be approximated arbitrarily well by products of matrices from subspaces V₁ and V₂.",
      "description": "The closure of V₁V₂ determines whether approximate factorizations exist even when exact ones do not. This concept plays a central role in understanding numerical stability, perturbation behavior, and feasibility of approximate matrix factorizations.",
      "properties": {
        "Goal": "Characterize attainable approximate factorizations.",
        "Applications": [
          "Approximate LU",
          "Low-rank approximations",
          "Structured matrix decompositions"
        ],
        "Methods": [
          "Topology of matrix spaces",
          "Perturbation analysis"
        ],
        "Examples": [
          "Every matrix is arbitrarily close to one with an LU factorization"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_Factoring.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Preconditioning": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "Preconditioning is the transformation of a linear system Ax = b into an equivalent system M^{-1}Ax = M^{-1}b (left preconditioning) or AM^{-1}y = b with x = M^{-1}y (right), where M ≈ A is a nonsingular matrix that is inexpensive to invert or solve with, designed to improve the convergence rate of iterative methods.",
      "description": "The goal is to cluster the eigenvalues of the preconditioned matrix away from zero, making Krylov subspace methods like CG, GMRES, or MINRES converge in significantly fewer iterations. Effective preconditioners balance accuracy (M close to A) with computational cost (O(n) or O(n log n) per application).",
      "properties": {
        "Goal": "Accelerate iterative solver convergence.",
        "Applications": [
          "Large sparse systems",
          "PDE solvers"
        ],
        "Methods": [
          "Incomplete LU",
          "Jacobi",
          "Multigrid",
          "Domain decomposition"
        ],
        "Examples": [
          "Left preconditioning: solve M y = c then A x = M y"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Jacobi Preconditioner": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "The Jacobi preconditioner is M = diag(A) + ωI, where diag(A) contains the diagonal entries of A and ω ∈ ℂ is an optional damping parameter (often ω = 0). It is the simplest splitting M = D, N = A - D.",
      "description": "Extremely cheap to apply (O(n) scaling) and parallelizable. Effective when A is diagonally dominant. Corresponds to the classical Jacobi iterative method. Often used as a baseline or building block in more sophisticated preconditioners.",
      "properties": {
        "Goal": "Damp high-frequency error components using only diagonal information.",
        "Applications": [
          "Smooth initial guess for multigrid",
          "Baseline for preconditioner comparison",
          "Diagonally dominant systems"
        ],
        "Methods": [
          "Extract diagonal",
          "Optional damping ω",
          "Inverse is element-wise division"
        ],
        "Examples": [
          "M_i = diag(A) + ωI with ω = 0 for standard Jacobi"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Sparse Approximate Inverse": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "A sparse approximate inverse (SAI) preconditioner computes a sparse matrix M such that ||I - MA||_F or ||I - AW||_F is minimized over all matrices W with a prescribed sparsity pattern, effectively approximating A^{-1} directly.",
      "description": "Application cost is O(nnz(M)) matrix-vector products. Excellent parallel performance due to explicit form. Often constructed via Frobenius norm minimization on independent columns or using QR factorizations of local submatrices.",
      "properties": {
        "Goal": "Construct explicit sparse approximation to A^{-1} for fast matrix-vector products",
        "Applications": [
          "Highly parallel architectures (GPU, many-core)",
          "Unstructured grids",
          "High-performance computing"
        ],
        "Methods": [
          "Frobenius norm minimization per column",
          "SPAID (sparse approximate inverse by distance)",
          "FSAI (factorized sparse approximate inverse)"
        ],
        "Examples": [
          "min_W ||AW - I||_F with W constrained to sparsity pattern of A^k"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Left Preconditioning": {
      "type": "Technique",
      "domain": "Numerical Linear Algebra",
      "definition": "Left preconditioning transforms Ax = b into M^{-1}Ax = M^{-1}b, preserving the solution x but altering the residual norm to ||M^{-1}(b - Ax)||.",
      "description": "Common in practice because it directly improves the convergence diagnostics used by Krylov methods (residual-based stopping criteria). Does not change the right-hand side in a way that affects eigenvalue clustering as strongly as right preconditioning.",
      "properties": {
        "Goal": "Improve convergence while keeping solution unchanged and monitoring preconditioned residuals.",
        "Applications": [
          "Standard choice in most libraries (PETSc, Trilinos)",
          "GMRES with ILU"
        ],
        "Methods": [
          "Apply M^{-1} to system matrix and RHS"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Right Preconditioning": {
      "type": "Technique",
      "domain": "Numerical Linear Algebra",
      "definition": "Right preconditioning transforms Ax = b into AM^{-1}y = b with x = M^{-1}y, preserving the residual norm ||b - Ax|| but solving for an intermediate variable y.",
      "description": "Often preferred when the preconditioner naturally approximates the inverse action from the right. Common in domain decomposition and multigrid methods. Requires extra step to recover x.",
      "properties": {
        "Goal": "Cluster eigenvalues of AM^{-1} while monitoring true residuals.",
        "Applications": [
          "Additive Schwarz",
          "Algebraic multigrid (AMG)",
          "When M approximates A from the right"
        ],
        "Methods": [
          "Solve AM^{-1}y = b, then x = M^{-1}y"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Splitting Methods": {
      "type": "Framework",
      "domain": "Iterative Methods",
      "definition": "A matrix splitting decomposes A = M - N where M is nonsingular and easy to invert. The iteration x^{k+1} = M^{-1}Nx^k + M^{-1}b converges if ρ(M^{-1}N) < 1.",
      "description": "Foundation of classical iterative methods (Jacobi, Gauss-Seidel, SOR) and modern preconditioning. The spectral radius of the iteration matrix B = M^{-1}N determines convergence rate.",
      "properties": {
        "Goal": "Construct fixed-point iterations via A = M - N with ρ(M^{-1}N) < 1.",
        "Applications": [
          "Stationary iterative methods",
          "Preconditioner design",
          "Convergence theory"
        ],
        "Methods": [
          "M = D (Jacobi)",
          "M = D+L (Gauss-Seidel)",
          "M = (D+ωL)/ω (SOR)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Sylvester Equation Solution": {
      "type": "Algorithm",
      "domain": "Matrix Equations",
      "definition": "The process of solving the matrix equation AX + XB = C for X, typically when spectra of A and B do not overlap.",
      "description": "Sylvester equation solvers rely on Schur decompositions and Bartels–Stewart algorithms. They play a central role in control theory, Lyapunov equations, and model reduction.",
      "properties": {
        "Goal": "Compute X efficiently when AX + XB = C.",
        "Applications": [
          "Control theory",
          "Lyapunov equations",
          "Model reduction"
        ],
        "Methods": [
          "Bartels–Stewart algorithm",
          "Schur decomposition"
        ],
        "Examples": [
          "Solving AX + XB = Q for stability analysis"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_matrix_equations.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Symmetric Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A real matrix satisfying Aᵀ = A.",
      "description": "Symmetric matrices enjoy real eigenvalues, orthogonal diagonalization, and strong numerical stability. They form the real counterpart of Hermitian matrices.",
      "properties": {
        "Goal": "Represent self-adjoint real operators.",
        "Applications": [
          "Optimization",
          "Eigenvalue problems",
          "PDE discretizations"
        ],
        "Methods": [
          "Orthogonal diagonalization"
        ],
        "Examples": [
          "Graph Laplacian matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_linear_algebra.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "Symmetric Matrix (real case)"
        ]
      }
    },
    "Symmetric Positive Definite Matrices": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "Real symmetric matrices A for which xᵀAx > 0 for all nonzero x.",
      "description": "SPD matrices arise in PDEs, covariance analysis, optimization, and CG methods. They guarantee unique Cholesky factorizations and fast iterative convergence.",
      "properties": {
        "Goal": "Characterize strictly positive curvature of quadratic forms.",
        "Applications": [
          "Conjugate Gradient",
          "Cholesky factorization",
          "Machine learning"
        ],
        "Methods": [
          "Eigenvalue analysis",
          "Quadratic forms"
        ],
        "Examples": [
          "Graph Laplacian + εI"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_spd.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Triangular Matrices": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "Matrices that are either upper or lower triangular.",
      "description": "Triangular matrices form the basis of LU factorization and back/forward substitution. They preserve structure in matrix decompositions and support fast algorithms.",
      "properties": {
        "Goal": "Define structured matrices enabling efficient solving.",
        "Applications": [
          "LU factorization",
          "QR reduction",
          "Matrix decomposition"
        ],
        "Methods": [
          "Back substitution",
          "Forward substitution"
        ],
        "Examples": [
          "Upper and lower triangular matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_matrix_structure.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Triangular Solves": {
      "type": "Algorithm",
      "domain": "Numerical Linear Algebra",
      "definition": "Algorithms that solve triangular linear systems via forward or backward substitution.",
      "description": "Triangular solves are low-cost O(n²) operations widely used inside LU, QR, Cholesky, and Krylov methods. They involve recursive elimination without pivoting.",
      "properties": {
        "Goal": "Solve triangular systems efficiently.",
        "Applications": [
          "LU solve",
          "QR solve",
          "Block triangular systems"
        ],
        "Methods": [
          "Forward substitution",
          "Backward substitution"
        ],
        "Examples": [
          "Solving Lx = b in O(n²) time"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_triangular_solves.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "Triangular Solves"
        ]
      }
    },
    "Tridiagonal Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A matrix whose nonzero entries lie on the main diagonal and the two adjacent diagonals.",
      "description": "Tridiagonal matrices arise in 1D PDE discretizations, eigenvalue algorithms, and Lanczos methods. They enable fast O(n) algorithms for solving linear systems.",
      "properties": {
        "Goal": "Represent sparse structured operators with minimal bandwidth.",
        "Applications": [
          "Lanczos algorithm",
          "Thomas algorithm",
          "Finite differences"
        ],
        "Methods": [
          "Specialized LU",
          "Orthogonal reduction"
        ],
        "Examples": [
          "1D Poisson matrix"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_tridiagonal.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "Tridiagonal Matrix (symmetric case)"
        ]
      }
    },
    "Unitary Similarity": {
      "type": "Concept",
      "domain": "Linear Algebra / Spectral Theory",
      "definition": "A similarity transformation of the form A → U*AU where U is unitary.",
      "description": "Unitary similarity preserves norms, eigenvalues, and stability, making it essential in Schur decomposition, unitarily diagonalizable matrices, and spectral algorithms.",
      "properties": {
        "Goal": "Transform matrices while preserving numerical stability and eigenvalues.",
        "Applications": [
          "Schur decomposition",
          "Matrix diagonalization"
        ],
        "Methods": [
          "Unitary transformations"
        ],
        "Examples": [
          "Hessenberg reduction via unitary similarity"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_unitary.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Unitary Transformation": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A transformation represented by a unitary matrix U such that U*U = I.",
      "description": "Unitary transformations preserve norms, orthogonality, and conditioning. They are essential for stable QR factorization, Hessenberg reduction, and SVD.",
      "properties": {
        "Goal": "Apply stable norm-preserving transformations.",
        "Applications": [
          "QR factorization",
          "Eigenvalue methods",
          "Matrix reductions"
        ],
        "Methods": [
          "Givens rotations",
          "Householder reflections"
        ],
        "Examples": [
          "Q in QR decomposition"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_orthogonal_transformations.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Upper Triangular Factor": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "The U matrix in LU decomposition, where all entries below the diagonal are zero.",
      "description": "The upper triangular factor U stores the result of Gaussian elimination. It is used in backward substitution and forms half of the LU factorization.",
      "properties": {
        "Goal": "Store eliminated system structure for solving Ax = b.",
        "Applications": [
          "LU factorization",
          "Backward substitution"
        ],
        "Methods": [
          "Gaussian elimination"
        ],
        "Examples": [
          "LU = L U with U triangular"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_lu_factorization.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Upper Triangular Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A matrix whose entries below the diagonal are all zero.",
      "description": "Upper triangular matrices naturally arise in QR and LU factorizations. They support fast backward substitution and reveal eigenvalues directly on the diagonal.",
      "properties": {
        "Goal": "Provide structure for efficient solves and spectral access.",
        "Applications": [
          "Schur decomposition",
          "LU factorization"
        ],
        "Methods": [
          "Back substitution"
        ],
        "Examples": [
          "R in QR factorization"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_triangular_matrices.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Vandermonde Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra / Approximation Theory",
      "definition": "A matrix whose rows or columns follow geometric progressions based on input nodes.",
      "description": "Vandermonde matrices appear in polynomial interpolation, Prony methods, and spectral approximations. They are often ill-conditioned, especially for large degrees.",
      "properties": {
        "Goal": "Represent polynomial basis evaluations compactly.",
        "Applications": [
          "Interpolation",
          "Polynomial fitting",
          "Signal processing"
        ],
        "Methods": [
          "Lagrange interpolation",
          "Basis evaluation"
        ],
        "Examples": [
          "V(i,j) = x_i^{j-1}"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_interpolation.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Vector Subspace": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A subset of a vector space closed under addition and scalar multiplication.",
      "description": "Vector subspaces are foundational structures of linear algebra. They define solution sets, null spaces, ranges, invariant subspaces, and decomposition structures.",
      "properties": {
        "Goal": "Define linear structure within a larger vector space.",
        "Applications": [
          "Null space",
          "Range",
          "Direct sum decompositions"
        ],
        "Methods": [
          "Linear span",
          "Projection operators"
        ],
        "Examples": [
          "Null(A)",
          "Range(A)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_vector_spaces.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Residual r = b - Ax": {
      "type": "Concept",
      "domain": "Iterative Methods",
      "definition": "The difference between the right-hand side and the current approximation’s predicted value in a linear system.",
      "description": "Residuals determine convergence, stopping criteria, and direction generation in iterative methods such as CG, GMRES, and Richardson iteration.",
      "properties": {
        "Goal": "Measure progress of iterative solvers.",
        "Applications": [
          "CG",
          "GMRES",
          "Jacobi",
          "Richardson iteration"
        ],
        "Methods": [
          "Residual norm minimization"
        ],
        "Examples": [
          "||r_k|| used as stopping criterion"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_iterative_methods.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "r = b - A x"
        ]
      }
    },
    "Matrix Square Root (Alternative Notation)": {
      "type": "Concept",
      "domain": "Matrix Functions",
      "definition": "A matrix X satisfying X² = A. This entry covers the symbol-level term sqrt(A).",
      "description": "Matrix square roots arise in diffusion processes, covariance models, and analytic matrix functions. Many algorithms compute √A using Newton iterations or spectral decompositions.",
      "properties": {
        "Goal": "Compute a matrix whose square equals A.",
        "Applications": [
          "Diffusion equations",
          "Covariance analysis"
        ],
        "Methods": [
          "Newton’s iteration",
          "Schur decomposition"
        ],
        "Examples": [
          "Principal square root from spectral decomposition"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_matrix_functions.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "sqrt(A)"
        ]
      }
    },
    "Cholesky Factorization": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "A decomposition of a Hermitian positive definite matrix A into A = R*R, where R is an upper triangular matrix with positive diagonal entries.",
      "description": "Cholesky factorization exploits symmetry and positive definiteness to reduce computational cost from O(n³) for general LU to approximately n³/3 flops while requiring only n²/2 storage. It is widely used in optimization, Monte Carlo simulations, and solving normal equations.",
      "properties": {
        "Goal": "Efficiently factorize Hermitian positive definite matrices with half the storage and one-third the operations of LU.",
        "Applications": [
          "Quadratic programming",
          "Kalman filtering",
          "Covariance decomposition",
          "Monte Carlo methods"
        ],
        "Methods": [
          "Block elimination",
          "Outer product form",
          "Inner product form"
        ],
        "Examples": [
          "A = R*R with R upper triangular and diag(R) > 0"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Positive Definite Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A Hermitian matrix A ∈ ℂ^{n×n} such that (Ax, x) > 0 for all nonzero x ∈ ℂ^n.",
      "description": "Positive definite matrices arise in energy minimization, covariance modeling, and elliptic PDEs. They guarantee unique Cholesky factors, stable inverses, and real positive eigenvalues. The property is preserved under congruence: M*A*M is positive definite if M is invertible.",
      "properties": {
        "Goal": "Model strictly convex quadratic forms and ensure numerical stability in factorizations.",
        "Applications": [
          "Optimization",
          "Statistics",
          "Physics simulations",
          "Control theory"
        ],
        "Methods": [
          "Sylvester's criterion",
          "Cholesky test",
          "Eigenvalue analysis"
        ],
        "Examples": [
          "Covariance matrices",
          "Hessians of convex functions",
          "A = R*R"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Sylvester Equation": {
      "type": "Concept",
      "domain": "Control Theory",
      "definition": "A matrix equation of the form AX − XB = C, where A, B, C are given matrices and X is unknown.",
      "description": "The Sylvester equation models linear system interconnections and appears in control design, model reduction, and eigenvalue assignment. When σ(A) ∩ σ(B) = ∅, it has a unique solution solvable in O(n³) via Schur triangulation or vectorization (kronecker form).",
      "properties": {
        "Goal": "Solve for coupling matrix X in interconnected linear systems or compute Lyapunov functions.",
        "Applications": [
          "Stability analysis",
          "Model order reduction",
          "Riccati equations",
          "Pole placement"
        ],
        "Methods": [
          "Schur method",
          "Hessenberg-Schur algorithm",
          "Bartels-Stewart",
          "Vectorization"
        ],
        "Examples": [
          "AX − XA* = −BB* (Lyapunov)",
          "AX − XB = C with diagonal A, B"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Discrete Fourier Transform (DFT)": {
      "type": "Method",
      "domain": "Signal Processing",
      "definition": "A linear transformation mapping a sequence x₀, ..., x_{n-1} to coefficients cⱼ = ∑ₖ xₖ ω^{jk}, where ω = e^{-2πi/n}, represented by the Vandermonde matrix Fₙ.",
      "description": "The DFT diagonalizes circulant matrices and enables fast convolution, filtering, and spectral analysis. The Fast Fourier Transform (FFT) computes it in O(n log n) using divide-and-conquer on power-of-two sizes.",
      "properties": {
        "Goal": "Decompose signals into frequency components and accelerate convolution/correlation",
        "Applications": [
          "Audio processing",
          "Image compression",
          "PDE solvers",
          "Polynomial multiplication"
        ],
        "Methods": [
          "Cooley-Tukey FFT",
          "Radix-2",
          "Split-radix",
          "Bluestein"
        ],
        "Examples": [
          "F₄ = [[1,1,1,1], [1,-1,1,-1], ...]",
          "FFT of length 2^l"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Fast Fourier Transform (FFT)": {
      "type": "Algorithm",
      "domain": "Numerical Algorithms",
      "definition": "An efficient algorithm for computing the DFT in O(n log n) operations by recursively splitting into even/odd indices when n is a power of two.",
      "description": "The Cooley-Tukey FFT reduces DFT complexity from O(n²) to O(n log n) using butterfly operations and twiddle factors. It is foundational in digital signal processing and enables real-time spectral analysis.",
      "properties": {
        "Goal": "Compute DFT with minimal arithmetic operations using recursive decomposition",
        "Applications": [
          "Spectral methods",
          "FFT-based convolution",
          "MRI reconstruction",
          "Audio synthesis"
        ],
        "Methods": [
          "Decimation-in-time",
          "Decimation-in-frequency",
          "Bit-reversal",
          "In-place computation"
        ],
        "Examples": [
          "Radix-2 butterfly: yⱼ = x_{2j} + ω^j x_{2j+1}"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Schur Triangulation": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "A similarity transformation A = Q T Q* where T is upper triangular and Q is unitary, revealing eigenvalues on the diagonal of T.",
      "description": "Schur form is the foundation for robust eigenvalue computation and solving Sylvester equations. The QR-based Francis algorithm computes it in O(n³) with high backward stability. Real Schur form handles complex conjugate pairs.",
      "properties": {
        "Goal": "Reduce matrix to triangular form under unitary similarity to expose eigenvalues and enable block algorithms.",
        "Applications": [
          "Eigenvalue problems",
          "Sylvester solvers",
          "Matrix functions",
          "Control theory"
        ],
        "Methods": [
          "Francis QR algorithm",
          "Hessenberg reduction",
          "Double shift"
        ],
        "Examples": [
          "A = Q T Q* with T upper triangular, diag(T) = eigenvalues"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Fill-in": {
      "type": "Concept",
      "domain": "Sparse Linear Algebra",
      "definition": "New nonzero entries created during matrix factorizations such as LU, Cholesky, or QR.",
      "description": "Fill-in affects storage requirements and computational cost. Minimizing fill-in is essential for sparse solvers and preconditioners. Ordering strategies such as minimum degree or nested dissection aim to reduce fill-in.",
      "properties": {
        "Goal": "Quantify and manage new nonzeros generated during factorization.",
        "Applications": [
          "Sparse LU factorization",
          "Cholesky factorization",
          "Preconditioner construction"
        ],
        "Methods": [
          "Graph-based ordering",
          "Approximate minimum degree",
          "Nested dissection"
        ],
        "Examples": [
          "Cholesky of 2D Laplacian grid matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_sparse.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Francis Shift": {
      "type": "Concept",
      "domain": "Eigenvalue Algorithms",
      "definition": "A shift strategy used in the QR algorithm to accelerate convergence to eigenvalues.",
      "description": "The implicit double-shift QR method introduced by Francis uses complex conjugate shifts to target eigenvalues more rapidly. This is a key technique in modern dense eigenvalue solvers.",
      "properties": {
        "Goal": "Accelerate QR eigenvalue algorithm convergence.",
        "Applications": [
          "QR algorithm",
          "Hessenberg QR iteration"
        ],
        "Methods": [
          "Implicit double shift",
          "Bulge-chasing"
        ],
        "Examples": [
          "Francis double-shift QR iteration"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_qr_algorithm.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Givens Rotation": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "A plane rotation used to introduce zeros in matrices, especially in QR factorization.",
      "description": "Givens rotations apply a rotation in a 2D coordinate plane to eliminate specific matrix elements. They are ideal for sparse matrices because they introduce minimal fill-in.",
      "properties": {
        "Goal": "Eliminate matrix entries while preserving orthogonality.",
        "Applications": [
          "QR factorization",
          "Least squares",
          "Hessenberg reduction"
        ],
        "Methods": [
          "Orthogonal transformations",
          "Sparse QR algorithms"
        ],
        "Examples": [
          "Givens QR for sparse least-squares problems"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_qr_factorization.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "GL(n, ℂ)": {
      "type": "Concept",
      "domain": "Linear Algebra / Group Theory",
      "definition": "The group of all invertible n×n matrices over the complex numbers.",
      "description": "GL(n,ℂ) is the fundamental group representing all linear automorphisms of ℂⁿ. Many matrix algorithms rely on structure and properties preserved under GL(n,ℂ) transformations.",
      "properties": {
        "Goal": "Describe the set of all invertible linear transformations.",
        "Applications": [
          "Matrix decomposition theory",
          "Similarity transformations",
          "Lie groups"
        ],
        "Methods": [
          "Determinant analysis",
          "Group operations"
        ],
        "Examples": [
          "A ∈ GL(n,ℂ) iff det(A) ≠ 0"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_linear_algebra.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "General Linear Group"
        ]
      }
    },
    "Hessenberg Form": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "A nearly triangular matrix where all entries below the first subdiagonal are zero.",
      "description": "Hessenberg form is the target of reduction prior to running the QR algorithm. Transforming a matrix into Hessenberg form preserves eigenvalues and reduces computational cost.",
      "properties": {
        "Goal": "Reduce general matrices to a structured form suitable for eigenvalue algorithms.",
        "Applications": [
          "QR algorithm",
          "Iterative eigenvalue methods"
        ],
        "Methods": [
          "Householder transformations"
        ],
        "Examples": [
          "Upper Hessenberg reduction of dense matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_hessenberg.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Hessenberg Reduction": {
      "type": "Algorithm",
      "domain": "Numerical Linear Algebra",
      "definition": "An algorithm that transforms a matrix into Hessenberg form using orthogonal transformations.",
      "description": "Reduction to Hessenberg form is a standard preprocessing step in dense eigenvalue computations. It reduces complexity in subsequent QR iterations while preserving eigenvalues.",
      "properties": {
        "Goal": "Efficiently produce a Hessenberg matrix from a general matrix.",
        "Applications": [
          "Eigenvalue computations",
          "QR algorithm initialization"
        ],
        "Methods": [
          "Householder reflections"
        ],
        "Examples": [
          "MATLAB's hess(A)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_eigenvalue_algorithms.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "High Parallelism": {
      "type": "Concept",
      "domain": "High-Performance Computing",
      "definition": "The property of an algorithm being able to exploit many processors simultaneously with minimal communication.",
      "description": "High parallelism is crucial for modern numerical algorithms on GPUs, clusters, and distributed systems. FFT, matrix multiplication, and multigrid methods often exhibit strong parallel characteristics.",
      "properties": {
        "Goal": "Increase performance by reducing sequential bottlenecks.",
        "Applications": [
          "FFT",
          "Multigrid",
          "Parallel linear solvers"
        ],
        "Methods": [
          "Task decomposition",
          "Domain decomposition"
        ],
        "Examples": [
          "FFT butterfly operations executed in parallel"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_parallel_computing.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Idempotency": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A property of an operator P satisfying P² = P.",
      "description": "Idempotent operators correspond to projection operators onto subspaces. They play a central role in matrix subspace theory, direct sum decompositions, and oblique projections.",
      "properties": {
        "Goal": "Characterize projection-like operators with stable repeated action.",
        "Applications": [
          "Projection operators",
          "Invariant subspaces"
        ],
        "Methods": [
          "Spectral decomposition",
          "Subspace projections"
        ],
        "Examples": [
          "Orthogonal projection matrix"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_projections.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "Idempotency P^2 = P"
        ]
      }
    },
    "Imaginary Unit j": {
      "type": "Concept",
      "domain": "Complex Analysis",
      "definition": "The imaginary unit j satisfying j² = −1, commonly used in electrical engineering notation.",
      "description": "In many numerical algorithms involving complex arithmetic, j corresponds to the imaginary axis. It is mathematically equivalent to the complex unit i in mathematics.",
      "properties": {
        "Goal": "Represent the imaginary axis in complex numbers.",
        "Applications": [
          "Fourier analysis",
          "Complex matrix computations"
        ],
        "Methods": [
          "Complex arithmetic"
        ],
        "Examples": [
          "e^{jθ} representation in FFT"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_complex_numbers.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "j"
        ]
      }
    },
    "Inverse Iteration": {
      "type": "Algorithm",
      "domain": "Eigenvalue Algorithms",
      "definition": "An iterative method for approximating eigenvectors by repeatedly solving shifted linear systems.",
      "description": "Inverse iteration computes eigenvectors associated with a selected eigenvalue by repeatedly solving (A - μI)x_{k+1} = x_k. When μ is close to an eigenvalue, convergence is rapid.",
      "properties": {
        "Goal": "Compute eigenvectors accurately for a given eigenvalue shift.",
        "Applications": [
          "Eigenvalue refinement",
          "Rayleigh Quotient Iteration"
        ],
        "Methods": [
          "Shifted linear solves",
          "Spectral shift"
        ],
        "Examples": [
          "Inverse iteration with Rayleigh quotient shift"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_eigenvalue_methods.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Iterative Methods": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Iterative methods are algorithms that generate a sequence of approximations x_j to the solution x of a linear system Ax = b, starting from an initial guess and refining it until convergence.",
      "description": "They are preferred for large sparse systems where direct methods like Gaussian elimination are O(n^3) and computationally expensive, with per-iteration costs often O(n^2) or less, such as O(n) for sparse or O(n log n) with structured matrices.",
      "properties": {
        "Goal": "Solve large linear systems efficiently without full factorization.",
        "Applications": [
          "PDE discretizations",
          "Optimization",
          "Eigenproblems"
        ],
        "Methods": [
          "Krylov subspace methods",
          "Conjugate gradient",
          "GMRES",
          "Preconditioned iterations"
        ],
        "Examples": [
          "x_{j+1} = x_j + correction",
          "Convergence when ||r_j|| small"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Linear System": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "A linear system is an equation of the form Ax = b where A is an n x n matrix, x is the unknown vector, and b is the right-hand side vector.",
      "description": "For large n (e.g., 10^4 to 10^8), iterative methods are used due to high O(n^3) cost of direct solvers, especially when A is sparse or structured.",
      "properties": {
        "Goal": "Find x such that Ax = b.",
        "Applications": [
          "Scientific simulations",
          "Machine learning",
          "Engineering"
        ],
        "Methods": [
          "Direct (LU, QR)",
          "Iterative (CG, GMRES)"
        ],
        "Examples": [
          "A in C^{n x n}, b in C^n"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Arnoldi Process": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "The Arnoldi process builds an orthonormal basis Qⱼ for the Krylov subspace Kⱼ(A; b) and a Hessenberg matrix Hⱼ such that A Qⱼ = Qⱼ₊₁ Ĥⱼ.",
      "description": "It uses Gram–Schmidt-like orthogonalisation to compute basis vectors qₖ recursively, enabling reduced-order projections for solving systems or eigenvalues.",
      "properties": {
        "Goal": "Orthogonalise Krylov basis for stable computations.",
        "Applications": [
          "GMRES",
          "Eigenvalue solvers",
          "Matrix functions"
        ],
        "Methods": [
          "Recursive computation: hₖ,ₖ₋₁ qₖ = A qₖ₋₁ − Σ hₗ,ₖ₋₁ qₗ"
        ],
        "Examples": [
          "Qⱼ₊₁ Ĥⱼ = A Qⱼ"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "GMRES": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Generalised Minimal Residual (GMRES) is an iterative method that finds xⱼ in x₀ + Kⱼ(A; r₀) minimising ‖b − A xⱼ‖₂ for nonsymmetric systems.",
      "description": "It uses Arnoldi to build the basis and solves a least-squares problem with the Hessenberg matrix at each step, restarting when j is large.",
      "properties": {
        "Goal": "Minimise residual norm over Krylov subspace.",
        "Applications": [
          "Nonsymmetric linear systems",
          "PDE solvers"
        ],
        "Methods": [
          "Arnoldi orthogonalisation",
          "Least squares on Ĥⱼ y − α e₁"
        ],
        "Examples": [
          "xⱼ = Qⱼ yⱼ where yⱼ minimises ‖Ĥⱼ y − α e₁‖"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Residual Norm": {
      "type": "Metric",
      "domain": "Numerical Analysis",
      "definition": "The residual norm ‖rⱼ‖ = ‖b − A xⱼ‖ measures how well the approximate solution xⱼ satisfies the system Ax = b.",
      "description": "In iterative methods, residuals decrease monotonically in GMRES, and are used to test convergence.",
      "properties": {
        "Goal": "Quantify approximation error in equation satisfaction.",
        "Applications": [
          "Convergence testing",
          "Stopping criteria"
        ],
        "Methods": [
          "Euclidean norm",
          "Relative residual"
        ],
        "Examples": [
          "‖rⱼ₊₁‖ ≤ ‖rⱼ‖ in GMRES"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "A-Norm Error": {
      "type": "Metric",
      "domain": "Numerical Analysis",
      "definition": "The A-norm error ‖x − xⱼ‖ₐ = ( (x − xⱼ), A (x − xⱼ) )^{1/2} measures the error in the energy norm for SPD A.",
      "description": "CG minimises this norm over the Krylov subspace, relating to the quadratic form minimised in the system.",
      "properties": {
        "Goal": "Quantify solution error in energy sense.",
        "Applications": [
          "CG convergence analysis",
          "Variational problems"
        ],
        "Methods": [
          "Defined via inner product (x, A y)"
        ],
        "Examples": [
          "Min ‖x − xⱼ‖ₐ in CG"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Polynomial Approximation": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "Polynomial approximation in iterative methods views xⱼ = pⱼ₋₁(A) b as a polynomial in A applied to b, minimising residuals or errors via min-max problems over polynomials.",
      "description": "Convergence bounds use Chebyshev or other polynomials to estimate rates based on eigenvalue distribution.",
      "properties": {
        "Goal": "Approximate A⁻¹ b via polynomials in A.",
        "Applications": [
          "Convergence analysis",
          "Accelerated methods"
        ],
        "Methods": [
          "Min-max over deg ≤ j-1",
          "Chebyshev acceleration"
        ],
        "Examples": [
          "min_{deg p ≤ j-1, p(0)=1} max_λ |p(λ)|"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Hessenberg Matrix": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "A Hessenberg matrix H is upper triangular except for the subdiagonal, arising in Arnoldi as the projection of A onto the Krylov basis.",
      "description": "It simplifies least-squares solves in GMRES and eigenvalue computations.",
      "properties": {
        "Goal": "Reduce matrix for efficient projections.",
        "Applications": [
          "GMRES minimisation",
          "QR algorithm"
        ],
        "Methods": [
          "From Arnoldi: Ĥⱼ with subdiagonal"
        ],
        "Examples": [
          "Hⱼ tridiagonal in Lanczos"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Jordan Canonical Form": {
      "type": "Concept",
      "domain": "Linear Algebra / Spectral Theory",
      "definition": "A block-diagonal matrix consisting of Jordan blocks that represent the structure of a linear operator up to similarity transformations.",
      "description": "The Jordan Canonical Form reveals eigenvalues, geometric multiplicities, and algebraic multiplicities of a matrix. It classifies matrices up to similarity and provides insight into non-diagonalizable operators.",
      "properties": {
        "Goal": "Classify matrices under similarity and reveal spectral structure.",
        "Applications": [
          "Differential equations",
          "Matrix functions",
          "Control theory"
        ],
        "Methods": [
          "Similarity transformations",
          "Generalized eigenvectors"
        ],
        "Examples": [
          "Jordan blocks for repeated eigenvalues"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_spectral_theory.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "Jordan Form"
        ]
      }
    },
    "Kronecker Product": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "An operation on two matrices that produces a block matrix, defined as A ⊗ B = [a_ij B].",
      "description": "Kronecker products encode tensor product structures and appear in matrix equations, vectorization identities, and discretizations of PDEs. They are a foundation of fast algorithms for large structured systems.",
      "properties": {
        "Goal": "Represent tensor products and structured matrix operations.",
        "Applications": [
          "Sylvester equations",
          "Quantum computing",
          "Tensor calculus"
        ],
        "Methods": [
          "Block matrix construction",
          "Vectorization identities"
        ],
        "Examples": [
          "vec(AXB) = (Bᵀ ⊗ A) vec(X)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_matrix_operations.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Lower Triangular Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A matrix whose entries above the main diagonal are all zero.",
      "description": "Lower triangular matrices appear naturally in LU factorization and recursive matrix algorithms. They support fast forward substitution for solving linear systems.",
      "properties": {
        "Goal": "Provide a structured matrix enabling efficient solves.",
        "Applications": [
          "LU factorization",
          "Forward substitution"
        ],
        "Methods": [
          "Matrix decomposition"
        ],
        "Examples": [
          "L in LU = L U"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_factorization.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Matrix Pencil": {
      "type": "Concept",
      "domain": "Generalized Eigenvalue Theory",
      "definition": "A parametric family of matrices of the form A - λB representing a generalized eigenvalue problem.",
      "description": "Matrix pencils are central to generalized eigenvalue problems, QZ algorithms, and control theory. They allow describing systems with singular B or differential-algebraic structure.",
      "properties": {
        "Goal": "Represent generalized eigenvalue problems.",
        "Applications": [
          "QZ algorithm",
          "Control synthesis",
          "DAEs"
        ],
        "Methods": [
          "Generalized Schur decomposition"
        ],
        "Examples": [
          "det(A - λB) = 0 gives generalized eigenvalues"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_generalized_eigenvalues.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Matrix Rank": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The number of linearly independent rows or columns of a matrix.",
      "description": "Matrix rank reveals fundamental properties of a matrix such as invertibility, nullity, and the dimension of its image. Rank plays a core role in solving linear systems, low-rank approximation, and SVD.",
      "properties": {
        "Goal": "Quantify the dimension of the range of a matrix.",
        "Applications": [
          "Linear systems",
          "Low-rank approximation",
          "SVD"
        ],
        "Methods": [
          "Row reduction",
          "Singular value decomposition"
        ],
        "Examples": [
          "Rank-deficient least squares problems"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_matrix_rank.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Minimal Polynomial": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "The monic polynomial of least degree such that p(A) = 0.",
      "description": "The minimal polynomial characterizes the algebraic structure of a matrix. Its degree determines the size of Jordan blocks and governs convergence of polynomial iterative methods.",
      "properties": {
        "Goal": "Capture the smallest polynomial annihilating a matrix.",
        "Applications": [
          "Jordan form",
          "Krylov methods",
          "Matrix functions"
        ],
        "Methods": [
          "Cayley-Hamilton theorem",
          "Companion matrices"
        ],
        "Examples": [
          "Diagonalizable matrices have minimal polynomial with simple roots"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_spectral_analysis.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Normal Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra / Spectral Theory",
      "definition": "A matrix that commutes with its conjugate transpose, satisfying AA* = A*A.",
      "description": "Normal matrices include Hermitian, unitary, and orthogonal matrices as special cases. They are diagonalizable by a unitary matrix and enjoy well-conditioned eigenvalue problems.",
      "properties": {
        "Goal": "Generalize diagonalizable and orthogonally diagonalizable matrices.",
        "Applications": [
          "Quantum mechanics",
          "Spectral analysis",
          "Matrix functions"
        ],
        "Methods": [
          "Unitary diagonalization"
        ],
        "Examples": [
          "Unitary matrices, Hermitian matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_spectral_theory.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Numerical Radius": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "The quantity w(A) = max_{‖x‖=1} |x*Ax|, representing the radius of the smallest disk containing the field of values.",
      "description": "The numerical radius provides a tighter bound than the spectral radius for non-normal matrices. It is related to operator norms and is used in stability analysis.",
      "properties": {
        "Goal": "Bound eigenvalues and assess stability.",
        "Applications": [
          "Operator theory",
          "Stability",
          "Non-normal matrices"
        ],
        "Methods": [
          "Field of values analysis"
        ],
        "Examples": [
          "w(A) ≤ ‖A‖ ≤ 2 w(A)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_field_of_values.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Pseudospectrum": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "The ε-pseudospectrum of A is the set of complex numbers z for which ‖(A - zI)⁻¹‖ > 1/ε or z is an eigenvalue of a nearby matrix.",
      "description": "Pseudospectra describe sensitivity of eigenvalues to perturbations and are crucial in understanding non-normal behavior, transient growth, and numerical stability.",
      "properties": {
        "Goal": "Assess robustness of eigenvalues under perturbations.",
        "Applications": [
          "Non-normal matrices",
          "Stability analysis",
          "Control theory"
        ],
        "Methods": [
          "Resolvent norm computation"
        ],
        "Examples": [
          "Highly non-normal matrices have large pseudospectra"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_pseudospectra.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Positive Semidefinite Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A Hermitian/Symmetric matrix A satisfying xᵀAx ≥ 0 for all x.",
      "description": "Positive semidefinite (PSD) matrices appear in optimization, covariance matrices, kernel methods, and PDE discretizations. They generalize positive definite matrices.",
      "properties": {
        "Goal": "Characterize matrices inducing non-negative quadratic forms.",
        "Applications": [
          "Optimization",
          "Machine learning",
          "Statistics"
        ],
        "Methods": [
          "Cholesky-like factorizations",
          "Eigenvalue analysis"
        ],
        "Examples": [
          "Covariance matrices in statistics"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_psd.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "Positive Semidefinite"
        ]
      }
    },
    "Quadratic Form": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A function of the form q(x) = xᵀAx for a symmetric matrix A.",
      "description": "Quadratic forms relate directly to eigenvalues, definiteness, and optimization landscapes. They appear in stability theory, energy minimization, and classification of matrices.",
      "properties": {
        "Goal": "Model energy-like quantities through symmetric matrices.",
        "Applications": [
          "Optimization",
          "Stability",
          "Statistics"
        ],
        "Methods": [
          "Spectral decomposition"
        ],
        "Examples": [
          "Second-order Taylor approximations"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_quadratic_forms.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Orthogonal Complement": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The orthogonal complement of a subspace V is the set of vectors perpendicular to all elements in V, denoted V^⊥.",
      "description": "For projections, if P is orthogonal, then R(P) ⊥ R(I - P), ensuring the decomposition is orthogonal.",
      "properties": {
        "Goal": "Decompose space into perpendicular subspaces.",
        "Applications": [
          "Gram-Schmidt",
          "Least squares",
          "Spectral methods"
        ],
        "Methods": [
          "Dot product zero condition",
          "Null space of transpose"
        ],
        "Examples": [
          "R(I - P) as complement of R(P)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Gershgorin Circle Theorem": {
      "type": "Theorem",
      "domain": "Matrix Analysis",
      "definition": "Every eigenvalue of A ∈ ℂ^{n×n} lies in at least one disk Dⱼ = {z : |z − aⱼⱼ| ≤ Rⱼ}, where Rⱼ = Σ_{l≠j} |aⱼₗ|}.",
      "description": "Provides eigenvalue localization without computation. Disks are centered at diagonal entries with radii equal to off-diagonal row sums. Useful for bounding spectral radius and detecting diagonal dominance.",
      "properties": {
        "Goal": "Locate eigenvalues in complex plane using only matrix entries.",
        "Applications": [
          "Convergence analysis",
          "Error bounding",
          "Preconditioning design"
        ],
        "Methods": [
          "Row-sum computation",
          "Union of disks",
          "Refinement via similarity"
        ],
        "Examples": [
          "Strictly diagonally dominant → eigenvalues in disjoint disks"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Power Iteration": {
      "type": "Method",
      "domain": "Eigenvalue Algorithms",
      "definition": "An iterative method that computes the dominant eigenvalue and eigenvector by repeatedly applying A to a vector and normalizing: q^{(k)} = A q^{(k-1)} / ||A q^{(k-1)}||, λ^{(k)} = (A q^{(k)}, q^{(k)}).",
      "description": "Converges linearly to the eigenvector corresponding to the largest-magnitude eigenvalue if |λ₁| > |λ₂| ≥ ... ≥ |λₙ|. Convergence rate is |λ₂/λ₁|. Inverse iteration targets smallest or shifted eigenvalues.",
      "properties": {
        "Goal": "Find dominant eigenpair with minimal storage and simple operations.",
        "Applications": [
          "PageRank",
          "Principal Component Analysis",
          "Vibration modes"
        ],
        "Methods": [
          "Rayleigh quotient",
          "Normalization",
          "Deflation",
          "Shift-and-invert"
        ],
        "Examples": [
          "q^{(k)} ≈ x₁ + O((λ₂/λ₁)^k)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Householder Reflection": {
      "type": "Transformation",
      "domain": "Numerical Linear Algebra",
      "definition": "A unitary matrix H = I − 2vv*/(v*v) that reflects a vector x across the hyperplane perpendicular to v, mapping x to σe₁.",
      "description": "Used in QR factorization and Hessenberg reduction to introduce zeros below the subdiagonal. Numerically stable and requires O(n) operations per reflection. Essential for implicit QR algorithm.",
      "properties": {
        "Goal": "Zero out selected entries via unitary similarity while preserving eigenvalues.",
        "Applications": [
          "QR decomposition",
          "Hessenberg form",
          "Tridiagonalization"
        ],
        "Methods": [
          "Sign choice for stability",
          "WY representation",
          "Blocked Householder"
        ],
        "Examples": [
          "H x = −sign(x₁) ||x|| e₁"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "QR Algorithm": {
      "type": "Algorithm",
      "domain": "Eigenvalue Computation",
      "definition": "An iterative method that computes the Schur form via repeated QR decompositions: A_{k+1} = R_k Q_k, with A_0 = A. With shifts, converges cubically to upper triangular form.",
      "description": "The de facto standard for dense eigenvalue problems. Implicit version uses Householder/Bulge chasing to reduce cost from O(n³) per iteration to O(n). Francis shift accelerates convergence.",
      "properties": {
        "Goal": "Compute all eigenvalues (and optionally eigenvectors) via unitary similarity to triangular form.",
        "Applications": [
          "MATLAB eig",
          "LAPACK",
          "Control theory",
          "PDE solvers"
        ],
        "Methods": [
          "Hessenberg reduction",
          "Francis double shift",
          "Deflation",
          "Balancing"
        ],
        "Examples": [
          "A_{k+1} = Q_k* A_k Q_k"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Generalized Eigenvalue Problem": {
      "type": "Problem",
      "domain": "Numerical Linear Algebra",
      "definition": "Find λ ∈ ℂ and nonzero x ∈ ℂⁿ such that Ax = λBx, with A, B ∈ ℂ^{n×n}.",
      "description": "Arises in structural dynamics, control, and Markov chains. Reduced to standard form if B invertible (M = B⁻¹A). QZ algorithm generalizes QR using unitary transformations to triangularize both matrices.",
      "properties": {
        "Goal": "Solve coupled systems or weighted eigenvalue problems.",
        "Applications": [
          "Vibration with constraints",
          "Markov chain stationary distribution",
          "Optimal control"
        ],
        "Methods": [
          "QZ algorithm",
          "Cholesky + standard EVP",
          "Shift-and-invert"
        ],
        "Examples": [
          "Ax = λBx with B positive definite"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Field of Values": {
      "type": "Concept",
      "domain": "Matrix Analysis",
      "definition": "The set F(A) = {x*Ax : x ∈ ℂⁿ, ||x||=1}, also known as the numerical range.",
      "description": "Convex, compact set containing all eigenvalues. Bounds spectral radius and condition number. For normal matrices, F(A) is the convex hull of eigenvalues.",
      "properties": {
        "Goal": "Characterize operator behavior beyond eigenvalues.",
        "Applications": [
          "Stability analysis",
          "Convergence of iterations",
          "Pseudospectrum approximation"
        ],
        "Methods": [
          "Rayleigh quotient",
          "Hausdorff-Toeplitz theorem",
          "Discretization"
        ],
        "Examples": [
          "F(A) = conv(σ(A)) if A normal"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "LU Factorization with Partial Pivoting": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "LU factorization with partial pivoting decomposes a matrix A ∈ ℂ^{n×n} into PA = LU, where P is a permutation matrix, L is unit lower triangular with |l_{ij}| ≤ 1 for i > j, and U is upper triangular. Partial pivoting selects the largest absolute entry in the current column as the pivot to minimize numerical instability.",
      "description": "This algorithm enhances the stability of Gaussian elimination by row permutations to avoid small pivots. It is the standard method for solving linear systems Ax = b in practice, balancing computational cost (O(n³)) with robustness against round-off errors in floating-point arithmetic.",
      "properties": {
        "Goal": "Stable triangular factorization of a matrix for solving linear systems and computing inverses.",
        "Applications": [
          "Solving Ax = b",
          "Matrix inversion",
          "Determinant computation",
          "Condition number estimation"
        ],
        "Methods": [
          "Gaussian elimination with row pivoting",
          "In-place storage using single array",
          "Compact WY representation for L"
        ],
        "Examples": [
          "4×4 matrix example with pivots 8, 17/4, -6/7, 2 showing growth control"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Partial Pivoting": {
      "type": "Strategy",
      "domain": "Numerical Linear Algebra",
      "definition": "A pivoting strategy in Gaussian elimination that, at step k, permutes rows so that the entry of largest magnitude in column k (from row k to n) becomes the pivot, ensuring |pivot| = max_{i≥k} |a_{ik}|.",
      "description": "Partial pivoting prevents division by small pivots, reducing amplification of round-off errors. It guarantees that all subdiagonal entries in L satisfy |l_{ij}| ≤ 1, bounding the growth factor ρ ≤ 2^{n-1} in theory, though typically much smaller in practice.",
      "properties": {
        "Goal": "Minimize numerical error propagation during elimination by choosing largest available pivot.",
        "Applications": [
          "LU factorization",
          "Linear system solving",
          "Matrix decomposition in finite precision"
        ],
        "Methods": [
          "Row interchange before elimination step",
          "Column scanning for max absolute value"
        ],
        "Examples": [
          "P1 swaps rows to bring largest entry to diagonal position"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Permutation Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A permutation matrix P is a square matrix with exactly one 1 in each row and column and 0s elsewhere. Multiplying PA permutes the rows of A; AP permutes columns.",
      "description": "In LU with partial pivoting, P represents the cumulative row interchanges. Since P^{-1} = P^T = P^*, it preserves norms: ||Px|| = ||x||. The final factorization is PA = LU.",
      "properties": {
        "Goal": "Represent row or column reordering in matrix factorizations.",
        "Applications": [
          "Pivoting in LU",
          "Reordering for sparsity",
          "Graph relabeling"
        ],
        "Methods": [
          "Identity with swapped rows",
          "Product of elementary permutation matrices"
        ],
        "Examples": [
          "P = [0 1; 1 0] swaps rows 1 and 2"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Growth Factor": {
      "type": "Metric",
      "domain": "Numerical Linear Algebra",
      "definition": "The growth factor rho in LU factorization with partial pivoting is defined as rho = max_{i,j} |u_{ij}| / max_{i,j} |a_{ij}|, measuring the largest entry in U relative to the original matrix A.",
      "description": "Controls backward stability: computed factors satisfy L_hat U_hat = P A + delta A with ||delta A|| / ||A|| = O(rho epsilon_machine). Partial pivoting keeps rho moderate in practice, though worst-case rho = 2^{n-1} is possible.",
      "properties": {
        "Goal": "Quantify element growth during Gaussian elimination to assess numerical stability.",
        "Applications": [
          "Backward error analysis",
          "Condition estimation",
          "Pivoting strategy evaluation"
        ],
        "Methods": [
          "Ratio of max |u_{ij}| to max |a_{ij}|",
          "Monitored during factorization"
        ],
        "Examples": [
          "Wilkinson's matrix gives rho approx 2^{n-1}"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Condition Number": {
      "type": "Metric",
      "domain": "Numerical Linear Algebra",
      "definition": "The condition number κ(W) of a nonsingular matrix W is κ(W) = ||W|| ⋅ ||W^{-1}||, with respect to any consistent matrix norm. For the 1-norm or ∞-norm, κ₁(W) = σ₁/σₙ where σ are singular values.",
      "description": "Measures sensitivity of linear system solution to perturbations. Large κ implies ill-conditioned system: small changes in input cause large output changes. In LU context, related to pivot size and growth.",
      "properties": {
        "Goal": "Quantify sensitivity of Ax = b to perturbations in A or b.",
        "Applications": [
          "Error bounds in linear solvers",
          "Preconditioning design",
          "Numerical stability analysis"
        ],
        "Methods": [
          "SVD-based: κ₂ = σ_max / σ_min",
          "1-norm estimation via Hager's method"
        ],
        "Examples": [
          "κ(A) ≈ 10^k ⇒ lose k digits of accuracy"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Gaussian Elimination": {
      "type": "Method",
      "domain": "Linear Algebra",
      "definition": "Gaussian elimination transforms a linear system Ax = b into upper triangular form Ux = c via row operations: adding multiples of one row to another. With pivoting, it forms the basis of LU factorization.",
      "description": "Core algorithm for solving linear systems. Without pivoting, unstable for small pivots. With partial pivoting, becomes robust standard method. Can be expressed as sequence of rank-1 updates or multiplier storage in L.",
      "properties": {
        "Goal": "Reduce system to triangular form for back substitution.",
        "Applications": [
          "Linear system solving",
          "Matrix factorization",
          "Determinant via product of diagonals"
        ],
        "Methods": [
          "Forward elimination",
          "Back substitution",
          "Pivoting variants"
        ],
        "Examples": [
          "4×4 system reduced step-by-step with P, L, U shown"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "A - μ I": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A shifted matrix operator formed by subtracting a scalar multiple of the identity matrix from A.",
      "description": "The matrix A - μI is central to spectral shift techniques, inverse iteration, Rayleigh Quotient Iteration, and many eigenvalue algorithms. Shifting by μ changes the eigenvalues to λ_i - μ, allowing selective amplification of eigen-components.",
      "properties": {
        "Goal": "Shift the spectrum of A so selected eigenvalues become easier to isolate or invert.",
        "Applications": [
          "Inverse Iteration",
          "Rayleigh Quotient Iteration",
          "Spectral Shift",
          "Shift-Invert Methods"
        ],
        "Methods": [
          "Shift-and-invert transformation",
          "Spectral manipulation"
        ],
        "Examples": [
          "(A - λ_min I) is used to emphasize smallest eigenvalues"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_spectral_methods.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "Shifted Matrix",
          "A minus mu I"
        ]
      }
    },
    "Backward Stability": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "A property of an algorithm where the computed solution is the exact solution to a slightly perturbed input problem.",
      "description": "Backward stability ensures that numerical errors arise only from small perturbations in the input, making the algorithm robust. LU with partial pivoting, QR factorization, and SVD-based algorithms are typical examples.",
      "properties": {
        "Goal": "Guarantee high numerical reliability even under rounding error.",
        "Applications": [
          "Linear system solving",
          "Eigenvalue computations",
          "Least squares"
        ],
        "Methods": [
          "Perturbation analysis",
          "Stability theory"
        ],
        "Examples": [
          "LU with partial pivoting is backward stable for many matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_backward_stability.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Brauer Cassini Ovals": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "A set of regions in the complex plane providing eigenvalue inclusion bounds based on a partition of the matrix.",
      "description": "Brauer’s theorem partitions the matrix into blocks and constructs Cassini ovals that enclose all eigenvalues. They generalize Gershgorin’s disks and often produce tighter bounds.",
      "properties": {
        "Goal": "Provide eigenvalue inclusion regions sharper than Gershgorin circles.",
        "Applications": [
          "Eigenvalue estimation",
          "Spectral bounds",
          "Iterative method convergence analysis"
        ],
        "Methods": [
          "Block partitioning",
          "Cassini oval construction"
        ],
        "Examples": [
          "Used to bound eigenvalues of non-normal matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_spectral_inclusion.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Cauchy Integral Formula": {
      "type": "Concept",
      "domain": "Complex Analysis / Matrix Functions",
      "definition": "A contour integral formula that represents analytic functions inside a closed curve using boundary values.",
      "description": "In matrix computations, the Cauchy integral formula is used to define analytic matrix functions such as f(A). It allows representing functions of matrices using contour integrals around the spectrum.",
      "properties": {
        "Goal": "Define analytic functions of matrices using contour integration.",
        "Applications": [
          "Matrix exponential",
          "Matrix sign function",
          "Matrix square root"
        ],
        "Methods": [
          "Contour integration",
          "Residue calculus"
        ],
        "Examples": [
          "f(A) = (1/(2πi)) ∮ f(z)(zI - A)⁻¹ dz"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_matrix_functions.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Chebyshev Polynomials": {
      "type": "Concept",
      "domain": "Approximation Theory",
      "definition": "A family of orthogonal polynomials minimizing the maximum error over [-1,1].",
      "description": "Chebyshev polynomials play a key role in iterative methods, eigenvalue solvers, spectral methods, and approximation theory. They provide optimal polynomial approximants and reduce oscillations.",
      "properties": {
        "Goal": "Achieve near-minimax polynomial approximation.",
        "Applications": [
          "Polynomial approximation",
          "Iterative methods acceleration",
          "Spectral discretization"
        ],
        "Methods": [
          "Orthogonal polynomials",
          "Clenshaw recurrence"
        ],
        "Examples": [
          "Chebyshev semi-iterative method"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_approximation.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Circulant Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A structured matrix where each row is a cyclic shift of the previous row.",
      "description": "Circulant matrices are diagonalizable by the discrete Fourier transform (DFT), making them computationally efficient for convolution, filtering, and spectral analysis. Fast multiplication using FFT leads to O(n log n) operations.",
      "properties": {
        "Goal": "Exploit cyclic structure for fast computations.",
        "Applications": [
          "Signal processing",
          "Fast convolution",
          "Spectral graph theory"
        ],
        "Methods": [
          "DFT diagonalization",
          "FFT-based multiplication"
        ],
        "Examples": [
          "Toeplitz-circulant approximation",
          "Convolution as circulant matrix-vector product"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_fft.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Complete Pivoting": {
      "type": "Algorithm",
      "domain": "Numerical Linear Algebra",
      "definition": "A pivoting strategy for Gaussian elimination where the largest element in the remaining submatrix is chosen as the pivot.",
      "description": "Complete pivoting reduces numerical instability by selecting globally maximal pivots. Although more stable than partial pivoting, it is computationally more expensive and therefore rarely used in large-scale problems.",
      "properties": {
        "Goal": "Improve numerical stability of Gaussian elimination.",
        "Applications": [
          "Gaussian elimination",
          "LU factorization"
        ],
        "Methods": [
          "Global pivot search",
          "Row and column permutations"
        ],
        "Examples": [
          "Used when numerical precision is critical in small matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_gaussian_elimination.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Congruence Transformation": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A transformation of the form A ↦ PᵀAP where P is nonsingular, often used to analyze symmetric matrices.",
      "description": "Congruence transformations preserve definiteness properties of matrices. They are commonly used in quadratic forms, inertia theory, and Lyapunov stability analysis.",
      "properties": {
        "Goal": "Preserve quadratic form properties while transforming matrices.",
        "Applications": [
          "Quadratic forms",
          "Lyapunov equations",
          "Symmetric matrix analysis"
        ],
        "Methods": [
          "Matrix decomposition",
          "Equivalence transformations"
        ],
        "Examples": [
          "Sylvester’s law of inertia"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_matrix_theory.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Control Synthesis": {
      "type": "Concept",
      "domain": "Control Theory",
      "definition": "The process of designing controllers that achieve desired system performance using mathematical models.",
      "description": "Control synthesis includes optimal control, state feedback, LQR design, and H∞ control. Many linear algebraic problems such as Lyapunov and Riccati equations appear in control synthesis.",
      "properties": {
        "Goal": "Design controllers that stabilize and optimize dynamic systems.",
        "Applications": [
          "Robotics",
          "Aerospace",
          "Feedback systems"
        ],
        "Methods": [
          "State-space design",
          "Linear algebraic equations",
          "Optimization"
        ],
        "Examples": [
          "LQR controller synthesis",
          "Pole placement"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_control.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Direct Sum": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A decomposition V = U ⊕ W where every vector in V has a unique representation as u + w with u ∈ U and w ∈ W.",
      "description": "Direct sum decompositions are fundamental in structure theory, invariant subspaces, and block matrix representations. They provide a clean separation of components across orthogonal or complementary subspaces.",
      "properties": {
        "Goal": "Split a vector space into independent, non-overlapping components.",
        "Applications": [
          "Invariant subspaces",
          "Block diagonalization",
          "Decomposing matrix subspaces"
        ],
        "Methods": [
          "Projection operators",
          "Complementary subspaces"
        ],
        "Examples": [
          "V = span(e1) ⊕ span(e2,e3)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_subspaces.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": [
          "Direct Sum Decomposition"
        ]
      }
    },
    "Domain Decomposition": {
      "type": "Algorithm",
      "domain": "Numerical PDEs / Scientific Computing",
      "definition": "A method for solving PDEs by dividing the computational domain into smaller subdomains and solving local problems.",
      "description": "Domain decomposition improves parallelism, reduces communication, and allows localized problem-solving. It forms the basis for Schwarz methods, additive and multiplicative preconditioners.",
      "properties": {
        "Goal": "Solve large PDE systems efficiently using parallel local solves.",
        "Applications": [
          "Elliptic PDEs",
          "Finite element methods",
          "Parallel solvers"
        ],
        "Methods": [
          "Schwarz iteration",
          "Overlapping and non-overlapping decompositions"
        ],
        "Examples": [
          "Additive Schwarz method"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_domain_decomposition.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Elliptic PDE": {
      "type": "Concept",
      "domain": "Partial Differential Equations",
      "definition": "A class of PDEs characterized by positive-definite differential operators, often requiring solution of large sparse systems.",
      "description": "Elliptic PDEs frequently arise in steady-state physical systems. Their discretizations lead to SPD linear systems that are ideal for conjugate gradients and multigrid solvers.",
      "properties": {
        "Goal": "Model steady-state physical processes such as diffusion, elasticity, and electrostatics.",
        "Applications": [
          "Finite element analysis",
          "Heat conduction",
          "Electrostatics"
        ],
        "Methods": [
          "Discretization",
          "Iterative solvers"
        ],
        "Examples": [
          "Poisson equation: −Δu = f"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_pdes.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Eigenvalue Decomposition": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "A decomposition A = VΛV⁻¹ where Λ is diagonal and V contains eigenvectors of A.",
      "description": "Eigenvalue decomposition exists for diagonalizable matrices and plays a central role in spectral analysis, diagonalization, and understanding matrix dynamics.",
      "properties": {
        "Goal": "Represent a matrix in terms of its eigenvalues and eigenvectors.",
        "Applications": [
          "Diagonalization",
          "Matrix functions",
          "Stability analysis"
        ],
        "Methods": [
          "Similarity transformation"
        ],
        "Examples": [
          "A diagonalizable matrix with distinct eigenvalues"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_spectral_theory.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Exponential Integrators": {
      "type": "Algorithm",
      "domain": "Numerical Differential Equations",
      "definition": "A class of time-stepping methods that use the matrix exponential to solve stiff differential equations.",
      "description": "By explicitly integrating the linear part of the system, exponential integrators reduce stiffness and enable stable large time steps. They use matrix exponential actions such as exp(A)v.",
      "properties": {
        "Goal": "Solve stiff ODEs efficiently with large stable time steps.",
        "Applications": [
          "Stiff ODEs",
          "Schrödinger equation",
          "Fluid dynamics"
        ],
        "Methods": [
          "Krylov subspace exponential actions",
          "φ-functions"
        ],
        "Examples": [
          "Exponential Euler method",
          "ETD Runge–Kutta"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "lecture_notes_exponential_integrators.pdf",
        "created_at": "2025-11-18",
        "version": "1.0",
        "synonyms": []
      }
    },
    "Matrix Product V1 V2": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The matrix product V1 V2 represents the set of all matrices formed by multiplying elements from subspaces V1 and V2, i.e., {V1 V2 : V1 ∈ V1, V2 ∈ V2}.",
      "description": "This construct is used to approximate or factor large matrices A by finding subspaces such that A lies in V1 V2, enabling low-parameter representations.",
      "properties": {
        "Goal": "Represent matrices as products of subspace elements for factorization.",
        "Applications": [
          "Matrix approximation",
          "Low-rank factoring",
          "Compression"
        ],
        "Methods": [
          "Subspace identification",
          "Optimization over subspaces"
        ],
        "Examples": [
          "A ≈ V1 V2 for V1, V2 low-dimensional"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Range of Projection": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The range R(P) of a projection P is the set {y : y = Px for some x}, representing the subspace onto which P projects.",
      "description": "For orthogonal projections, R(P) is perpendicular to R(I - P), forming an orthogonal decomposition of the space.",
      "properties": {
        "Goal": "Define the projected subspace.",
        "Applications": [
          "Subspace identification",
          "Dimensionality reduction"
        ],
        "Methods": [
          "Column span of P",
          "Fixed points of P"
        ],
        "Examples": [
          "R(P) = span{columns of P}"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Algorithmic Factoring": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Algorithmic factoring refers to computational methods for decomposing matrices into structured forms like products of subspaces or triangular factors.",
      "description": "It leverages Krylov subspaces and projections to factor matrices efficiently, especially for large-scale problems.",
      "properties": {
        "Goal": "Factor matrices using algorithmic techniques.",
        "Applications": [
          "Large linear systems",
          "Eigenproblems",
          "Approximations"
        ],
        "Methods": [
          "Subspace iteration",
          "Projection-based factoring"
        ],
        "Examples": [
          "A = V1 W^{-1} using subspaces"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Invariant Subspace": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "An invariant subspace W for matrix A satisfies A W ⊆ W, meaning A maps W into itself.",
      "description": "Invariant subspaces are key in spectral theory and factoring, often identified via Krylov methods or projections.",
      "properties": {
        "Goal": "Find subspaces stable under matrix action.",
        "Applications": [
          "Eigen decomposition",
          "Schur form",
          "Model reduction"
        ],
        "Methods": [
          "Subspace iteration",
          "Arnoldi process"
        ],
        "Examples": [
          "Eigenvector spans 1D invariant subspace"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Square Matrix A": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A square matrix A ∈ ℂ^{n×n} is a matrix with equal rows and columns, central to linear transformations and eigenvalue problems.",
      "description": "In factoring contexts, A is decomposed using subspaces, projections, or iterations for computational efficiency.",
      "properties": {
        "Goal": "Represent linear operators on finite-dimensional spaces.",
        "Applications": [
          "Systems of equations",
          "Transformations",
          "Spectral analysis"
        ],
        "Methods": [
          "Factorization",
          "Iteration",
          "Diagonalization"
        ],
        "Examples": [
          "A with complex entries"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    }
  },
  "edges": [
    {
      "source": "Singular Value Decomposition",
      "type": "uses",
      "target": "Unitary Matrix"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "uses",
      "target": "Singular Value"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "enables",
      "target": "Low Rank Approximation"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "related_to",
      "target": "Principal Component Analysis"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "solves",
      "target": "Matrix Approximation Problem"
    },
    {
      "source": "Singular Value",
      "type": "part_of",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Singular Value",
      "type": "computed_from",
      "target": "Eigenvalue"
    },
    {
      "source": "Singular Value",
      "type": "determines",
      "target": "Matrix Rank"
    },
    {
      "source": "Singular Value",
      "type": "equals",
      "target": "Operator Norm"
    },
    {
      "source": "Unitary Matrix",
      "type": "used_in",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Unitary Matrix",
      "type": "preserves",
      "target": "Euclidean Norm"
    },
    {
      "source": "Unitary Matrix",
      "type": "part_of",
      "target": "QR Decomposition"
    },
    {
      "source": "Unitary Matrix",
      "type": "generalizes",
      "target": "Orthogonal Matrix"
    },
    {
      "source": "Low Rank Approximation",
      "type": "solved_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Low Rank Approximation",
      "type": "uses",
      "target": "Frobenius Norm"
    },
    {
      "source": "Low Rank Approximation",
      "type": "enables",
      "target": "Principal Component Analysis"
    },
    {
      "source": "Low Rank Approximation",
      "type": "minimizes",
      "target": "Reconstruction Error"
    },
    {
      "source": "Frobenius Norm",
      "type": "used_in",
      "target": "Low Rank Approximation"
    },
    {
      "source": "Frobenius Norm",
      "type": "equivalent_to",
      "target": "Euclidean Norm"
    },
    {
      "source": "Frobenius Norm",
      "type": "sum_of",
      "target": "Singular Value Squared"
    },
    {
      "source": "Frobenius Norm",
      "type": "invariant_under",
      "target": "Unitary Transformation"
    },
    {
      "source": "Operator Norm",
      "type": "equals",
      "target": "Largest Singular Value"
    },
    {
      "source": "Operator Norm",
      "type": "part_of",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Operator Norm",
      "type": "measures",
      "target": "Linear Transformation Stretch"
    },
    {
      "source": "Matrix Function",
      "type": "computed_via",
      "target": "Newton's Iteration"
    },
    {
      "source": "Matrix Function",
      "type": "applied_to",
      "target": "Square Matrix A"
    },
    {
      "source": "Matrix Function",
      "type": "used_in",
      "target": "Eigenvalue Problems"
    },
    {
      "source": "Matrix Function",
      "type": "related_to",
      "target": "Matrix Square Root"
    },
    {
      "source": "Matrix Square Root",
      "type": "instance_of",
      "target": "Matrix Function"
    },
    {
      "source": "Matrix Square Root",
      "type": "computed_by",
      "target": "Newton's Iteration for Square Root"
    },
    {
      "source": "Matrix Square Root",
      "type": "requires",
      "target": "Positive Definite A (for principal)"
    },
    {
      "source": "Matrix Square Root",
      "type": "related_to",
      "target": "Eigenvalue Decomposition"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "computes",
      "target": "Matrix Square Root"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "variant_of",
      "target": "Newton's Method"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "requires",
      "target": "Matrix Inversion"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "converges_to",
      "target": "sqrt(A)"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "solved_by",
      "target": "Iterative Methods"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "shifted_as",
      "target": "A - μ I"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "spectrum",
      "target": "Λ(A)"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "related_to",
      "target": "Singular Value Problem"
    },
    {
      "source": "Spectral Shift",
      "type": "transforms",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Spectral Shift",
      "type": "enables",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Spectral Shift",
      "type": "used_in",
      "target": "Inverse Iteration"
    },
    {
      "source": "Spectral Shift",
      "type": "estimates",
      "target": "Approximate μ"
    },
    {
      "source": "Positive Definite Shift",
      "type": "variant_of",
      "target": "Spectral Shift"
    },
    {
      "source": "Positive Definite Shift",
      "type": "ensures",
      "target": "Positive Definiteness"
    },
    {
      "source": "Positive Definite Shift",
      "type": "used_for",
      "target": "Conjugate Gradient"
    },
    {
      "source": "Positive Definite Shift",
      "type": "based_on",
      "target": "Eigenvalue Estimate ˆμ"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "of",
      "target": "Square Matrix A"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "shifted_by",
      "target": "Spectral Shift"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "estimated_via",
      "target": "Iterative Methods"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "related_to",
      "target": "Jordan Form"
    },
    {
      "source": "Matrix Computations",
      "type": "part_of",
      "target": "Numerical Analysis"
    },
    {
      "source": "Matrix Computations",
      "type": "used_in",
      "target": "Scientific Computing"
    },
    {
      "source": "Matrix Computations",
      "type": "depends_on",
      "target": "Linear Algebra"
    },
    {
      "source": "Matrix Computations",
      "type": "related_to",
      "target": "Partial Differential Equations"
    },
    {
      "source": "Matrix Computations",
      "type": "related_to",
      "target": "Matrix Factorization"
    },
    {
      "source": "Inner Product",
      "type": "generalization_of",
      "target": "Dot Product"
    },
    {
      "source": "Inner Product",
      "type": "used_in",
      "target": "Gram-Schmidt Process"
    },
    {
      "source": "Inner Product",
      "type": "foundation_for",
      "target": "Norm"
    },
    {
      "source": "Inner Product",
      "type": "related_to",
      "target": "Orthogonality"
    },
    {
      "source": "Computational Complexity (Matrix Multiplication)",
      "type": "applies_to",
      "target": "Matrix Multiplication"
    },
    {
      "source": "Computational Complexity (Matrix Multiplication)",
      "type": "contrasts_with",
      "target": "Subcubic Algorithms"
    },
    {
      "source": "Computational Complexity (Matrix Multiplication)",
      "type": "related_to",
      "target": "Strassen Algorithm"
    },
    {
      "source": "Matrix Computations",
      "type": "includes",
      "target": "LU Factorization"
    },
    {
      "source": "Matrix Computations",
      "type": "includes",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Matrix Computations",
      "type": "foundation_for",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Matrix Computations",
      "type": "related_to",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "LU Factorization",
      "type": "part_of",
      "target": "Matrix Factorization"
    },
    {
      "source": "LU Factorization",
      "type": "used_in",
      "target": "Linear System Solving"
    },
    {
      "source": "LU Factorization",
      "type": "approximated_by",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "LU Factorization",
      "type": "related_to",
      "target": "Gram-Schmidt Orthogonalization"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "part_of",
      "target": "Matrix Factorization"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "alternative_to",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "related_to",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "used_for",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "solved_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "related_to",
      "target": "LU Factorization"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "approximated_by",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "used_in",
      "target": "Partial Differential Equations"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "alternative_to",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "used_for",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "related_to",
      "target": "Gram-Schmidt Orthogonalization"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "applied_to",
      "target": "Triangular Matrices"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "used_in",
      "target": "QR Factorization"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "related_to",
      "target": "LU Factorization"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "applied_in",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "foundation_for",
      "target": "Orthogonal Matrices"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "achieved_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "alternative_via",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "used_in",
      "target": "Matrix Factorization"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "measures",
      "target": "Frobenius Norm"
    },
    {
      "source": "Hermitian Matrix",
      "type": "special_case_of",
      "target": "Symmetric Matrix (real case)"
    },
    {
      "source": "Hermitian Matrix",
      "type": "approximated_by",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Hermitian Matrix",
      "type": "related_to",
      "target": "Unitary Matrix"
    },
    {
      "source": "Hermitian Matrix",
      "type": "decomposed_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "uses",
      "target": "Krylov Subspace"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "variant_of",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "has_variant",
      "target": "Preconditioned Conjugate Gradient"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "requires",
      "target": "Symmetric Positive Definite Matrices"
    },
    {
      "source": "Preconditioned Conjugate Gradient",
      "type": "extends",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Preconditioned Conjugate Gradient",
      "type": "uses",
      "target": "Preconditioning"
    },
    {
      "source": "Preconditioned Conjugate Gradient",
      "type": "uses",
      "target": "Krylov Subspace"
    },
    {
      "source": "Preconditioned Conjugate Gradient",
      "type": "related_to",
      "target": "Incomplete LU Factorization"
    },
    {
      "source": "Krylov Subspace",
      "type": "foundation_for",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Krylov Subspace",
      "type": "used_in",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Krylov Subspace",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Krylov Subspace",
      "type": "related_to",
      "target": "Polynomial Approximation"
    },
    {
      "source": "Krylov Subspace Methods",
      "type": "generalization_of",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Krylov Subspace Methods",
      "type": "generalization_of",
      "target": "GMRES"
    },
    {
      "source": "Krylov Subspace Methods",
      "type": "built_on",
      "target": "Krylov Subspace"
    },
    {
      "source": "Matrix Factorization",
      "type": "includes",
      "target": "LU Factorization"
    },
    {
      "source": "Matrix Factorization",
      "type": "includes",
      "target": "QR Factorization"
    },
    {
      "source": "Matrix Factorization",
      "type": "includes",
      "target": "Cholesky Factorization"
    },
    {
      "source": "Matrix Factorization",
      "type": "includes",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Matrix Factorization",
      "type": "includes",
      "target": "Schur Decomposition"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "approximation_of",
      "target": "LU Factorization"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "used_in",
      "target": "Preconditioned Conjugate Gradient"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "related_to",
      "target": "Sparsity Structure"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "used_in",
      "target": "Spectral Radius"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "used_in",
      "target": "Gershgorin Circle Theorem"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "computed_by",
      "target": "Schur Decomposition"
    },
    {
      "source": "Projection Operator",
      "type": "has_property",
      "target": "Idempotency"
    },
    {
      "source": "Projection Operator",
      "type": "includes",
      "target": "Orthogonal Projection"
    },
    {
      "source": "Projection Operator",
      "type": "includes",
      "target": "Oblique Projection"
    },
    {
      "source": "Projection Operator",
      "type": "used_in",
      "target": "Matrix Subspace"
    },
    {
      "source": "Projection Operator",
      "type": "used_in",
      "target": "Factorization Algorithms"
    },
    {
      "source": "Matrix Subspace",
      "type": "generalization_of",
      "target": "Vector Subspace"
    },
    {
      "source": "Matrix Subspace",
      "type": "used_in",
      "target": "Krylov Subspace"
    },
    {
      "source": "Matrix Subspace",
      "type": "foundation_for",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "Matrix Subspace",
      "type": "related_to",
      "target": "Invariant Subspace"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "subtype_of",
      "target": "Matrix Subspace"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "enables",
      "target": "LU Factorization"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "defines",
      "target": "Inv(V)"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "related_to",
      "target": "GL(n,ℂ)"
    },
    {
      "source": "Inv(V)",
      "type": "derived_from",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "Inv(V)",
      "type": "is_a",
      "target": "Matrix Subspace"
    },
    {
      "source": "Inv(V)",
      "type": "preserves_under",
      "target": "Similarity Transformation"
    },
    {
      "source": "Inv(V)",
      "type": "used_in",
      "target": "LU Factorization"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "requires",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "uses",
      "target": "Inv(V)"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "extends",
      "target": "LU Factorization"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "related_to",
      "target": "Schur Complement"
    },
    {
      "source": "Krylov Subspace",
      "type": "subtype_of",
      "target": "Matrix Subspace"
    },
    {
      "source": "Krylov Subspace",
      "type": " invariant_under",
      "target": "Power Iteration"
    },
    {
      "source": "Krylov Subspace",
      "type": "foundation_for",
      "target": "GMRES"
    },
    {
      "source": "Krylov Subspace",
      "type": "bounded_by",
      "target": "Minimal Polynomial"
    },
    {
      "source": "Matrix Polynomials",
      "type": "generalizes",
      "target": "Scalar Polynomial"
    },
    {
      "source": "Matrix Polynomials",
      "type": "satisfies",
      "target": "Cayley-Hamilton Theorem"
    },
    {
      "source": "Matrix Polynomials",
      "type": "defines",
      "target": "Minimal Polynomial"
    },
    {
      "source": "Matrix Polynomials",
      "type": "preserves",
      "target": "Matrix Subspace"
    },
    {
      "source": "Projection Operator",
      "type": "special_case_of",
      "target": "Idempotent Operator"
    },
    {
      "source": "Projection Operator",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Projection Operator",
      "type": "orthogonal_to",
      "target": "I−P"
    },
    {
      "source": "Projection Operator",
      "type": "related_to",
      "target": "Oblique Projection"
    },
    {
      "source": "QZ Algorithm",
      "type": "related_to",
      "target": "Matrix Pencil"
    },
    {
      "source": "QZ Algorithm",
      "type": "generalization_of",
      "target": "QR Algorithm"
    },
    {
      "source": "Reflection",
      "type": "generalization_of",
      "target": "Householder Reflection"
    },
    {
      "source": "Reflection",
      "type": "related_to",
      "target": "Orthogonal Matrix"
    },
    {
      "source": "Row Reduction",
      "type": "foundation_for",
      "target": "Gaussian Elimination"
    },
    {
      "source": "Row Reduction",
      "type": "related_to",
      "target": "Matrix Rank"
    },
    {
      "source": "Schur Complement",
      "type": "related_to",
      "target": "Block Matrix"
    },
    {
      "source": "Schur Complement",
      "type": "related_to",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Search Directions (in CG)",
      "type": "subcomponent_of",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Search Directions (in CG)",
      "type": "related_to",
      "target": "Krylov Subspace"
    },
    {
      "source": "Similarity Transformation",
      "type": "used_in",
      "target": "Jordan Canonical Form"
    },
    {
      "source": "Similarity Transformation",
      "type": "used_in",
      "target": "Eigenvalue Decomposition"
    },
    {
      "source": "Spectral Gap",
      "type": "related_to",
      "target": "Spectrum Λ(A)"
    },
    {
      "source": "Spectral Gap",
      "type": "related_to",
      "target": "Power Iteration"
    },
    {
      "source": "Spectral Mapping Theorem",
      "type": "foundation_for",
      "target": "Matrix Function"
    },
    {
      "source": "Spectral Mapping Theorem",
      "type": "related_to",
      "target": "Spectrum Λ(A)"
    },
    {
      "source": "Spectral Radius",
      "type": "related_to",
      "target": "Power Iteration"
    },
    {
      "source": "Spectral Radius",
      "type": "related_to",
      "target": "Numerical Radius"
    },
    {
      "source": "Strassen Algorithm",
      "type": "subcategory_of",
      "target": "Subcubic Algorithms"
    },
    {
      "source": "Strassen Algorithm",
      "type": "related_to",
      "target": "Computational Complexity (Matrix Multiplication)"
    },
    {
      "source": "Subcubic Algorithms",
      "type": "generalization_of",
      "target": "Strassen Algorithm"
    },
    {
      "source": "Subcubic Algorithms",
      "type": "related_to",
      "target": "Matrix Multiplication"
    },
    {
      "source": "Subspace Decomposition",
      "type": "related_to",
      "target": "Direct Sum"
    },
    {
      "source": "Subspace Decomposition",
      "type": "related_to",
      "target": "Matrix Subspace"
    },
    {
      "source": "Subspace Iteration",
      "type": "related_to",
      "target": "Power Iteration"
    },
    {
      "source": "Subspace Iteration",
      "type": "related_to",
      "target": "Krylov Subspace"
    },
    {
      "source": "Sparsity Structure",
      "type": "defines",
      "target": "Standard Matrix Subspace"
    },
    {
      "source": "Sparsity Structure",
      "type": "used_in",
      "target": "Incomplete LU Factorization"
    },
    {
      "source": "Sparsity Structure",
      "type": "related_to",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Standard Matrix Subspace",
      "type": "based_on",
      "target": "Sparsity Structure"
    },
    {
      "source": "Standard Matrix Subspace",
      "type": "used_in",
      "target": "LU Factorization within Subspace"
    },
    {
      "source": "Standard Matrix Subspace",
      "type": "related_to",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Orthogonal Projector (Matrix Subspace)",
      "type": "projects_onto",
      "target": "Matrix Subspace"
    },
    {
      "source": "Orthogonal Projector (Matrix Subspace)",
      "type": "used_in",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Orthogonal Projector (Matrix Subspace)",
      "type": "related_to",
      "target": "Projection Operator"
    },
    {
      "source": "Invertible Matrix Subspace",
      "type": "opposes",
      "target": "Singular Matrix Subspace"
    },
    {
      "source": "Invertible Matrix Subspace",
      "type": "related_to",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "Invertible Matrix Subspace",
      "type": "enables",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Singular Matrix Subspace",
      "type": "opposes",
      "target": "Invertible Matrix Subspace"
    },
    {
      "source": "Singular Matrix Subspace",
      "type": "subset_of",
      "target": "Matrix Subspace"
    },
    {
      "source": "Singular Matrix Subspace",
      "type": "related_to",
      "target": "Low Rank Approximation"
    },
    {
      "source": "Polynomially Closed Matrix Subspace",
      "type": "supports",
      "target": "Invertible Matrix Subspace"
    },
    {
      "source": "Polynomially Closed Matrix Subspace",
      "type": "related_to",
      "target": "Matrix Polynomials"
    },
    {
      "source": "Polynomially Closed Matrix Subspace",
      "type": "enables",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Closure of V₁V₂",
      "type": "extends",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Closure of V₁V₂",
      "type": "used_in",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Closure of V₁V₂",
      "type": "related_to",
      "target": "Matrix Approximation Problem"
    },
    {
      "source": "Preconditioning",
      "type": "improves",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Preconditioning",
      "type": "improves",
      "target": "GMRES"
    },
    {
      "source": "Preconditioning",
      "type": "requires",
      "target": "Approximate Inverse"
    },
    {
      "source": "Preconditioning",
      "type": "reduces",
      "target": "Spectral Radius"
    },
    {
      "source": "Preconditioning",
      "type": "clusters",
      "target": "Eigenvalues"
    },
    {
      "source": "Preconditioning",
      "type": "used_with",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "is_a",
      "target": "Diagonal Preconditioner"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "part_of",
      "target": "Splitting Methods"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "used_in",
      "target": "Jacobi Iteration"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "simpler_than",
      "target": "Gauss-Seidel Preconditioner"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "approximates",
      "target": "LU Factorization"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "used_as",
      "target": "Preconditioner"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "extends",
      "target": "Gaussian Elimination"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "controls",
      "target": "Fill-in"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "approximates",
      "target": "Matrix Inverse"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "avoids",
      "target": "Triangular Solves"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "competes_with",
      "target": "Incomplete LU"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "enables",
      "target": "High Parallelism"
    },
    {
      "source": "Left Preconditioning",
      "type": "variant_of",
      "target": "Preconditioning"
    },
    {
      "source": "Left Preconditioning",
      "type": "preserves",
      "target": "Solution Vector"
    },
    {
      "source": "Left Preconditioning",
      "type": "changes",
      "target": "Residual Norm"
    },
    {
      "source": "Left Preconditioning",
      "type": "contrasts",
      "target": "Right Preconditioning"
    },
    {
      "source": "Right Preconditioning",
      "type": "variant_of",
      "target": "Preconditioning"
    },
    {
      "source": "Right Preconditioning",
      "type": "preserves",
      "target": "Residual Norm"
    },
    {
      "source": "Right Preconditioning",
      "type": "requires",
      "target": "Extra Solve"
    },
    {
      "source": "Right Preconditioning",
      "type": "used_in",
      "target": "Domain Decomposition"
    },
    {
      "source": "Splitting Methods",
      "type": "underlies",
      "target": "Jacobi Method"
    },
    {
      "source": "Splitting Methods",
      "type": "underlies",
      "target": "Gauss-Seidel Method"
    },
    {
      "source": "Splitting Methods",
      "type": "generalizes",
      "target": "Richardson Iteration"
    },
    {
      "source": "Splitting Methods",
      "type": "foundation_of",
      "target": "Preconditioning"
    },
    {
      "source": "Sylvester Equation Solution",
      "type": "related_to",
      "target": "Sylvester Equation"
    },
    {
      "source": "Sylvester Equation Solution",
      "type": "used_in",
      "target": "Lyapunov Equation"
    },
    {
      "source": "Symmetric Matrix",
      "type": "generalization_of",
      "target": "Symmetric Positive Definite Matrices"
    },
    {
      "source": "Symmetric Matrix",
      "type": "related_to",
      "target": "Hermitian Matrix"
    },
    {
      "source": "Symmetric Positive Definite Matrices",
      "type": "subtype_of",
      "target": "Symmetric Matrix"
    },
    {
      "source": "Symmetric Positive Definite Matrices",
      "type": "related_to",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Triangular Matrices",
      "type": "related_to",
      "target": "Lower Triangular Matrix"
    },
    {
      "source": "Triangular Matrices",
      "type": "related_to",
      "target": "Upper Triangular Matrix"
    },
    {
      "source": "Triangular Solves",
      "type": "related_to",
      "target": "LU Factorization"
    },
    {
      "source": "Triangular Solves",
      "type": "related_to",
      "target": "Triangular Matrices"
    },
    {
      "source": "Tridiagonal Matrix",
      "type": "used_in",
      "target": "Lanczos Algorithm"
    },
    {
      "source": "Tridiagonal Matrix",
      "type": "related_to",
      "target": "SPD Matrices"
    },
    {
      "source": "Unitary Similarity",
      "type": "related_to",
      "target": "Unitary Matrix"
    },
    {
      "source": "Unitary Similarity",
      "type": "related_to",
      "target": "Schur Triangulation"
    },
    {
      "source": "Unitary Transformation",
      "type": "generalization_of",
      "target": "Givens Rotation"
    },
    {
      "source": "Unitary Transformation",
      "type": "generalization_of",
      "target": "Householder Reflection"
    },
    {
      "source": "Upper Triangular Factor",
      "type": "subcomponent_of",
      "target": "LU Factorization"
    },
    {
      "source": "Upper Triangular Factor",
      "type": "related_to",
      "target": "Triangular Matrices"
    },
    {
      "source": "Upper Triangular Matrix",
      "type": "related_to",
      "target": "Triangular Matrices"
    },
    {
      "source": "Upper Triangular Matrix",
      "type": "related_to",
      "target": "QR Algorithm"
    },
    {
      "source": "Vandermonde Matrix",
      "type": "related_to",
      "target": "Polynomial Approximation"
    },
    {
      "source": "Vandermonde Matrix",
      "type": "related_to",
      "target": "Matrix Rank"
    },
    {
      "source": "Vector Subspace",
      "type": "foundation_for",
      "target": "Matrix Subspace"
    },
    {
      "source": "Vector Subspace",
      "type": "related_to",
      "target": "Orthogonal Complement"
    },
    {
      "source": "Residual r = b - Ax",
      "type": "related_to",
      "target": "Residual Norm"
    },
    {
      "source": "Residual r = b - Ax",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Matrix Square Root (Alternative Notation)",
      "type": "synonym_of",
      "target": "Matrix Square Root"
    },
    {
      "source": "Cholesky Factorization",
      "type": "specializes",
      "target": "LU Factorization"
    },
    {
      "source": "Cholesky Factorization",
      "type": "requires",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Cholesky Factorization",
      "type": "outputs",
      "target": "Upper Triangular Factor"
    },
    {
      "source": "Cholesky Factorization",
      "type": "related_to",
      "target": "LDL* Decomposition"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "generalization_of",
      "target": "Positive Semidefinite"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "enables",
      "target": "Cholesky Factorization"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "preserved_under",
      "target": "Congruence Transformation"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "related_to",
      "target": "Elliptic PDE"
    },
    {
      "source": "Sylvester Equation",
      "type": "generalizes",
      "target": "Lyapunov Equation"
    },
    {
      "source": "Sylvester Equation",
      "type": "solved_via",
      "target": "Schur Triangulation"
    },
    {
      "source": "Sylvester Equation",
      "type": "used_in",
      "target": "Control Synthesis"
    },
    {
      "source": "Sylvester Equation",
      "type": "related_to",
      "target": "Kronecker Product"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "computed_by",
      "target": "Fast Fourier Transform"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "diagonalizes",
      "target": "Circulant Matrix"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "unitary_up_to_scaling",
      "target": "Fₙ*/√n"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "related_to",
      "target": "Vandermonde Matrix"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "implements",
      "target": "Discrete Fourier Transform"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "complexity_reduction",
      "target": "O(n²) to O(n log n)"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "uses",
      "target": "Twiddle Factors"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "related_to",
      "target": "Butterfly Diagram"
    },
    {
      "source": "Schur Triangulation",
      "type": "generalizes",
      "target": "Hessenberg Form"
    },
    {
      "source": "Schur Triangulation",
      "type": "enables",
      "target": "Sylvester Equation Solution"
    },
    {
      "source": "Schur Triangulation",
      "type": "computed_by",
      "target": "QR Algorithm"
    },
    {
      "source": "Schur Triangulation",
      "type": "related_to",
      "target": "Unitary Similarity"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "generalizes",
      "target": "Discrete Fourier Transform (DFT)"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "related_to",
      "target": "Circulant Matrix"
    },
    {
      "source": "Fill-in",
      "type": "related_to",
      "target": "Sparse Approximate Inverse"
    },
    {
      "source": "Fill-in",
      "type": "related_to",
      "target": "Sparsity Structure"
    },
    {
      "source": "Francis Shift",
      "type": "used_in",
      "target": "QR Algorithm"
    },
    {
      "source": "Francis Shift",
      "type": "related_to",
      "target": "Hessenberg Matrix"
    },
    {
      "source": "Givens Rotation",
      "type": "variant_of",
      "target": "QR Factorization"
    },
    {
      "source": "Givens Rotation",
      "type": "related_to",
      "target": "Householder Reflection"
    },
    {
      "source": "GL(n, ℂ)",
      "type": "related_to",
      "target": "Invertible Matrix Subspace"
    },
    {
      "source": "GL(n, ℂ)",
      "type": "related_to",
      "target": "Matrix Function"
    },
    {
      "source": "Hessenberg Form",
      "type": "related_to",
      "target": "Hessenberg Matrix"
    },
    {
      "source": "Hessenberg Form",
      "type": "used_in",
      "target": "QR Algorithm"
    },
    {
      "source": "Hessenberg Reduction",
      "type": "produces",
      "target": "Hessenberg Form"
    },
    {
      "source": "Hessenberg Reduction",
      "type": "used_in",
      "target": "QR Algorithm"
    },
    {
      "source": "High Parallelism",
      "type": "related_to",
      "target": "Fast Fourier Transform (FFT)"
    },
    {
      "source": "High Parallelism",
      "type": "related_to",
      "target": "Matrix Multiplication"
    },
    {
      "source": "Idempotency",
      "type": "related_to",
      "target": "Projection Operator"
    },
    {
      "source": "Idempotency",
      "type": "related_to",
      "target": "Oblique Projection"
    },
    {
      "source": "Imaginary Unit j",
      "type": "related_to",
      "target": "Fast Fourier Transform (FFT)"
    },
    {
      "source": "Imaginary Unit j",
      "type": "related_to",
      "target": "Discrete Fourier Transform (DFT)"
    },
    {
      "source": "Inverse Iteration",
      "type": "uses",
      "target": "A - μ I"
    },
    {
      "source": "Inverse Iteration",
      "type": "variant_of",
      "target": "Rayleigh Quotient Iteration"
    },
    {
      "source": "Iterative Methods",
      "type": "alternative_to",
      "target": "Direct Methods"
    },
    {
      "source": "Iterative Methods",
      "type": "uses",
      "target": "Krylov Subspace"
    },
    {
      "source": "Iterative Methods",
      "type": "improved_by",
      "target": "Preconditioning"
    },
    {
      "source": "Iterative Methods",
      "type": "measures_convergence",
      "target": "Residual Norm"
    },
    {
      "source": "Linear System",
      "type": "solved_by",
      "target": "Iterative Methods"
    },
    {
      "source": "Linear System",
      "type": "preconditioned_as",
      "target": "M^{-1} A x = M^{-1} b"
    },
    {
      "source": "Linear System",
      "type": "residual",
      "target": "r = b - A x"
    },
    {
      "source": "Linear System",
      "type": "convergence_bound",
      "target": "Polynomial Minimization"
    },
    {
      "source": "Krylov Subspace",
      "type": "generated_by",
      "target": "Matrix Powers"
    },
    {
      "source": "Krylov Subspace",
      "type": "orthogonalized_by",
      "target": "Arnoldi Process"
    },
    {
      "source": "Krylov Subspace",
      "type": "invariant_under",
      "target": "A"
    },
    {
      "source": "Arnoldi Process",
      "type": "orthogonalises",
      "target": "Krylov Subspace"
    },
    {
      "source": "Arnoldi Process",
      "type": "produces",
      "target": "Hessenberg Matrix"
    },
    {
      "source": "Arnoldi Process",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Arnoldi Process",
      "type": "similar_to",
      "target": "Lanczos Algorithm"
    },
    {
      "source": "GMRES",
      "type": "based_on",
      "target": "Arnoldi Process"
    },
    {
      "source": "GMRES",
      "type": "minimises",
      "target": "Residual Norm"
    },
    {
      "source": "GMRES",
      "type": "for",
      "target": "Nonsymmetric Matrices"
    },
    {
      "source": "GMRES",
      "type": "variant_of",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "equivalent_to",
      "target": "Lanczos Algorithm (for SPD)"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "minimises",
      "target": "A-Norm Error"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "for",
      "target": "Symmetric Positive Definite Matrices"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "improved_by",
      "target": "Preconditioning"
    },
    {
      "source": "Preconditioning",
      "type": "improves",
      "target": "Iterative Methods"
    },
    {
      "source": "Preconditioning",
      "type": "approximates",
      "target": "A⁻¹"
    },
    {
      "source": "Preconditioning",
      "type": "used_in",
      "target": "PCG"
    },
    {
      "source": "Preconditioning",
      "type": "related_to",
      "target": "Condition Number"
    },
    {
      "source": "Residual Norm",
      "type": "minimised_by",
      "target": "GMRES"
    },
    {
      "source": "Residual Norm",
      "type": "orthogonal_to",
      "target": "Search Directions (in CG)"
    },
    {
      "source": "Residual Norm",
      "type": "computed_in",
      "target": "Iterative Methods"
    },
    {
      "source": "Residual Norm",
      "type": "bounds_convergence",
      "target": "Polynomial Approximation"
    },
    {
      "source": "A-Norm Error",
      "type": "minimised_by",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "A-Norm Error",
      "type": "related_to",
      "target": "Quadratic Form"
    },
    {
      "source": "A-Norm Error",
      "type": "for",
      "target": "SPD Matrices"
    },
    {
      "source": "A-Norm Error",
      "type": "bounds_via",
      "target": "Chebyshev Polynomials"
    },
    {
      "source": "Polynomial Approximation",
      "type": "underlies",
      "target": "Krylov Methods Convergence"
    },
    {
      "source": "Polynomial Approximation",
      "type": "uses",
      "target": "Eigenvalue Spectrum"
    },
    {
      "source": "Polynomial Approximation",
      "type": "improved_by",
      "target": "Preconditioning (clustering eigenvalues)"
    },
    {
      "source": "Polynomial Approximation",
      "type": "related_to",
      "target": "Condition Number"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "produced_by",
      "target": "Arnoldi Process"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "used_in",
      "target": "GMRES Least Squares"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "similar_to",
      "target": "Tridiagonal Matrix (symmetric case)"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "decomposed_by",
      "target": "QR Factorisation"
    },
    {
      "source": "Jordan Canonical Form",
      "type": "related_to",
      "target": "Eigenvalue Decomposition"
    },
    {
      "source": "Jordan Canonical Form",
      "type": "related_to",
      "target": "Minimal Polynomial"
    },
    {
      "source": "Kronecker Product",
      "type": "related_to",
      "target": "Sylvester Equation"
    },
    {
      "source": "Kronecker Product",
      "type": "related_to",
      "target": "Matrix Multiplication"
    },
    {
      "source": "Lower Triangular Matrix",
      "type": "related_to",
      "target": "LU Factorization"
    },
    {
      "source": "Lower Triangular Matrix",
      "type": "related_to",
      "target": "Upper Triangular Matrix"
    },
    {
      "source": "Matrix Pencil",
      "type": "related_to",
      "target": "Generalized Eigenvalue Problem"
    },
    {
      "source": "Matrix Pencil",
      "type": "used_in",
      "target": "QZ Algorithm"
    },
    {
      "source": "Matrix Rank",
      "type": "related_to",
      "target": "Singular Value"
    },
    {
      "source": "Matrix Rank",
      "type": "related_to",
      "target": "Low Rank Approximation"
    },
    {
      "source": "Minimal Polynomial",
      "type": "related_to",
      "target": "Jordan Canonical Form"
    },
    {
      "source": "Minimal Polynomial",
      "type": "related_to",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Normal Matrix",
      "type": "generalization_of",
      "target": "Hermitian Matrix"
    },
    {
      "source": "Normal Matrix",
      "type": "generalization_of",
      "target": "Unitary Matrix"
    },
    {
      "source": "Numerical Radius",
      "type": "related_to",
      "target": "Field of Values"
    },
    {
      "source": "Numerical Radius",
      "type": "related_to",
      "target": "Spectral Radius"
    },
    {
      "source": "Pseudospectrum",
      "type": "related_to",
      "target": "Spectrum Λ(A)"
    },
    {
      "source": "Pseudospectrum",
      "type": "related_to",
      "target": "Field of Values"
    },
    {
      "source": "Positive Semidefinite Matrix",
      "type": "generalization_of",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Positive Semidefinite Matrix",
      "type": "related_to",
      "target": "Quadratic Form"
    },
    {
      "source": "Quadratic Form",
      "type": "related_to",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Quadratic Form",
      "type": "related_to",
      "target": "Positive Semidefinite Matrix"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "generalizes_to",
      "target": "Generalized Eigenvalue Problem"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "solved_by",
      "target": "QR Algorithm"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "foundation_for",
      "target": "Matrix Functions"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "related_to",
      "target": "Jordan Canonical Form"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "bounds",
      "target": "Spectrum"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "refined_by",
      "target": "Brauer Cassini Ovals"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "used_in",
      "target": "Initial Eigenvalue Estimation"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "related_to",
      "target": "Spectral Radius"
    },
    {
      "source": "Power Iteration",
      "type": "special_case_of",
      "target": "Subspace Iteration"
    },
    {
      "source": "Power Iteration",
      "type": "enhanced_by",
      "target": "Rayleigh Quotient Iteration"
    },
    {
      "source": "Power Iteration",
      "type": "requires",
      "target": "Spectral Gap"
    },
    {
      "source": "Power Iteration",
      "type": "related_to",
      "target": "Inverse Iteration"
    },
    {
      "source": "Householder Reflection",
      "type": "building_block_of",
      "target": "QR Algorithm"
    },
    {
      "source": "Householder Reflection",
      "type": "unitary",
      "target": "Reflection"
    },
    {
      "source": "Householder Reflection",
      "type": "used_in",
      "target": "Hessenberg Reduction"
    },
    {
      "source": "Householder Reflection",
      "type": "related_to",
      "target": "Givens Rotation"
    },
    {
      "source": "QR Algorithm",
      "type": "computes",
      "target": "Schur Form"
    },
    {
      "source": "QR Algorithm",
      "type": "uses",
      "target": "Householder Reflection"
    },
    {
      "source": "QR Algorithm",
      "type": "accelerated_by",
      "target": "Francis Shift"
    },
    {
      "source": "QR Algorithm",
      "type": "related_to",
      "target": "Hessenberg Form"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "solved_by",
      "target": "QZ Algorithm"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "reduces_to",
      "target": "Standard Eigenvalue Problem"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "generalizes",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "related_to",
      "target": "Matrix Pencil"
    },
    {
      "source": "Matrix Function",
      "type": "computed_via",
      "target": "Schur Decomposition"
    },
    {
      "source": "Matrix Function",
      "type": "preserves",
      "target": "Spectral Mapping Theorem"
    },
    {
      "source": "Matrix Function",
      "type": "used_in",
      "target": "Exponential Integrators"
    },
    {
      "source": "Matrix Function",
      "type": "related_to",
      "target": "Cauchy Integral Formula"
    },
    {
      "source": "Field of Values",
      "type": "contains",
      "target": "Spectrum"
    },
    {
      "source": "Field of Values",
      "type": "equals_for",
      "target": "Normal Matrix"
    },
    {
      "source": "Field of Values",
      "type": "bounds",
      "target": "Numerical Radius"
    },
    {
      "source": "Field of Values",
      "type": "related_to",
      "target": "Pseudospectrum"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "improves",
      "target": "Gaussian Elimination"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "uses",
      "target": "Permutation Matrix"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "produces",
      "target": "Lower Triangular Matrix"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "produces",
      "target": "Upper Triangular Matrix"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "controls",
      "target": "Growth Factor"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "enables",
      "target": "Backward Stable Solver"
    },
    {
      "source": "Partial Pivoting",
      "type": "part_of",
      "target": "LU Factorization with Partial Pivoting"
    },
    {
      "source": "Partial Pivoting",
      "type": "reduces",
      "target": "Round-off Error"
    },
    {
      "source": "Partial Pivoting",
      "type": "bounds",
      "target": "Growth Factor"
    },
    {
      "source": "Partial Pivoting",
      "type": "contrasts",
      "target": "Complete Pivoting"
    },
    {
      "source": "Permutation Matrix",
      "type": "used_in",
      "target": "LU Factorization with Partial Pivoting"
    },
    {
      "source": "Permutation Matrix",
      "type": "preserves",
      "target": "Euclidean Norm"
    },
    {
      "source": "Permutation Matrix",
      "type": "inverts_to",
      "target": "Itself"
    },
    {
      "source": "Permutation Matrix",
      "type": "represents",
      "target": "Row Permutation"
    },
    {
      "source": "Growth Factor",
      "type": "bounded_by",
      "target": "Partial Pivoting"
    },
    {
      "source": "Growth Factor",
      "type": "affects",
      "target": "Backward Stability"
    },
    {
      "source": "Growth Factor",
      "type": "measured_in",
      "target": "Upper Triangular Factor"
    },
    {
      "source": "Condition Number",
      "type": "estimated_via",
      "target": "LU Factorization"
    },
    {
      "source": "Condition Number",
      "type": "equals",
      "target": "Ratio of Extreme Singular Values"
    },
    {
      "source": "Condition Number",
      "type": "impacts",
      "target": "Solution Accuracy"
    },
    {
      "source": "Gaussian Elimination",
      "type": "foundation_of",
      "target": "LU Factorization"
    },
    {
      "source": "Gaussian Elimination",
      "type": "enhanced_by",
      "target": "Partial Pivoting"
    },
    {
      "source": "Gaussian Elimination",
      "type": "performs",
      "target": "Row Reduction"
    },
    {
      "source": "A - μ I",
      "type": "related_to",
      "target": "Spectral Shift"
    },
    {
      "source": "A - μ I",
      "type": "used_in",
      "target": "Inverse Iteration"
    },
    {
      "source": "Backward Stability",
      "type": "related_to",
      "target": "Growth Factor"
    },
    {
      "source": "Backward Stability",
      "type": "related_to",
      "target": "Gaussian Elimination"
    },
    {
      "source": "Brauer Cassini Ovals",
      "type": "related_to",
      "target": "Gershgorin Circle Theorem"
    },
    {
      "source": "Brauer Cassini Ovals",
      "type": "generalization_of",
      "target": "Gershgorin Circle Theorem"
    },
    {
      "source": "Cauchy Integral Formula",
      "type": "foundation_for",
      "target": "Matrix Function"
    },
    {
      "source": "Cauchy Integral Formula",
      "type": "related_to",
      "target": "Spectrum Λ(A)"
    },
    {
      "source": "Chebyshev Polynomials",
      "type": "related_to",
      "target": "Polynomial Approximation"
    },
    {
      "source": "Chebyshev Polynomials",
      "type": "used_in",
      "target": "Iterative Methods"
    },
    {
      "source": "Circulant Matrix",
      "type": "related_to",
      "target": "Discrete Fourier Transform (DFT)"
    },
    {
      "source": "Circulant Matrix",
      "type": "related_to",
      "target": "Fast Fourier Transform (FFT)"
    },
    {
      "source": "Complete Pivoting",
      "type": "variant_of",
      "target": "LU Factorization with Partial Pivoting"
    },
    {
      "source": "Complete Pivoting",
      "type": "related_to",
      "target": "Growth Factor"
    },
    {
      "source": "Congruence Transformation",
      "type": "related_to",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Congruence Transformation",
      "type": "related_to",
      "target": "Lyapunov Equation"
    },
    {
      "source": "Control Synthesis",
      "type": "related_to",
      "target": "Lyapunov Equation"
    },
    {
      "source": "Control Synthesis",
      "type": "related_to",
      "target": "Matrix Pencil"
    },
    {
      "source": "Direct Sum",
      "type": "related_to",
      "target": "Orthogonal Complement"
    },
    {
      "source": "Direct Sum",
      "type": "related_to",
      "target": "Matrix Subspace"
    },
    {
      "source": "Domain Decomposition",
      "type": "related_to",
      "target": "Elliptic PDE"
    },
    {
      "source": "Domain Decomposition",
      "type": "used_in",
      "target": "Preconditioning"
    },
    {
      "source": "Elliptic PDE",
      "type": "related_to",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Elliptic PDE",
      "type": "related_to",
      "target": "Symmetric Positive Definite Matrices"
    },
    {
      "source": "Eigenvalue Decomposition",
      "type": "related_to",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Eigenvalue Decomposition",
      "type": "subtype_of",
      "target": "Spectral Decomposition"
    },
    {
      "source": "Exponential Integrators",
      "type": "related_to",
      "target": "Matrix Function"
    },
    {
      "source": "Exponential Integrators",
      "type": "related_to",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Krylov Subspace",
      "type": "generated_from",
      "target": "Matrix Powers"
    },
    {
      "source": "Krylov Subspace",
      "type": "used_in",
      "target": "Iterative Methods"
    },
    {
      "source": "Krylov Subspace",
      "type": "related_to",
      "target": "Invariant Subspace"
    },
    {
      "source": "Krylov Subspace",
      "type": "dimension",
      "target": "j"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "approximates",
      "target": "Square Matrix A"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "related_to",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "uses",
      "target": "Subspaces V1 V2"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "enables",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Projection Operator",
      "type": "satisfies",
      "target": "Idempotency P^2 = P"
    },
    {
      "source": "Projection Operator",
      "type": "defines",
      "target": "Range R(P)"
    },
    {
      "source": "Projection Operator",
      "type": "complements_with",
      "target": "I - P"
    },
    {
      "source": "Projection Operator",
      "type": "used_in",
      "target": "Subspace Decomposition"
    },
    {
      "source": "Range of Projection",
      "type": "defined_by",
      "target": "Projection Operator"
    },
    {
      "source": "Range of Projection",
      "type": "orthogonal_to",
      "target": "R(I - P)"
    },
    {
      "source": "Range of Projection",
      "type": "part_of",
      "target": "Direct Sum Decomposition"
    },
    {
      "source": "Range of Projection",
      "type": "related_to",
      "target": "Kernel of I - P"
    },
    {
      "source": "Orthogonal Complement",
      "type": "property_of",
      "target": "Orthogonal Projection"
    },
    {
      "source": "Orthogonal Complement",
      "type": "ensures",
      "target": "Direct Sum"
    },
    {
      "source": "Orthogonal Complement",
      "type": "related_to",
      "target": "Inner Product Space"
    },
    {
      "source": "Orthogonal Complement",
      "type": "used_in",
      "target": "Subspace Factoring"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "uses",
      "target": "Krylov Subspace"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "incorporates",
      "target": "Projection Operator"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "alternative_to",
      "target": "Direct Factorization"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "applied_to",
      "target": "Square Matrix A"
    },
    {
      "source": "Invariant Subspace",
      "type": "related_to",
      "target": "Krylov Subspace"
    },
    {
      "source": "Invariant Subspace",
      "type": "projected_onto",
      "target": "Projection Operator"
    },
    {
      "source": "Invariant Subspace",
      "type": "used_in",
      "target": "Matrix Factoring"
    },
    {
      "source": "Invariant Subspace",
      "type": "contains",
      "target": "Eigenvectors"
    },
    {
      "source": "Square Matrix A",
      "type": "factored_by",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Square Matrix A",
      "type": "generates",
      "target": "Krylov Subspace"
    },
    {
      "source": "Square Matrix A",
      "type": "acts_on",
      "target": "Invariant Subspace"
    },
    {
      "source": "Square Matrix A",
      "type": "projected_by",
      "target": "Projection Operator"
    }
  ],
  "metadata": {
    "last_built": "2025-11-19 11:17:54",
    "node_count": 140,
    "edge_count": 486
  }
}