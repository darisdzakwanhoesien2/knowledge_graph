{
  "nodes": {
    "Thresholding": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Thresholding is an image segmentation technique that converts a grayscale image into a binary image by assigning pixel values above a threshold to one class and below to another.",
      "description": "It is one of the simplest and most widely used segmentation methods, effective when objects and background have sufficiently different intensity distributions.",
      "properties": {
        "Goal": "Create a binary mask separating foreground from background.",
        "Applications": [
          "Document binarization",
          "Motion detection",
          "Edge-based segmentation"
        ],
        "Methods": [
          "Global thresholding",
          "Local/adaptive thresholding",
          "Otsu's method"
        ],
        "Examples": [
          "Converting medical scans to highlight tumors",
          "Isolating text in scanned pages"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04-binary-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Preprocessing": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Preprocessing refers to operations applied to an input image before the main analysis to enhance relevant features and suppress noise or unwanted variations.",
      "description": "In binary image analysis, it improves the quality of subsequent thresholding and segmentation by reducing noise, normalizing illumination, and enhancing contrast.",
      "properties": {
        "Goal": "Improve image quality for robust downstream processing.",
        "Applications": [
          "Noise reduction",
          "Contrast enhancement",
          "Illumination correction"
        ],
        "Methods": [
          "Smoothing filters",
          "Histogram equalization",
          "Bilateral filtering"
        ],
        "Examples": [
          "Applying Gaussian blur before thresholding",
          "Removing salt-and-pepper noise"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04-binary-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Segmentation": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Segmentation is the process of partitioning an image into meaningful regions or objects, often producing a binary mask where each region is labeled.",
      "description": "In binary vision, it typically results in a foreground/background separation and is a critical step before recognition or measurement.",
      "properties": {
        "Goal": "Isolate objects of interest from the background.",
        "Applications": [
          "Object counting",
          "Defect detection",
          "Character recognition"
        ],
        "Methods": [
          "Thresholding",
          "Edge detection",
          "Region growing"
        ],
        "Examples": [
          "Separating cells in microscopy",
          "Extracting text blocks"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04-binary-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Post-processing": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Post-processing involves refinement operations applied to segmented binary images to correct errors, remove noise, and improve object integrity.",
      "description": "Common operations include morphological filtering to close gaps, remove small islands, or smooth boundaries in binary masks.",
      "properties": {
        "Goal": "Clean and refine binary segmentation results.",
        "Applications": [
          "Hole filling",
          "Noise removal",
          "Boundary smoothing"
        ],
        "Methods": [
          "Morphological closing",
          "Opening",
          "Connected component filtering"
        ],
        "Examples": [
          "Removing small speckles after thresholding",
          "Filling holes in segmented objects"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04-binary-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Shape Representation": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Shape representation involves extracting compact and descriptive features from binary object silhouettes to enable recognition and classification.",
      "description": "It transforms raw pixel data into structural descriptors such as contours, moments, or geometric properties for higher-level analysis.",
      "properties": {
        "Goal": "Encode object geometry in a recognition-friendly format.",
        "Applications": [
          "Object classification",
          "Pose estimation",
          "Defect analysis"
        ],
        "Methods": [
          "Contour extraction",
          "Moment invariants",
          "Fourier descriptors"
        ],
        "Examples": [
          "Circularity measure",
          "Bounding box",
          "Convex hull"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04-binary-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Labeling": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Labeling is the process of assigning a unique identifier to each connected component in a binary image, enabling individual object analysis.",
      "description": "Also known as connected component labeling, it is essential for counting, measuring, and tracking distinct objects in a scene.",
      "properties": {
        "Goal": "Identify and enumerate separate objects in a binary mask.",
        "Applications": [
          "Particle counting",
          "Cell tracking",
          "Character segmentation"
        ],
        "Methods": [
          "Two-pass algorithm",
          "Union-find",
          "Recursive labeling"
        ],
        "Examples": [
          "Labeling individual coins in an image",
          "Numbering text lines"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04-binary-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Recognition": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Recognition in binary image analysis refers to classifying or identifying objects based on their shape, size, or extracted features from labeled regions.",
      "description": "It maps low-level binary patterns to semantic categories using prior knowledge, templates, or learned models.",
      "properties": {
        "Goal": "Assign meaningful labels to detected objects.",
        "Applications": [
          "OCR",
          "Symbol recognition",
          "Part identification"
        ],
        "Methods": [
          "Template matching",
          "Feature classification",
          "Statistical pattern recognition"
        ],
        "Examples": [
          "Identifying machine parts on a conveyor",
          "Reading license plates"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04-binary-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Binary Image Analysis Pipeline": {
      "type": "Framework",
      "domain": "Computer Vision",
      "definition": "The binary image analysis pipeline is a sequential workflow consisting of preprocessing, segmentation, post-processing, labeling, shape representation, and recognition to extract meaningful information from images.",
      "description": "It represents a classical, modular approach to vision tasks where images are progressively transformed from raw pixels to semantic understanding via binary intermediates.",
      "properties": {
        "Goal": "Separate objects of interest from the background using intensity thresholds.",
        "Applications": [
          "Object segmentation",
          "Document analysis",
          "Medical imaging",
          "Industrial inspection"
        ],
        "Methods": [
          "Thresholding",
          "Otsu's method",
          "Adaptive thresholding"
        ],
        "Examples": [
          "Scanned text documents",
          "Silhouettes",
          "Mask images"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04-binary-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "3D Perception": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "3D perception refers to the process of recovering the three-dimensional structure and spatial relationships of a scene from one or more 2D images, enabling machines to understand depth, shape, and layout.",
      "description": "It bridges 2D image analysis with real-world geometry, using monocular, stereo, or motion cues to estimate depth, reconstruct surfaces, and interpret scene organization.",
      "properties": {
        "Goal": "Reconstruct 3D world from 2D visual data",
        "Applications": [
          "3D modeling",
          "Autonomous navigation",
          "Augmented reality"
        ],
        "Methods": [
          "Structure from Motion",
          "Multi-view Stereo",
          "Dense Matching"
        ],
        "Examples": [
          "Multi-view stereo reconstruction",
          "Optical illusions from ambiguous 3D interpretations"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10-3D_perception-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Depth from Stereo": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Depth from stereo is the recovery of 3D scene depth by triangulating corresponding points in a pair of images captured from slightly different viewpoints (baseline).",
      "description": "It relies on disparity—the horizontal shift of a point between left and right images—to compute depth inversely proportional to disparity via calibrated camera geometry.",
      "properties": {
        "Goal": "Compute per-pixel depth using binocular disparity.",
        "Applications": [
          "3D reconstruction",
          "Robot navigation",
          "Virtual reality"
        ],
        "Methods": [
          "Stereo matching",
          "Triangulation",
          "Rectification",
          "Disparity map refinement"
        ],
        "Examples": [
          "Z = (f * B) / d, where d is disparity"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10-3D_perception-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Structure from Motion": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Structure from Motion (SfM) is the simultaneous estimation of 3D scene structure and camera motion from a set of 2D images taken from different viewpoints, typically using feature correspondences.",
      "description": "It solves for camera poses and sparse 3D points via bundle adjustment, forming the basis for dense reconstruction and large-scale 3D modeling.",
      "properties": {
        "Goal": "Estimate 3D geometry and motion from image sequences.",
        "Applications": [
          "3D reconstruction",
          "AR/VR",
          "Robot localization"
        ],
        "Methods": [
          "Feature matching",
          "Bundle adjustment",
          "Triangulation"
        ],
        "Examples": [
          "Exam 2019 - Method explanation",
          "Exam 2020 - Stereo extension"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10-3D_perception-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Monocular Depth Cues": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Monocular depth cues are visual signals in a single image that provide information about relative or absolute depth without requiring multiple viewpoints.",
      "description": "They mimic human perception mechanisms and include occlusion, perspective, shading, texture gradient, and known object size, enabling depth estimation from single images.",
      "properties": {
        "Goal": "Infer 3D layout from 2D image cues.",
        "Applications": [
          "Single-image depth estimation",
          "Image editing",
          "Autonomous navigation"
        ],
        "Methods": [
          "Shape from shading",
          "Depth from defocus",
          "Learning-based monocular depth"
        ],
        "Examples": [
          "Linear perspective in roads",
          "Occlusion of distant objects"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10-3D_perception-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Shape from Shading": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Shape from shading recovers surface orientation (normals) by analyzing how light intensity varies across a surface under known illumination, assuming Lambertian reflectance.",
      "description": "It solves an inverse optics problem to estimate local surface tilt from image gradients, enabling 3D reconstruction from a single image.",
      "properties": {
        "Goal": "Estimate surface normals from intensity gradients.",
        "Applications": [
          "Planetary surface mapping",
          "Medical imaging",
          "Industrial inspection"
        ],
        "Methods": [
          "Reflectance map",
          "Variational optimization",
          "Photometric stereo"
        ],
        "Examples": [
          "Reconstructing face shape from one photo"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10-3D_perception-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Photometric Stereo": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Photometric stereo uses multiple images of a static object under different known lighting directions to estimate per-pixel surface normals by solving a linear system of reflectance equations.",
      "description": "It decouples albedo and geometry, providing dense, high-quality normal maps even for textureless surfaces, assuming Lambertian reflectance.",
      "properties": {
        "Goal": "Recover detailed surface orientation from shading cues.",
        "Applications": [
          "Shape-from-shading",
          "Industrial inspection",
          "Material analysis"
        ],
        "Methods": [
          "Lambertian reflectance model",
          "Linear intensity equations"
        ],
        "Examples": [
          "Exam 2017 - Principle question",
          "Exam 2018 - Example"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10-3D_perception-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Multi-view Stereo": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Multi-view stereo (MVS) computes dense 3D reconstruction from multiple calibrated images by estimating depth or disparity in each view and fusing consistent measurements into a coherent 3D model.",
      "description": "It extends stereo to arbitrary camera configurations, producing detailed point clouds or meshes using photo-consistency and regularization.",
      "properties": {
        "Goal": "Dense 3D reconstruction from image sets",
        "Applications": [
          "3D scanning",
          "Virtual reality",
          "Archaeology"
        ],
        "Methods": [
          "Feature matching",
          "Depth map fusion"
        ],
        "Examples": [
          "Reconstruction of architectural columns and dinosaur models from photographs"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10-3D_perception-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Triangulation": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Triangulation is the process of determining the 3D position of a point by intersecting rays from two or more calibrated cameras that observe the corresponding 2D image points.",
      "description": "It is the geometric foundation of passive 3D reconstruction, converting image correspondences into metric 3D coordinates using camera projection matrices.",
      "properties": {
        "Goal": "Compute 3D point from 2D correspondences and camera parameters.",
        "Applications": [
          "3D reconstruction",
          "Robot localization",
          "Augmented reality"
        ],
        "Methods": [
          "Linear triangulation",
          "Mid-point method",
          "Optimal triangulation"
        ],
        "Examples": [
          "X = intersection of two rays in space"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10-3D_perception-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Image Matching": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Image matching is the process of establishing correspondences between two or more images of the same scene taken from different viewpoints, times, or sensors to determine geometric or photometric relationships.",
      "description": "It forms the foundation for applications requiring image alignment, such as panorama stitching, 3D reconstruction, and visual tracking. Matching can be sparse (feature-based) or dense (pixel-wise).",
      "properties": {
        "Goal": "Find reliable point or region correspondences across images.",
        "Applications": [
          "Structure from Motion",
          "Stereo vision",
          "Image retrieval",
          "Augmented reality"
        ],
        "Methods": [
          "Feature-based matching",
          "Area-based matching",
          "Direct methods",
          "Learning-based matching"
        ],
        "Examples": [
          "Matching SIFT features",
          "Dense optical flow",
          "Template correlation"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09-matching-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Epipolar Geometry": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Epipolar geometry describes the intrinsic projective relationship between two views of a 3D scene, constraining the location of corresponding points to lie on corresponding epipolar lines.",
      "description": "It reduces the search space for matching from 2D to 1D and is encapsulated by the fundamental matrix (for uncalibrated cameras) or essential matrix (for calibrated cameras).",
      "properties": {
        "Goal": "Constrain correspondence search using stereo geometry.",
        "Applications": [
          "Stereo matching",
          "Camera calibration",
          "Motion estimation"
        ],
        "Methods": [
          "Fundamental matrix estimation",
          "Essential matrix",
          "Epipolar line computation"
        ],
        "Examples": [
          "Left image point maps to line in right image"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09-matching-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Fundamental Matrix": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The fundamental matrix is a 3x3 rank-2 matrix that encapsulates the epipolar geometry between two uncalibrated images, mapping points in one image to epipolar lines in the other.",
      "description": "It is estimated from point correspondences and used to enforce geometric consistency in matching and reconstruction. F satisfies x'ᵀ F x = 0 for corresponding points x and x'.",
      "properties": {
        "Goal": "Encode projective geometry between two views without calibration.",
        "Applications": [
          "Correspondence validation",
          "Camera pose estimation",
          "Image rectification"
        ],
        "Methods": [
          "8-point algorithm",
          "7-point algorithm",
          "RANSAC estimation"
        ],
        "Examples": [
          "F computed from 8+ matches",
          "Used in guided matching"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09-matching-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Essential Matrix": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The essential matrix is a 3x3 matrix that describes the epipolar geometry between two calibrated cameras, encoding relative rotation and translation (up to scale) between their coordinate systems.",
      "description": "It is a specialized version of the fundamental matrix when intrinsic parameters are known. E = K'ᵀ F K, and it has exactly two equal non-zero singular values.",
      "properties": {
        "Goal": "Recover relative pose from calibrated image pairs.",
        "Applications": [
          "5-point algorithm",
          "Visual odometry",
          "SLAM initialization"
        ],
        "Methods": [
          "5-point algorithm",
          "Nister's method",
          "Decomposition into R and t"
        ],
        "Examples": [
          "E from 5+ matches in calibrated stereo"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09-matching-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Stereo Matching": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Stereo matching is the process of finding corresponding pixels between a pair of rectified stereo images to compute depth via triangulation, producing a disparity map.",
      "description": "It exploits epipolar constraints to reduce search to 1D and uses similarity measures (e.g., SAD, NCC) with aggregation and optimization to handle ambiguity and occlusions.",
      "properties": {
        "Goal": "Compute dense depth from calibrated stereo pairs.",
        "Applications": [
          "3D reconstruction",
          "Robot navigation",
          "Autonomous driving"
        ],
        "Methods": [
          "Local block matching",
          "Semi-global matching",
          "Graph cuts",
          "Deep stereo"
        ],
        "Examples": [
          "Disparity map from left-right image pair"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09-matching-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Image Rectification": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Image rectification is the process of warping a stereo image pair so that epipolar lines become horizontal and conjugate points have the same y-coordinate, simplifying correspondence search.",
      "description": "It enables efficient scanline-based stereo algorithms and is typically performed using the fundamental matrix or known camera parameters.",
      "properties": {
        "Goal": "Align epipolar lines for efficient stereo matching.",
        "Applications": [
          "Dense stereo",
          "Disparity estimation",
          "Multi-view stereo"
        ],
        "Methods": [
          "Hartley’s method",
          "Polar rectification",
          "Calibrated rectification"
        ],
        "Examples": [
          "Transforming converging epipoles to infinity"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09-matching-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Homography": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "A homography is a 3x3 projective transformation that maps points from one plane to another (or between two views of the same plane), preserving collinearity but not distances or angles.",
      "description": "It is used to align images of planar scenes, remove perspective distortion, and enable direct pixel-wise comparison in matching.",
      "properties": {
        "Goal": "Map image coordinates between planar views.",
        "Applications": [
          "Panorama stitching",
          "Image mosaicking",
          "Augmented reality",
          "Document scanning"
        ],
        "Methods": [
          "DLT (Direct Linear Transformation)",
          "RANSAC estimation from 4+ points"
        ],
        "Examples": [
          "Warping book page to frontal view"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09-matching-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Pinhole Model": {
      "type": "Model",
      "domain": "Computer Vision",
      "definition": "The pinhole model is a simplified geometric approximation of image formation in which light rays from a 3D scene pass through an infinitesimally small aperture (pinhole) and project onto an image plane, forming an inverted image.",
      "description": "It serves as the foundational camera model in computer vision, enabling the mathematical description of perspective projection without lens distortions. Real cameras approximate this model using lenses to focus light.",
      "properties": {
        "Goal": "Model how 3D points project onto a 2D image plane.",
        "Applications": [
          "Camera calibration",
          "3D reconstruction",
          "Stereo imaging"
        ],
        "Methods": [
          "Homogeneous coordinate projection",
          "Matrix-based projection equations"
        ],
        "Examples": [
          "Exam 2014 - Projection geometry",
          "Exam 2016 - Stereo setup"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02-imaging-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Perspective Projection": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Perspective projection is the process by which 3D points in the world are mapped to 2D points on an image plane through a projection center, simulating how the human eye or a pinhole camera perceives depth.",
      "description": "It results in closer objects appearing larger and parallel lines converging at vanishing points. The projection is governed by similar triangles and the focal length of the imaging system.",
      "properties": {
        "Goal": "Transform 3D world coordinates (X, Y, Z) into 2D image coordinates (x, y).",
        "Applications": [
          "3D reconstruction",
          "Augmented reality",
          "Photogrammetry"
        ],
        "Methods": [
          "Pinhole camera equations: x = f * X/Z, y = f * Y/Z"
        ],
        "Examples": [
          "Railroad tracks appearing to converge in photographs"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02-imaging-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Camera Obscura": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Camera obscura is an optical device that projects an inverted image of a scene through a small hole onto a surface inside a darkened chamber.",
      "description": "It is the historical and physical precursor to modern photographic cameras and the conceptual basis for the pinhole camera model in computer vision.",
      "properties": {
        "Goal": "Demonstrate natural image formation via light projection.",
        "Applications": [
          "Artistic drawing aid",
          "Historical optics experiments"
        ],
        "Methods": [
          "Single aperture light projection"
        ],
        "Examples": [
          "Artists using camera obscura for realistic perspective in paintings"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02-imaging-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Perspective Distortion": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Perspective distortion refers to the apparent deformation of objects in images due to the viewpoint and the geometry of perspective projection, even when the lens is perfect.",
      "description": "It causes nonlinear transformations between views: straight lines remain straight, but angles, shapes, and relative sizes change depending on distance and camera orientation.",
      "properties": {
        "Goal": "N/A",
        "Applications": [
          "Wide-angle photography",
          "Architectural imaging"
        ],
        "Methods": [
          "N/A"
        ],
        "Examples": [
          "Tall buildings appearing to lean inward when photographed from below"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02-imaging-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Focal Length": {
      "type": "Metric",
      "domain": "Computer Vision",
      "definition": "Focal length is the distance between the camera's optical center and the image plane where a sharp image of an object at infinity is formed.",
      "description": "It determines the scale of the projected image and the field of view. In the pinhole model, it directly appears in the projection equations x = f * X/Z and y = f * Y/Z.",
      "properties": {
        "Goal": "Control magnification and field of view in imaging systems.",
        "Applications": [
          "Zoom lenses",
          "Camera calibration",
          "Depth estimation"
        ],
        "Methods": [
          "Intrinsic parameter in camera matrix"
        ],
        "Examples": [
          "Telephoto (long f) vs. wide-angle (short f) lenses"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02-imaging-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Image Plane": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The image plane is the two-dimensional surface onto which 3D scene points are projected to form a 2D image in the pinhole camera model.",
      "description": "In real cameras, it corresponds to the sensor plane (CCD/CMOS); in theoretical models, it can be placed in front (virtual) or behind (real) the projection center.",
      "properties": {
        "Goal": "Capture the 2D projection of the 3D world.",
        "Applications": [
          "Sensor design",
          "Projection geometry"
        ],
        "Methods": [
          "Coordinate transformation"
        ],
        "Examples": [
          "Real image plane (behind lens), Virtual image plane (front projection)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02-imaging-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Simplified Imaging Model": {
      "type": "Framework",
      "domain": "Computer Vision",
      "definition": "A simplified imaging model breaks down image formation into geometric, optical, and sensor components to abstract the camera as a measurable system.",
      "description": "It separates concerns: geometry (projection), optics (light focusing), and sensor (light measurement), forming the basis for camera modeling and calibration.",
      "properties": {
        "Goal": "Provide a modular understanding of the imaging pipeline.",
        "Applications": [
          "Camera modeling",
          "Photometric calibration"
        ],
        "Methods": [
          "Decomposition into sub-models"
        ],
        "Examples": [
          "Geometric → Pinhole, Optical → Thin lens, Sensor → Radiometric response"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02-imaging-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "SIFT": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A feature detection and description algorithm that identifies keypoints and computes descriptors invariant to scale, rotation, and partial illumination changes.",
      "description": "SIFT builds a scale-space using Difference-of-Gaussians, detects local extrema, and forms gradient-based descriptors robust to geometric and photometric transformations.",
      "properties": {
        "Goal": "Detect stable image features for matching across scales and rotations.",
        "Applications": [
          "Image matching",
          "Object recognition",
          "3D reconstruction"
        ],
        "Methods": [
          "Difference-of-Gaussians",
          "Gradient histogram descriptors",
          "Keypoint matching"
        ],
        "Examples": [
          "Exam 2015 - Definition",
          "Exam 2018 - Descriptor explanation"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Harris Corner Detector": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A feature detector that identifies corners by measuring local intensity variations using the autocorrelation matrix.",
      "description": "The Harris detector finds points with significant intensity change in orthogonal directions, forming the basis of many tracking and matching algorithms.",
      "properties": {
        "Goal": "Detect stable, repeatable interest points corresponding to corners or junctions.",
        "Applications": [
          "Feature tracking",
          "Camera calibration",
          "Image alignment"
        ],
        "Methods": [
          "Image gradients",
          "Structure tensor",
          "Corner response function R = det(M) - k*trace(M)^2"
        ],
        "Examples": [
          "L-junctions",
          "T-junctions",
          "Building corners"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Hough Transform": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A feature extraction technique used to detect parametric shapes such as lines, circles, or ellipses in images.",
      "description": "The transform maps image edge points into a parameter space where shapes correspond to peaks, enabling robust detection despite noise or occlusion.",
      "properties": {
        "Goal": "Detect geometric primitives via voting in parameter space.",
        "Applications": [
          "Line detection",
          "Circle detection",
          "Shape analysis"
        ],
        "Methods": [
          "Parameter-space accumulation",
          "Threshold-based peak detection"
        ],
        "Examples": [
          "Exam 2015 - Principle question",
          "Exam 2018 - Example usage"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Random Sample Consensus (RANSAC)": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "An iterative algorithm to estimate model parameters from data containing outliers by repeatedly sampling minimal subsets and testing consensus.",
      "description": "RANSAC is robust to outliers and commonly used in fitting geometric models such as lines, planes, or homographies.",
      "properties": {
        "Goal": "Estimate model parameters robustly in the presence of outliers.",
        "Applications": [
          "Feature matching cleanup",
          "Pose estimation",
          "Image stitching"
        ],
        "Methods": [
          "Random sampling",
          "Model fitting",
          "Inlier counting",
          "Iteration until confidence"
        ],
        "Examples": [
          "Estimating F from 1000 matches with 60% outliers"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Local Binary Patterns (LBP)": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A texture descriptor that encodes the local spatial pattern of pixel intensities into binary codes based on neighbor comparisons.",
      "description": "LBP provides a rotation- and grayscale-invariant way to represent texture; it’s lightweight and robust, making it popular for real-time classification.",
      "properties": {
        "Goal": "Describe local texture via binary comparisons with center pixel.",
        "Applications": [
          "Face recognition",
          "Biomedical texture analysis",
          "Industrial surface inspection"
        ],
        "Methods": [
          "Circular neighborhood sampling",
          "Binary encoding",
          "Histogram aggregation"
        ],
        "Examples": [
          "LBP(8,1) code",
          "Uniform LBP",
          "Rotation-invariant LBP"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "K-Means Clustering": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "An unsupervised learning algorithm that partitions data into K clusters by minimizing within-cluster variance.",
      "description": "Used to group similar pixels, features, or image patches, serving as a basis for segmentation and visual vocabulary creation.",
      "properties": {
        "Goal": "Group data points into clusters based on feature similarity.",
        "Applications": [
          "Image segmentation",
          "Bag-of-Words clustering",
          "Color quantization"
        ],
        "Methods": [
          "Iterative centroid update",
          "Euclidean distance minimization"
        ],
        "Examples": [
          "Exam 2016 - Principle",
          "Exam 2019 - Usage explanation"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Otsu’s Thresholding Method": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "An automatic global thresholding technique that separates foreground and background by minimizing intra-class intensity variance.",
      "description": "Commonly used in image preprocessing for binarization tasks, providing an optimal threshold without supervision.",
      "properties": {
        "Goal": "Automatically determine an optimal threshold to separate regions.",
        "Applications": [
          "Image segmentation",
          "Preprocessing for OCR",
          "Object extraction"
        ],
        "Methods": [
          "Histogram-based variance minimization"
        ],
        "Examples": [
          "Exam 2018 - Principle",
          "Exam 2019 - Usage question"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Harris–Laplace Detector": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A multi-scale feature detector combining corner detection (Harris) with scale selection (Laplacian) to identify stable features across resolutions.",
      "description": "Enhances standard corner detectors by integrating scale information for improved invariance.",
      "properties": {
        "Goal": "Detect scale-invariant interest points.",
        "Applications": [
          "Feature matching",
          "Object tracking",
          "Scale-space analysis"
        ],
        "Methods": [
          "Corner response computation",
          "Laplacian-of-Gaussian scale selection"
        ],
        "Examples": [
          "Exam 2016 - Extended question"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Feature descriptor": {
      "type": "Concept",
      "domain": "Feature Extraction",
      "definition": "A representation of an image patch or interest point that captures its essential characteristics, designed to be robust to variations in illumination, viewpoint, and scale.",
      "description": "",
      "properties": {},
      "metadata": {
        "created_by": "Daris",
        "source": "02-features-00.pdf",
        "created_at": "2023-10-27",
        "version": "1.0"
      }
    },
    "Maximally Stable Extremal Regions (MSER)": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A method for detecting regions in an image that are stable across a wide range of thresholds, often used for text detection and object recognition.",
      "description": "",
      "properties": {},
      "metadata": {
        "created_by": "Daris",
        "source": "02-features-00.pdf",
        "created_at": "2023-10-27",
        "version": "1.0"
      }
    },
    "Texture": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Texture refers to the spatial arrangement and variation of intensity or color values in an image region, representing surface patterns independent of overall color or brightness.",
      "description": "It provides critical information about material properties and structural repetition. Texture analysis enables segmentation, classification, and synthesis in natural and artificial scenes.",
      "properties": {
        "Goal": "Capture local spatial patterns and repetitions in image intensity or color.",
        "Applications": [
          "Material classification",
          "Image segmentation",
          "Defect detection",
          "Content-based retrieval"
        ],
        "Methods": [
          "Statistical methods",
          "Filter banks",
          "Structural analysis"
        ],
        "Examples": [
          "Grass",
          "Sand",
          "Brick wall",
          "Checkerboard",
          "Striped fabric"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05-texture-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Texture Segmentation": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Texture segmentation is the process of partitioning an image into regions with homogeneous texture properties, separating areas of different surface patterns.",
      "description": "It extends intensity-based segmentation by incorporating local spatial statistics or frequency content to distinguish textured regions.",
      "properties": {
        "Goal": "Group pixels into regions sharing similar textural appearance.",
        "Applications": [
          "Medical imaging",
          "Remote sensing",
          "Fabric inspection",
          "Scene understanding"
        ],
        "Methods": [
          "Supervised classification",
          "Unsupervised clustering",
          "Filter response analysis"
        ],
        "Examples": [
          "Separating grass from sky",
          "Isolating wood grain from metal"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05-texture-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Texture Descriptors": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Texture descriptors are quantitative features extracted from image patches to characterize local texture properties in a compact, discriminative form.",
      "description": "They transform raw pixel neighborhoods into feature vectors suitable for classification, clustering, or similarity comparison.",
      "properties": {
        "Goal": "Encode texture appearance into robust, comparable feature representations.",
        "Applications": [
          "Texture classification",
          "Retrieval",
          "Synthesis",
          "Anomaly detection"
        ],
        "Methods": [
          "Gray-level co-occurrence matrix",
          "Local Binary Patterns",
          "Gabor filters",
          "Histogram of gradients"
        ],
        "Examples": [
          "GLCM contrast",
          "LBP codes",
          "Filter bank energy"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05-texture-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Statistical Texture Analysis": {
      "type": "Framework",
      "domain": "Computer Vision",
      "definition": "Statistical texture analysis models texture as the spatial distribution of gray-level values, using statistical measures to characterize local intensity variations.",
      "description": "It assumes texture arises from repeated patterns or random processes and uses first- or second-order statistics to describe regions.",
      "properties": {
        "Goal": "Decompose texture into primitives and syntactic rules.",
        "Applications": [
          "Fabric design",
          "Pattern recognition",
          "Synthetic texture generation"
        ],
        "Methods": [
          "Morphological operations",
          "Grammar-based modeling",
          "Texton mapping"
        ],
        "Examples": [
          "Brick wall = rectangle + grid",
          "Checkerboard = square + alternation"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05-texture-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Gray-Level Co-occurrence Matrix": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The Gray-Level Co-occurrence Matrix (GLCM) is a second-order statistical method that counts the frequency of intensity pairs at a given spatial offset in an image.",
      "description": "It captures spatial relationships between pixel values and derives texture features like contrast, correlation, energy, and homogeneity.",
      "properties": {
        "Goal": "Model joint probability of intensity pairs at specific displacements.",
        "Applications": [
          "Texture classification",
          "Medical image analysis",
          "Satellite imagery"
        ],
        "Methods": [
          "Matrix construction",
          "Feature extraction (contrast, entropy, etc.)"
        ],
        "Examples": [
          "GLCM at (1,0) for horizontal texture",
          "Haralick features"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05-texture-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Filter Bank Methods": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Filter bank methods apply a set of linear filters (e.g., Gabor, wavelet) tuned to different frequencies and orientations to decompose texture into frequency-scale components.",
      "description": "They model texture in the frequency domain, capturing multi-scale and multi-orientation patterns inspired by human visual cortex.",
      "properties": {
        "Goal": "Describe texture using responses to multiple spatial-frequency filters.",
        "Applications": [
          "Texture classification",
          "Defect detection",
          "Material identification"
        ],
        "Methods": [
          "Gabor filters",
          "Laws masks",
          "Energy feature computation"
        ],
        "Examples": [
          "Exam 2016 - Texture task",
          "Exam 2019 - Conceptual question"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05-texture-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Motion": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Motion in computer vision refers to the apparent displacement of image brightness patterns between consecutive frames in a video sequence, caused by relative movement between the camera and the scene.",
      "description": "It provides critical information about scene dynamics, enabling applications such as tracking, segmentation, and 3D reconstruction. Motion analysis distinguishes between camera motion, object motion, and complex non-rigid deformations.",
      "properties": {
        "Goal": "Estimate and interpret changes in image appearance over time.",
        "Applications": [
          "Video stabilization",
          "Object tracking",
          "Action recognition",
          "Autonomous navigation"
        ],
        "Methods": [
          "Optical flow",
          "Feature tracking",
          "Block matching",
          "Differential methods"
        ],
        "Examples": [
          "Car moving across frames",
          "Person walking",
          "Camera panning"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08-motion-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Optical Flow": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Optical flow is the dense 2D vector field representing the apparent motion of brightness patterns between two consecutive image frames, assigning a displacement vector to each pixel.",
      "description": "It models local image changes under the brightness constancy assumption and is used to estimate scene motion, segment moving objects, and support higher-level video analysis.",
      "properties": {
        "Goal": "Compute per-pixel motion vectors between image pairs.",
        "Applications": [
          "Motion compensation",
          "Video compression",
          "Robot navigation",
          "Medical imaging"
        ],
        "Methods": [
          "Lucas-Kanade (sparse)",
          "Horn-Schunck (dense)",
          "Farneback",
          "Deep learning-based flow"
        ],
        "Examples": [
          "Flow field around a moving car",
          "Expansion flow from camera zoom"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08-motion-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Brightness Constancy Assumption": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The brightness constancy assumption states that the intensity of a moving point remains constant between consecutive frames, i.e., I(x, t) = I(x + d, t + 1), where d is the displacement.",
      "description": "It is the foundational constraint in differential optical flow methods, enabling the formulation of the optical flow constraint equation from spatiotemporal image gradients.",
      "properties": {
        "Goal": "Link pixel intensity across time for motion estimation.",
        "Applications": [
          "Optical flow computation",
          "Motion detection",
          "Tracking"
        ],
        "Methods": [
          "Gradient-based flow",
          "Aperture problem solving"
        ],
        "Examples": [
          "Valid for Lambertian surfaces under constant illumination"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08-motion-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Aperture Problem": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The aperture problem arises when observing motion through a local window: only the motion component perpendicular to an edge can be measured, leaving the parallel component ambiguous.",
      "description": "It explains why local motion measurements are underconstrained and requires integration over larger regions or use of corner features to resolve full 2D motion.",
      "properties": {
        "Goal": "Explain the fundamental ambiguity in local motion detection.",
        "Applications": [
          "Optical flow",
          "Edge tracking",
          "Motion estimation"
        ],
        "Methods": [
          "Gradient constraint equation",
          "Lucas–Kanade method",
          "Global smoothness enforcement"
        ],
        "Examples": [
          "Exam 2015 - Definition",
          "Exam 2019 - Optical flow task"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08-motion-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Horn-Schunck Method": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The Horn-Schunck method is a dense optical flow algorithm that minimizes a global energy functional combining the brightness constancy error and a smoothness term across the entire image.",
      "description": "It produces smooth flow fields by enforcing spatial coherence, making it suitable for dense motion estimation even in textureless regions.",
      "properties": {
        "Goal": "Compute smooth, dense optical flow over the full image.",
        "Applications": [
          "Motion segmentation",
          "Video editing",
          "Fluid flow visualization"
        ],
        "Methods": [
          "Variational optimization",
          "Euler-Lagrange equations",
          "Iterative solvers"
        ],
        "Examples": [
          "Global flow in translating scenes",
          "Dense flow in medical ultrasound"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08-motion-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Motion Segmentation": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Motion segmentation is the process of partitioning a video into regions corresponding to independently moving objects based on their motion patterns.",
      "description": "It leverages optical flow or feature trajectories to group pixels with coherent motion, enabling object-level video analysis.",
      "properties": {
        "Goal": "Separate independently moving objects in dynamic scenes.",
        "Applications": [
          "Video surveillance",
          "Autonomous driving",
          "Action analysis"
        ],
        "Methods": [
          "Flow clustering",
          "Layered motion models",
          "Graph cuts"
        ],
        "Examples": [
          "Separating pedestrian from background",
          "Isolating multiple cars"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08-motion-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Feature Tracking": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Feature tracking involves detecting distinctive points in one frame and finding their corresponding locations in subsequent frames using local search and similarity metrics.",
      "description": "It is a sparse motion representation widely used in real-time systems due to low computational cost and robustness when features are well-distributed.",
      "properties": {
        "Goal": "Maintain persistent point correspondences across video frames.",
        "Applications": [
          "SLAM",
          "Structure from Motion",
          "Camera pose estimation"
        ],
        "Methods": [
          "KLT tracker",
          "Descriptor matching",
          "Sub-pixel refinement"
        ],
        "Examples": [
          "Tracking Harris corners over time",
          "Sparse trajectory bundles"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08-motion-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Specular Reflection": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Specular reflection is the mirror-like reflection of light from a surface in which light from a single incoming direction is reflected into a single outgoing direction, following the law of reflection.",
      "description": "It produces highlights and sharp reflections, characteristic of smooth, shiny surfaces. In computer vision, specular highlights are often modeled separately from diffuse reflection to improve surface reconstruction and material estimation.",
      "properties": {
        "Goal": "Model mirror-like light behavior on smooth surfaces.",
        "Applications": [
          "Shape from shading",
          "Material classification",
          "Highlight removal"
        ],
        "Methods": [
          "Law of reflection: angle of incidence = angle of reflection"
        ],
        "Examples": [
          "Shiny metal surfaces",
          "Water reflections",
          "Glass"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03-color-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Diffuse Reflection": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Diffuse reflection occurs when light striking a rough surface is scattered in many directions, with equal radiance in all directions (Lambertian reflection).",
      "description": "It is responsible for the matte appearance of non-shiny surfaces. In vision, it is used to estimate surface orientation and albedo under known lighting.",
      "properties": {
        "Goal": "Model light scattering from rough, matte surfaces.",
        "Applications": [
          "Photometric stereo",
          "Albedo estimation",
          "Shape recovery"
        ],
        "Methods": [
          "Lambert's cosine law: intensity ∝ cos(θ)"
        ],
        "Examples": [
          "Paper",
          "Cloth",
          "Painted walls"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03-color-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Light": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Light is electromagnetic radiation within the visible spectrum (approximately 380–700 nm) that can be perceived by the human visual system.",
      "description": "In computer vision, light is modeled as energy propagating through space, interacting with matter via reflection, transmission, and absorption. Its spectral composition determines perceived color.",
      "properties": {
        "Goal": "Enable visual perception and image formation.",
        "Applications": [
          "Illumination modeling",
          "Color analysis",
          "Radiometry"
        ],
        "Methods": [
          "Spectral power distribution",
          "Wavelength-based analysis"
        ],
        "Examples": [
          "Sunlight",
          "LED illumination",
          "Laser light"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03-color-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Visible Spectrum": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The visible spectrum is the portion of the electromagnetic spectrum with wavelengths between approximately 380 and 700 nanometers, detectable by the human eye.",
      "description": "Different wavelengths within this range are perceived as different colors, from violet (short) to red (long). It forms the basis for trichromatic color vision in humans and most imaging sensors.",
      "properties": {
        "Goal": "Define the range of light responsible for color vision.",
        "Applications": [
          "Color imaging",
          "Spectral analysis",
          "Display technology"
        ],
        "Methods": [
          "Spectrophotometry",
          "Prism dispersion"
        ],
        "Examples": [
          "380 nm → violet",
          "550 nm → green",
          "700 nm → red"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03-color-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Illuminating Source": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "An illuminating source is any object or phenomenon that emits electromagnetic radiation, contributing directly to scene illumination.",
      "description": "The perceived color depends on the spectral power distribution of the emitted light. Multiple sources combine additively. Examples include natural (sun) and artificial (bulbs) light.",
      "properties": {
        "Goal": "Provide primary light energy to a scene.",
        "Applications": [
          "Color constancy",
          "White balancing",
          "Relighting"
        ],
        "Methods": [
          "Additive color mixing",
          "Spectral emission modeling"
        ],
        "Examples": [
          "Sun",
          "Incandescent bulb",
          "Fluorescent light",
          "Monitor screen"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03-color-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Reflecting Source": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "A reflecting source is a surface that reflects incident light from illuminating sources, contributing to the light reaching the observer or camera.",
      "description": "Its color depends on which wavelengths are absorbed and which are reflected (subtractive mixing). Pigments and dyes are common examples.",
      "properties": {
        "Goal": "Modulate incident light to produce colored appearance.",
        "Applications": [
          "Color segmentation",
          "Material recognition",
          "Reflectance estimation"
        ],
        "Methods": [
          "Subtractive color mixing",
          "Spectral reflectance curves"
        ],
        "Examples": [
          "Red paint",
          "Green leaf",
          "Blue fabric"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03-color-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Additive Rule": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The additive rule states that the total spectrum observed from multiple illuminating sources is the sum of their individual spectral power distributions.",
      "description": "This linear superposition applies to light sources emitting directly into the scene. It is fundamental to color mixing in displays and multi-light illumination models.",
      "properties": {
        "Goal": "Predict combined effect of multiple light emitters.",
        "Applications": [
          "Stage lighting",
          "Display calibration",
          "Multi-spectral imaging"
        ],
        "Methods": [
          "Spectral summation: E_total(λ) = Σ E_i(λ)"
        ],
        "Examples": [
          "Mixing red + green light → yellow"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03-color-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Subtractive Rule": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The subtractive rule describes how the perceived color of mixed reflecting sources depends on the remaining wavelengths after selective absorption by each material.",
      "description": "Used in printing and pigments, it models how filters or dyes remove parts of the spectrum. The result is the intersection of reflected wavelengths.",
      "properties": {
        "Goal": "Model color formation via selective light absorption.",
        "Applications": [
          "Color printing (CMYK)",
          "Paint mixing",
          "Filter design"
        ],
        "Methods": [
          "Multiplicative reflectance: R_total(λ) = Π R_i(λ)"
        ],
        "Examples": [
          "Yellow + magenta filter → red transmission"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03-color-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Reflection": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Reflection is the process by which light bounces off a surface, changing direction without being absorbed or transmitted.",
      "description": "It can be specular (mirror-like) or diffuse (scattered), depending on surface roughness. Essential for image formation in most vision systems.",
      "properties": {
        "Goal": "Redirect incident light toward observer or sensor.",
        "Applications": [
          "Shape reconstruction",
          "BRDF estimation",
          "Photometric analysis"
        ],
        "Methods": [
          "Law of reflection",
          "Fresnel equations"
        ],
        "Examples": [
          "Mirror",
          "Polished metal",
          "Matte paper"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03-color-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Transmission": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Transmission is the passage of light through a material without significant scattering or absorption, typically in transparent or translucent media.",
      "description": "It allows light to propagate through objects like glass or water, often with refraction. Important in underwater imaging and optical systems.",
      "properties": {
        "Goal": "Allow light to pass through matter.",
        "Applications": [
          "Lens modeling",
          "Underwater vision",
          "Medical imaging"
        ],
        "Methods": [
          "Beer-Lambert law",
          "Refraction (Snell's law)"
        ],
        "Examples": [
          "Clear glass",
          "Water",
          "Air"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03-color-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Absorption": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Absorption is the process by which the energy of light is converted into another form (usually heat) when interacting with matter.",
      "description": "It selectively removes certain wavelengths, determining the color of reflecting objects. Critical for understanding material appearance and spectral selectivity.",
      "properties": {
        "Goal": "Convert light energy into internal energy.",
        "Applications": [
          "Spectral imaging",
          "Color filtering",
          "Thermal imaging"
        ],
        "Methods": [
          "Absorption coefficient",
          "Beer-Lambert law"
        ],
        "Examples": [
          "Black surface absorbing all light",
          "Red filter absorbing blue/green"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03-color-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Convolutional Neural Network (CNN)": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A deep learning architecture composed of convolutional, pooling, and fully connected layers that automatically learn hierarchical visual features.",
      "description": "CNNs dominate modern computer vision tasks by learning spatial hierarchies of features directly from raw image data without handcrafted descriptors.",
      "properties": {
        "Goal": "Automatically learn discriminative visual representations.",
        "Applications": [
          "Image classification",
          "Object detection",
          "Semantic segmentation"
        ],
        "Methods": [
          "Backpropagation",
          "Convolutional filtering",
          "Pooling operations"
        ],
        "Examples": [
          "Exam 2018 - Modern methods",
          "Exam 2019 - High-level discussion"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Bag-of-Words (BoW) Representation": {
      "type": "Representation",
      "domain": "Feature Representation",
      "definition": "An image representation that models visual content as a histogram of discrete visual words learned from feature descriptors.",
      "description": "BoW models are popular for image classification and retrieval due to their simplicity and effectiveness, despite losing spatial information.",
      "properties": {
        "Goal": "Represent images as collections of visual words for classification or retrieval.",
        "Applications": [
          "Image classification",
          "Content-based image retrieval",
          "Object recognition"
        ],
        "Methods": [
          "Feature extraction (e.g., SIFT, SURF)",
          "Visual vocabulary creation (e.g., K-Means)",
          "Histogram generation"
        ],
        "Examples": [
          "Image search engines",
          "Category recognition"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "02-features-00.pdf",
        "created_at": "2023-10-27",
        "version": "1.0"
      }
    },
    "Visual words": {
      "type": "Concept",
      "domain": "Feature Representation",
      "definition": "A cluster center in the feature space, representing a common visual pattern or feature, used to build Bag-of-Words representations.",
      "description": "",
      "properties": {},
      "metadata": {
        "created_by": "Daris",
        "source": "02-features-00.pdf",
        "created_at": "2023-10-27",
        "version": "1.0"
      }
    },
    "Image segmentation": {
      "type": "Task/Capability",
      "domain": "Segmentation",
      "definition": "The process of partitioning a digital image into multiple segments (sets of pixels) to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze.",
      "description": "Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.",
      "properties": {
        "Goal": "Divide an image into coherent regions for further analysis.",
        "Applications": [
          "Object detection",
          "Medical imaging",
          "Scene understanding"
        ],
        "Methods": [
          "Thresholding",
          "Region growing",
          "Graph-based segmentation"
        ],
        "Examples": [
          "Exam 2019 - Principle",
          "Exam 2020 - Segmentation method"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "04-binary-00.pdf",
        "created_at": "2023-10-27",
        "version": "1.0"
      }
    },
    "Background Subtraction": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A motion-based segmentation approach that isolates moving foreground objects by comparing each frame to a background model.",
      "description": "Widely used in video surveillance, motion analysis, and dynamic scene understanding.",
      "properties": {
        "Goal": "Separate moving objects from a static or slowly changing background.",
        "Applications": [
          "Surveillance",
          "Traffic monitoring",
          "Gesture recognition"
        ],
        "Methods": [
          "Frame differencing",
          "Gaussian mixture models",
          "Otsu thresholding"
        ],
        "Examples": [
          "Exam 2018 - Segmentation question"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Lucas–Kanade Optical Flow": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A differential method for optical flow estimation assuming constant motion within a local neighborhood.",
      "description": "Solves the aperture problem by enforcing spatial smoothness; widely used for motion tracking and video stabilization.",
      "properties": {
        "Goal": "Estimate local motion at feature points with sub-pixel accuracy.",
        "Applications": [
          "Feature tracking",
          "Visual odometry",
          "Augmented reality"
        ],
        "Methods": [
          "Local window analysis",
          "Weighted least squares",
          "Pyramidal refinement"
        ],
        "Examples": [
          "Tracking facial landmarks",
          "Corner point tracking in video"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Texture Analysis": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "The process of quantifying image surface characteristics using spatial variations in intensity or color.",
      "description": "Includes statistical, structural, and filter-based approaches to characterize surface patterns or material properties.",
      "properties": {
        "Goal": "Extract numerical features that describe texture patterns.",
        "Applications": [
          "Texture classification",
          "Surface inspection",
          "Remote sensing"
        ],
        "Methods": [
          "Co-occurrence matrices",
          "Filter banks",
          "LBP histograms"
        ],
        "Examples": [
          "Exam 2015 - Seashell texture",
          "Exam 2018 - Filter bank task"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Edge Detection": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A fundamental operation that detects local discontinuities in intensity to outline object boundaries.",
      "description": "Typical operators like Sobel, Prewitt, or Canny emphasize intensity gradients to delineate shapes for segmentation and recognition.",
      "properties": {
        "Goal": "Identify object boundaries via intensity gradients.",
        "Applications": [
          "Shape analysis",
          "Hough Transform",
          "Segmentation preprocessing"
        ],
        "Methods": [
          "Gradient computation",
          "Thresholding",
          "Non-maximum suppression"
        ],
        "Examples": [
          "Exam 2015 - Preprocessing",
          "Exam 2018 - Edge-based Hough task"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Graph-Based Segmentation": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "An image segmentation method that models the image as a graph, where pixels or regions are nodes and edge weights represent similarity.",
      "description": "Cuts or merges in the graph minimize a global cost function, yielding coherent region boundaries.",
      "properties": {
        "Goal": "Group pixels by minimizing inter-region dissimilarity.",
        "Applications": [
          "Object segmentation",
          "Superpixel generation",
          "Video segmentation"
        ],
        "Methods": [
          "Normalized cuts",
          "Minimum spanning tree",
          "Spectral clustering"
        ],
        "Examples": [
          "Exam 2018 - Segmentation discussion"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Local Features": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Local features are distinctive image patterns that are detected at specific points (keypoints) and described in a way that enables reliable matching across different views, scales, or illumination conditions.",
      "description": "They serve as the foundation for tasks requiring correspondence between images, such as stitching, 3D reconstruction, object recognition, and tracking. A complete local feature pipeline includes detection, description, and matching.",
      "properties": {
        "Goal": "Establish robust point correspondences between images under geometric and photometric transformations.",
        "Applications": [
          "Image matching",
          "Panorama stitching",
          "Structure from Motion",
          "Object recognition"
        ],
        "Methods": [
          "Keypoint detection",
          "Descriptor extraction",
          "Feature matching"
        ],
        "Examples": [
          "Harris corners",
          "SIFT keypoints",
          "ORB features"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06-features-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Keypoint Detection": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Keypoint detection is the process of identifying salient, repeatable locations in an image where local features can be reliably extracted and matched across different views.",
      "description": "Good keypoints are typically corners, blobs, or junctions that remain stable under small transformations. Detection is the first stage in the local feature pipeline.",
      "properties": {
        "Goal": "Locate stable points invariant to translation, rotation, and scale (to varying degrees).",
        "Applications": [
          "Feature-based alignment",
          "Visual odometry",
          "Augmented reality"
        ],
        "Methods": [
          "Harris",
          "FAST",
          "DoG (SIFT)",
          "MSER"
        ],
        "Examples": [
          "Corner points",
          "Blob centers",
          "Region extrema"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06-features-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Feature Description": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Feature description involves computing a compact, discriminative vector (descriptor) from the image patch around a detected keypoint to enable robust matching.",
      "description": "The descriptor captures local appearance and is designed to be invariant to scale, rotation, and illumination changes. It is the second stage after keypoint detection.",
      "properties": {
        "Goal": "Encode local image appearance into a matching-friendly representation.",
        "Applications": [
          "Wide baseline matching",
          "Object retrieval",
          "Loop closure in SLAM"
        ],
        "Methods": [
          "SIFT",
          "SURF",
          "BRIEF",
          "ORB",
          "Histogram of gradients"
        ],
        "Examples": [
          "128D SIFT vector",
          "64D SURF",
          "256-bit BRIEF"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06-features-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Feature Matching": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Feature matching is the process of finding correspondences between descriptors from two images by measuring similarity (e.g., Euclidean distance or Hamming distance).",
      "description": "It establishes pairwise associations between keypoints, enabling geometric verification (e.g., via RANSAC) to filter outliers. It is the final stage in local feature pipelines.",
      "properties": {
        "Goal": "Identify correct point-to-point correspondences across images.",
        "Applications": [
          "Image stitching",
          "3D reconstruction",
          "Visual tracking"
        ],
        "Methods": [
          "Nearest neighbor",
          "Ratio test",
          "Cross-checking",
          "FLANN"
        ],
        "Examples": [
          "Matching SIFT descriptors",
          "Hamming distance for binary ORB"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06-features-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Image Gradient": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The image gradient is a vector field representing the directional change in intensity at each pixel, computed as the partial derivatives in x and y directions.",
      "description": "It is fundamental to edge and corner detection. The magnitude indicates edge strength, and the direction indicates edge orientation.",
      "properties": {
        "Goal": "Quantify local intensity changes for feature detection.",
        "Applications": [
          "Edge detection",
          "Corner detection",
          "Optical flow"
        ],
        "Methods": [
          "Sobel operator",
          "Prewitt",
          "Finite differences"
        ],
        "Examples": [
          "∇I = [∂I/∂x, ∂I/∂y]"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06-features-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Structure Tensor": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The structure tensor (or second-moment matrix) is a 2x2 matrix that summarizes the predominant directions and strength of gradients in a local image neighborhood.",
      "description": "It is used in corner detection to distinguish corners (high variation in all directions), edges (variation in one direction), and flat regions (low variation).",
      "properties": {
        "Goal": "Analyze local gradient distribution for feature classification.",
        "Applications": [
          "Corner detection",
          "Motion estimation",
          "Texture analysis"
        ],
        "Methods": [
          "M = Σ w [Ix², IxIy; IxIy, Iy²]",
          "Eigenvalue analysis"
        ],
        "Examples": [
          "Harris response",
          "Shi-Tomasi corner measure"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06-features-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Radial Distortion": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "A lens distortion where straight lines appear curved due to nonlinear magnification that varies with distance from the image center.",
      "description": "Commonly corrected during camera calibration using polynomial or division models to improve geometric accuracy.",
      "properties": {
        "Goal": "Model and correct optical distortion effects in imaging systems.",
        "Applications": [
          "Camera calibration",
          "3D measurement",
          "Photogrammetry"
        ],
        "Methods": [
          "Polynomial distortion model",
          "Inverse distortion mapping"
        ],
        "Examples": [
          "Exam 2014 - Lens modeling",
          "Exam 2018 - Definition question"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "HSV Color Space": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "A color representation model that describes colors in terms of hue, saturation, and value, which better aligns with human color perception.",
      "description": "HSV is commonly used in segmentation and tracking tasks because hue can be more stable under illumination variations.",
      "properties": {
        "Goal": "Represent and manipulate color information in perceptually meaningful terms.",
        "Applications": [
          "Color-based segmentation",
          "Object tracking",
          "Skin detection"
        ],
        "Methods": [
          "RGB-to-HSV conversion",
          "Thresholding by hue and saturation"
        ],
        "Examples": [
          "Exam 2015 - Definition",
          "Exam 2019 - Basic term question"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Depth of Field": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "The range of distances within a scene that appear acceptably sharp in an image.",
      "description": "Controlled by aperture size, focal length, and sensor distance; important in focus estimation and 3D reconstruction.",
      "properties": {
        "Goal": "Quantify and control image sharpness across depth layers.",
        "Applications": [
          "Focus measurement",
          "Autofocus systems",
          "3D reconstruction"
        ],
        "Methods": [
          "Optical modeling",
          "Focus metric computation"
        ],
        "Examples": [
          "Exam 2020 - Definition question"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Epipolar Constraint": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "A geometric relationship stating that a point in one image must lie on a specific line (the epipolar line) in the other image when both views observe the same 3D point.",
      "description": "Derived from camera projection matrices and the essential matrix; simplifies stereo correspondence search.",
      "properties": {
        "Goal": "Reduce 2D stereo correspondence search to 1D along epipolar lines.",
        "Applications": [
          "Stereo vision",
          "Structure-from-Motion",
          "Camera calibration"
        ],
        "Methods": [
          "Essential matrix computation",
          "Epipolar geometry modeling"
        ],
        "Examples": [
          "Exam 2018 - Theoretical question",
          "Exam 2020 - Stereo derivation"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Metamers": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "Different spectral distributions that produce the same color perception under specific lighting conditions.",
      "description": "Important in color science and sensor calibration, explaining why cameras and human vision may differ in color interpretation.",
      "properties": {
        "Goal": "Understand perceptual equivalence in color representation.",
        "Applications": [
          "Color calibration",
          "Illumination modeling",
          "Spectral imaging"
        ],
        "Methods": [
          "Spectral measurement",
          "Color matching functions"
        ],
        "Examples": [
          "Exam 2017 - Definition",
          "Exam 2019 - Conceptual question"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Chromatic Aberration": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "An optical phenomenon where different wavelengths of light focus at different distances, causing color fringes in images.",
      "description": "Corrected via lens design or software post-processing to ensure color alignment in multi-channel imaging.",
      "properties": {
        "Goal": "Reduce color blurring caused by wavelength-dependent refraction.",
        "Applications": [
          "Lens design",
          "Image restoration",
          "Color correction"
        ],
        "Methods": [
          "Spectral lens calibration",
          "Image deconvolution"
        ],
        "Examples": [
          "Exam 2018 - Definition question"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Camera Extrinsics": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "Parameters that describe the position and orientation of a camera in a world coordinate system.",
      "description": "Extrinsics link the camera reference frame to world coordinates, essential for triangulation and multi-view alignment.",
      "properties": {
        "Goal": "Map coordinates between camera and world spaces.",
        "Applications": [
          "Stereo calibration",
          "SLAM",
          "3D reconstruction"
        ],
        "Methods": [
          "Rotation-translation matrix estimation",
          "PnP algorithms"
        ],
        "Examples": [
          "Exam 2014 - Camera calibration task"
        ]
      },
      "metadata": {
        "created_by": "Daris",
        "source": "Machine Vision exams (2014–2020)",
        "created_at": "2025-11-04",
        "version": "1.0"
      }
    },
    "Pattern Recognition": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Pattern recognition is the process of automatically detecting and classifying structured patterns or regularities in data, enabling machines to interpret complex signals such as images, speech, or text.",
      "description": "In computer vision, it involves identifying objects, faces, actions, or scenes by learning discriminative features from labeled data and applying statistical or structural models for decision-making.",
      "properties": {
        "Goal": "Assign meaningful labels to input data based on learned patterns.",
        "Applications": [
          "Object recognition",
          "Face detection",
          "Medical diagnosis",
          "Document analysis"
        ],
        "Methods": [
          "Statistical classification",
          "Template matching",
          "Neural networks",
          "Syntactic analysis"
        ],
        "Examples": [
          "Recognizing handwritten digits",
          "Identifying tumors in MRI scans"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07-recognition-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Statistical Pattern Recognition": {
      "type": "Framework",
      "domain": "Computer Vision",
      "definition": "Statistical pattern recognition models patterns as random variables and uses probability theory to make decisions based on feature measurements and class-conditional densities.",
      "description": "It assumes patterns are represented by feature vectors in a high-dimensional space and applies Bayesian decision theory to minimize classification error.",
      "properties": {
        "Goal": "Minimize classification error using probabilistic models.",
        "Applications": [
          "Face recognition",
          "Character recognition",
          "Speech processing"
        ],
        "Methods": [
          "Bayes classifier",
          "Maximum likelihood",
          "Gaussian mixture models",
          "k-NN"
        ],
        "Examples": [
          "Classifying iris patterns",
          "Spam email filtering"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07-recognition-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Structural Pattern Recognition": {
      "type": "Framework",
      "domain": "Computer Vision",
      "definition": "Structural pattern recognition represents patterns using symbolic data structures such as strings, trees, or graphs, emphasizing relational and syntactic organization.",
      "description": "It is suitable for patterns with explicit compositional structure, using grammars and parsing to recognize hierarchical relationships.",
      "properties": {
        "Goal": "Recognize patterns based on their compositional and relational structure.",
        "Applications": [
          "Syntactic OCR",
          "Fingerprint analysis",
          "Chemical structure recognition"
        ],
        "Methods": [
          "Formal grammars",
          "Graph matching",
          "Parsing algorithms"
        ],
        "Examples": [
          "Recognizing sentence structure",
          "Identifying molecular bonds"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07-recognition-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Feature Vector": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "A feature vector is a fixed-length numerical representation of an input pattern, where each element corresponds to a measured or derived attribute.",
      "description": "It transforms raw data into a format suitable for statistical classifiers, enabling distance-based or probabilistic decision-making in pattern space.",
      "properties": {
        "Goal": "Map complex patterns into comparable numerical space.",
        "Applications": [
          "Classification",
          "Clustering",
          "Regression"
        ],
        "Methods": [
          "Feature extraction",
          "Dimensionality reduction",
          "Normalization"
        ],
        "Examples": [
          "HOG features for pedestrians",
          "MFCC for speech",
          "Pixel intensities"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07-recognition-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Classifier": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "A classifier is a function that maps input feature vectors to discrete class labels, typically learned from labeled training examples.",
      "description": "It forms the decision stage in pattern recognition systems, using learned boundaries or probabilities to assign new observations to predefined categories.",
      "properties": {
        "Goal": "Assign input patterns to correct categories with minimal error.",
        "Applications": [
          "Object detection",
          "Face verification",
          "Anomaly detection"
        ],
        "Methods": [
          "Linear discriminant",
          "SVM",
          "Decision trees",
          "Neural networks"
        ],
        "Examples": [
          "k-NN",
          "Naive Bayes",
          "Random Forest"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07-recognition-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Template Matching": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Template matching is a pattern recognition technique that searches for a known pattern (template) in an image by sliding it across all locations and measuring similarity.",
      "description": "It is simple and effective for rigid, well-defined patterns under controlled conditions but sensitive to scale, rotation, and illumination changes.",
      "properties": {
        "Goal": "Locate instances of a known pattern within an image.",
        "Applications": [
          "Character recognition",
          "Defect detection",
          "Logo identification"
        ],
        "Methods": [
          "Correlation",
          "Normalized cross-correlation",
          "SSD"
        ],
        "Examples": [
          "Finding 'STOP' signs",
          "Matching printed circuit patterns"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07-recognition-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Bayes Classifier": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The Bayes classifier assigns an input pattern to the class with the highest posterior probability, computed using Bayes' theorem and class-conditional likelihoods.",
      "description": "It is statistically optimal when probability distributions are known, forming the theoretical foundation for many practical classifiers.",
      "properties": {
        "Goal": "Minimize expected classification error using probabilistic inference.",
        "Applications": [
          "Spam filtering",
          "Medical diagnosis",
          "Document classification"
        ],
        "Methods": [
          "P(class|features) = P(features|class) * P(class) / P(features)"
        ],
        "Examples": [
          "Naive Bayes",
          "Gaussian Bayes classifier"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07-recognition-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Object Recognition": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Object recognition is the task of identifying and localizing specific objects within images or video, assigning them semantic class labels.",
      "description": "It combines detection (where) and classification (what) and is a core capability in autonomous systems, surveillance, and augmented reality.",
      "properties": {
        "Goal": "Detect and classify objects in visual scenes.",
        "Applications": [
          "Autonomous driving",
          "Robotics",
          "Image search",
          "Security"
        ],
        "Methods": [
          "Sliding window",
          "Region proposals",
          "Deep learning (CNNs)"
        ],
        "Examples": [
          "Detecting pedestrians",
          "Recognizing furniture in rooms"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07-recognition-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Computer Vision": {
      "type": "Concept",
      "domain": "Artificial Intelligence",
      "definition": "Computer vision is an interdisciplinary field that enables computers to gain high-level understanding from digital images or videos by automating tasks that the human visual system can perform.",
      "description": "It involves processing visual data to extract meaningful information, often converting images into higher-level representations. Closely related to artificial intelligence, it focuses on application-oriented tasks rather than theoretical aspects.",
      "properties": {
        "Goal": "Automate visual perception and interpretation",
        "Applications": [
          "Image recognition",
          "3D reconstruction",
          "Video analysis",
          "Object detection"
        ],
        "Methods": [
          "Machine Learning",
          "Deep Neural Networks"
        ],
        "Examples": [
          "N/A"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01-intro-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Machine Vision": {
      "type": "Concept",
      "domain": "Artificial Intelligence",
      "definition": "Machine vision refers to the application-oriented implementation of computer vision technologies, emphasizing practical deployment over theoretical research.",
      "description": "It is more focused on real-world industrial and engineering applications compared to the broader and more academic field of computer vision.",
      "properties": {
        "Goal": "Practical automation of visual tasks",
        "Applications": [
          "Industrial inspection",
          "Robotics",
          "Quality control"
        ],
        "Methods": [
          "N/A"
        ],
        "Examples": [
          "N/A"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01-intro-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Image Processing": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Image processing involves the transformation of images into other images, often as a preprocessing step to support higher-level computer vision tasks.",
      "description": "It includes operations like filtering, enhancement, and compression, typically producing modified images rather than semantic interpretations.",
      "properties": {
        "Goal": "Transform and enhance image data",
        "Applications": [
          "Noise reduction",
          "Edge detection",
          "Image restoration"
        ],
        "Methods": [
          "Filtering",
          "Morphological operations"
        ],
        "Examples": [
          "N/A"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01-intro-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Image Understanding": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Image understanding is the process of making decisions based on images and constructing scene descriptions.",
      "description": "It goes beyond mere image transformation to interpret content, recognize objects, and infer relationships within visual scenes.",
      "properties": {
        "Goal": "Semantic interpretation of visual data",
        "Applications": [
          "Scene description",
          "Object recognition",
          "Activity understanding"
        ],
        "Methods": [
          "N/A"
        ],
        "Examples": [
          "N/A"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01-intro-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Image Recognition": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Image recognition is the ability of software to identify objects, places, people, writing, and actions in images.",
      "description": "It enables systems to classify and localize entities within visual data, forming a core capability in modern computer vision applications.",
      "properties": {
        "Goal": "Identify and classify visual elements",
        "Applications": [
          "Facial recognition",
          "Object detection",
          "Scene labeling"
        ],
        "Methods": [
          "Deep Learning",
          "Convolutional Neural Networks"
        ],
        "Examples": [
          "Classification + Localization",
          "Object Detection",
          "Semantic Segmentation",
          "Instance Segmentation"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01-intro-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Computational Photography": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Computational photography uses algorithmic techniques to enhance or extend the capabilities of digital imaging beyond traditional photography limits.",
      "description": "It combines multiple exposures, focuses, or sensor data to produce images with improved dynamic range, sharpness, or reduced noise.",
      "properties": {
        "Goal": "Overcome hardware limitations via computation",
        "Applications": [
          "Deblurring",
          "HDR imaging",
          "Light field capture"
        ],
        "Methods": [
          "Image fusion",
          "Motion deblurring"
        ],
        "Examples": [
          "Deblurring motion-blurred images",
          "Fusing short and long exposures for night scenes"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01-intro-00.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    }
  },
  "edges": [
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "produced_by",
      "target": "Thresholding"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "input_to",
      "target": "Morphological Operations"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "used_in",
      "target": "Connected Component Labeling"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "basis_for",
      "target": "Shape Analysis"
    },
    {
      "source": "Thresholding",
      "type": "produces",
      "target": "Binary Image Analysis Pipeline"
    },
    {
      "source": "Thresholding",
      "type": "part_of",
      "target": "Segmentation"
    },
    {
      "source": "Thresholding",
      "type": "precedes",
      "target": "Morphological Processing"
    },
    {
      "source": "Thresholding",
      "type": "enhanced_by",
      "target": "Preprocessing"
    },
    {
      "source": "Preprocessing",
      "type": "precedes",
      "target": "Segmentation"
    },
    {
      "source": "Preprocessing",
      "type": "improves",
      "target": "Thresholding"
    },
    {
      "source": "Preprocessing",
      "type": "part_of",
      "target": "Binary Image Analysis Pipeline"
    },
    {
      "source": "Preprocessing",
      "type": "uses",
      "target": "Image Filtering"
    },
    {
      "source": "Segmentation",
      "type": "follows",
      "target": "Preprocessing"
    },
    {
      "source": "Segmentation",
      "type": "precedes",
      "target": "Post-processing"
    },
    {
      "source": "Segmentation",
      "type": "produces",
      "target": "Binary Image Analysis Pipeline"
    },
    {
      "source": "Segmentation",
      "type": "enables",
      "target": "Shape Representation"
    },
    {
      "source": "Post-processing",
      "type": "follows",
      "target": "Segmentation"
    },
    {
      "source": "Post-processing",
      "type": "precedes",
      "target": "Labeling"
    },
    {
      "source": "Post-processing",
      "type": "uses",
      "target": "Morphological Operations"
    },
    {
      "source": "Post-processing",
      "type": "improves",
      "target": "Binary Image Analysis Pipeline"
    },
    {
      "source": "Shape Representation",
      "type": "follows",
      "target": "Labeling"
    },
    {
      "source": "Shape Representation",
      "type": "input_to",
      "target": "Recognition"
    },
    {
      "source": "Shape Representation",
      "type": "uses",
      "target": "Connected Components"
    },
    {
      "source": "Shape Representation",
      "type": "extracts",
      "target": "Geometric Features"
    },
    {
      "source": "Labeling",
      "type": "follows",
      "target": "Post-processing"
    },
    {
      "source": "Labeling",
      "type": "precedes",
      "target": "Shape Representation"
    },
    {
      "source": "Labeling",
      "type": "uses",
      "target": "Connected Components"
    },
    {
      "source": "Labeling",
      "type": "enables",
      "target": "Object Measurement"
    },
    {
      "source": "Recognition",
      "type": "follows",
      "target": "Shape Representation"
    },
    {
      "source": "Recognition",
      "type": "uses",
      "target": "Geometric Features"
    },
    {
      "source": "Recognition",
      "type": "part_of",
      "target": "Binary Image Analysis Pipeline"
    },
    {
      "source": "Recognition",
      "type": "outputs",
      "target": "Semantic Labels"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Preprocessing"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Segmentation"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Post-processing"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Labeling"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Shape Representation"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Recognition"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "foundation_for",
      "target": "Classical Computer Vision"
    },
    {
      "source": "3D Perception",
      "type": "enables",
      "target": "Scene Understanding"
    },
    {
      "source": "3D Perception",
      "type": "uses",
      "target": "Image Matching"
    },
    {
      "source": "3D Perception",
      "type": "includes",
      "target": "Depth Estimation"
    },
    {
      "source": "3D Perception",
      "type": "includes",
      "target": "3D Reconstruction"
    },
    {
      "source": "3D Perception",
      "type": "complements",
      "target": "2D Vision"
    },
    {
      "source": "Depth from Stereo",
      "type": "is_a",
      "target": "Passive 3D Perception"
    },
    {
      "source": "Depth from Stereo",
      "type": "requires",
      "target": "Camera Calibration"
    },
    {
      "source": "Depth from Stereo",
      "type": "uses",
      "target": "Stereo Matching"
    },
    {
      "source": "Depth from Stereo",
      "type": "outputs",
      "target": "Depth Map"
    },
    {
      "source": "Depth from Stereo",
      "type": "limited_by",
      "target": "Baseline Length"
    },
    {
      "source": "Structure from Motion",
      "type": "is_a",
      "target": "Multi-view 3D Reconstruction"
    },
    {
      "source": "Structure from Motion",
      "type": "uses",
      "target": "Feature Matching"
    },
    {
      "source": "Structure from Motion",
      "type": "uses",
      "target": "Random Sample Consensus (RANSAC)"
    },
    {
      "source": "Structure from Motion",
      "type": "input_to",
      "target": "Multi-view Stereo"
    },
    {
      "source": "Structure from Motion",
      "type": "enables",
      "target": "Dense Reconstruction"
    },
    {
      "source": "Monocular Depth Cues",
      "type": "enables",
      "target": "Single Image 3D"
    },
    {
      "source": "Monocular Depth Cues",
      "type": "includes",
      "target": "Occlusion"
    },
    {
      "source": "Monocular Depth Cues",
      "type": "includes",
      "target": "Perspective"
    },
    {
      "source": "Monocular Depth Cues",
      "type": "includes",
      "target": "Shading"
    },
    {
      "source": "Monocular Depth Cues",
      "type": "contrasts_with",
      "target": "Binocular Cues"
    },
    {
      "source": "Shape from Shading",
      "type": "is_a",
      "target": "Monocular Depth Cues"
    },
    {
      "source": "Shape from Shading",
      "type": "assumes",
      "target": "Lambertian Reflectance"
    },
    {
      "source": "Shape from Shading",
      "type": "requires",
      "target": "Known Light Source"
    },
    {
      "source": "Shape from Shading",
      "type": "extended_by",
      "target": "Photometric Stereo"
    },
    {
      "source": "Shape from Shading",
      "type": "outputs",
      "target": "Surface Normals"
    },
    {
      "source": "Photometric Stereo",
      "type": "extends",
      "target": "Shape from Shading"
    },
    {
      "source": "Photometric Stereo",
      "type": "requires",
      "target": "Multiple Known Lights"
    },
    {
      "source": "Photometric Stereo",
      "type": "handles",
      "target": "Unknown Albedo"
    },
    {
      "source": "Photometric Stereo",
      "type": "input_to",
      "target": "Surface Integration"
    },
    {
      "source": "Photometric Stereo",
      "type": "robust_to",
      "target": "Cast Shadows (with care)"
    },
    {
      "source": "Multi-view Stereo",
      "type": "follows",
      "target": "Structure from Motion"
    },
    {
      "source": "Multi-view Stereo",
      "type": "generalizes",
      "target": "Stereo Matching"
    },
    {
      "source": "Multi-view Stereo",
      "type": "uses",
      "target": "Photo-consistency"
    },
    {
      "source": "Multi-view Stereo",
      "type": "outputs",
      "target": "Dense Point Cloud"
    },
    {
      "source": "Multi-view Stereo",
      "type": "enables",
      "target": "Mesh Generation"
    },
    {
      "source": "Triangulation",
      "type": "core_of",
      "target": "Stereo Vision"
    },
    {
      "source": "Triangulation",
      "type": "core_of",
      "target": "Structure from Motion"
    },
    {
      "source": "Triangulation",
      "type": "requires",
      "target": "Camera Calibration"
    },
    {
      "source": "Triangulation",
      "type": "requires",
      "target": "Point Correspondences"
    },
    {
      "source": "Triangulation",
      "type": "outputs",
      "target": "3D Point"
    },
    {
      "source": "Image Matching",
      "type": "enables",
      "target": "3D Reconstruction"
    },
    {
      "source": "Image Matching",
      "type": "includes",
      "target": "Feature Matching"
    },
    {
      "source": "Image Matching",
      "type": "includes",
      "target": "Dense Matching"
    },
    {
      "source": "Image Matching",
      "type": "uses",
      "target": "Local Features"
    },
    {
      "source": "Image Matching",
      "type": "related_to",
      "target": "Geometric Transformation"
    },
    {
      "source": "Epipolar Geometry",
      "type": "foundation_for",
      "target": "Stereo Matching"
    },
    {
      "source": "Epipolar Geometry",
      "type": "encapsulated_by",
      "target": "Fundamental Matrix"
    },
    {
      "source": "Epipolar Geometry",
      "type": "encapsulated_by",
      "target": "Essential Matrix"
    },
    {
      "source": "Epipolar Geometry",
      "type": "used_in",
      "target": "Wide Baseline Matching"
    },
    {
      "source": "Epipolar Geometry",
      "type": "related_to",
      "target": "Baseline"
    },
    {
      "source": "Fundamental Matrix",
      "type": "represents",
      "target": "Epipolar Geometry"
    },
    {
      "source": "Fundamental Matrix",
      "type": "estimated_from",
      "target": "Point Correspondences"
    },
    {
      "source": "Fundamental Matrix",
      "type": "generalizes",
      "target": "Essential Matrix"
    },
    {
      "source": "Fundamental Matrix",
      "type": "input_to",
      "target": "Image Rectification"
    },
    {
      "source": "Fundamental Matrix",
      "type": "decomposed_into",
      "target": "Camera Matrices"
    },
    {
      "source": "Essential Matrix",
      "type": "special_case_of",
      "target": "Fundamental Matrix"
    },
    {
      "source": "Essential Matrix",
      "type": "requires",
      "target": "Camera Calibration"
    },
    {
      "source": "Essential Matrix",
      "type": "decomposed_into",
      "target": "Rotation Matrix"
    },
    {
      "source": "Essential Matrix",
      "type": "decomposed_into",
      "target": "Translation Direction"
    },
    {
      "source": "Essential Matrix",
      "type": "used_in",
      "target": "Relative Pose Estimation"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "used_in",
      "target": "Fundamental Matrix Estimation"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "used_in",
      "target": "Homography Estimation"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "filters",
      "target": "Outliers"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "complemented_by",
      "target": "LO-RANSAC"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "foundation_for",
      "target": "Robust Fitting"
    },
    {
      "source": "Stereo Matching",
      "type": "uses",
      "target": "Epipolar Geometry"
    },
    {
      "source": "Stereo Matching",
      "type": "requires",
      "target": "Image Rectification"
    },
    {
      "source": "Stereo Matching",
      "type": "outputs",
      "target": "Disparity Map"
    },
    {
      "source": "Stereo Matching",
      "type": "enables",
      "target": "Depth Estimation"
    },
    {
      "source": "Stereo Matching",
      "type": "part_of",
      "target": "Passive 3D Vision"
    },
    {
      "source": "Image Rectification",
      "type": "precedes",
      "target": "Stereo Matching"
    },
    {
      "source": "Image Rectification",
      "type": "uses",
      "target": "Fundamental Matrix"
    },
    {
      "source": "Image Rectification",
      "type": "produces",
      "target": "Rectified Images"
    },
    {
      "source": "Image Rectification",
      "type": "simplifies",
      "target": "Correspondence Search"
    },
    {
      "source": "Homography",
      "type": "special_case_of",
      "target": "Projective Transformation"
    },
    {
      "source": "Homography",
      "type": "estimated_from",
      "target": "Planar Correspondences"
    },
    {
      "source": "Homography",
      "type": "used_in",
      "target": "Image Stitching"
    },
    {
      "source": "Homography",
      "type": "contrasts_with",
      "target": "Fundamental Matrix"
    },
    {
      "source": "Homography",
      "type": "induced_by",
      "target": "Planar Scene"
    },
    {
      "source": "Pinhole Model",
      "type": "basis_for",
      "target": "Perspective Projection"
    },
    {
      "source": "Pinhole Model",
      "type": "approximated_by",
      "target": "Camera"
    },
    {
      "source": "Pinhole Model",
      "type": "used_in",
      "target": "Camera Calibration"
    },
    {
      "source": "Pinhole Model",
      "type": "related_to",
      "target": "Camera Obscura"
    },
    {
      "source": "Perspective Projection",
      "type": "extends",
      "target": "Pinhole Model"
    },
    {
      "source": "Perspective Projection",
      "type": "causes",
      "target": "Perspective Distortion"
    },
    {
      "source": "Perspective Projection",
      "type": "used_in",
      "target": "3D Perception"
    },
    {
      "source": "Perspective Projection",
      "type": "inverse_of",
      "target": "Triangulation"
    },
    {
      "source": "Camera Obscura",
      "type": "inspired",
      "target": "Pinhole Model"
    },
    {
      "source": "Camera Obscura",
      "type": "predecessor_of",
      "target": "Photographic Camera"
    },
    {
      "source": "Camera Obscura",
      "type": "demonstrates",
      "target": "Perspective Projection"
    },
    {
      "source": "Perspective Distortion",
      "type": "result_of",
      "target": "Perspective Projection"
    },
    {
      "source": "Perspective Distortion",
      "type": "differs_from",
      "target": "Lens Distortion"
    },
    {
      "source": "Perspective Distortion",
      "type": "mitigated_by",
      "target": "Viewpoint Normalization"
    },
    {
      "source": "Focal Length",
      "type": "parameter_in",
      "target": "Pinhole Model"
    },
    {
      "source": "Focal Length",
      "type": "affects",
      "target": "Field of View"
    },
    {
      "source": "Focal Length",
      "type": "used_in",
      "target": "Perspective Projection"
    },
    {
      "source": "Image Plane",
      "type": "part_of",
      "target": "Pinhole Model"
    },
    {
      "source": "Image Plane",
      "type": "corresponds_to",
      "target": "Sensor Plane"
    },
    {
      "source": "Image Plane",
      "type": "receives",
      "target": "Projected Rays"
    },
    {
      "source": "Simplified Imaging Model",
      "type": "includes",
      "target": "Geometric Model"
    },
    {
      "source": "Simplified Imaging Model",
      "type": "includes",
      "target": "Optical Model"
    },
    {
      "source": "Simplified Imaging Model",
      "type": "includes",
      "target": "Sensor Model"
    },
    {
      "source": "Simplified Imaging Model",
      "type": "foundation_for",
      "target": "Camera Calibration"
    },
    {
      "source": "SIFT",
      "type": "extends",
      "target": "Feature descriptor"
    },
    {
      "source": "SIFT",
      "type": "used_in",
      "target": "Bag-of-Words (BoW) Representation"
    },
    {
      "source": "SIFT",
      "type": "related_to",
      "target": "Harris Corner Detector"
    },
    {
      "source": "SIFT",
      "type": "supports",
      "target": "Structure-from-Motion"
    },
    {
      "source": "Harris Corner Detector",
      "type": "foundation_for",
      "target": "SIFT"
    },
    {
      "source": "Harris Corner Detector",
      "type": "used_in",
      "target": "Optical flow"
    },
    {
      "source": "Harris Corner Detector",
      "type": "related_to",
      "target": "K-means clustering"
    },
    {
      "source": "Harris Corner Detector",
      "type": "evaluated_by",
      "target": "Precision-Recall"
    },
    {
      "source": "Hough Transform",
      "type": "used_in",
      "target": "Image segmentation"
    },
    {
      "source": "Hough Transform",
      "type": "contrasts_with",
      "target": "Random Sample Consensus (RANSAC)"
    },
    {
      "source": "Hough Transform",
      "type": "supports",
      "target": "Shape recognition"
    },
    {
      "source": "Hough Transform",
      "type": "requires",
      "target": "Edge detection"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "related_to",
      "target": "Least squares estimation"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "used_in",
      "target": "Triangulation"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "contrasts_with",
      "target": "Hough Transform"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "foundation_for",
      "target": "Affine transformation estimation"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "extends",
      "target": "Texture analysis"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "used_with",
      "target": "Euclidean distance"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "compared_to",
      "target": "Filter bank methods"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "supports",
      "target": "Seashell classification"
    },
    {
      "source": "K-Means Clustering",
      "type": "used_in",
      "target": "Image segmentation"
    },
    {
      "source": "K-Means Clustering",
      "type": "foundation_for",
      "target": "Bag-of-Words (BoW) Representation"
    },
    {
      "source": "K-Means Clustering",
      "type": "related_to",
      "target": "Unsupervised learning"
    },
    {
      "source": "K-Means Clustering",
      "type": "evaluated_by",
      "target": "Confusion matrix"
    },
    {
      "source": "Otsu’s Thresholding Method",
      "type": "used_in",
      "target": "Background subtraction"
    },
    {
      "source": "Otsu’s Thresholding Method",
      "type": "contrasts_with",
      "target": "K-Means Clustering"
    },
    {
      "source": "Otsu’s Thresholding Method",
      "type": "supports",
      "target": "Image segmentation"
    },
    {
      "source": "Harris–Laplace Detector",
      "type": "extends",
      "target": "Harris Corner Detector"
    },
    {
      "source": "Harris–Laplace Detector",
      "type": "related_to",
      "target": "SIFT"
    },
    {
      "source": "Harris–Laplace Detector",
      "type": "supports",
      "target": "Structure-from-Motion"
    },
    {
      "source": "Photometric Stereo",
      "type": "used_in",
      "target": "Surface reconstruction"
    },
    {
      "source": "Photometric Stereo",
      "type": "related_to",
      "target": "Diffuse reflection"
    },
    {
      "source": "Photometric Stereo",
      "type": "supports",
      "target": "Structure-from-Motion"
    },
    {
      "source": "Texture",
      "type": "complements",
      "target": "Color"
    },
    {
      "source": "Texture",
      "type": "used_in",
      "target": "Texture Segmentation"
    },
    {
      "source": "Texture",
      "type": "analyzed_by",
      "target": "Texture Descriptors"
    },
    {
      "source": "Texture",
      "type": "differs_from",
      "target": "Shape"
    },
    {
      "source": "Texture",
      "type": "related_to",
      "target": "Surface Material"
    },
    {
      "source": "Texture Segmentation",
      "type": "extends",
      "target": "Segmentation"
    },
    {
      "source": "Texture Segmentation",
      "type": "uses",
      "target": "Texture Descriptors"
    },
    {
      "source": "Texture Segmentation",
      "type": "precedes",
      "target": "Object Recognition"
    },
    {
      "source": "Texture Segmentation",
      "type": "part_of",
      "target": "Mid-level Vision"
    },
    {
      "source": "Texture Descriptors",
      "type": "input_to",
      "target": "Texture Segmentation"
    },
    {
      "source": "Texture Descriptors",
      "type": "output_of",
      "target": "Feature Extraction"
    },
    {
      "source": "Texture Descriptors",
      "type": "enables",
      "target": "Texture Classification"
    },
    {
      "source": "Texture Descriptors",
      "type": "includes",
      "target": "Local Binary Patterns (LBP)"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "is_a",
      "target": "Texture Descriptors"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "robust_to",
      "target": "Illumination Changes"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "used_in",
      "target": "Texture Classification"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "extended_by",
      "target": "Multi-scale LBP"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "includes",
      "target": "Gray-Level Co-occurrence Matrix"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "contrasts_with",
      "target": "Statistical Texture Analysis"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "foundation_for",
      "target": "Texture Classification"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "used_in",
      "target": "Texture Segmentation"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "part_of",
      "target": "Statistical Texture Analysis"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "input_to",
      "target": "Haralick Features"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "analyzes",
      "target": "Spatial Dependency"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "used_in",
      "target": "Texture Descriptors"
    },
    {
      "source": "Filter Bank Methods",
      "type": "is_a",
      "target": "Texture Descriptors"
    },
    {
      "source": "Filter Bank Methods",
      "type": "inspired_by",
      "target": "Human Visual Cortex"
    },
    {
      "source": "Filter Bank Methods",
      "type": "used_in",
      "target": "Texture Segmentation"
    },
    {
      "source": "Filter Bank Methods",
      "type": "includes",
      "target": "Gabor Filters"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "contrasts_with",
      "target": "Statistical Texture Analysis"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "uses",
      "target": "Textons"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "applies_to",
      "target": "Regular Textures"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "foundation_for",
      "target": "Texture Synthesis"
    },
    {
      "source": "Motion",
      "type": "analyzed_by",
      "target": "Optical Flow"
    },
    {
      "source": "Motion",
      "type": "used_in",
      "target": "Motion Segmentation"
    },
    {
      "source": "Motion",
      "type": "enables",
      "target": "3D Reconstruction"
    },
    {
      "source": "Motion",
      "type": "related_to",
      "target": "Temporal Analysis"
    },
    {
      "source": "Motion",
      "type": "caused_by",
      "target": "Relative Movement"
    },
    {
      "source": "Optical Flow",
      "type": "is_a",
      "target": "Motion Estimation"
    },
    {
      "source": "Optical Flow",
      "type": "assumes",
      "target": "Brightness Constancy"
    },
    {
      "source": "Optical Flow",
      "type": "used_in",
      "target": "Motion Segmentation"
    },
    {
      "source": "Optical Flow",
      "type": "extended_by",
      "target": "Deep Optical Flow"
    },
    {
      "source": "Optical Flow",
      "type": "contrasts_with",
      "target": "Feature Tracking"
    },
    {
      "source": "Brightness Constancy Assumption",
      "type": "core_of",
      "target": "Differential Optical Flow"
    },
    {
      "source": "Brightness Constancy Assumption",
      "type": "violated_by",
      "target": "Specular Highlights"
    },
    {
      "source": "Brightness Constancy Assumption",
      "type": "violated_by",
      "target": "Occlusion"
    },
    {
      "source": "Brightness Constancy Assumption",
      "type": "complemented_by",
      "target": "Smoothness Constraint"
    },
    {
      "source": "Aperture Problem",
      "type": "limitation_of",
      "target": "Local Motion Estimation"
    },
    {
      "source": "Aperture Problem",
      "type": "solved_by",
      "target": "Global Optimization"
    },
    {
      "source": "Aperture Problem",
      "type": "solved_by",
      "target": "Corner Features"
    },
    {
      "source": "Aperture Problem",
      "type": "related_to",
      "target": "Optical Flow Constraint"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "is_a",
      "target": "Sparse Optical Flow"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "assumes",
      "target": "Local Smoothness"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "uses",
      "target": "Image Gradient"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "robust_to",
      "target": "Small Motions"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "extended_by",
      "target": "Pyramidal Lucas-Kanade"
    },
    {
      "source": "Horn-Schunck Method",
      "type": "is_a",
      "target": "Dense Optical Flow"
    },
    {
      "source": "Horn-Schunck Method",
      "type": "uses",
      "target": "Global Smoothness"
    },
    {
      "source": "Horn-Schunck Method",
      "type": "complements",
      "target": "Lucas–Kanade Optical Flow"
    },
    {
      "source": "Horn-Schunck Method",
      "type": "foundation_for",
      "target": "Variational Flow Methods"
    },
    {
      "source": "Motion Segmentation",
      "type": "uses",
      "target": "Optical Flow"
    },
    {
      "source": "Motion Segmentation",
      "type": "enables",
      "target": "Object Tracking"
    },
    {
      "source": "Motion Segmentation",
      "type": "part_of",
      "target": "Video Analysis"
    },
    {
      "source": "Motion Segmentation",
      "type": "related_to",
      "target": "Layered Representation"
    },
    {
      "source": "Feature Tracking",
      "type": "alternative_to",
      "target": "Dense Optical Flow"
    },
    {
      "source": "Feature Tracking",
      "type": "uses",
      "target": "Local Features"
    },
    {
      "source": "Feature Tracking",
      "type": "input_to",
      "target": "Structure from Motion"
    },
    {
      "source": "Feature Tracking",
      "type": "implemented_as",
      "target": "KLT Tracker"
    },
    {
      "source": "Specular Reflection",
      "type": "contrasts_with",
      "target": "Diffuse Reflection"
    },
    {
      "source": "Specular Reflection",
      "type": "part_of",
      "target": "Bidirectional Reflectance Distribution Function"
    },
    {
      "source": "Specular Reflection",
      "type": "used_in",
      "target": "Phong Reflection Model"
    },
    {
      "source": "Specular Reflection",
      "type": "related_to",
      "target": "Fresnel Equations"
    },
    {
      "source": "Diffuse Reflection",
      "type": "contrasts_with",
      "target": "Specular Reflection"
    },
    {
      "source": "Diffuse Reflection",
      "type": "part_of",
      "target": "Bidirectional Reflectance Distribution Function"
    },
    {
      "source": "Diffuse Reflection",
      "type": "modeled_by",
      "target": "Lambertian Reflectance"
    },
    {
      "source": "Diffuse Reflection",
      "type": "used_in",
      "target": "Shape from Shading"
    },
    {
      "source": "Light",
      "type": "measured_in",
      "target": "Wavelength"
    },
    {
      "source": "Light",
      "type": "interacts_via",
      "target": "Reflection"
    },
    {
      "source": "Light",
      "type": "interacts_via",
      "target": "Transmission"
    },
    {
      "source": "Light",
      "type": "interacts_via",
      "target": "Absorption"
    },
    {
      "source": "Light",
      "type": "perceived_as",
      "target": "Color"
    },
    {
      "source": "Visible Spectrum",
      "type": "part_of",
      "target": "Electromagnetic Spectrum"
    },
    {
      "source": "Visible Spectrum",
      "type": "perceived_by",
      "target": "Human Visual System"
    },
    {
      "source": "Visible Spectrum",
      "type": "captured_by",
      "target": "RGB Sensor"
    },
    {
      "source": "Visible Spectrum",
      "type": "basis_for",
      "target": "Color"
    },
    {
      "source": "Illuminating Source",
      "type": "contrasts_with",
      "target": "Reflecting Source"
    },
    {
      "source": "Illuminating Source",
      "type": "follows",
      "target": "Additive Rule"
    },
    {
      "source": "Illuminating Source",
      "type": "emits",
      "target": "Light"
    },
    {
      "source": "Illuminating Source",
      "type": "affects",
      "target": "Scene Illumination"
    },
    {
      "source": "Reflecting Source",
      "type": "contrasts_with",
      "target": "Illuminating Source"
    },
    {
      "source": "Reflecting Source",
      "type": "follows",
      "target": "Subtractive Rule"
    },
    {
      "source": "Reflecting Source",
      "type": "reflects",
      "target": "Light"
    },
    {
      "source": "Reflecting Source",
      "type": "depends_on",
      "target": "Surface Material"
    },
    {
      "source": "Additive Rule",
      "type": "applies_to",
      "target": "Illuminating Source"
    },
    {
      "source": "Additive Rule",
      "type": "basis_for",
      "target": "RGB Color Model"
    },
    {
      "source": "Additive Rule",
      "type": "contrasts_with",
      "target": "Subtractive Rule"
    },
    {
      "source": "Subtractive Rule",
      "type": "applies_to",
      "target": "Reflecting Source"
    },
    {
      "source": "Subtractive Rule",
      "type": "basis_for",
      "target": "CMYK Color Model"
    },
    {
      "source": "Subtractive Rule",
      "type": "contrasts_with",
      "target": "Additive Rule"
    },
    {
      "source": "Reflection",
      "type": "includes",
      "target": "Specular Reflection"
    },
    {
      "source": "Reflection",
      "type": "includes",
      "target": "Diffuse Reflection"
    },
    {
      "source": "Reflection",
      "type": "contrasts_with",
      "target": "Transmission"
    },
    {
      "source": "Reflection",
      "type": "contrasts_with",
      "target": "Absorption"
    },
    {
      "source": "Reflection",
      "type": "governed_by",
      "target": "Surface Properties"
    },
    {
      "source": "Transmission",
      "type": "contrasts_with",
      "target": "Reflection"
    },
    {
      "source": "Transmission",
      "type": "contrasts_with",
      "target": "Absorption"
    },
    {
      "source": "Transmission",
      "type": "causes",
      "target": "Refraction"
    },
    {
      "source": "Transmission",
      "type": "modeled_in",
      "target": "Optical Model"
    },
    {
      "source": "Absorption",
      "type": "contrasts_with",
      "target": "Reflection"
    },
    {
      "source": "Absorption",
      "type": "contrasts_with",
      "target": "Transmission"
    },
    {
      "source": "Absorption",
      "type": "determines",
      "target": "Perceived Color"
    },
    {
      "source": "Absorption",
      "type": "modeled_by",
      "target": "Spectral Absorption Curve"
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "type": "extends",
      "target": "Feature descriptor"
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "type": "related_to",
      "target": "Bag-of-Words (BoW) Representation"
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "type": "used_in",
      "target": "Image segmentation"
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "type": "contrasts_with",
      "target": "SIFT"
    },
    {
      "source": "Bag-of-Words (BoW) Representation",
      "type": "relies_on",
      "target": "Feature descriptor"
    },
    {
      "source": "Bag-of-Words (BoW) Representation",
      "type": "uses",
      "target": "K-Means Clustering"
    },
    {
      "source": "Bag-of-Words (BoW) Representation",
      "type": "related_to",
      "target": "Visual words"
    },
    {
      "source": "Bag-of-Words (BoW) Representation",
      "type": "contrasts_with",
      "target": "Spatial pyramid matching"
    },
    {
      "source": "Image segmentation",
      "type": "is_a_goal_of",
      "target": "Computer Vision"
    },
    {
      "source": "Image segmentation",
      "type": "uses",
      "target": "Thresholding"
    },
    {
      "source": "Image segmentation",
      "type": "uses",
      "target": "K-Means Clustering"
    },
    {
      "source": "Image segmentation",
      "type": "related_to",
      "target": "Object detection"
    },
    {
      "source": "Background Subtraction",
      "type": "extends",
      "target": "Image segmentation"
    },
    {
      "source": "Background Subtraction",
      "type": "uses",
      "target": "Otsu’s Thresholding Method"
    },
    {
      "source": "Background Subtraction",
      "type": "supports",
      "target": "Optical flow"
    },
    {
      "source": "Background Subtraction",
      "type": "related_to",
      "target": "Motion estimation"
    },
    {
      "source": "Image segmentation",
      "type": "foundation_for",
      "target": "Background Subtraction"
    },
    {
      "source": "Image segmentation",
      "type": "used_with",
      "target": "HSV Color Space"
    },
    {
      "source": "Image segmentation",
      "type": "supported_by",
      "target": "Otsu’s Thresholding Method"
    },
    {
      "source": "Image segmentation",
      "type": "used_in",
      "target": "Texture classification"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "extends",
      "target": "Optical flow"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "addresses",
      "target": "Aperture Problem"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "used_in",
      "target": "Stereo matching"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "supports",
      "target": "Structure-from-Motion"
    },
    {
      "source": "Structure from Motion",
      "type": "extends",
      "target": "Triangulation"
    },
    {
      "source": "Structure from Motion",
      "type": "requires",
      "target": "Camera Extrinsics"
    },
    {
      "source": "Structure from Motion",
      "type": "related_to",
      "target": "Epipolar Constraint"
    },
    {
      "source": "Structure from Motion",
      "type": "used_with",
      "target": "Random Sample Consensus (RANSAC)"
    },
    {
      "source": "Texture Analysis",
      "type": "foundation_for",
      "target": "Local Binary Patterns (LBP)"
    },
    {
      "source": "Texture Analysis",
      "type": "related_to",
      "target": "Texture Classification"
    },
    {
      "source": "Texture Analysis",
      "type": "used_with",
      "target": "Color features"
    },
    {
      "source": "Texture Analysis",
      "type": "evaluated_by",
      "target": "Mahalanobis distance"
    },
    {
      "source": "Filter Bank Methods",
      "type": "contrasts_with",
      "target": "Local Binary Patterns (LBP)"
    },
    {
      "source": "Filter Bank Methods",
      "type": "supports",
      "target": "Texture Classification"
    },
    {
      "source": "Filter Bank Methods",
      "type": "evaluated_by",
      "target": "Euclidean distance"
    },
    {
      "source": "Filter Bank Methods",
      "type": "used_with",
      "target": "K-Means Clustering"
    },
    {
      "source": "Edge Detection",
      "type": "foundation_for",
      "target": "Hough Transform"
    },
    {
      "source": "Edge Detection",
      "type": "used_with",
      "target": "Random Sample Consensus (RANSAC)"
    },
    {
      "source": "Edge Detection",
      "type": "supports",
      "target": "Segmentation"
    },
    {
      "source": "Edge Detection",
      "type": "related_to",
      "target": "Optical Flow"
    },
    {
      "source": "Graph-Based Segmentation",
      "type": "extends",
      "target": "Image segmentation"
    },
    {
      "source": "Graph-Based Segmentation",
      "type": "related_to",
      "target": "K-Means Clustering"
    },
    {
      "source": "Graph-Based Segmentation",
      "type": "supports",
      "target": "Object detection"
    },
    {
      "source": "Graph-Based Segmentation",
      "type": "evaluated_by",
      "target": "Confusion matrix"
    },
    {
      "source": "Local Features",
      "type": "used_in",
      "target": "Image Matching"
    },
    {
      "source": "Local Features",
      "type": "enables",
      "target": "3D Reconstruction"
    },
    {
      "source": "Local Features",
      "type": "includes",
      "target": "Keypoint Detection"
    },
    {
      "source": "Local Features",
      "type": "includes",
      "target": "Feature Description"
    },
    {
      "source": "Local Features",
      "type": "precedes",
      "target": "Feature Matching"
    },
    {
      "source": "Harris Corner Detector",
      "type": "is_a",
      "target": "Keypoint Detection"
    },
    {
      "source": "Harris Corner Detector",
      "type": "uses",
      "target": "Image Gradient"
    },
    {
      "source": "Harris Corner Detector",
      "type": "precedes",
      "target": "Feature Description"
    },
    {
      "source": "Harris Corner Detector",
      "type": "robust_to",
      "target": "Translation"
    },
    {
      "source": "Harris Corner Detector",
      "type": "not_robust_to",
      "target": "Scale Changes"
    },
    {
      "source": "Keypoint Detection",
      "type": "part_of",
      "target": "Local Features"
    },
    {
      "source": "Keypoint Detection",
      "type": "precedes",
      "target": "Feature Description"
    },
    {
      "source": "Keypoint Detection",
      "type": "input_to",
      "target": "Descriptor Extraction"
    },
    {
      "source": "Keypoint Detection",
      "type": "includes",
      "target": "Harris Corner Detector"
    },
    {
      "source": "Feature Description",
      "type": "part_of",
      "target": "Local Features"
    },
    {
      "source": "Feature Description",
      "type": "follows",
      "target": "Keypoint Detection"
    },
    {
      "source": "Feature Description",
      "type": "input_to",
      "target": "Feature Matching"
    },
    {
      "source": "Feature Description",
      "type": "uses",
      "target": "Orientation Normalization"
    },
    {
      "source": "Feature Matching",
      "type": "part_of",
      "target": "Local Features"
    },
    {
      "source": "Feature Matching",
      "type": "follows",
      "target": "Feature Description"
    },
    {
      "source": "Feature Matching",
      "type": "uses",
      "target": "Descriptor Distance"
    },
    {
      "source": "Feature Matching",
      "type": "filtered_by",
      "target": "Random Sample Consensus (RANSAC)"
    },
    {
      "source": "Image Gradient",
      "type": "used_in",
      "target": "Harris Corner Detector"
    },
    {
      "source": "Image Gradient",
      "type": "basis_for",
      "target": "Structure Tensor"
    },
    {
      "source": "Image Gradient",
      "type": "input_to",
      "target": "Edge Detection"
    },
    {
      "source": "Image Gradient",
      "type": "related_to",
      "target": "Edge"
    },
    {
      "source": "Structure Tensor",
      "type": "used_in",
      "target": "Harris Corner Detector"
    },
    {
      "source": "Structure Tensor",
      "type": "computed_from",
      "target": "Image Gradient"
    },
    {
      "source": "Structure Tensor",
      "type": "enables",
      "target": "Corner Classification"
    },
    {
      "source": "Structure Tensor",
      "type": "related_to",
      "target": "Autocorrelation Function"
    },
    {
      "source": "Pinhole Model",
      "type": "extends",
      "target": "Camera extrinsics"
    },
    {
      "source": "Pinhole Model",
      "type": "used_in",
      "target": "Triangulation"
    },
    {
      "source": "Pinhole Model",
      "type": "foundation_for",
      "target": "Epipolar constraint"
    },
    {
      "source": "Pinhole Model",
      "type": "contrasts_with",
      "target": "Perspective-3-point (P3P) problem"
    },
    {
      "source": "Radial Distortion",
      "type": "related_to",
      "target": "Pinhole Model"
    },
    {
      "source": "Radial Distortion",
      "type": "corrected_by",
      "target": "Camera calibration"
    },
    {
      "source": "Radial Distortion",
      "type": "influences",
      "target": "Triangulation accuracy"
    },
    {
      "source": "HSV Color Space",
      "type": "used_in",
      "target": "Image segmentation"
    },
    {
      "source": "HSV Color Space",
      "type": "contrasts_with",
      "target": "RGB color space"
    },
    {
      "source": "HSV Color Space",
      "type": "supports",
      "target": "Texture analysis"
    },
    {
      "source": "Depth of Field",
      "type": "related_to",
      "target": "Aperture problem"
    },
    {
      "source": "Depth of Field",
      "type": "affects",
      "target": "Structure-from-Motion"
    },
    {
      "source": "Depth of Field",
      "type": "used_in",
      "target": "Depth estimation"
    },
    {
      "source": "Epipolar Constraint",
      "type": "extends",
      "target": "Pinhole Model"
    },
    {
      "source": "Epipolar Constraint",
      "type": "used_in",
      "target": "Triangulation"
    },
    {
      "source": "Epipolar Constraint",
      "type": "foundation_for",
      "target": "Stereo matching"
    },
    {
      "source": "Epipolar Constraint",
      "type": "requires",
      "target": "Projection matrices"
    },
    {
      "source": "Aperture Problem",
      "type": "related_to",
      "target": "Optical flow"
    },
    {
      "source": "Aperture Problem",
      "type": "addressed_by",
      "target": "Lucas–Kanade method"
    },
    {
      "source": "Aperture Problem",
      "type": "contrasts_with",
      "target": "Depth of Field"
    },
    {
      "source": "Metamers",
      "type": "related_to",
      "target": "HSV Color Space"
    },
    {
      "source": "Metamers",
      "type": "contrasts_with",
      "target": "Chromatic aberration"
    },
    {
      "source": "Metamers",
      "type": "used_in",
      "target": "Color constancy models"
    },
    {
      "source": "Chromatic Aberration",
      "type": "contrasts_with",
      "target": "Metamers"
    },
    {
      "source": "Chromatic Aberration",
      "type": "influences",
      "target": "Color calibration"
    },
    {
      "source": "Chromatic Aberration",
      "type": "affects",
      "target": "Image quality assessment"
    },
    {
      "source": "Camera Extrinsics",
      "type": "extends",
      "target": "Pinhole Model"
    },
    {
      "source": "Camera Extrinsics",
      "type": "used_in",
      "target": "Triangulation"
    },
    {
      "source": "Camera Extrinsics",
      "type": "required_for",
      "target": "Structure-from-Motion"
    },
    {
      "source": "Pattern Recognition",
      "type": "enables",
      "target": "Object Recognition"
    },
    {
      "source": "Pattern Recognition",
      "type": "part_of",
      "target": "Artificial Intelligence"
    },
    {
      "source": "Pattern Recognition",
      "type": "uses",
      "target": "Feature Extraction"
    },
    {
      "source": "Pattern Recognition",
      "type": "includes",
      "target": "Statistical Pattern Recognition"
    },
    {
      "source": "Pattern Recognition",
      "type": "includes",
      "target": "Structural Pattern Recognition"
    },
    {
      "source": "Statistical Pattern Recognition",
      "type": "part_of",
      "target": "Pattern Recognition"
    },
    {
      "source": "Statistical Pattern Recognition",
      "type": "uses",
      "target": "Feature Vector"
    },
    {
      "source": "Statistical Pattern Recognition",
      "type": "foundation_for",
      "target": "Machine Learning Classifiers"
    },
    {
      "source": "Statistical Pattern Recognition",
      "type": "contrasts_with",
      "target": "Structural Pattern Recognition"
    },
    {
      "source": "Structural Pattern Recognition",
      "type": "part_of",
      "target": "Pattern Recognition"
    },
    {
      "source": "Structural Pattern Recognition",
      "type": "uses",
      "target": "Symbolic Representation"
    },
    {
      "source": "Structural Pattern Recognition",
      "type": "contrasts_with",
      "target": "Statistical Pattern Recognition"
    },
    {
      "source": "Structural Pattern Recognition",
      "type": "applies_to",
      "target": "Hierarchical Patterns"
    },
    {
      "source": "Feature Vector",
      "type": "input_to",
      "target": "Classifier"
    },
    {
      "source": "Feature Vector",
      "type": "output_of",
      "target": "Feature Extraction"
    },
    {
      "source": "Feature Vector",
      "type": "used_in",
      "target": "Statistical Pattern Recognition"
    },
    {
      "source": "Feature Vector",
      "type": "transformed_by",
      "target": "PCA"
    },
    {
      "source": "Classifier",
      "type": "core_component_of",
      "target": "Statistical Pattern Recognition"
    },
    {
      "source": "Classifier",
      "type": "uses",
      "target": "Feature Vector"
    },
    {
      "source": "Classifier",
      "type": "trained_on",
      "target": "Training Data"
    },
    {
      "source": "Classifier",
      "type": "outputs",
      "target": "Class Label"
    },
    {
      "source": "Template Matching",
      "type": "is_a",
      "target": "Pattern Recognition"
    },
    {
      "source": "Template Matching",
      "type": "requires",
      "target": "Template"
    },
    {
      "source": "Template Matching",
      "type": "sensitive_to",
      "target": "Geometric Variations"
    },
    {
      "source": "Template Matching",
      "type": "extended_by",
      "target": "Deformable Templates"
    },
    {
      "source": "Bayes Classifier",
      "type": "is_a",
      "target": "Classifier"
    },
    {
      "source": "Bayes Classifier",
      "type": "foundation_for",
      "target": "Probabilistic Classification"
    },
    {
      "source": "Bayes Classifier",
      "type": "requires",
      "target": "Prior Probability"
    },
    {
      "source": "Bayes Classifier",
      "type": "uses",
      "target": "Likelihood Model"
    },
    {
      "source": "Object Recognition",
      "type": "is_a",
      "target": "Pattern Recognition"
    },
    {
      "source": "Object Recognition",
      "type": "requires",
      "target": "Feature Extraction"
    },
    {
      "source": "Object Recognition",
      "type": "enabled_by",
      "target": "Local Features"
    },
    {
      "source": "Object Recognition",
      "type": "modernized_by",
      "target": "Deep Learning"
    },
    {
      "source": "Computer Vision",
      "type": "related_to",
      "target": "Artificial Intelligence"
    },
    {
      "source": "Computer Vision",
      "type": "related_to",
      "target": "Machine Learning"
    },
    {
      "source": "Computer Vision",
      "type": "related_to",
      "target": "Image Processing"
    },
    {
      "source": "Computer Vision",
      "type": "used_in",
      "target": "Image Recognition"
    },
    {
      "source": "Computer Vision",
      "type": "used_in",
      "target": "3D Perception"
    },
    {
      "source": "Machine Vision",
      "type": "is_a",
      "target": "Computer Vision"
    },
    {
      "source": "Machine Vision",
      "type": "contrasted_with",
      "target": "Computer Vision"
    },
    {
      "source": "Image Processing",
      "type": "supports",
      "target": "Computer Vision"
    },
    {
      "source": "Image Processing",
      "type": "differs_from",
      "target": "Image Understanding"
    },
    {
      "source": "Image Understanding",
      "type": "part_of",
      "target": "Computer Vision"
    },
    {
      "source": "Image Understanding",
      "type": "requires",
      "target": "Image Processing"
    },
    {
      "source": "Image Recognition",
      "type": "part_of",
      "target": "Computer Vision"
    },
    {
      "source": "Image Recognition",
      "type": "uses",
      "target": "Machine Learning"
    },
    {
      "source": "3D Perception",
      "type": "part_of",
      "target": "Computer Vision"
    },
    {
      "source": "3D Perception",
      "type": "related_to",
      "target": "Human Visual System"
    },
    {
      "source": "3D Perception",
      "type": "uses",
      "target": "Perceptual Cues"
    },
    {
      "source": "Multi-view Stereo",
      "type": "used_for",
      "target": "3D Perception"
    },
    {
      "source": "Multi-view Stereo",
      "type": "part_of",
      "target": "Structure from Motion"
    },
    {
      "source": "Computational Photography",
      "type": "extends",
      "target": "Image Processing"
    },
    {
      "source": "Computational Photography",
      "type": "part_of",
      "target": "Computer Vision"
    }
  ],
  "metadata": {
    "last_built": "2025-11-06 21:05:20",
    "node_count": 108,
    "edge_count": 443
  }
}