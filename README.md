# knowledge_graph

python merging.py
python generate_flashcard.py
streamlit run app_streamlit.py

Tools for Prompting:
1. https://grok.com/c/8ba9b602-ab42-400e-b047-bca0bc2f4049

1. Numerical Matrix: 1, 4, 7, 10: https://grok.com/c/da3ed06b-500e-4bdd-a602-32ddfc9fa6a9
2. Numerical Matrix: 2, 5, 8, 11: https://grok.com/c/8ba9b602-ab42-400e-b047-bca0bc2f4049
3. Numerical Matrix: 3, 6, 9: https://grok.com/c/eb638b03-5f0a-49f1-b948-102ecb4e79b7

1. Introduction to Optimization: Interesting issue that there is no much connection for the nodes/sparse https://grok.com/c/862353ec-7595-4648-ac67-b3c4192ed7b6

Let's extract topics from this PDF and check with the list of completed_nodes and incompleted_nodes

New codes: https://chatgpt.com/c/6971c79d-1f78-8333-97a3-37ab402e0cc8

Yes â€” this is exactly the right move.
Once your data becomes:

```
json_nodes/<subject_name>/*.json
```

you unlock **true scalability, clean provenance, curriculum mapping, and automation**.

Below is a **production-grade directory contract** tailored to your project and how you actually use it (graph merging, flashcards, Streamlit analytics, clustering, curriculum mining).

---

# ğŸ§± Canonical Project Directory Structure

```
education/knowledge_graph/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ json_nodes/                         â­ Raw Knowledge Sources (Authoritative)
â”‚   â”œâ”€â”€ introduction_to_optimization/
â”‚   â”‚   â”œâ”€â”€ 01_data.json
â”‚   â”‚   â”œâ”€â”€ 02_constraints.json
â”‚   â”‚   â””â”€â”€ metadata.json              â† optional subject metadata
â”‚   â”‚
â”‚   â”œâ”€â”€ machine_vision/
â”‚   â”‚   â”œâ”€â”€ 01_data.json
â”‚   â”‚   â”œâ”€â”€ segmentation.json
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â”‚
â”‚   â”œâ”€â”€ linear_algebra/
â”‚   â”‚   â”œâ”€â”€ eigen.json
â”‚   â”‚   â”œâ”€â”€ svd.json
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â”‚
â”‚   â””â”€â”€ probability/
â”‚       â”œâ”€â”€ random_variables.json
â”‚       â””â”€â”€ metadata.json
â”‚
â”‚
â”œâ”€â”€ pipelines/                         ğŸšœ Data Engineering Pipelines
â”‚   â”œâ”€â”€ merge_graph.py                 â† replaces merging.py
â”‚   â”œâ”€â”€ normalize_graph.py
â”‚   â”œâ”€â”€ validate_schema.py
â”‚   â”œâ”€â”€ generate_flashcards.py
â”‚   â””â”€â”€ extract_subject_index.py
â”‚
â”‚
â”œâ”€â”€ data/                              ğŸ“¦ Generated Artifacts
â”‚   â”œâ”€â”€ graphs/
â”‚   â”‚   â”œâ”€â”€ merged_graph.json
â”‚   â”‚   â”œâ”€â”€ graph_summary.json
â”‚   â”‚   â””â”€â”€ graph_versions/
â”‚   â”‚
â”‚   â”œâ”€â”€ flashcards/
â”‚   â”‚   â”œâ”€â”€ flashcards.json
â”‚   â”‚   â”œâ”€â”€ flashcards_by_subject.json
â”‚   â”‚   â””â”€â”€ exports/
â”‚   â”‚
â”‚   â”œâ”€â”€ indexes/
â”‚   â”‚   â”œâ”€â”€ subject_index.json
â”‚   â”‚   â””â”€â”€ node_subject_map.json
â”‚   â”‚
â”‚   â””â”€â”€ logs/
â”‚       â””â”€â”€ merge_log.txt
â”‚
â”‚
â”œâ”€â”€ app/                               ğŸ–¥ Streamlit Application
â”‚   â”œâ”€â”€ app_streamlit.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 0_Global.py
â”‚   â”‚   â”œâ”€â”€ 1_Global_Narrative.py
â”‚   â”‚   â”œâ”€â”€ 2_Find_Connection.py
â”‚   â”‚   â”œâ”€â”€ 3_Learn_From_Node.py
â”‚   â”‚   â”œâ”€â”€ 4_Flashcards.py
â”‚   â”‚   â”œâ”€â”€ 5_Flashcards_By_Subject.py
â”‚   â”‚   â””â”€â”€ 6_Node_Cleanup.py
â”‚   â”‚
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ graph_loader.py
â”‚       â”œâ”€â”€ filters.py
â”‚       â””â”€â”€ visualizations.py
â”‚
â”‚
â”œâ”€â”€ schemas/                           ğŸ“ Data Contracts
â”‚   â”œâ”€â”€ entity.schema.json
â”‚   â”œâ”€â”€ graph.schema.json
â”‚   â”œâ”€â”€ flashcard.schema.json
â”‚   â””â”€â”€ subject.schema.json
â”‚
â”‚
â”œâ”€â”€ tools/                             ğŸ›  Dev Utilities
â”‚   â”œâ”€â”€ visualize_graph.py
â”‚   â”œâ”€â”€ structure_code.py
â”‚   â””â”€â”€ migration_scripts/
â”‚
â””â”€â”€ notebooks/                         ğŸ“Š Experimental / Analysis
    â”œâ”€â”€ graph_exploration.ipynb
    â””â”€â”€ curriculum_analysis.ipynb
```

---

# ğŸ¯ Why This Structure Works for You

This structure supports:

âœ… Subject isolation
âœ… Provenance tracking
âœ… Curriculum analytics
âœ… Scalable ingestion
âœ… Multiple pipelines
âœ… Clean Streamlit separation
âœ… Versioned outputs
âœ… Future ML / embeddings

This mirrors the way you already build research tooling in your ESG projects.

---

---

# ğŸ“ json_nodes Contract (Most Important)

Your raw data becomes:

```
json_nodes/<subject_name>/<files>.json
```

### Example

```
json_nodes/
â”œâ”€â”€ introduction_to_optimization/
â”‚   â”œâ”€â”€ 01_data.json
â”‚   â”œâ”€â”€ constraints.json
â”‚   â””â”€â”€ metadata.json
â”‚
â”œâ”€â”€ machine_vision/
â”‚   â”œâ”€â”€ 01_data.json
â”‚   â””â”€â”€ metadata.json
```

---

## âœ… Subject Folder Rules

Each subject folder:

| Rule          | Description                              |
| ------------- | ---------------------------------------- |
| Folder name   | snake_case canonical subject ID          |
| Files         | Any number of JSON entity or graph files |
| metadata.json | Optional subject-level metadata          |

---

---

# ğŸ“˜ Example `metadata.json` (Optional but Recommended)

```
json_nodes/introduction_to_optimization/metadata.json
```

```json
{
  "subject_id": "introduction_to_optimization",
  "display_name": "Introduction to Optimization",
  "course": "Numerical Optimization",
  "level": "Undergraduate",
  "tags": ["convexity", "gradients", "constraints"]
}
```

Later you can show:

ğŸ“ Course grouping
ğŸ“Š Difficulty progression
ğŸ“š Curriculum mapping

---

---

# ğŸšœ Pipeline Responsibility Contracts

### `pipelines/merge_graph.py`

Responsible for:

```
json_nodes/*/*  â†’  data/graphs/merged_graph.json
```

Injects:

```json
metadata: {
   subjects: [...],
   subject_ids: [...],
   source_files: [...]
}
```

---

### `pipelines/generate_flashcards.py`

Responsible for:

```
merged_graph.json â†’ data/flashcards/flashcards.json
```

Preserves:

```
subjects
domains
entity
sources
```

---

---

# ğŸ–¥ Streamlit App Contract

App reads ONLY from:

```
data/
```

Never directly from:

```
json_nodes/
```

This keeps UI stable even if ingestion changes.

---

---

# ğŸ§­ Naming Conventions (Critical for Scaling)

| Element          | Convention       |
| ---------------- | ---------------- |
| Subject folder   | snake_case       |
| Subject display  | Title Case       |
| Entity name      | Human readable   |
| Relation types   | snake_case       |
| Pipeline scripts | verb_object.py   |
| Output artifacts | explicit folders |

---

---

# ğŸ” Migration Strategy (Your Current Repo)

You can migrate safely:

### Step 1 â€” Create folders

```bash
mkdir -p json_nodes/introduction_to_optimization
mkdir -p json_nodes/machine_vision
```

Move files accordingly.

---

### Step 2 â€” Update merging loader

Change loader to recursively scan:

```
json_nodes/*/*.json
```

Instead of flat folder.

---

### Step 3 â€” Rebuild graph

```bash
python pipelines/merge_graph.py
python pipelines/generate_flashcards.py
streamlit run app/app_streamlit.py
```

---

---

# ğŸš€ If You Want

Next I can:

âœ… Give you the exact updated recursive loader code
âœ… Auto-detect subject from folder name
âœ… Auto-load subject metadata.json
âœ… Build subject index automatically
âœ… Upgrade Streamlit filters
âœ… Add subject analytics dashboards
âœ… Add curriculum sequencing

Just tell me which piece you want to implement first.

Perfect ğŸ‘
Letâ€™s implement this **end-to-end properly** so that:

âœ… Your data lives in

```
json_nodes/<subject_name>/*.json
```

âœ… Subjects are automatically inferred from folder names
âœ… Provenance is preserved
âœ… Graph builds cleanly
âœ… Flashcards are generated with subject metadata
âœ… Streamlit keeps working without hacks
âœ… Scales to unlimited subjects

Youâ€™ll be able to just drop new folders and everything updates automatically.

---

# ğŸ§± Target Directory (Minimal Working Version)

Create these folders (if not yet):

```
education/knowledge_graph/
â”œâ”€â”€ json_nodes/
â”‚   â”œâ”€â”€ introduction_to_optimization/
â”‚   â”‚   â””â”€â”€ 01_data.json
â”‚   â””â”€â”€ machine_vision/
â”‚       â””â”€â”€ 01_data.json
â”‚
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ merge_graph.py
â”‚   â””â”€â”€ generate_flashcards.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ graphs/
â”‚   â””â”€â”€ flashcards/
â”‚
â””â”€â”€ app/
    â””â”€â”€ app_streamlit.py   (your existing app can stay)
```

---

---

# ğŸšœ PIPELINE 1 â€” Recursive Graph Merge with Subjects

Create file:

```
pipelines/merge_graph.py
```

---

## âœ… Full Code â€” `pipelines/merge_graph.py`

```python
import json
from pathlib import Path
from datetime import datetime

# ======================================================
# PATH CONFIG
# ======================================================

BASE_DIR = Path(__file__).resolve().parents[1]

JSON_NODES_DIR = BASE_DIR / "json_nodes"
OUTPUT_GRAPH = BASE_DIR / "data" / "graphs" / "merged_graph.json"

OUTPUT_GRAPH.parent.mkdir(parents=True, exist_ok=True)


# ======================================================
# UTILITIES
# ======================================================

def display_name_from_slug(slug: str) -> str:
    """Convert snake_case â†’ Title Case."""
    return slug.replace("_", " ").title()


def iter_subject_files():
    """
    Yield:
        subject_id, subject_name, json_file_path
    """
    for subject_dir in JSON_NODES_DIR.iterdir():
        if not subject_dir.is_dir():
            continue

        subject_id = subject_dir.name
        subject_name = display_name_from_slug(subject_id)

        for json_file in subject_dir.glob("*.json"):
            if json_file.name.lower() == "metadata.json":
                continue
            yield subject_id, subject_name, json_file


def load_json(path: Path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


# ======================================================
# MERGE LOGIC
# ======================================================

def merge_graph():
    graph = {
        "nodes": {},
        "edges": [],
        "metadata": {
            "built_at": datetime.utcnow().isoformat(),
            "subjects": {}
        }
    }

    print("ğŸ” Scanning subject folders...")

    for subject_id, subject_name, json_path in iter_subject_files():
        print(f"   â€¢ {subject_name:<35} â† {json_path.name}")

        payload = load_json(json_path)

        # Register subject globally
        graph["metadata"]["subjects"].setdefault(subject_id, {
            "subject_id": subject_id,
            "display_name": subject_name,
            "files": []
        })
        graph["metadata"]["subjects"][subject_id]["files"].append(json_path.name)

        # ----------------------------------------
        # Case 1 â€” Graph JSON
        # ----------------------------------------
        if isinstance(payload, dict) and "nodes" in payload and "edges" in payload:

            for node_name, node_data in payload["nodes"].items():
                node = graph["nodes"].setdefault(node_name, node_data)

                meta = node.setdefault("metadata", {})
                meta.setdefault("subjects", set()).add(subject_id)
                meta.setdefault("source_files", set()).add(json_path.name)

            for edge in payload["edges"]:
                if edge not in graph["edges"]:
                    graph["edges"].append(edge)

        # ----------------------------------------
        # Case 2 â€” Entity JSON
        # ----------------------------------------
        elif isinstance(payload, dict) and "entity" in payload:
            entity = payload["entity"]

            node = graph["nodes"].setdefault(entity, {
                "type": payload.get("type", "Concept"),
                "domain": payload.get("domain", ""),
                "definition": payload.get("definition", ""),
                "description": payload.get("description", ""),
                "properties": payload.get("properties", {}),
                "metadata": {}
            })

            meta = node.setdefault("metadata", {})
            meta.setdefault("subjects", set()).add(subject_id)
            meta.setdefault("source_files", set()).add(json_path.name)

            for rel in payload.get("relations", []):
                edge = {
                    "source": entity,
                    "type": rel.get("type", "related_to"),
                    "target": rel.get("target")
                }
                if edge not in graph["edges"]:
                    graph["edges"].append(edge)

        # ----------------------------------------
        # Case 3 â€” List of entities
        # ----------------------------------------
        elif isinstance(payload, list):
            for item in payload:
                if not isinstance(item, dict) or "entity" not in item:
                    continue

                entity = item["entity"]

                node = graph["nodes"].setdefault(entity, {
                    "type": item.get("type", "Concept"),
                    "domain": item.get("domain", ""),
                    "definition": item.get("definition", ""),
                    "description": item.get("description", ""),
                    "properties": item.get("properties", {}),
                    "metadata": {}
                })

                meta = node.setdefault("metadata", {})
                meta.setdefault("subjects", set()).add(subject_id)
                meta.setdefault("source_files", set()).add(json_path.name)

                for rel in item.get("relations", []):
                    edge = {
                        "source": entity,
                        "type": rel.get("type", "related_to"),
                        "target": rel.get("target")
                    }
                    if edge not in graph["edges"]:
                        graph["edges"].append(edge)

        else:
            print(f"âš ï¸ Skipped unsupported JSON format: {json_path}")

    # ----------------------------------------
    # Normalize metadata sets â†’ lists
    # ----------------------------------------
    for node in graph["nodes"].values():
        meta = node.get("metadata", {})
        meta["subjects"] = sorted(list(meta.get("subjects", [])))
        meta["source_files"] = sorted(list(meta.get("source_files", [])))

    print("\nâœ… Merge completed")
    print(f"   Nodes : {len(graph['nodes'])}")
    print(f"   Edges : {len(graph['edges'])}")
    print(f"   Subjects : {len(graph['metadata']['subjects'])}")

    return graph


# ======================================================
# SAVE
# ======================================================

def save_graph(graph):
    OUTPUT_GRAPH.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_GRAPH, "w", encoding="utf-8") as f:
        json.dump(graph, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ Graph saved to: {OUTPUT_GRAPH}")


# ======================================================
# MAIN
# ======================================================

if __name__ == "__main__":
    graph = merge_graph()
    save_graph(graph)
```

---

---

# ğŸƒ PIPELINE 2 â€” Subject-Aware Flashcard Generator

Create file:

```
pipelines/generate_flashcards.py
```

---

## âœ… Full Code â€” `pipelines/generate_flashcards.py`

```python
import json
from pathlib import Path

# ======================================================
# PATH CONFIG
# ======================================================

BASE_DIR = Path(__file__).resolve().parents[1]

GRAPH_FILE = BASE_DIR / "data" / "graphs" / "merged_graph.json"
FLASHCARD_FILE = BASE_DIR / "data" / "flashcards" / "flashcards.json"

FLASHCARD_FILE.parent.mkdir(parents=True, exist_ok=True)


# ======================================================
# GENERATION
# ======================================================

def generate_flashcards():
    if not GRAPH_FILE.exists():
        raise FileNotFoundError(f"Graph not found: {GRAPH_FILE}")

    with open(GRAPH_FILE, "r", encoding="utf-8") as f:
        graph = json.load(f)

    flashcards = []

    for entity, props in graph["nodes"].items():

        # Skip empty placeholder nodes
        if not any([
            props.get("definition"),
            props.get("description"),
            props.get("properties")
        ]):
            continue

        meta = props.get("metadata", {})

        subjects = meta.get("subjects", [])
        sources = meta.get("source_files", [])

        domain = props.get("domain", "General")
        definition = props.get("definition", "No definition available.")
        description = props.get("description", "")
        properties = props.get("properties", {})

        facts = []
        for k, v in properties.items():
            if isinstance(v, list):
                v = ", ".join(map(str, v))
            facts.append(f"**{k}:** {v}")

        flashcards.append({
            "entity": entity,
            "domain": domain,
            "subjects": subjects,
            "sources": sources,
            "front": f"ğŸ§© {entity}\nğŸ“˜ Domain: {domain}\nğŸ“š Subjects: {', '.join(subjects)}",
            "back": (
                f"**Definition:** {definition}\n\n"
                f"**Description:** {description}\n\n"
                + "\n".join(facts)
            ).strip()
        })

    with open(FLASHCARD_FILE, "w", encoding="utf-8") as f:
        json.dump(flashcards, f, indent=2, ensure_ascii=False)

    print(f"âœ… Generated {len(flashcards)} flashcards")
    print(f"ğŸ’¾ Saved to {FLASHCARD_FILE}")


# ======================================================
# MAIN
# ======================================================

if __name__ == "__main__":
    generate_flashcards()
```

---

---

# ğŸ–¥ STEP 3 â€” Update Flashcard UI Filter

Modify:

```
pages/5_Flashcard_per_subject.py
```

---

### ğŸ” Replace your filtering section with:

```python
# -------------------------------
# SUBJECT FILTERING
# -------------------------------

all_subjects = sorted({
    s for card in flashcards
    for s in card.get("subjects", [])
})

selected_subjects = st.multiselect(
    "Choose one or more subjects:",
    options=all_subjects,
    default=all_subjects
)

filtered_cards = [
    c for c in flashcards
    if any(s in selected_subjects for s in c.get("subjects", []))
]
```

Everything else can remain unchanged.

---

---

# â–¶ï¸ How To Run

From project root:

```bash
python pipelines/merge_graph.py
python pipelines/generate_flashcards.py
streamlit run app/app_streamlit.py
```

---

---

# âœ… What You Now Have

âœ” Recursive subject ingestion
âœ” Unlimited subjects
âœ” Provenance preserved
âœ” Flashcards filtered by subject
âœ” No brittle filename parsing
âœ” Future-proof structure

Drop a new folder:

```
json_nodes/deep_learning/01_data.json
```

Re-run pipelines â†’ instantly available everywhere.

---

---

# ğŸš€ If You Want Next

I can also give you:

âœ… Subject â†’ Chapter auto-mapping
âœ… Subject coverage dashboard
âœ… Missing concept detector
âœ… Curriculum sequencing
âœ… Export to Anki
âœ… Knowledge gap analytics

Just tell me ğŸ‘
