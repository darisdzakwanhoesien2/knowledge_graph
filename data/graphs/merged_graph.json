{
  "nodes": {
    "Continuous Optimization": {
      "type": "Concept",
      "domain": "Optimization",
      "definition": "Formulating a problem mathematically such that the objective function $f: \\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ and the constraints $c_{i}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ are considered over the continuous space $\\mathbb{R}^{n}$",
      "description": "The dimension $n$ and the number of constraints can be large. It contrasts with discrete optimization, where $\\mathbb{R}^{n}$ is replaced by a discrete set.",
      "properties": {
        "Goal": "Consider problem (1.1) on p. 3",
        "Applications": [],
        "Methods": [
          "Smooth optimization (using partial derivatives and gradients)"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Objective Function ($f$)": {
      "type": "Concept",
      "domain": "Optimization",
      "definition": "The function $f: \\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ that results from modeling or is given in an optimization problem.",
      "description": "The objective function is the quantity that is minimized (or maximized, by minimizing $-f$). Its gradient ($\\nabla f$) indicates the directions in which $f$ decreases.",
      "properties": {
        "Goal": "To be minimized or maximized",
        "Applications": [],
        "Methods": [],
        "Examples": [
          "$f(x_{1},x_{2})=x_{1}^{4}+x_{2}^{6}$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Gradient (\\nabla f)": {
      "type": "Concept",
      "domain": "Optimization, Calculus",
      "definition": "The gradient of $f$, denoted $\\nabla f$, which provides information about the local behavior of the function.",
      "description": "It appears often in computations, especially in smooth optimization, because it tells in which directions the function $f$ decreases. At a point $x$ on a level set $L_c(f)$, $\\nabla f(x)$ is orthogonal against the tangent plane of the level set.",
      "properties": {
        "Goal": "Determine direction of function decrease",
        "Applications": [
          "Smooth optimization computations",
          "Finding tangent planes/lines"
        ],
        "Methods": [
          "Computing partial derivatives"
        ],
        "Examples": [
          "$\\nabla f(x)=[4x_{1}^{3}, 6x_{2}^{5}]^T$ for $f(x_{1},x_{2})=x_{1}^{4}+x_{2}^{6}$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Level Set ($L_c(f)$)": {
      "type": "Concept",
      "domain": "Optimization, Calculus",
      "definition": "The set of points $x \\in \\mathbb{R}^n$ where the function $f$ takes a constant value $c$, defined as $L_c(f) = \\{ x \\in \\mathbb{R}^n \\mid f(x) = c \\}$.",
      "description": "These sets are typically of dimension $n-1$. When $n=2$, they are curves and are frequently referred to as 'contours of $f$'.",
      "properties": {
        "Goal": "Visualize or understand the function's landscape",
        "Applications": [
          "Drawing pictures to illustrate mathematics when $n=2$ or 3"
        ],
        "Methods": [
          "Requires computing the gradient $\\nabla f(x)$ to determine orthogonality to the tangent plane"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Convexity": {
      "type": "Concept",
      "domain": "Optimization",
      "definition": "A simplifying property of function $f$ in a convex set $S\\subset\\mathbb{R}^{n}$ where the line segment between any two points of its graph lies on or above the graph.",
      "description": "If $f$ is smooth, convexity is equivalent to having a positive semi-definite Hessian matrix everywhere in $S$, meaning the graph of $f$ is 'bowl-like' everywhere. Convexity makes problem solving easier.",
      "properties": {
        "Goal": "Simplify optimization problem solving",
        "Applications": [
          "Convex Optimization"
        ],
        "Methods": [
          "Checking if the Hessian matrix is positive semi-definite"
        ],
        "Examples": [
          "$f(x_{1},x_{2})=x_{1}^{4}+x_{2}^{6}$ is convex"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Linear Program (LP)": {
      "type": "Concept",
      "domain": "Optimization",
      "definition": "A type of optimization problem where both the objective function $f$ and the constraints functions $c_{i}$ are linear (plus possibly a constant).",
      "description": "Linear programs started modern numerical optimization requiring electronic computers. The feasible set for an LP is always a convex polygon.",
      "properties": {
        "Goal": "Solve optimization problems with strictly linear structures",
        "Applications": [
          "Modern numerical optimization"
        ],
        "Methods": [
          "Algorithms specific to LP (implied by 'LP alone is a vast area')"
        ],
        "Examples": [
          "Example (1.3 a-d) on p. 4"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Newton's Method": {
      "type": "Method",
      "domain": "Numerical Optimization",
      "definition": "An iterative method for solving a nonlinear system of equations $r(x)=0$. It computes the next iterate $x_{k+1}$ by solving for the zero of the tangent line approximation of $r(x)$ at $x_k$.",
      "description": "For $n=1$, the iterate is $x_{k+1}=x_{k}-\\frac{r(x_{k})}{r^{\\prime}(x_{k})}$. For $n>2$, the derivative is replaced with the Jacobian, $x_{k+1}=x_{k}-J(x_{k})^{-1}r(x_{k})$. This method is important for devising iterative methods for solving optimization problems.",
      "properties": {
        "Goal": "Solve a nonlinear system of equations $r(x)=0$",
        "Applications": [
          "Devising iterative methods for optimization problems"
        ],
        "Methods": [
          "Uses Taylor's theorem",
          "Uses Jacobian $J(x_k)$ (if $n>2$)"
        ],
        "Examples": [
          "Numerical example (11.31) on p. 281"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Speed of Convergence (Order p)": {
      "type": "Metric",
      "domain": "Numerical Optimization",
      "definition": "A measure used for iterative methods, quantifying how quickly the iterates $x_k$ approach the solution $x$ according to the inequality $||x-x_{k+1}||\\le C||x-x_{k}||^{p}$ for some $C>0$.",
      "description": "The order $p$ dictates the rate of convergence. If $p=1$, convergence is linear (requiring $0<C<1$); if $p=2$, convergence is quadratic and is considered fast, often sufficient for very good approximations in just 2-4 iterates.",
      "properties": {
        "Goal": "Measure the effectiveness and efficiency of an iterative algorithm",
        "Applications": [
          "Evaluating Optimization Algorithms"
        ],
        "Methods": [],
        "Examples": [
          "Linear ($p=1$), Quadratic ($p=2$), Cubic ($p=3$)"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Unconstrained Optimization": {
      "type": "Concept",
      "domain": "Optimization",
      "definition": "A mathematical minimization problem that seeks to find $x \\in \\mathbb{R}^n$ minimizing $f(x)$ with no constraints, so the feasible region $\\Omega$ is the entire space $\\mathbb{R}^n$.",
      "description": "This structure notably simplifies the optimization problem because the feasible region is $\\mathbb{R}^{n}$. The solution relies primarily on advanced calculus techniques involving the gradient of $f$ and Taylor's theorem, requiring $f$ to be sufficiently smooth.",
      "properties": {
        "Goal": "Minimizing the objective function $f(x)$ over $x \\in \\mathbb{R}^{n}$",
        "Applications": [],
        "Methods": [
          "Line Search Methods",
          "Trust Region Methods",
          "Application of Taylor's theorem"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Local Minimizer ($x^*$)": {
      "type": "Concept",
      "domain": "Optimization",
      "definition": "A point $x^{*}\\in\\mathbb{R}^{n}$ that is a global minimizer when restricted to some neighborhood of $x^{*}$ intersected with the feasible region $\\Omega$.",
      "description": "Algorithms typically find only local minimizers, which may provide a poor estimate of the global solution. However, in convex optimization problems, any local minimizer is guaranteed to be a global minimizer.",
      "properties": {
        "Goal": "Identify points satisfying $f(x^{*})\\le f(x)$ locally",
        "Applications": [],
        "Methods": [
          "Checking First-Order Necessary Condition (FONC)",
          "Checking Second-Order Sufficient Conditions (SOSC)"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "First-Order Necessary Condition (FONC)": {
      "type": "Concept",
      "domain": "Unconstrained Optimization",
      "definition": "A necessary condition that a local minimizer $x^{*}$ of a smooth function $f$ must satisfy, requiring that the gradient vanishes: $\\nabla f(x^{*})=0$.",
      "description": "This condition is derived using Taylor's theorem (2.4) and means that the point $x^{*}$ is a stationary or critical point of $f$. If this condition is not met (i.e., $\\nabla f(x^{*})\\ne0$), the function must decrease when moving in the direction $-\\nabla f(x^{*})$.",
      "properties": {
        "Goal": "Identify stationary points that are candidates for minimizers",
        "Applications": [
          "Finding stationary points by solving the linear system $\\nabla f(x)=0$"
        ],
        "Methods": [
          "Application of Taylor's theorem (2.4)"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Positive Semidefinite Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra, Optimization",
      "definition": "A symmetric matrix $A\\in\\mathbb{R}^{n\\times n}$ where all its eigenvalues are non-negative, which is mathematically equivalent to the condition $x^{T}Ax\\ge0$ for any vector $x$.",
      "description": "This matrix concept is crucial for the 'second derivative test' in optimization. For a stationary point to be a local minimizer, its Hessian matrix $\\nabla^{2}f(x^{*})$ must satisfy the positive semidefinite property, which forms the Second-Order Necessary Condition.",
      "properties": {
        "Goal": "Determine the curvature behavior (non-negative) of a function in all directions",
        "Applications": [
          "Defining Second-Order Necessary Condition (SONC)"
        ],
        "Methods": [
          "Checking eigenvalues"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization",
          "numerical_matrix"
        ],
        "source_files": [
          "02_data.json",
          "extra_00_05.json"
        ]
      }
    },
    "Second-Order Sufficient Conditions (SOSC)": {
      "type": "Concept",
      "domain": "Unconstrained Optimization",
      "definition": "Conditions guaranteeing that a point $x^{*}$ is a strict local minimizer, requiring that $x^{*}$ is a stationary point ($\\nabla f(x^{*})=0$) and that the Hessian matrix $\\nabla^{2}f(x^{*})$ is positive definite.",
      "description": "Derived using Taylor's theorem (2.6), SOSC guarantees that $f$ increases in all directions when moving away from $x^{*}$, thereby confirming a strict local minimum. These conditions are sufficient but not necessary; for instance, $f(x)=x^4$ at $x=0$ is a local minimizer but fails the positive definiteness test.",
      "properties": {
        "Goal": "Confirm a strict local minimizer exists at a stationary point",
        "Applications": [
          "Higher-dimensional second derivative test"
        ],
        "Methods": [
          "Checking for positive definiteness of $\\nabla^{2}f(x^{*})$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Saddle Point": {
      "type": "Concept",
      "domain": "Optimization",
      "definition": "A stationary point $x^*$ where the function $f$ grows in some directions and decreases in other directions when moving away from $x^{*}$.",
      "description": "Saddle points are critical points where the First-Order Necessary Condition (FONC) holds, but they are neither local minimizers nor maximizers, indicating mixed curvature in the Hessian.",
      "properties": {
        "Goal": "Classify stationary points that are not local minimizers.",
        "Applications": [],
        "Methods": [
          "Checking the Hessian matrix $\\nabla^{2}f(x^{*})$ for positive and negative eigenvalues"
        ],
        "Examples": [
          "The point $(\\frac{-1}{\\sqrt{3}},0)$ for the function $f(x)=x_{1}(x_{1}^{2}-1)+x_{2}^{2}$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Line Search Methods": {
      "type": "Method",
      "domain": "Unconstrained Optimization",
      "definition": "A family of iterative methods used to solve unconstrained minimization problems by improving the current iterate $x_{k}$ in two sequential steps: choosing a descent direction $p_{k}$ and computing the optimal step length $\\alpha$.",
      "description": "The second step involves solving a one-dimensional subproblem, $min_{\\alpha>0}f(x_{k}+\\alpha p_{k})$, which finds the optimal distance to move in the chosen direction $p_k$. This subproblem is computationally simpler than the original $n$-dimensional problem.",
      "properties": {
        "Goal": "Iteratively find a local minimizer",
        "Applications": [
          "Solving unconstrained minimization problems"
        ],
        "Methods": [
          "Choosing $p_k$ (descent direction)",
          "Solving $min_{\\alpha>0}f(x_{k}+\\alpha p_{k})$ for $\\alpha$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Descent Direction ($p_k$)": {
      "type": "Concept",
      "domain": "Optimization, Line Search Methods",
      "definition": "A direction $p_{k}$ chosen in a Line Search Method such that the objective function $f$ decreases when moving from the current iterate $x_{k}$ into that direction.",
      "description": "Choosing $p_k$ is the first step in Line Search Methods and must precede the determination of the step length $\\alpha$. Since $f$ must decrease, the computed step length $\\alpha$ must be positive.",
      "properties": {
        "Goal": "Ensure that the iteration $x_{k+1}$ results in a lower function value than $f(x_k)$",
        "Applications": [
          "Used in the first step of Line Search Methods"
        ],
        "Methods": [
          "Cleverly computing $p_k$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Quadratic Function (Optimization Tool)": {
      "type": "Concept",
      "domain": "Optimization",
      "definition": "A simple function structure used as a tool to model objective functions, even if the original function is not quadratic.",
      "description": "Quadratic functions appear in many applications and are useful for analyzing algorithm behavior, such as the speed of convergence of the steepest descent method. They are employed in both unconstrained and constrained optimization contexts, often replacing the original function via Taylor approximation.",
      "properties": {
        "Goal": "Simplify analysis and serve as an approximation model for non-quadratic functions",
        "Applications": [
          "Unconstrained optimization",
          "Constrained optimization",
          "Analyzing Steepest Descent speed"
        ],
        "Methods": [
          "Approximating original function via Taylor's theorem"
        ],
        "Examples": [
          "Solving the linear system $Qx=b$ where $\\nabla^2 f(x)=Q$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Descent Direction Condition": {
      "type": "Metric",
      "domain": "Unconstrained Optimization, Line Search Methods",
      "definition": "The algebraic condition required for a direction $p_{k}$ to ensure the objective function $f$ decreases when moving from $x_{k}$ into $p_{k}$, given by $\\nabla f(x_{k})^{T}p_{k}<0$.",
      "description": "This condition ensures that the derivative of the one-dimensional function $\\phi(\\alpha)=f(x_{k}+\\alpha p_{k})$ evaluated at $\\alpha=0$ is negative, confirming an initial decrease in $f$. It is based on the first degree Taylor's polynomial.",
      "properties": {
        "Goal": "Ensure progress towards a minimizer in Line Search Methods",
        "Applications": [
          "Choosing $p_k$ in Line Search methods"
        ],
        "Methods": [
          "Using the chain rule to find $\\phi^{\\prime}(0)$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Direction of Steepest Descent": {
      "type": "Method",
      "domain": "Unconstrained Optimization, Line Search Methods",
      "definition": "The choice of descent direction $p_{k}=-\\nabla f(x_{k})$, which yields the largest decrease in $f$ locally from $x_{k}$ for small $\\alpha>0$.",
      "description": "This is the simplest and cheapest method for choosing a descent direction, equivalent to setting $B_k=I$ in the generalized direction formula (11). Despite appearing to be the 'best choice' based on local decrease, it can suffer from slow linear convergence, especially if the objective function's condition number is large.",
      "properties": {
        "Goal": "Choose the fastest local rate of descent",
        "Applications": [
          "Steepest Descent Method"
        ],
        "Methods": [
          "Setting $p_k = -\\nabla f(x_k)$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Newton's Method (Optimization)": {
      "type": "Method",
      "domain": "Unconstrained Optimization",
      "definition": "A method for finding a stationary point $x^{*}$ by applying the standard Newton's method to solve the nonlinear system $\\nabla f(x)=0$, resulting in the iterative update $x_{k+1}=x_{k}-(\\nabla^{2}f(x_{k}))^{-1}\\nabla f(x_{k})$.",
      "description": "In the context of line search, this corresponds to using the Hessian $B_{k}=\\nabla^{2}f(x_{k})$ as the approximating matrix and setting the step length $\\alpha=1$ at every step, which is a notable advantage. It replaces the objective function $f$ with a simple quadratic function and, when close to the solution, exhibits rapid quadratic convergence.",
      "properties": {
        "Goal": "Find stationary points rapidly",
        "Applications": [
          "Small dimensional optimization problems"
        ],
        "Methods": [
          "Uses $B_k = \\nabla^2 f(x_k)$",
          "Sets $\\alpha=1$"
        ],
        "Examples": [
          "$min(x_{1}-2)^{4}+(x_{1}-2x_{2})^{2}$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Steepest Descent Method": {
      "type": "Method",
      "domain": "Unconstrained Optimization",
      "definition": "An iterative Line Search Method that consistently uses the Direction of Steepest Descent ($p_k = -\\nabla f(x_k)$) for its search direction, corresponding to setting $B_{k}=I$ in the generalized direction formula (11).",
      "description": "This method is computationally cheap because it avoids forming or inverting the Hessian. However, its convergence speed is typically linear, and if the condition number of the Hessian approximation $Q$ is large, convergence can be extremely slow.",
      "properties": {
        "Goal": "Iteratively minimize $f$ with minimal computational cost per step",
        "Applications": [
          "Used when $n$ is large and Hessian computation is too expensive"
        ],
        "Methods": [
          "Using exact step length $\\alpha_{k}$ derived from quadratic model"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Condition Number (\\kappa(Q))": {
      "type": "Metric",
      "domain": "Numerical Analysis, Optimization",
      "definition": "The ratio of the largest eigenvalue ($\\lambda_n$) to the smallest eigenvalue ($\\lambda_1$) of the matrix $Q$, defined as $\\kappa(Q) = \\frac{\\lambda_n}{\\lambda_1}$, used to analyze the convergence rate of iterative methods.",
      "description": "For the Steepest Descent method applied to a quadratic function, the condition number dictates the linear convergence rate, $C=\\frac{\\kappa(Q)-1}{\\kappa(Q)+1}$. A large condition number means $C$ is close to 1, indicating very slow convergence, requiring thousands of steps to reach an approximation.",
      "properties": {
        "Goal": "Quantify the geometry of the function's contours and predict convergence speed",
        "Applications": [
          "Analysis of Steepest Descent Method"
        ],
        "Methods": [
          "Calculation using eigenvalues of Q"
        ],
        "Examples": [
          "If $\\kappa(Q)=800$, $C=0.9975$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Quasi-Newton Methods": {
      "type": "Method",
      "domain": "Unconstrained Optimization",
      "definition": "A family of iterative methods aimed at inexpensively producing a matrix $B_{k}$ that approximates the Hessian matrix $\\nabla^{2}f(x_{k})$, used to compute the descent direction $p_{k}=-B_{k}^{-1}\\nabla f(x_{k})$.",
      "description": "These methods are used when forming and solving the full Hessian system (12) is too time-consuming, typically aiming for superlinear convergence ($1<p<2$)—a speed that is faster than linear but usually slower than quadratic. They rely on small rank-updates to improve the approximation iteratively.",
      "properties": {
        "Goal": "Achieve fast convergence without the expense of calculating the full Hessian",
        "Applications": [
          "Unconstrained minimization when $n$ is large"
        ],
        "Methods": [
          "Rank-updating schemes ($B_{k+1}=B_{k}+F_{k}$ with $rank(F_k)\\le 2$)"
        ],
        "Examples": [
          "SR1 method",
          "BFGS method"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Secant Condition": {
      "type": "Concept",
      "domain": "Quasi-Newton Methods",
      "definition": "A condition used to define the updated Hessian approximation $B_{k+1}$ in Quasi-Newton methods, derived from the first-degree Taylor approximation of $\\nabla f$ and requiring $B_{k+1}(x_{k+1}-x_{k})=\\nabla f(x_{k+1})-\\nabla f(x_{k})$.",
      "description": "This condition dictates how the updated matrix $B_{k+1}$ must relate the change in position ($s_k = x_{k+1}-x_{k}$) to the change in gradient ($y_k = \\nabla f(x_{k+1})-\\nabla f(x_{k})$). It is a finite difference approximation of the relationship between the Hessian and the gradient changes.",
      "properties": {
        "Goal": "Provide a relationship to constrain the rank-update formula $F_k$",
        "Applications": [
          "Deriving SR1 update",
          "Deriving BFGS method"
        ],
        "Methods": [
          "Using Taylor's polynomial of $\\nabla f$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "SR1 Update": {
      "type": "Method",
      "domain": "Quasi-Newton Methods",
      "definition": "The symmetric rank-one (SR1) update formula used to incrementally modify the Hessian approximation $B_{k}$ using a rank-one matrix update to satisfy the secant condition (17).",
      "description": "It is the simplest rank-update scheme, defined by $B_{k+1}=B_{k}+\\frac{1}{(y_{k}-B_{k}s_{k})^{T}s_{k}}(y_{k}-B_{k}s_{k})(y_{k}-B_{k}s_{k})^{T}$. This update can fail if the denominator is zero or very small, in which case the matrix $B_k$ is usually left unchanged.",
      "properties": {
        "Goal": "Update the Hessian approximation inexpensively while retaining symmetry",
        "Applications": [
          "Quasi-Newton Methods"
        ],
        "Methods": [
          "Enforcing $B_{k+1}$ symmetry",
          "Satisfying the Secant Condition"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Bisection Method (Step Length)": {
      "type": "Method",
      "domain": "Numerical Optimization, Line Search Methods",
      "definition": "A basic algorithm for finding a zero of a continuous, univariate function, specifically applied here to find $\\alpha_{k}$ by locating a zero of $\\phi^{\\prime}(\\alpha)=\\nabla f(x_{k}+\\alpha p_{k})^{T}p_{k}$.",
      "description": "The method iteratively halves an interval $[0, \\hat{\\alpha}]$ that is known to contain a zero of $\\phi^{\\prime}$ because $\\phi^{\\prime}$ changes sign across the interval. Since $\\phi^{\\prime}(0)<0$, it requires guessing a point $\\hat{\\alpha}>0$ such that $\\phi^{\\prime}(\\hat{\\alpha})>0$.",
      "properties": {
        "Goal": "Find an approximate optimal step length $\\alpha_{k}$ for the 1D subproblem (7)",
        "Applications": [
          "Univariate zero finding"
        ],
        "Methods": [
          "Requires only function evaluations (no derivatives)",
          "Halving the search interval"
        ],
        "Examples": [
          "Example with $\\phi^{\\prime}(\\alpha)=\\alpha^{6}-\\alpha-1$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Nonlinear Least Squares Problems": {
      "type": "Concept",
      "domain": "Optimization, Data Fitting",
      "definition": "Structured unconstrained optimization problems that arise when minimizing the sum of squares of $m$ residual functions $r_{j}(x)$, where $r_{j}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ are smooth functions, often under the assumption $m\\ge n$.",
      "description": "These problems frequently appear when tuning a model to achieve the best fit for given data, such as in machine learning or neural network applications. Due to their structure, simplifying tricks can be applied, particularly when combined with Line Search and Quasi-Newton methods.",
      "properties": {
        "Goal": "Find parameters $x$ that minimize the residual errors between a model and observed data",
        "Applications": [
          "Machine learning",
          "Neural network applications",
          "Model tuning"
        ],
        "Methods": [
          "Structured Quasi-Newton methods"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Bisection Method": {
      "type": "Root-Finding Algorithm",
      "domain": "Numerical Optimization / Line Search",
      "definition": "A robust bracketing method that locates a root of a continuous function $\\phi(\\alpha)$ by repeatedly halving an interval $[a, b]$ where $\\phi(a) \\le 0 \\le \\phi(b)$ (or vice versa) until the interval length is below a tolerance.",
      "description": "Used in line search to find a step length $\\alpha_k > 0$ satisfying the Wolfe conditions. Converges **linearly** with rate $C = \\frac{1}{2}$ because the uncertainty interval is halved at each iteration.",
      "properties": {
        "Goal": "Find $\\alpha_k$ such that $\\phi(\\alpha_k) \\le \\phi(0) + c_1 \\alpha_k \\phi'(0)$ and $|\\phi'(\\alpha_k)| \\le c_2 |\\phi'(0)|$ (Wolfe conditions).",
        "Applications": [
          "Exact line search fallback",
          "Backtracking line search when Armijo fails"
        ],
        "Methods": [
          "Initialize bracket $[\\alpha_{\\text{low}}, \\alpha_{\\text{high}}]$ with opposite signs or satisfying bounds",
          "Set midpoint $\\alpha_{\\text{mid}} = \\frac{\\alpha_{\\text{low}} + \\alpha_{\\text{high}}}{2}$",
          "Shrink interval based on $\\phi(\\alpha_{\\text{mid}})$ sign or Wolfe violation"
        ],
        "Examples": [
          "Solving $\\phi'(\\alpha) = \\alpha^5 - \\alpha - 1 = 0$ in $[1, 2]$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Least Squares Problems": {
      "type": "Optimization Problem Type",
      "domain": "Unconstrained Optimization",
      "definition": "Minimize $f(x) = \\frac{1}{2} \\| r(x) \\|^2 = \\frac{1}{2} \\sum_{j=1}^m r_j(x)^2$, where $r: \\mathbb{R}^n \\to \\mathbb{R}^m$ is the vector of **residual functions** $r_j(x)$.",
      "description": "Arises when overdetermined systems $r_j(x) = 0$ ($m \\ge n$) have no exact solution. Transforms root-finding into minimization. Special structure: $\\nabla f(x) = J(x)^T r(x)$, $\\nabla^2 f(x) = J(x)^T J(x) + \\sum r_j(x) \\nabla^2 r_j(x)$.",
      "properties": {
        "Goal": "Find best-fit parameters $x^*$ minimizing sum of squared residuals.",
        "Applications": [
          "Data fitting",
          "Machine learning (regression)",
          "Parameter estimation"
        ],
        "Methods": [
          "Gauss–Newton (ignores second-order residual terms)",
          "Levenberg–Marquardt (adds damping)",
          "Full Newton (uses exact Hessian)"
        ],
        "Examples": [
          "Linear regression: $r_j(x) = a_j^T x - b_j$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Linear Least Squares Problem (Linear Regression)": {
      "type": "Optimization Problem Type",
      "domain": "Linear Algebra / Regression",
      "definition": "Minimize $f(x) = \\frac{1}{2} \\| A x - b \\|^2$, where $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $m \\ge n$.",
      "description": "Closed-form solution via **normal equations**: $A^T A x = A^T b$. Numerically stable via **QR factorization**: $A = QR$, solve $R x = Q^T b$.",
      "properties": {
        "Goal": "Find $x^*$ minimizing $\\sum (a_j^T x - b_j)^2$.",
        "Applications": [
          "Linear regression",
          "Polynomial fitting"
        ],
        "Methods": [
          "Normal equations (if $A^T A$ well-conditioned)",
          "QR decomposition (preferred)",
          "SVD (for rank-deficient cases)"
        ],
        "Examples": [
          "Fit $y = x_1 + x_2 t$: residuals $r_j = x_1 + x_2 t_j - y_j$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Gauss–Newton Method": {
      "type": "Quasi-Newton Algorithm",
      "domain": "Nonlinear Least Squares",
      "definition": "Iterative method for nonlinear least squares: at $x_k$, solve $J_k^T J_k p_k = -J_k^T r_k$ for step $p_k$, where $J_k = J(x_k)$ is the Jacobian of $r$.",
      "description": "Approximates Hessian as $B_k = J_k^T J_k$, neglecting $\\sum r_j \\nabla^2 r_j$. Valid when residuals are small at solution. Step satisfies $p_k^T \\nabla f_k = -\\| J_k p_k \\|^2 \\le 0$ (descent unless $J_k p_k = 0$).",
      "properties": {
        "Goal": "Efficiently solve nonlinear least squares without computing second derivatives.",
        "Applications": [
          "Curve fitting",
          "Nonlinear regression"
        ],
        "Methods": [
          "Compute $J_k$, $r_k$",
          "Solve $(J_k^T J_k) p_k = -J_k^T r_k$ (via QR/Cholesky)",
          "Line search: $x_{k+1} = x_k + \\alpha_k p_k$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Residual Functions ($r_j(x)$)": {
      "type": "Model Component",
      "domain": "Least Squares",
      "definition": "Scalar functions $r_j: \\mathbb{R}^n \\to \\mathbb{R}$, $j=1,\\dots,m$, representing model misfit: $r_j(x) = \\text{model}_j(x) - \\text{data}_j$.",
      "description": "Stacked into vector $r(x) = [r_1(x), \\dots, r_m(x)]^T$. Objective: $f(x) = \\frac{1}{2} \\| r(x) \\|^2$. Gradient: $\\nabla f(x) = J(x)^T r(x)$, where $J(x) = \\nabla r(x)$ is $m \\times n$ Jacobian.",
      "properties": {
        "Goal": "Quantify error between model and observations.",
        "Applications": [
          "Define $f(x)$, $\\nabla f(x)$, $J(x)$"
        ],
        "Methods": [],
        "Examples": [
          "Linear: $r_j(x) = a_j^T x - b_j$",
          "Nonlinear: $r_j(x) = e^{x_1 t_j} + x_2 \\cos(x_3 t_j) - y_j$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Levenberg–Marquardt Method": {
      "type": "Trust-Region Algorithm",
      "domain": "Nonlinear Least Squares",
      "definition": "Trust-region method for nonlinear least squares: solve $\\min_{\\|p\\| \\le \\Delta_k} \\| J_k p + r_k \\|^2$ approximately by solving $(J_k^T J_k + \\mu_k I) p = -J_k^T r_k$, adjusting $\\mu_k > 0$ to control step size.",
      "description": "Combines Gauss–Newton ($\\mu_k = 0$) and steepest descent ($\\mu_k \\to \\infty$). Robust when $J_k^T J_k$ is singular or residuals are large.",
      "properties": {
        "Goal": "Solve nonlinear least squares with global convergence.",
        "Applications": [
          "Curve fitting",
          "Computer vision (bundle adjustment)"
        ],
        "Methods": [
          "Choose $\\mu_k$ via trust-region logic or heuristic",
          "Solve damped normal equations",
          "Update $\\Delta_k$ based on $\\rho_k$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Constrained Optimization Problem": {
      "type": "Optimization Problem Type",
      "domain": "Optimization Theory",
      "definition": "Minimize an objective function $f(x)$ subject to equality constraints $c_i(x) = 0$ for $i \\in \\mathcal{E}$ and inequality constraints $c_i(x) \\ge 0$ for $i \\in \\mathcal{I}$, defining the feasible set $\\Omega = \\{x \\in \\mathbb{R}^n \\mid c_i(x) = 0, \\, i \\in \\mathcal{E}; \\, c_i(x) \\ge 0, \\, i \\in \\mathcal{I}\\}$.",
      "description": "Extends unconstrained optimization by restricting the search to $\\Omega$. Most algorithms find only **local** minimizers within a neighborhood intersected with $\\Omega$.",
      "properties": {
        "Goal": "Find $x^* \\in \\Omega$ such that $f(x^*) \\le f(x)$ for all $x \\in \\Omega \\cap \\mathcal{N}(x^*)$ (local minimizer).",
        "Applications": [
          "Engineering design",
          "Economics",
          "Machine learning (e.g., SVM, constrained regression)"
        ],
        "Methods": [
          "KKT conditions for stationarity",
          "Active-set methods",
          "Interior-point methods",
          "Penalty/barrier methods"
        ],
        "Examples": [
          "$\\min x_1 + x_2$ s.t. $x_1^2 + x_2^2 = 2$ (equality)",
          "$\\min x_1 + x_2$ s.t. $2 - x_1^2 - x_2^2 \\ge 0$ (inequality)",
          "QP: $\\min \\frac{1}{2} x^T Q x - b^T x$ s.t. $C x \\le d$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Karush-Kuhn-Tucker (KKT) Conditions": {
      "type": "First-Order Necessary Conditions",
      "domain": "Constrained Optimization Theory",
      "definition": "For a local minimizer $x^* \\in \\Omega$ under a constraint qualification (e.g., LICQ), there exist $\\lambda^*_i$ (Lagrange multipliers) such that:\\n1. **Stationarity**: $\\nabla f(x^*) = \\sum_{i \\in \\mathcal{E} \\cup \\mathcal{I}} \\lambda^*_i \\nabla c_i(x^*)$\\\\\n2. **Primal feasibility**: $c_i(x^*) = 0$ ($i \\in \\mathcal{E}$), $c_i(x^*) \\ge 0$ ($i \\in \\mathcal{I}$)$\\\\\n3. **Dual feasibility**: $\\lambda^*_i \\ge 0$ ($i \\in \\mathcal{I}$)$\\\\\n4. **Complementary slackness**: $\\lambda^*_i c_i(x^*) = 0$ ($i \\in \\mathcal{I}$)$\\\\\n(Theorem 12.1, p. 321)",
      "description": "Generalizes Lagrange multiplier method to inequalities. First-order necessary conditions for local optimality under CQ. Equivalent to $\\nabla_x \\mathcal{L}(x^*, \\lambda^*) = 0$, feasibility, and complementarity.",
      "properties": {
        "Goal": "Characterize candidate local minimizers via a system of equations and inequalities.",
        "Applications": [
          "Optimality certification",
          "Algorithm termination (e.g., SQP, IPM)",
          "Duality theory"
        ],
        "Methods": [
          "Solve $\\nabla_x \\mathcal{L} = 0$, $\\lambda_i c_i = 0$, $\\lambda_i \\ge 0$, $c_i \\ge 0$",
          "Newton-type methods on KKT system"
        ],
        "Examples": [
          "QP: solve $Q x - b = C^T \\lambda$, $\\lambda \\ge 0$, $\\lambda \\circ (C x - d) = 0$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Lagrangian Function ($\\mathcal{L}(x, \\lambda)$)": {
      "type": "Auxiliary Function",
      "domain": "Constrained Optimization Theory",
      "definition": "$\\mathcal{L}(x, \\lambda) = f(x) - \\sum_{i \\in \\mathcal{E}} \\lambda_i c_i(x) - \\sum_{i \\in \\mathcal{I}} \\lambda_i c_i(x)$",
      "description": "Combines objective and constraints into a single function. KKT stationarity is $\\nabla_x \\mathcal{L}(x^*, \\lambda^*) = 0$. For equality-only: $\\mathcal{L}(x, \\lambda) = f(x) - \\lambda^T c(x)$",
      "properties": {
        "Goal": "Convert constrained problem into unconstrained saddle-point problem.",
        "Applications": [
          "KKT derivation",
          "Duality",
          "SQP methods"
        ],
        "Methods": [
          "Form $\\mathcal{L}$, set $\\nabla_x \\mathcal{L} = 0$",
          "Dual function: $q(\\lambda) = \\inf_x \\mathcal{L}(x, \\lambda)$"
        ],
        "Examples": [
          "Eigenvalue: $\\mathcal{L}(x, \\lambda) = x^T Q x - \\lambda (x^T x - 1)$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Tangent Cone ($T_\\Omega(x)$)": {
      "type": "Geometric Set",
      "domain": "Constrained Optimization Geometry",
      "definition": "The set of directions $d$ such that there exists a sequence $x_k \\in \\Omega$, $x_k \\to x$, and $t_k \\downarrow 0$ with $(x_k - x)/t_k \\to d$.",
      "description": "Limit of feasible directions from $x$. Contains all $d$ for which a feasible path exists in $\\Omega$. Under LICQ, $T_\\Omega(x) = \\mathcal{F}(x)$ (linearized cone).",
      "properties": {
        "Goal": "Describe local geometry of feasible set at $x$.",
        "Applications": [
          "First-order necessary conditions (Theorem 12.3)",
          "Convergence analysis"
        ],
        "Methods": [],
        "Examples": [
          "Equality $c(x)=0$: $T_\\Omega(x) = \\ker \\nabla c(x)^T$",
          "Inequality at boundary: half-space tangent"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Linear Independence Constraint Qualification (LICQ)": {
      "type": "Constraint Qualification",
      "domain": "Constrained Optimization Theory",
      "definition": "At $x \\in \\Omega$, the gradients $\\{\\nabla c_i(x) \\mid i \\in \\mathcal{A}(x)\\}$ are linearly independent.",
      "description": "Strong CQ ensuring KKT multipliers are unique and $T_\\Omega(x) = \\mathcal{F}(x)$. Prevents degenerate constraint configurations.",
      "properties": {
        "Goal": "Ensure well-posedness of KKT system and geometric regularity.",
        "Applications": [
          "Guarantee KKT necessity",
          "Uniqueness of $\\lambda^*$"
        ],
        "Methods": [
          "Check rank of Jacobian of active constraints = |$\\mathcal{A}(x)$|"
        ],
        "Examples": [
          "Holds for $x^T x = 1$, $x \\ne 0$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Active Set of Indices ($\\mathcal{A}(x)$)": {
      "type": "Index Set",
      "domain": "Constrained Optimization",
      "definition": "$\\mathcal{A}(x) = \\mathcal{E} \\cup \\{i \\in \\mathcal{I} \\mid c_i(x) = 0\\}$ — indices of binding constraints at $x$.",
      "description": "Determines which constraints are active (tight). Only these affect local geometry and KKT conditions.",
      "properties": {
        "Goal": "Identify binding constraints for local analysis.",
        "Applications": [
          "LICQ check",
          "Active-set algorithms",
          "Reduced KKT system"
        ],
        "Methods": [],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Farkas' Lemma": {
      "type": "Alternative Theorem",
      "domain": "Convex Analysis / Linear Algebra",
      "definition": "For $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, exactly one holds:\\n1. $\\exists x \\ge 0$ s.t. $A x = b$\\\\\n2. $\\exists y$ s.t. $A^T y \\ge 0$, $b^T y < 0$",
      "description": "Fundamental result in conic duality. Used to prove KKT: if $-\\nabla f(x) \\notin \\mathcal{K}$ (cone of active gradients), then descent direction exists.",
      "properties": {
        "Goal": "Prove impossibility of infeasibility via separating hyperplane.",
        "Applications": [
          "Proof of KKT (via cone separation)",
          "LP duality",
          "Support vector machines"
        ],
        "Methods": [
          "Geometric: separating hyperplane",
          "Algebraic: Gordan, Motzkin theorems"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Trust Region Methods (TRMs)": {
      "type": "Optimization Algorithm",
      "domain": "Unconstrained Optimization",
      "definition": "An iterative method for solving unconstrained optimization problems $\\min f(x)$ by solving a sequence of constrained quadratic subproblems within a trust region around the current iterate $x_k$.",
      "description": "At each iteration, a step $p_k$ is computed by minimizing a quadratic model $m_k(p)$ of $f(x_k + p)$ subject to $\\|p\\| \\le \\Delta_k$. The radius $\\Delta_k$ is adjusted based on how well the model predicts the actual function decrease.",
      "properties": {
        "Goal": "Find a local minimizer of $f(x)$ with robust global convergence properties.",
        "Applications": [
          "Large-scale nonlinear optimization",
          "Non-convex problems where line search may fail"
        ],
        "Methods": [
          "Solve Trust Region Subproblem (52) at each $x_k$",
          "Use KKT conditions to characterize optimal $p_k$",
          "Update $\\Delta_k$ using comparison ratio $\\rho_k$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Trust Region Subproblem (52)": {
      "type": "Constrained Quadratic Program",
      "domain": "Trust Region Methods",
      "definition": "$\\min_{p \\in \\mathbb{R}^n} m_k(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta_k$, where $m_k(p) = f_k + g_k^T p + \\frac{1}{2} p^T B_k p$.",
      "description": "Core subproblem solved at each TRM iteration. The solution $p_k$ satisfies $(B_k + \\lambda I)p_k = -g_k$, $\\lambda \\ge 0$, $\\lambda(\\|p_k\\| - \\Delta_k) = 0$, and $B_k + \\lambda I \\succeq 0$.",
      "properties": {
        "Goal": "Compute a step $p_k$ that sufficiently reduces the model within the trust region.",
        "Applications": [
          "Step generation in TRM"
        ],
        "Methods": [
          "Case 1 (interior): $\\lambda = 0$, solve $B_k p = -g_k$ if $\\|p\\| \\le \\Delta_k$",
          "Case 2 (boundary): solve $(B_k + \\lambda I)p = -g_k$, $\\|p\\| = \\Delta_k$ via root-finding in $\\lambda$",
          "Approximate via Krylov subspace methods"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Model Function ($m_k(p)$)": {
      "type": "Quadratic Approximation",
      "domain": "Trust Region Methods",
      "definition": "Second-order Taylor expansion of $f$ around $x_k$: $m_k(p) = f(x_k) + \\nabla f(x_k)^T p + \\frac{1}{2} p^T B_k p$, where $B_k \\approx \\nabla^2 f(x_k)$.",
      "description": "Local surrogate for $f$. When $B_k = \\nabla^2 f(x_k)$, the approximation error satisfies $|f(x_k + p) - m_k(p)| = O(\\|p\\|^3)$.",
      "properties": {
        "Goal": "Provide a tractable local model for minimization within the trust region.",
        "Applications": [
          "Defining Trust Region Subproblem (52)"
        ],
        "Methods": [],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Comparison Ratio ($\\rho_k$)": {
      "type": "Performance Metric",
      "domain": "Trust Region Methods",
      "definition": "$\\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}$ — ratio of actual to predicted reduction.",
      "description": "Measures model quality. If $\\rho_k \\ge \\eta_1 > 0$ (e.g., 0.1), accept step and possibly increase $\\Delta_k$. If $\\rho_k < \\eta_1$, reject step and shrink $\\Delta_k$.",
      "properties": {
        "Goal": "Assess reliability of quadratic model $m_k$ to control trust region radius.",
        "Applications": [
          "Step acceptance/rejection",
          "Adaptive radius update"
        ],
        "Methods": [
          "Accept if $\\rho_k \\ge 0.1$, increase $\\Delta_k$ if $\\rho_k \\ge 0.75$",
          "Reject and shrink $\\Delta_k$ if $\\rho_k < 0.1$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Krylov Subspace Approximation (TR Subproblem)": {
      "type": "Approximation Technique",
      "domain": "Trust Region Subproblem Solving",
      "definition": "Approximate solution of the TR subproblem by restricting $p$ to a low-dimensional Krylov subspace $\\mathcal{K}_j = \\operatorname{span}\\{g_k, B_k^{-1}g_k, \\ldots, (B_k^{-1})^{j-1}g_k\\}$.",
      "description": "For $j=2$, reduces to 2D problem in $\\operatorname{span}\\{g_k, B_k^{-1}g_k\\}$. Fast and effective when $B_k$ is large/sparse. Equivalent to CG steps on the Newton system.",
      "properties": {
        "Goal": "Solve TR subproblem approximately when exact solution is costly.",
        "Applications": [
          "Large-scale TRMs"
        ],
        "Methods": [
          "Project subproblem onto $\\mathcal{K}_j$",
          "Solve reduced 2D TR subproblem in $(\\alpha, \\beta)$",
          "Use Steihaug-Toint truncated CG"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Lagrangian for TR Subproblem ($\\mathcal{L}(p,\\lambda)$)": {
      "type": "Lagrangian Function",
      "domain": "Trust Region Methods",
      "definition": "$\\mathcal{L}(p, \\lambda) = m_k(p) + \\lambda (\\|p\\|^2 - \\Delta_k^2)$, where constraint $g(p) = \\Delta_k^2 - \\|p\\|^2 \\ge 0$.",
      "description": "Used to derive KKT conditions: $\\nabla_p \\mathcal{L} = g_k + B_k p + 2\\lambda p = 0$, $\\lambda \\ge 0$, $\\lambda(\\|p\\|^2 - \\Delta_k^2) = 0$, $B_k + 2\\lambda I \\succeq 0$.",
      "properties": {
        "Goal": "Formulate optimality conditions for the TR subproblem.",
        "Applications": [
          "Deriving $(B_k + \\lambda I)p = -g_k$ with $\\lambda \\ge 0$"
        ],
        "Methods": [
          "Stationarity: $g_k + (B_k + 2\\lambda I)p = 0$",
          "Complementary slackness and dual feasibility"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Krylov Subspace Approximation (TRM)": {
      "type": "Approximation Technique",
      "domain": "Trust Region Methods",
      "definition": "A technique to approximately solve the Trust Region Subproblem (52) by restricting the step $$ p $$ to a low-dimensional Krylov subspace, typically $$ \\operatorname{span}\\{q, B^{-1}q\\} $$, reducing the problem to a 2D constrained optimization.",
      "description": "Instead of solving the full $$ n $$-dimensional quadratic subproblem exactly, it solves a reduced 2D problem in coefficients $$ (\\alpha, \\beta) $$ using an orthonormal basis $$ \\{\\hat{q}, \\tilde{q}\\} $$ of the subspace (58). This is equivalent to a 2-step Krylov method for the linear system (54).",
      "properties": {
        "Goal": "Efficiently approximate the solution to the Trust Region Subproblem (52).",
        "Applications": [
          "Step computation in Trust Region Methods when $$ n $$ is large"
        ],
        "Methods": [
          "Construct $$ p = [\\hat{q}, \\tilde{q}] \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix} $$ with orthonormal basis of $$ \\operatorname{span}\\{q, B^{-1}q\\} $$ (58).",
          "Minimize reduced quadratic model subject to $$ \\|[\\alpha, \\beta]^T\\| \\le \\Delta_k $$.",
          "Extend to higher-order Krylov: $$ \\operatorname{span}\\{q, B^{-1}q, \\ldots, (B^{-1})^j q\\} $$ for small $$ j $$."
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Second Order Necessary Conditions (Constrained)": {
      "type": "Necessary Condition/Theorem",
      "domain": "Constrained Optimization Theory",
      "definition": "At a local minimizer $$ x^* $$ with Lagrange multipliers $$ \\lambda^* $$, the Hessian of the Lagrangian must satisfy $$ w^T \\nabla_{xx}^2 \\mathcal{L}(x^*, \\lambda^*) w \\ge 0 $$ for all $$ w \\in \\mathcal{C}(x^*, \\lambda^*) $$ (Theorem 12.5, p. 332).",
      "description": "This is a necessary condition for local optimality. It applies only on the critical cone — directions where the linearized constraints are satisfied and the reduced gradient vanishes.",
      "properties": {
        "Goal": "Verify whether a KKT point $$ x^* $$ can be a local minimizer.",
        "Applications": [
          "Classification of stationary points"
        ],
        "Methods": [
          "Evaluate quadratic form $$ w^T \\nabla_{xx}^2 \\mathcal{L}(x^*, \\lambda^*) w $$ on $$ \\mathcal{C}(x^*, \\lambda^*) $$",
          "For 1D active constraint: check second derivative along feasible arc"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Second Order Sufficient Conditions (Constrained)": {
      "type": "Sufficient Condition/Theorem",
      "domain": "Constrained Optimization Theory",
      "definition": "If $$ x^* $$ is a KKT point and $$ w^T \\nabla_{xx}^2 \\mathcal{L}(x^*, \\lambda^*) w > 0 $$ for all $$ w \\in \\mathcal{C}(x^*, \\lambda^*) $$, $$ w \\ne 0 $$, then $$ x^* $$ is a strict local minimizer (Theorem 12.6, p. 333).",
      "description": "Stronger than necessary conditions, this guarantees local optimality when the Lagrangian Hessian is positive definite on the critical cone.",
      "properties": {
        "Goal": "Confirm that a KKT point $$ x^* $$ is a strict local minimizer.",
        "Applications": [
          "Proving local optimality in constrained problems"
        ],
        "Methods": [
          "Verify positive definiteness of $$ \\nabla_{xx}^2 \\mathcal{L} $$ on $$ \\mathcal{C}(x^*, \\lambda^*) $$",
          "For linear constraints: check $$ \\nabla^2 f(x^*) \\succ 0 $$ on null space of active constraints"
        ],
        "Examples": [
          "In example (29), point $$ (-1,-1) $$ with $$ \\lambda^* = 1/2 $$ has positive definite $$ \\nabla_{xx}^2 \\mathcal{L} $$ on critical cone → local minimizer."
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Critical Cone ($$ \\mathcal{C}(x^*, \\lambda^*) $$)": {
      "type": "Geometric Concept/Set",
      "domain": "Constrained Optimization Geometry",
      "definition": "The set of directions $$ w $$ such that:\n1. $$ w $$ is in the linearized feasible cone: $$ \\nabla c_i(x^*)^T w \\ge 0 $$ for all $$ i $$ with $$ c_i(x^*)=0 $$,\n2. $$ \\nabla f(x^*)^T w = 0 $$ (critical directions).",
      "description": "Subset of the feasible direction cone $$ \\mathcal{F}(x^*) $$ where first-order improvement is zero. Second-order conditions are checked only on this cone. For equality constraints, it is the null space of $$ \\nabla c(x^*)^T $$.",
      "properties": {
        "Goal": "Define the subspace for second-order optimality tests.",
        "Applications": [
          "Second-order necessary/sufficient conditions"
        ],
        "Methods": [
          "Active inequalities with $$ \\lambda_i^* > 0 $$: $$ w \\perp \\nabla c_i(x^*) $$",
          "Active inequalities with $$ \\lambda_i^* = 0 $$: $$ \\nabla c_i(x^*)^T w \\ge 0 $$",
          "Equality constraints: $$ \\nabla c_i(x^*)^T w = 0 $$"
        ],
        "Examples": [
          "Example 12.7 (59) at $$ (0,0) $$: $$ \\mathcal{C} = \\{(0, w_2) \\mid w_2 \\ge 0\\} $$"
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Linear Program (LP) in Standard Form (64)": {
      "type": "Optimization Problem Type",
      "domain": "Linear Programming",
      "definition": "$$ \\min~c^T x $$ subject to $$ Ax = b $$, $$ x \\ge 0 $$, where $$ A \\in \\mathbb{R}^{m \\times n} $$, $$ m < n $$, and $$ A $$ has full row rank.",
      "description": "Standard form used by the Simplex Method. All constraints are equalities and non-negativity. Conversion from inequality form requires slack variables.",
      "properties": {
        "Goal": "Minimize linear cost over polyhedral feasible set.",
        "Applications": [
          "Resource allocation, production planning, logistics"
        ],
        "Methods": [
          "Simplex Method",
          "Interior Point Methods"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "07_data.json",
          "08_data.json"
        ]
      }
    },
    "Linear Program (LP) in Canonical Form (65)": {
      "type": "Optimization Problem Type",
      "domain": "Linear Programming",
      "definition": "$$ \\min~c^T x $$ subject to $$ Ax \\le b $$, $$ x \\ge 0 $$.",
      "description": "Natural modeling form with inequality constraints. Feasible region is a polyhedron (polygon in 2D). Converted to standard form via slack variables.",
      "properties": {
        "Goal": "Minimize linear objective over inequality-constrained domain.",
        "Applications": [
          "Initial formulation of real-world LP problems"
        ],
        "Methods": [
          "Add slack variables: $$ Ax + z = b $$, $$ z \\ge 0 $$ → standard form (66)"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "07_data.json",
          "08_data.json"
        ]
      }
    },
    "Slack Variables ($$ z $$)": {
      "type": "Auxiliary Variable",
      "domain": "Linear Programming",
      "definition": "Non-negative variables $$ z \\ge 0 $$ introduced to convert inequalities $$ Ax \\le b $$ into equalities $$ Ax + z = b $$.",
      "description": "Each inequality gets one slack variable. Transforms canonical form into standard form suitable for Simplex Method.",
      "properties": {
        "Goal": "Enable equality-based algorithms on inequality-constrained LPs.",
        "Applications": [
          "Conversion from canonical to standard form"
        ],
        "Methods": [
          "Augment constraint matrix: $$ [A \\ I] $$, variable vector: $$ [x; z] $$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Simplex Method": {
      "type": "Optimization Algorithm",
      "domain": "Linear Programming",
      "definition": "An iterative algorithm that solves LPs in standard form by moving from vertex to adjacent vertex of the feasible polyhedron, reducing cost at each pivot.",
      "description": "Exploits vertex optimality: optimal solution lies at a vertex. Avoids checking all $2^n$ KKT combinations by following improving edges. Converges in finite steps (non-degenerate case).",
      "properties": {
        "Goal": "Find global minimum of linear program.",
        "Applications": [
          "Solving LPs in standard form (64)"
        ],
        "Methods": [
          "Phase I: Find initial basic feasible solution",
          "Phase II: Pivot to reduce cost until optimality ($$ s \\ge 0 $$)",
          "Use reduced costs $$ s_j = c_j - \\lambda^T A_{:j} $$ to select entering variable"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "07_data.json",
          "08_data.json"
        ]
      }
    },
    "Basic Feasible Solution (Vertex)": {
      "type": "Solution Point/Geometric Vertex",
      "domain": "Linear Programming",
      "definition": "A solution $x$ to $Ax=b$ in standard form such that $x\\ge0$ and exactly $n-m$ components are zero (assuming full row rank of $A$).",
      "description": "These points correspond to the vertices of the feasible polyhedron. The Simplex method moves exclusively between such points.",
      "properties": {
        "Goal": "Serve as endpoints for descent steps along feasible edges.",
        "Applications": [
          "Starting point for Simplex Phase II (if $B^{-1}b \\ge 0$)"
        ],
        "Methods": [],
        "Examples": [
          "For problem (67), initial vertex: $x_{1}=8$, $x_{2}=9$ (basic), $x_{3}=x_{4}=x_{5}=0$ (non-basic)."
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Minimum Ratio Rule": {
      "type": "Selection Criterion/Rule",
      "domain": "Simplex Method",
      "definition": "The rule to determine the leaving basic variable by computing $\\theta^* = \\min\\left\\{\\frac{(B^{-1}b)_{i}}{(B^{-1}u)_{i}} \\mid (B^{-1}u)_{i} > 0\\right\\}$, where $u$ is the column of the entering variable in $N$.",
      "description": "This rule ensures the step size along the edge is maximal while preserving feasibility, identifying the next vertex.",
      "properties": {
        "Goal": "Determine the leaving variable and step size to the next vertex.",
        "Applications": [
          "Step 3 of the Simplex Method"
        ],
        "Methods": [
          "Compute ratios $\\frac{(B^{-1}b)_{i}}{(B^{-1}u)_{i}}$ only for $i$ where $(B^{-1}u)_{i} > 0$."
        ],
        "Examples": [
          "In example (67), ratios $\\frac{8}{2}=4$ and $\\frac{9}{3}=3$; minimum is 3, so $x_{2}$ leaves."
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Critical Cone ($\\mathcal{C}(x^{*},\\lambda^{*})$)": {
      "type": "Geometric Concept/Set",
      "domain": "Constrained Optimization Theory",
      "definition": "The set of directions $w\\in\\mathbb{R}^{n}$ such that $\\nabla c_{i}(x^{*})^{T}w = 0$ for all active constraints $i$ (i.e., $c_{i}(x^{*})=0$ and $\\lambda^{*}_{i}>0$), and $\\nabla c_{i}(x^{*})^{T}w \\ge 0$ for active constraints with $\\lambda^{*}_{i}=0$.",
      "description": "Used to restrict second-order necessary/sufficient conditions at a KKT point. For the eigenvalue problem $\\min x^{T}Qx$ s.t. $\\|x\\|=1$, it is the tangent plane $\\{w \\mid x^{*T}w=0\\}$.",
      "properties": {
        "Goal": "Define the subspace on which the Hessian of the Lagrangian must be positive semidefinite (second-order conditions).",
        "Applications": [
          "Second-Order Necessary Conditions (Theorem 12.5)"
        ],
        "Methods": [],
        "Examples": [
          "For constraint $c(x) = \\|x\\|^2 - 1 = 0$, $\\mathcal{C}(x^{*},\\lambda^{*}) = \\{w \\mid x^{*T}w = 0\\}$."
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Second Order Necessary Condition (Eigenvalue Problem)": {
      "type": "Necessary Condition/Verification",
      "domain": "Constrained Optimization Theory",
      "definition": "At a local minimizer $x^{*}$ of $\\min x^{T}Qx$ s.t. $\\|x\\|=1$, the restricted Hessian satisfies $w^{T}(Q - \\lambda^{*}I)w \\ge 0$ for all $w \\in \\mathcal{C}(x^{*},\\lambda^{*}) = \\{w \\mid x^{*T}w=0\\}$.",
      "description": "Equivalent to $\\lambda^{*}$ being the smallest eigenvalue of $Q$. If $\\lambda^{*}$ is not the smallest, $x^{*}$ cannot be a local minimizer.",
      "properties": {
        "Goal": "Verify whether a KKT point $x^{*}$ (unit eigenvector) is a local minimizer.",
        "Applications": [
          "Analyzing constrained eigenvalue problems"
        ],
        "Methods": [
          "Check if $\\lambda^{*}$ is the smallest eigenvalue of $Q$."
        ],
        "Examples": [
          "If $\\lambda^{*}$ is the smallest eigenvalue and multiplicity one, $x^{*}$ is a local minimizer."
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Duality Theory (Convex)": {
      "type": "Optimization Concept/Theory",
      "domain": "Convex Optimization",
      "definition": "A framework for analyzing the primal problem $\\min~f(x)$ s.t. $c(x)\\ge0$ via the Lagrangian and dual problem, valid under convexity assumptions on $f$ and concavity on $c_j$.",
      "description": "Requires $f$ convex and $c_j$ concave so that the Lagrangian $\\mathcal{L}(x,\\lambda) = f(x) - \\lambda^{T}c(x)$ is convex in $x$ for fixed $\\lambda\\ge0$. Enables weak/strong duality results.",
      "properties": {
        "Goal": "Analyze and solve constrained convex optimization problems using dual variables.",
        "Applications": [
          "Linear Programming",
          "Quadratic Programming"
        ],
        "Methods": [
          "Form Lagrangian $\\mathcal{L}(x,\\lambda)=f(x)-\\lambda^{T}c(x)$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Duality Theory (Convex Optimization)": {
      "type": "Optimization Concept/Theory",
      "domain": "Convex Optimization",
      "definition": "A major concept in optimization focused on the relationship between a primal constrained minimization problem ($\\min~f(x)$ subject to $c(x)\\ge0$) and its corresponding dual problem.",
      "description": "Duality requires strong assumptions, namely that the objective function $f$ must be convex, and the constraint functions $c_{j}$ must be concave (meaning $-c_{j}$ are convex). This guarantees that the Lagrangian $\\mathcal{L}(x,\\lambda)$ is convex in $x$ for any fixed $\\lambda\\ge0$.",
      "properties": {
        "Goal": "Analyze and potentially solve constrained optimization problems using dual variables.",
        "Applications": [
          "Linear Programming (LP)",
          "Quadratic Programming (QP)"
        ],
        "Methods": [
          "Formulating the Lagrangian function $\\mathcal{L}(x,\\lambda)=f(x)-\\lambda^{T}c(x)$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Dual Objective Function ($q(\\lambda)$)": {
      "type": "Mathematical Function",
      "domain": "Duality Theory",
      "definition": "The infimum of the Lagrangian function over $x$ for a fixed $\\lambda\\ge0$: $q(\\lambda)=\\inf_{x}\\mathcal{L}(x,\\lambda)$ (74).",
      "description": "This function is defined only for those $\\lambda\\ge0$ for which the infimum is finite ($q(\\lambda)> -\\infty$). The dual function $q$ is always concave (hence $-q$ is convex).",
      "properties": {
        "Goal": "Define the objective function for the Dual Problem (76).",
        "Applications": [
          "Formulating the Dual Problem (76)"
        ],
        "Methods": [
          "Finding critical points by setting $\\nabla_{x}\\mathcal{L}(x,\\lambda)=0$ (assuming smoothness)"
        ],
        "Examples": [
          "For $\\min\\frac{1}{2}(x_{1}^{2}+x_{2}^{2})$ subject to $x_{1}-1\\ge0$, $q(\\lambda)=-\\frac{1}{2}\\lambda^{2}+\\lambda$."
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Dual Problem (76)": {
      "type": "Optimization Problem Type",
      "domain": "Duality Theory",
      "definition": "The optimization problem defined by maximizing the dual objective function $q(\\lambda)$ subject to the constraint $\\lambda\\ge0$: $\\max~q(\\lambda)$ subject to $\\lambda\\ge0$.",
      "description": "Solving the dual problem provides information on the original (primal) problem (73). Due to weak duality, the optimal value of (76) is less than or equal to the optimal value of the primal problem.",
      "properties": {
        "Goal": "Maximize $q(\\lambda)$ to find the best lower bound for the primal problem's optimal value.",
        "Applications": [
          "Solving the Dual LP (79)"
        ],
        "Methods": [
          "Solving the KKT conditions for $x$ once the optimal $\\lambda^*$ is found."
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Weak Duality": {
      "type": "Theorem/Concept",
      "domain": "Duality Theory",
      "definition": "The theorem stating that for any feasible primal solution $x$ and any $\\lambda\\ge0$ such that $q(\\lambda)> -\\infty$, the dual objective function value is always less than or equal to the primal objective value: $q(\\lambda)\\le f(x)$.",
      "description": "This inequality holds because $c(x)\\ge0$ and $\\lambda\\ge0$ imply $-\\lambda^{T}c(x)\\le0$, so $\\mathcal{L}(x,\\lambda)\\le f(x)$ and thus $q(\\lambda)\\le f(x)$. Weak duality gives a lower bound on the primal optimal value.",
      "properties": {
        "Goal": "Provide a simple lower bound for the optimal value of the primal problem.",
        "Applications": [
          "Verifying optimality: if $q(\\lambda)=f(x)$ for feasible $x,\\lambda$, then $x$ and $\\lambda$ are optimal"
        ],
        "Methods": [],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Strong Duality (Theorem 12.12)": {
      "type": "Theorem/Concept",
      "domain": "Duality Theory",
      "definition": "The property that the optimal values of the primal problem $f(x^*)$ and the dual problem $q(\\lambda^*)$ are equal, i.e., $q(\\lambda^{*})=f(x^{*})$. Under convexity and a constraint qualification (e.g., Slater's condition), strong duality holds.",
      "description": "Theorem 12.12 states that if $x^{*}$ is a solution to the primal problem (73) and there exists $\\lambda^{*}$ such that $(x^{*},\\lambda^{*})$ satisfies the KKT conditions (under convexity assumptions), then $\\lambda^{*}$ solves the dual problem (76) and $f(x^*)=q(\\lambda^*)$.",
      "properties": {
        "Goal": "Confirm that a found pair $(x,\\lambda)$ is globally optimal for both the primal and dual problems.",
        "Applications": [
          "Solving optimization problems by finding $\\lambda^*$ first and then $x$"
        ],
        "Methods": [
          "Proof combines convexity of $\\mathcal{L}(x,\\lambda^{*})$ in $x$, KKT conditions, and weak duality"
        ],
        "Examples": [
          "In example (75), the value of both the primal and dual problems is $\\frac{1}{2}$."
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Dual Linear Program (79)": {
      "type": "Optimization Problem Type",
      "domain": "Linear Programming/Duality",
      "definition": "The dual problem corresponding to the primal LP $\\min~c^{T}x$ subject to $Ax\\ge b$ ($Ax-b\\ge0$): $\\max~b^{T}\\lambda$ subject to $A^{T}\\lambda = c$ and $\\lambda\\ge0$.",
      "description": "The dual objective function $q(\\lambda)$ for this LP is $b^{T}\\lambda$ when the stationarity condition $A^{T}\\lambda = c$ holds (otherwise $q(\\lambda)=-\\infty$). This transformation is useful if the dual problem is easier to solve than the primal.",
      "properties": {
        "Goal": "Solve the primal LP (78) indirectly by solving the dual problem.",
        "Applications": [
          "Alternative solution method for LP problems"
        ],
        "Methods": [
          "If solved for $\\lambda^{*}$, use $\\lambda^{*}$ to simplify the KKT conditions for $x$"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Interior Point Methods": {
      "type": "Optimization Algorithm",
      "domain": "Constrained Optimization",
      "definition": "A class of algorithms designed to overcome the combinatorial difficulty associated with optimization problems having many inequality constraints.",
      "description": "These methods are typically required for Quadratic Programming (QP) problems where the inequality constraints lead to $2^{m}$ possible active sets, making direct application of active-set or corner-based methods impractical.",
      "properties": {
        "Goal": "Efficiently solve constrained optimization problems with numerous inequality constraints.",
        "Applications": [
          "Quadratic Programming (44)"
        ],
        "Methods": [],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Simplex Method Worst-Case Complexity": {
      "type": "Performance Metric/Bound",
      "domain": "Linear Programming",
      "definition": "The maximum theoretical number of vertices the Simplex method might have to visit in the worst case, given by the number of feasible basic solutions $\\binom{m}{n}$ (for $m$ constraints and $n$ variables in standard form with $m\\ge n$).",
      "description": "Although the worst-case scenario is exponential in $n$ (as illustrated by contrived examples like the Klee-Minty cube), the Simplex method is regarded as efficient in practice, typically requiring at most $2m$ to $3m$ pivots.",
      "properties": {
        "Goal": "Define the upper bound of computational effort required by the Simplex method in the worst case.",
        "Applications": [],
        "Methods": [],
        "Examples": [
          "For a problem with $m=6$ equality constraints and $n=2$ non-basic variables (i.e., $6$ choose $2$), there are $\\binom{6}{2}=15$ basic feasible solutions."
        ]
      },
      "metadata": {
        "subjects": [
          "introduction_to_optimization"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Pinhole Camera Model": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "A simplified geometric model of image formation in which light rays pass through a single small aperture and project an inverted image onto an image plane.",
      "description": "Used as the mathematical basis for camera calibration and projection matrix derivation in both single-view and multi-view geometry.",
      "properties": {
        "Goal": "Model how 3D points project onto a 2D image plane.",
        "Applications": [
          "Camera calibration",
          "3D reconstruction",
          "Stereo imaging"
        ],
        "Methods": [
          "Homogeneous coordinate projection",
          "Matrix-based projection equations"
        ],
        "Examples": [
          "Exam 2014 - Projection geometry",
          "Exam 2016 - Stereo setup"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_01_core_concept.json"
        ]
      }
    },
    "Radial Distortion": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "A lens distortion where straight lines appear curved due to nonlinear magnification that varies with distance from the image center.",
      "description": "Commonly corrected during camera calibration using polynomial or division models to improve geometric accuracy.",
      "properties": {
        "Goal": "Model and correct optical distortion effects in imaging systems.",
        "Applications": [
          "Camera calibration",
          "3D measurement",
          "Photogrammetry"
        ],
        "Methods": [
          "Polynomial distortion model",
          "Inverse distortion mapping"
        ],
        "Examples": [
          "Exam 2014 - Lens modeling",
          "Exam 2018 - Definition question"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_01_core_concept.json"
        ]
      }
    },
    "HSV Color Space": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "A color representation model that describes colors in terms of hue, saturation, and value, which better aligns with human color perception.",
      "description": "HSV is commonly used in segmentation and tracking tasks because hue can be more stable under illumination variations.",
      "properties": {
        "Goal": "Represent and manipulate color information in perceptually meaningful terms.",
        "Applications": [
          "Color-based segmentation",
          "Object tracking",
          "Skin detection"
        ],
        "Methods": [
          "RGB-to-HSV conversion",
          "Thresholding by hue and saturation"
        ],
        "Examples": [
          "Exam 2015 - Definition",
          "Exam 2019 - Basic term question"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_01_core_concept.json"
        ]
      }
    },
    "Depth of Field": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "The range of distances within a scene that appear acceptably sharp in an image.",
      "description": "Controlled by aperture size, focal length, and sensor distance; important in focus estimation and 3D reconstruction.",
      "properties": {
        "Goal": "Quantify and control image sharpness across depth layers.",
        "Applications": [
          "Focus measurement",
          "Autofocus systems",
          "3D reconstruction"
        ],
        "Methods": [
          "Optical modeling",
          "Focus metric computation"
        ],
        "Examples": [
          "Exam 2020 - Definition question"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_01_core_concept.json"
        ]
      }
    },
    "Epipolar Constraint": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "A geometric relationship stating that a point in one image must lie on a specific line (the epipolar line) in the other image when both views observe the same 3D point.",
      "description": "Derived from camera projection matrices and the essential matrix; simplifies stereo correspondence search.",
      "properties": {
        "Goal": "Reduce 2D stereo correspondence search to 1D along epipolar lines.",
        "Applications": [
          "Stereo vision",
          "Structure-from-Motion",
          "Camera calibration"
        ],
        "Methods": [
          "Essential matrix computation",
          "Epipolar geometry modeling"
        ],
        "Examples": [
          "Exam 2018 - Theoretical question",
          "Exam 2020 - Stereo derivation"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_01_core_concept.json"
        ]
      }
    },
    "Aperture Problem": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "An ambiguity in local motion estimation where only the component of motion perpendicular to an image gradient can be measured.",
      "description": "Occurs in optical flow estimation; resolved using spatial or temporal coherence constraints.",
      "properties": {
        "Goal": "Explain the fundamental ambiguity in local motion detection.",
        "Applications": [
          "Optical flow",
          "Edge tracking",
          "Motion estimation"
        ],
        "Methods": [
          "Gradient constraint equation",
          "Lucas–Kanade method",
          "Global smoothness enforcement"
        ],
        "Examples": [
          "Exam 2015 - Definition",
          "Exam 2019 - Optical flow task"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_01_core_concept.json",
          "08_data.json"
        ]
      }
    },
    "Metamers": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "Different spectral distributions that produce the same color perception under specific lighting conditions.",
      "description": "Important in color science and sensor calibration, explaining why cameras and human vision may differ in color interpretation.",
      "properties": {
        "Goal": "Understand perceptual equivalence in color representation.",
        "Applications": [
          "Color calibration",
          "Illumination modeling",
          "Spectral imaging"
        ],
        "Methods": [
          "Spectral measurement",
          "Color matching functions"
        ],
        "Examples": [
          "Exam 2017 - Definition",
          "Exam 2019 - Conceptual question"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_01_core_concept.json"
        ]
      }
    },
    "Chromatic Aberration": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "An optical phenomenon where different wavelengths of light focus at different distances, causing color fringes in images.",
      "description": "Corrected via lens design or software post-processing to ensure color alignment in multi-channel imaging.",
      "properties": {
        "Goal": "Reduce color blurring caused by wavelength-dependent refraction.",
        "Applications": [
          "Lens design",
          "Image restoration",
          "Color correction"
        ],
        "Methods": [
          "Spectral lens calibration",
          "Image deconvolution"
        ],
        "Examples": [
          "Exam 2018 - Definition question"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_01_core_concept.json"
        ]
      }
    },
    "Camera Extrinsics": {
      "type": "Concept",
      "domain": "Machine Vision",
      "definition": "Parameters that describe the position and orientation of a camera in a world coordinate system.",
      "description": "Extrinsics link the camera reference frame to world coordinates, essential for triangulation and multi-view alignment.",
      "properties": {
        "Goal": "Map coordinates between camera and world spaces.",
        "Applications": [
          "Stereo calibration",
          "SLAM",
          "3D reconstruction"
        ],
        "Methods": [
          "Rotation-translation matrix estimation",
          "PnP algorithms"
        ],
        "Examples": [
          "Exam 2014 - Camera calibration task"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_01_core_concept.json"
        ]
      }
    },
    "SIFT": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A feature detection and description algorithm that identifies keypoints and computes descriptors invariant to scale, rotation, and partial illumination changes.",
      "description": "SIFT builds a scale-space using Difference-of-Gaussians, detects local extrema, and forms gradient-based descriptors robust to geometric and photometric transformations.",
      "properties": {
        "Goal": "Detect stable image features for matching across scales and rotations.",
        "Applications": [
          "Image matching",
          "Object recognition",
          "3D reconstruction"
        ],
        "Methods": [
          "Difference-of-Gaussians",
          "Gradient histogram descriptors",
          "Keypoint matching"
        ],
        "Examples": [
          "Exam 2015 - Definition",
          "Exam 2018 - Descriptor explanation"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json"
        ]
      }
    },
    "Harris Corner Detector": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A feature detector that identifies corners by measuring local intensity variations using the autocorrelation matrix.",
      "description": "The Harris detector finds points with significant intensity change in orthogonal directions, forming the basis of many tracking and matching algorithms.",
      "properties": {
        "Goal": "Detect stable corner-like features in images.",
        "Applications": [
          "Feature tracking",
          "Image registration",
          "Object recognition"
        ],
        "Methods": [
          "Autocorrelation matrix",
          "Corner response function",
          "Non-maximum suppression"
        ],
        "Examples": [
          "Exam 2015 - Principle question",
          "Exam 2018 - Method explanation"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json",
          "06_data.json"
        ]
      }
    },
    "Hough Transform": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A feature extraction technique used to detect parametric shapes such as lines, circles, or ellipses in images.",
      "description": "The transform maps image edge points into a parameter space where shapes correspond to peaks, enabling robust detection despite noise or occlusion.",
      "properties": {
        "Goal": "Detect geometric primitives via voting in parameter space.",
        "Applications": [
          "Line detection",
          "Circle detection",
          "Shape analysis"
        ],
        "Methods": [
          "Parameter-space accumulation",
          "Threshold-based peak detection"
        ],
        "Examples": [
          "Exam 2015 - Principle question",
          "Exam 2018 - Example usage"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json"
        ]
      }
    },
    "Random Sample Consensus (RANSAC)": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "An iterative algorithm to estimate model parameters from data containing outliers by repeatedly sampling minimal subsets and testing consensus.",
      "description": "RANSAC is robust to outliers and commonly used in fitting geometric models such as lines, planes, or homographies.",
      "properties": {
        "Goal": "Estimate parameters robustly in the presence of noise and outliers.",
        "Applications": [
          "Line fitting",
          "Homography estimation",
          "Triangulation refinement"
        ],
        "Methods": [
          "Iterative sampling",
          "Consensus evaluation",
          "Model re-estimation"
        ],
        "Examples": [
          "Exam 2015 - Theory",
          "Exam 2018 - Usage example"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json"
        ]
      }
    },
    "Local Binary Patterns (LBP)": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A texture descriptor that encodes the local spatial pattern of pixel intensities into binary codes based on neighbor comparisons.",
      "description": "LBP provides a rotation- and grayscale-invariant way to represent texture; it’s lightweight and robust, making it popular for real-time classification.",
      "properties": {
        "Goal": "Represent texture structures in a compact and invariant form.",
        "Applications": [
          "Texture classification",
          "Face recognition",
          "Material analysis"
        ],
        "Methods": [
          "Neighborhood thresholding",
          "Histogram of binary codes",
          "Rotation-invariant encoding"
        ],
        "Examples": [
          "Exam 2017 - Texture task",
          "Exam 2020 - Texture classification"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json"
        ]
      }
    },
    "K-Means Clustering": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "An unsupervised learning algorithm that partitions data into K clusters by minimizing within-cluster variance.",
      "description": "Used to group similar pixels, features, or image patches, serving as a basis for segmentation and visual vocabulary creation.",
      "properties": {
        "Goal": "Group data points into clusters based on feature similarity.",
        "Applications": [
          "Image segmentation",
          "Bag-of-Words clustering",
          "Color quantization"
        ],
        "Methods": [
          "Iterative centroid update",
          "Euclidean distance minimization"
        ],
        "Examples": [
          "Exam 2016 - Principle",
          "Exam 2019 - Usage explanation"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json"
        ]
      }
    },
    "Otsu’s Thresholding Method": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "An automatic global thresholding technique that separates foreground and background by minimizing intra-class intensity variance.",
      "description": "Commonly used in image preprocessing for binarization tasks, providing an optimal threshold without supervision.",
      "properties": {
        "Goal": "Automatically determine an optimal threshold to separate regions.",
        "Applications": [
          "Image segmentation",
          "Preprocessing for OCR",
          "Object extraction"
        ],
        "Methods": [
          "Histogram-based variance minimization"
        ],
        "Examples": [
          "Exam 2018 - Principle",
          "Exam 2019 - Usage question"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json"
        ]
      }
    },
    "Harris–Laplace Detector": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A multi-scale feature detector combining corner detection (Harris) with scale selection (Laplacian) to identify stable features across resolutions.",
      "description": "Enhances standard corner detectors by integrating scale information for improved invariance.",
      "properties": {
        "Goal": "Detect scale-invariant interest points.",
        "Applications": [
          "Feature matching",
          "Object tracking",
          "Scale-space analysis"
        ],
        "Methods": [
          "Corner response computation",
          "Laplacian-of-Gaussian scale selection"
        ],
        "Examples": [
          "Exam 2016 - Extended question"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json"
        ]
      }
    },
    "Photometric Stereo": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A technique for estimating surface normals by observing an object under multiple known lighting directions.",
      "description": "Allows reconstruction of fine surface details by exploiting reflectance differences under controlled illumination.",
      "properties": {
        "Goal": "Recover detailed surface orientation from shading cues.",
        "Applications": [
          "Shape-from-shading",
          "Industrial inspection",
          "Material analysis"
        ],
        "Methods": [
          "Lambertian reflectance model",
          "Linear intensity equations"
        ],
        "Examples": [
          "Exam 2017 - Principle question",
          "Exam 2018 - Example"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json",
          "10_data.json"
        ]
      }
    },
    "Feature descriptor": {
      "type": "Concept",
      "domain": "Feature Extraction",
      "definition": "A representation of an image patch or interest point that captures its essential characteristics, designed to be robust to variations in illumination, viewpoint, and scale.",
      "description": "",
      "properties": {},
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json"
        ]
      }
    },
    "Bag-of-Words": {
      "type": "Concept",
      "domain": "Feature Representation",
      "definition": "A sparse vector representation of an image (or document) based on the frequency of visual words (or terms) from a predefined vocabulary.",
      "description": "",
      "properties": {},
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json"
        ]
      }
    },
    "Maximally Stable Extremal Regions (MSER)": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A method for detecting regions in an image that are stable across a wide range of thresholds, often used for text detection and object recognition.",
      "description": "",
      "properties": {},
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_01_feature_extraction_detection_methods.json"
        ]
      }
    },
    "Convolutional Neural Network (CNN)": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A deep learning architecture composed of convolutional, pooling, and fully connected layers that automatically learn hierarchical visual features.",
      "description": "CNNs dominate modern computer vision tasks by learning spatial hierarchies of features directly from raw image data without handcrafted descriptors.",
      "properties": {
        "Goal": "Automatically learn discriminative visual representations.",
        "Applications": [
          "Image classification",
          "Object detection",
          "Semantic segmentation"
        ],
        "Methods": [
          "Backpropagation",
          "Convolutional filtering",
          "Pooling operations"
        ],
        "Examples": [
          "Exam 2018 - Modern methods",
          "Exam 2019 - High-level discussion"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json"
        ]
      }
    },
    "Bag-of-Words (BoW) Representation": {
      "type": "Representation",
      "domain": "Feature Representation",
      "definition": "An image representation that models visual content as a histogram of discrete visual words learned from feature descriptors.",
      "description": "BoW models are popular for image classification and retrieval due to their simplicity and effectiveness, despite losing spatial information.",
      "properties": {
        "Goal": "Represent images as collections of visual words for classification or retrieval.",
        "Applications": [
          "Image classification",
          "Content-based image retrieval",
          "Object recognition"
        ],
        "Methods": [
          "Feature extraction (e.g., SIFT, SURF)",
          "Visual vocabulary creation (e.g., K-Means)",
          "Histogram generation"
        ],
        "Examples": [
          "Image search engines",
          "Category recognition"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json"
        ]
      }
    },
    "Visual words": {
      "type": "Concept",
      "domain": "Feature Representation",
      "definition": "A cluster center in the feature space, representing a common visual pattern or feature, used to build Bag-of-Words representations.",
      "description": "",
      "properties": {},
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json"
        ]
      }
    },
    "Image segmentation": {
      "type": "Task/Capability",
      "domain": "Segmentation",
      "definition": "The process of partitioning a digital image into multiple segments (sets of pixels) to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze.",
      "description": "Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.",
      "properties": {
        "Goal": "Simplify and/or change the representation of an image into something more meaningful.",
        "Applications": [
          "Object recognition",
          "Medical imaging",
          "Autonomous driving",
          "Image editing"
        ],
        "Methods": [
          "Thresholding",
          "Clustering (e.g., K-Means)",
          "Region-based methods",
          "Edge-based methods",
          "Deep Learning (e.g., U-Net)"
        ],
        "Examples": [
          "Separating foreground from background",
          "Identifying different organs in a medical scan"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json"
        ]
      }
    },
    "Background Subtraction": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A motion-based segmentation approach that isolates moving foreground objects by comparing each frame to a background model.",
      "description": "Widely used in video surveillance, motion analysis, and dynamic scene understanding.",
      "properties": {
        "Goal": "Separate moving objects from a static or slowly changing background.",
        "Applications": [
          "Surveillance",
          "Traffic monitoring",
          "Gesture recognition"
        ],
        "Methods": [
          "Frame differencing",
          "Gaussian mixture models",
          "Otsu thresholding"
        ],
        "Examples": [
          "Exam 2018 - Segmentation question"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json"
        ]
      }
    },
    "Image Segmentation": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "The process of partitioning an image into semantically meaningful regions based on intensity, color, or texture.",
      "description": "Segmentation forms the foundation for object detection, recognition, and measurement by assigning pixels to distinct labels.",
      "properties": {
        "Goal": "Divide an image into coherent regions for further analysis.",
        "Applications": [
          "Object detection",
          "Medical imaging",
          "Scene understanding"
        ],
        "Methods": [
          "Thresholding",
          "Region growing",
          "Graph-based segmentation"
        ],
        "Examples": [
          "Exam 2019 - Principle",
          "Exam 2020 - Segmentation method"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json"
        ]
      }
    },
    "Lucas–Kanade Optical Flow": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A differential method for optical flow estimation assuming constant motion within a local neighborhood.",
      "description": "Solves the aperture problem by enforcing spatial smoothness; widely used for motion tracking and video stabilization.",
      "properties": {
        "Goal": "Estimate pixel displacement between consecutive frames.",
        "Applications": [
          "Motion tracking",
          "Stabilization",
          "3D reconstruction"
        ],
        "Methods": [
          "Gradient constraint equation",
          "Least-squares solution over a window"
        ],
        "Examples": [
          "Exam 2019 - Optical flow task"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json"
        ]
      }
    },
    "Structure-from-Motion (SfM)": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A technique that recovers 3D structure and camera motion from multiple overlapping 2D images.",
      "description": "Combines feature matching, triangulation, and bundle adjustment to produce dense reconstructions and camera pose estimates.",
      "properties": {
        "Goal": "Estimate 3D geometry and motion from image sequences.",
        "Applications": [
          "3D reconstruction",
          "AR/VR",
          "Robot localization"
        ],
        "Methods": [
          "Feature matching",
          "Bundle adjustment",
          "Triangulation"
        ],
        "Examples": [
          "Exam 2019 - Method explanation",
          "Exam 2020 - Stereo extension"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json"
        ]
      }
    },
    "Texture Analysis": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "The process of quantifying image surface characteristics using spatial variations in intensity or color.",
      "description": "Includes statistical, structural, and filter-based approaches to characterize surface patterns or material properties.",
      "properties": {
        "Goal": "Extract numerical features that describe texture patterns.",
        "Applications": [
          "Texture classification",
          "Surface inspection",
          "Remote sensing"
        ],
        "Methods": [
          "Co-occurrence matrices",
          "Filter banks",
          "LBP histograms"
        ],
        "Examples": [
          "Exam 2015 - Seashell texture",
          "Exam 2018 - Filter bank task"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json"
        ]
      }
    },
    "Filter Bank Methods": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A texture analysis technique using a set of filters (e.g., Gabor, Laws, or wavelets) to capture multi-scale, multi-orientation texture information.",
      "description": "Used to extract texture features robust to illumination and rotation changes, often preceding classification or retrieval.",
      "properties": {
        "Goal": "Describe texture using responses to multiple spatial-frequency filters.",
        "Applications": [
          "Texture classification",
          "Defect detection",
          "Material identification"
        ],
        "Methods": [
          "Gabor filters",
          "Laws masks",
          "Energy feature computation"
        ],
        "Examples": [
          "Exam 2016 - Texture task",
          "Exam 2019 - Conceptual question"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json",
          "05_data.json"
        ]
      }
    },
    "Edge Detection": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "A fundamental operation that detects local discontinuities in intensity to outline object boundaries.",
      "description": "Typical operators like Sobel, Prewitt, or Canny emphasize intensity gradients to delineate shapes for segmentation and recognition.",
      "properties": {
        "Goal": "Identify object boundaries via intensity gradients.",
        "Applications": [
          "Shape analysis",
          "Hough Transform",
          "Segmentation preprocessing"
        ],
        "Methods": [
          "Gradient computation",
          "Thresholding",
          "Non-maximum suppression"
        ],
        "Examples": [
          "Exam 2015 - Preprocessing",
          "Exam 2018 - Edge-based Hough task"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json"
        ]
      }
    },
    "Graph-Based Segmentation": {
      "type": "Method",
      "domain": "Machine Vision",
      "definition": "An image segmentation method that models the image as a graph, where pixels or regions are nodes and edge weights represent similarity.",
      "description": "Cuts or merges in the graph minimize a global cost function, yielding coherent region boundaries.",
      "properties": {
        "Goal": "Group pixels by minimizing inter-region dissimilarity.",
        "Applications": [
          "Object segmentation",
          "Superpixel generation",
          "Video segmentation"
        ],
        "Methods": [
          "Normalized cuts",
          "Minimum spanning tree",
          "Spectral clustering"
        ],
        "Examples": [
          "Exam 2018 - Segmentation discussion"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "00_02_02_learning_segmentation_hybrid_algorithms.json"
        ]
      }
    },
    "Mahalanobis Distance": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Mahalanobis distance is a statistical distance measure that quantifies the separation between a data point and a distribution by accounting for the covariance structure of the features.",
      "description": "It generalizes Euclidean distance by incorporating the inverse covariance matrix, effectively normalizing the feature space according to the data's natural spread and correlations. This makes it robust to scale differences and correlated dimensions, unlike raw Euclidean distance.",
      "properties": {
        "Goal": "Measure similarity in multivariate feature spaces with correlated dimensions",
        "Applications": [
          "Outlier detection",
          "Template matching",
          "Robust feature comparison",
          "Anomaly detection in texture"
        ],
        "Methods": [
          "Covariance estimation",
          "Matrix inversion",
          "Whitening transformation"
        ],
        "Examples": [
          "Comparing image patches using learned texture covariance",
          "Detecting defective regions in industrial inspection"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "00_extra_01.json"
        ]
      }
    },
    "K-Nearest Neighbor Classification": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "K-Nearest Neighbor (k-NN) classification is a non-parametric, instance-based learning algorithm that assigns a test sample to the majority class among its k closest training examples in feature space.",
      "description": "It requires no explicit training phase; instead, it stores the entire training set and performs classification at query time using a distance metric. The decision boundary is locally adaptive and can model complex, non-linear patterns, but computational cost grows with dataset size.",
      "properties": {
        "Goal": "Classify unknown samples based on proximity to labeled data",
        "Applications": [
          "Texture classification",
          "Image retrieval",
          "Face recognition",
          "Medical image diagnosis"
        ],
        "Methods": [
          "Distance computation",
          "Majority voting",
          "Weighted voting"
        ],
        "Examples": [
          "Classifying fabric types using GLCM features",
          "Retrieving visually similar images from a database"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "00_extra_01.json"
        ]
      }
    },
    "Gabor Filters": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Gabor filters are linear bandpass filters defined as the product of a Gaussian envelope and a complex sinusoid, used to model orientation- and frequency-selective responses similar to those in the human visual cortex.",
      "description": "They are widely used for texture analysis because they can be tuned to specific spatial frequencies and orientations. A filter bank of Gabor filters at multiple scales and orientations extracts rich local texture descriptors that are robust to small translations and distortions.",
      "properties": {
        "Goal": "Extract multi-scale, multi-orientation texture features",
        "Applications": [
          "Texture segmentation",
          "Fingerprint enhancement",
          "Iris recognition",
          "Face detection"
        ],
        "Methods": [
          "Convolution with 2D Gabor kernel",
          "Filter bank construction",
          "Energy or magnitude response"
        ],
        "Examples": [
          "Segmenting wood grain patterns",
          "Enhancing ridge structures in fingerprints"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "00_extra_01.json"
        ]
      }
    },
    "Gray-Level Co-occurrence Matrix": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Gray-Level Co-occurrence Matrix (GLCM) is a second-order statistical method that characterizes texture by computing how often pairs of pixel intensity values occur at a given spatial offset in an image.",
      "description": "It captures spatial relationships between pixel intensities, enabling the extraction of texture features such as contrast, homogeneity, and correlation. GLCM is rotation-sensitive unless averaged over multiple angles and is commonly computed on quantized gray-level images to reduce dimensionality.",
      "properties": {
        "Goal": "Quantify spatial distribution of intensity pairs for texture description",
        "Applications": [
          "Material classification",
          "Medical image analysis",
          "Remote sensing",
          "Defect detection"
        ],
        "Methods": [
          "Matrix construction over offset (d, θ)",
          "Normalization",
          "Haralick feature extraction"
        ],
        "Examples": [
          "Classifying rock types in geological imaging",
          "Detecting tumors via tissue texture"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "00_extra_01.json",
          "05_data.json"
        ]
      }
    },
    "Binary Mask": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "A binary mask is a binary image where each pixel indicates whether it belongs to the foreground (typically 1 or white) or background (typically 0 or black), used to isolate regions of interest.",
      "description": "It serves as a spatial filter to selectively process or analyze specific parts of an image. Binary masks are fundamental in segmentation pipelines, enabling operations like feature extraction or morphological processing on only the foreground regions.",
      "properties": {
        "Goal": "Isolate and define regions of interest in an image",
        "Applications": [
          "Object segmentation",
          "Region-based feature extraction",
          "Masking in image editing",
          "ROI processing"
        ],
        "Methods": [
          "Thresholding",
          "Edge detection + filling",
          "Manual annotation"
        ],
        "Examples": [
          "White shell regions on black background",
          "Foreground object in medical imaging"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "00_extra_02.json"
        ]
      }
    },
    "Erosion": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Erosion is a morphological operation that shrinks foreground objects in a binary or grayscale image by removing pixels from object boundaries using a structuring element.",
      "description": "It eliminates small noise, detaches weakly connected components, and thins objects. The result depends on the size and shape of the structuring element (e.g., disk, square). In binary images, a foreground pixel remains only if all pixels in the structuring element neighborhood are foreground.",
      "properties": {
        "Goal": "Shrink objects and remove small noise or protrusions",
        "Applications": [
          "Noise reduction",
          "Boundary smoothing",
          "Separation of touching objects",
          "Skeletonization"
        ],
        "Methods": [
          "Structuring element convolution",
          "Hit-or-miss transform"
        ],
        "Examples": [
          "Removing salt noise from binary thresholded image",
          "Separating overlapping shells before labeling"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "00_extra_02.json"
        ]
      }
    },
    "Dilation": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Dilation is a morphological operation that expands foreground objects in a binary or grayscale image by adding pixels to object boundaries using a structuring element.",
      "description": "It fills small holes, connects nearby components, and thickens objects. A foreground pixel is set if at least one pixel in the structuring element neighborhood is foreground. Commonly used to restore object size after erosion or to close gaps.",
      "properties": {
        "Goal": "Expand objects and fill small gaps or holes",
        "Applications": [
          "Hole filling",
          "Connecting broken structures",
          "Boundary expansion",
          "Image restoration"
        ],
        "Methods": [
          "Structuring element convolution",
          "Minkowski addition"
        ],
        "Examples": [
          "Filling cracks in segmented shells",
          "Closing small gaps in blood vessel segmentation"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "00_extra_02.json"
        ]
      }
    },
    "Opening": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Opening is a morphological operation defined as erosion followed by dilation using the same structuring element, used to remove small objects and noise while preserving larger structures.",
      "description": "It acts as a non-linear filter that eliminates thin protrusions, small islands, and noise without significantly altering the shape or size of larger foreground regions. Opening is idempotent and useful for cleaning binary segmentation results.",
      "properties": {
        "Goal": "Remove small noise and detach weakly connected components",
        "Applications": [
          "Noise suppression",
          "Object separation",
          "Preprocessing for connected component analysis",
          "Fingerprint enhancement"
        ],
        "Methods": [
          "Erosion then Dilation",
          "Structuring element selection (disk, cross)"
        ],
        "Examples": [
          "Removing salt-and-pepper noise from thresholded seashell image",
          "Cleaning small debris before labeling"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "00_extra_02.json"
        ]
      }
    },
    "Closing": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Closing is a morphological operation defined as dilation followed by erosion using the same structuring element, used to fill small holes and connect nearby components while preserving overall shape.",
      "description": "It closes small gaps and holes within foreground objects without enlarging them significantly. Like opening, it is idempotent and commonly applied after thresholding to produce solid, compact regions suitable for further analysis.",
      "properties": {
        "Goal": "Fill small holes and connect disjoint parts of objects",
        "Applications": [
          "Hole filling",
          "Gap closure",
          "Smoothing object interiors",
          "Post-processing segmentation"
        ],
        "Methods": [
          "Dilation then Erosion",
          "Structuring element (disk recommended for isotropy)"
        ],
        "Examples": [
          "Filling dark spots inside segmented seashells",
          "Connecting broken edges in vessel segmentation"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "00_extra_02.json"
        ]
      }
    },
    "Connected Component Labeling": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Connected Component Labeling is an algorithm that assigns a unique label to each connected group of foreground pixels in a binary image, identifying individual objects.",
      "description": "It scans the image and groups 4-connected or 8-connected foreground pixels into distinct components. Commonly implemented via two-pass algorithms (raster scan + union-find) or recursive flood-fill. Essential for counting, locating, and analyzing separate objects in segmented images.",
      "properties": {
        "Goal": "Identify and enumerate distinct objects in a binary image",
        "Applications": [
          "Object counting",
          "Individual ROI extraction",
          "Shape analysis",
          "Tracking initialization"
        ],
        "Methods": [
          "Two-pass labeling",
          "Union-find data structure",
          "Flood-fill recursion"
        ],
        "Examples": [
          "Labeling each seashell in a multi-object image",
          "Counting cells in microscopy"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "00_extra_02.json"
        ]
      }
    },
    "Computer Vision": {
      "type": "Concept",
      "domain": "Artificial Intelligence",
      "definition": "Computer vision is an interdisciplinary field that enables computers to gain high-level understanding from digital images or videos by automating tasks that the human visual system can perform.",
      "description": "It involves processing visual data to extract meaningful information, often converting images into higher-level representations. Closely related to artificial intelligence, it focuses on application-oriented tasks rather than theoretical aspects.",
      "properties": {
        "Goal": "Automate visual perception and interpretation",
        "Applications": [
          "Image recognition",
          "3D reconstruction",
          "Video analysis",
          "Object detection"
        ],
        "Methods": [
          "Machine Learning",
          "Deep Neural Networks"
        ],
        "Examples": [
          "N/A"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "01_data.json",
          "initial.json"
        ]
      }
    },
    "Machine Vision": {
      "type": "Concept",
      "domain": "Artificial Intelligence",
      "definition": "Machine vision refers to the application-oriented implementation of computer vision technologies, emphasizing practical deployment over theoretical research.",
      "description": "It is more focused on real-world industrial and engineering applications compared to the broader and more academic field of computer vision.",
      "properties": {
        "Goal": "Practical automation of visual tasks",
        "Applications": [
          "Industrial inspection",
          "Robotics",
          "Quality control"
        ],
        "Methods": [
          "N/A"
        ],
        "Examples": [
          "N/A"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Image Processing": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Image processing involves the transformation of images into other images, often as a preprocessing step to support higher-level computer vision tasks.",
      "description": "It includes operations like filtering, enhancement, and compression, typically producing modified images rather than semantic interpretations.",
      "properties": {
        "Goal": "Transform and enhance image data",
        "Applications": [
          "Noise reduction",
          "Edge detection",
          "Image restoration"
        ],
        "Methods": [
          "Filtering",
          "Morphological operations"
        ],
        "Examples": [
          "N/A"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Image Understanding": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Image understanding is the process of making decisions based on images and constructing scene descriptions.",
      "description": "It goes beyond mere image transformation to interpret content, recognize objects, and infer relationships within visual scenes.",
      "properties": {
        "Goal": "Semantic interpretation of visual data",
        "Applications": [
          "Scene description",
          "Object recognition",
          "Activity understanding"
        ],
        "Methods": [
          "N/A"
        ],
        "Examples": [
          "N/A"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Image Recognition": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Image recognition is the ability of software to identify objects, places, people, writing, and actions in images.",
      "description": "It enables systems to classify and localize entities within visual data, forming a core capability in modern computer vision applications.",
      "properties": {
        "Goal": "Identify and classify visual elements",
        "Applications": [
          "Facial recognition",
          "Object detection",
          "Scene labeling"
        ],
        "Methods": [
          "Deep Learning",
          "Convolutional Neural Networks"
        ],
        "Examples": [
          "Classification + Localization",
          "Object Detection",
          "Semantic Segmentation",
          "Instance Segmentation"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "01_data.json",
          "initial.json"
        ]
      }
    },
    "3D Perception": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "3D perception involves inferring three-dimensional structure and properties from 2D images or video sequences.",
      "description": "It addresses the ill-posed problem of recovering depth and geometry from projections, relying on cues like interposition, relative size, vanishing points, and foreshortening.",
      "properties": {
        "Goal": "Reconstruct 3D world from 2D visual data",
        "Applications": [
          "3D modeling",
          "Autonomous navigation",
          "Augmented reality"
        ],
        "Methods": [
          "Structure from Motion",
          "Multi-view Stereo",
          "Dense Matching"
        ],
        "Examples": [
          "Multi-view stereo reconstruction",
          "Optical illusions from ambiguous 3D interpretations"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "01_data.json",
          "10_data.json"
        ]
      }
    },
    "Multi-view Stereo": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Multi-view stereo is a technique for reconstructing detailed 3D models from multiple overlapping images taken from different viewpoints.",
      "description": "It computes dense correspondence across views to generate high-resolution 3D surface models, commonly used in photogrammetry and cultural heritage preservation.",
      "properties": {
        "Goal": "Dense 3D reconstruction from image sets",
        "Applications": [
          "3D scanning",
          "Virtual reality",
          "Archaeology"
        ],
        "Methods": [
          "Feature matching",
          "Depth map fusion"
        ],
        "Examples": [
          "Reconstruction of architectural columns and dinosaur models from photographs"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "01_data.json",
          "10_data.json"
        ]
      }
    },
    "Computational Photography": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Computational photography uses algorithmic techniques to enhance or extend the capabilities of digital imaging beyond traditional photography limits.",
      "description": "It combines multiple exposures, focuses, or sensor data to produce images with improved dynamic range, sharpness, or reduced noise.",
      "properties": {
        "Goal": "Overcome hardware limitations via computation",
        "Applications": [
          "Deblurring",
          "HDR imaging",
          "Light field capture"
        ],
        "Methods": [
          "Image fusion",
          "Motion deblurring"
        ],
        "Examples": [
          "Deblurring motion-blurred images",
          "Fusing short and long exposures for night scenes"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "01_data.json"
        ]
      }
    },
    "Pinhole Model": {
      "type": "Model",
      "domain": "Computer Vision",
      "definition": "The pinhole model is a simplified geometric approximation of image formation in which light rays from a 3D scene pass through an infinitesimally small aperture (pinhole) and project onto an image plane, forming an inverted image.",
      "description": "It serves as the foundational camera model in computer vision, enabling the mathematical description of perspective projection without lens distortions. Real cameras approximate this model using lenses to focus light.",
      "properties": {
        "Goal": "Model the geometric relationship between 3D world points and their 2D projections on the image plane.",
        "Applications": [
          "Camera calibration",
          "3D reconstruction",
          "Structure from motion"
        ],
        "Methods": [
          "Perspective projection equations",
          "Homogeneous coordinates"
        ],
        "Examples": [
          "Deriving image coordinates from world coordinates using focal length f"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Perspective Projection": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Perspective projection is the process by which 3D points in the world are mapped to 2D points on an image plane through a projection center, simulating how the human eye or a pinhole camera perceives depth.",
      "description": "It results in closer objects appearing larger and parallel lines converging at vanishing points. The projection is governed by similar triangles and the focal length of the imaging system.",
      "properties": {
        "Goal": "Transform 3D world coordinates (X, Y, Z) into 2D image coordinates (x, y).",
        "Applications": [
          "3D reconstruction",
          "Augmented reality",
          "Photogrammetry"
        ],
        "Methods": [
          "Pinhole camera equations: x = f * X/Z, y = f * Y/Z"
        ],
        "Examples": [
          "Railroad tracks appearing to converge in photographs"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Camera Obscura": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Camera obscura is an optical device that projects an inverted image of a scene through a small hole onto a surface inside a darkened chamber.",
      "description": "It is the historical and physical precursor to modern photographic cameras and the conceptual basis for the pinhole camera model in computer vision.",
      "properties": {
        "Goal": "Demonstrate natural image formation via light projection.",
        "Applications": [
          "Artistic drawing aid",
          "Historical optics experiments"
        ],
        "Methods": [
          "Single aperture light projection"
        ],
        "Examples": [
          "Artists using camera obscura for realistic perspective in paintings"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Perspective Distortion": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Perspective distortion refers to the apparent deformation of objects in images due to the viewpoint and the geometry of perspective projection, even when the lens is perfect.",
      "description": "It causes nonlinear transformations between views: straight lines remain straight, but angles, shapes, and relative sizes change depending on distance and camera orientation.",
      "properties": {
        "Goal": "N/A",
        "Applications": [
          "Wide-angle photography",
          "Architectural imaging"
        ],
        "Methods": [
          "N/A"
        ],
        "Examples": [
          "Tall buildings appearing to lean inward when photographed from below"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Focal Length": {
      "type": "Metric",
      "domain": "Computer Vision",
      "definition": "Focal length is the distance between the camera's optical center and the image plane where a sharp image of an object at infinity is formed.",
      "description": "It determines the scale of the projected image and the field of view. In the pinhole model, it directly appears in the projection equations x = f * X/Z and y = f * Y/Z.",
      "properties": {
        "Goal": "Control magnification and field of view in imaging systems.",
        "Applications": [
          "Zoom lenses",
          "Camera calibration",
          "Depth estimation"
        ],
        "Methods": [
          "Intrinsic parameter in camera matrix"
        ],
        "Examples": [
          "Telephoto (long f) vs. wide-angle (short f) lenses"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Image Plane": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The image plane is the two-dimensional surface onto which 3D scene points are projected to form a 2D image in the pinhole camera model.",
      "description": "In real cameras, it corresponds to the sensor plane (CCD/CMOS); in theoretical models, it can be placed in front (virtual) or behind (real) the projection center.",
      "properties": {
        "Goal": "Capture the 2D projection of the 3D world.",
        "Applications": [
          "Sensor design",
          "Projection geometry"
        ],
        "Methods": [
          "Coordinate transformation"
        ],
        "Examples": [
          "Real image plane (behind lens), Virtual image plane (front projection)"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Simplified Imaging Model": {
      "type": "Framework",
      "domain": "Computer Vision",
      "definition": "A simplified imaging model breaks down image formation into geometric, optical, and sensor components to abstract the camera as a measurable system.",
      "description": "It separates concerns: geometry (projection), optics (light focusing), and sensor (light measurement), forming the basis for camera modeling and calibration.",
      "properties": {
        "Goal": "Provide a modular understanding of the imaging pipeline.",
        "Applications": [
          "Camera modeling",
          "Photometric calibration"
        ],
        "Methods": [
          "Decomposition into sub-models"
        ],
        "Examples": [
          "Geometric → Pinhole, Optical → Thin lens, Sensor → Radiometric response"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "02_data.json"
        ]
      }
    },
    "Specular Reflection": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Specular reflection is the mirror-like reflection of light from a surface in which light from a single incoming direction is reflected into a single outgoing direction, following the law of reflection.",
      "description": "It produces highlights and sharp reflections, characteristic of smooth, shiny surfaces. In computer vision, specular highlights are often modeled separately from diffuse reflection to improve surface reconstruction and material estimation.",
      "properties": {
        "Goal": "Model mirror-like light behavior on smooth surfaces.",
        "Applications": [
          "Shape from shading",
          "Material classification",
          "Highlight removal"
        ],
        "Methods": [
          "Law of reflection: angle of incidence = angle of reflection"
        ],
        "Examples": [
          "Shiny metal surfaces",
          "Water reflections",
          "Glass"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Diffuse Reflection": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Diffuse reflection occurs when light striking a rough surface is scattered in many directions, with equal radiance in all directions (Lambertian reflection).",
      "description": "It is responsible for the matte appearance of non-shiny surfaces. In vision, it is used to estimate surface orientation and albedo under known lighting.",
      "properties": {
        "Goal": "Model light scattering from rough, matte surfaces.",
        "Applications": [
          "Photometric stereo",
          "Albedo estimation",
          "Shape recovery"
        ],
        "Methods": [
          "Lambert's cosine law: intensity ∝ cos(θ)"
        ],
        "Examples": [
          "Paper",
          "Cloth",
          "Painted walls"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Light": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Light is electromagnetic radiation within the visible spectrum (approximately 380–700 nm) that can be perceived by the human visual system.",
      "description": "In computer vision, light is modeled as energy propagating through space, interacting with matter via reflection, transmission, and absorption. Its spectral composition determines perceived color.",
      "properties": {
        "Goal": "Enable visual perception and image formation.",
        "Applications": [
          "Illumination modeling",
          "Color analysis",
          "Radiometry"
        ],
        "Methods": [
          "Spectral power distribution",
          "Wavelength-based analysis"
        ],
        "Examples": [
          "Sunlight",
          "LED illumination",
          "Laser light"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Visible Spectrum": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The visible spectrum is the portion of the electromagnetic spectrum with wavelengths between approximately 380 and 700 nanometers, detectable by the human eye.",
      "description": "Different wavelengths within this range are perceived as different colors, from violet (short) to red (long). It forms the basis for trichromatic color vision in humans and most imaging sensors.",
      "properties": {
        "Goal": "Define the range of light responsible for color vision.",
        "Applications": [
          "Color imaging",
          "Spectral analysis",
          "Display technology"
        ],
        "Methods": [
          "Spectrophotometry",
          "Prism dispersion"
        ],
        "Examples": [
          "380 nm → violet",
          "550 nm → green",
          "700 nm → red"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Illuminating Source": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "An illuminating source is any object or phenomenon that emits electromagnetic radiation, contributing directly to scene illumination.",
      "description": "The perceived color depends on the spectral power distribution of the emitted light. Multiple sources combine additively. Examples include natural (sun) and artificial (bulbs) light.",
      "properties": {
        "Goal": "Provide primary light energy to a scene.",
        "Applications": [
          "Color constancy",
          "White balancing",
          "Relighting"
        ],
        "Methods": [
          "Additive color mixing",
          "Spectral emission modeling"
        ],
        "Examples": [
          "Sun",
          "Incandescent bulb",
          "Fluorescent light",
          "Monitor screen"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Reflecting Source": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "A reflecting source is a surface that reflects incident light from illuminating sources, contributing to the light reaching the observer or camera.",
      "description": "Its color depends on which wavelengths are absorbed and which are reflected (subtractive mixing). Pigments and dyes are common examples.",
      "properties": {
        "Goal": "Modulate incident light to produce colored appearance.",
        "Applications": [
          "Color segmentation",
          "Material recognition",
          "Reflectance estimation"
        ],
        "Methods": [
          "Subtractive color mixing",
          "Spectral reflectance curves"
        ],
        "Examples": [
          "Red paint",
          "Green leaf",
          "Blue fabric"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Additive Rule": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The additive rule states that the total spectrum observed from multiple illuminating sources is the sum of their individual spectral power distributions.",
      "description": "This linear superposition applies to light sources emitting directly into the scene. It is fundamental to color mixing in displays and multi-light illumination models.",
      "properties": {
        "Goal": "Predict combined effect of multiple light emitters.",
        "Applications": [
          "Stage lighting",
          "Display calibration",
          "Multi-spectral imaging"
        ],
        "Methods": [
          "Spectral summation: E_total(λ) = Σ E_i(λ)"
        ],
        "Examples": [
          "Mixing red + green light → yellow"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Subtractive Rule": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The subtractive rule describes how the perceived color of mixed reflecting sources depends on the remaining wavelengths after selective absorption by each material.",
      "description": "Used in printing and pigments, it models how filters or dyes remove parts of the spectrum. The result is the intersection of reflected wavelengths.",
      "properties": {
        "Goal": "Model color formation via selective light absorption.",
        "Applications": [
          "Color printing (CMYK)",
          "Paint mixing",
          "Filter design"
        ],
        "Methods": [
          "Multiplicative reflectance: R_total(λ) = Π R_i(λ)"
        ],
        "Examples": [
          "Yellow + magenta filter → red transmission"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Reflection": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Reflection is the process by which light bounces off a surface, changing direction without being absorbed or transmitted.",
      "description": "It can be specular (mirror-like) or diffuse (scattered), depending on surface roughness. Essential for image formation in most vision systems.",
      "properties": {
        "Goal": "Redirect incident light toward observer or sensor.",
        "Applications": [
          "Shape reconstruction",
          "BRDF estimation",
          "Photometric analysis"
        ],
        "Methods": [
          "Law of reflection",
          "Fresnel equations"
        ],
        "Examples": [
          "Mirror",
          "Polished metal",
          "Matte paper"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "numerical_matrix"
        ],
        "source_files": [
          "03_data.json",
          "extra_00_06.json"
        ]
      }
    },
    "Transmission": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Transmission is the passage of light through a material without significant scattering or absorption, typically in transparent or translucent media.",
      "description": "It allows light to propagate through objects like glass or water, often with refraction. Important in underwater imaging and optical systems.",
      "properties": {
        "Goal": "Allow light to pass through matter.",
        "Applications": [
          "Lens modeling",
          "Underwater vision",
          "Medical imaging"
        ],
        "Methods": [
          "Beer-Lambert law",
          "Refraction (Snell's law)"
        ],
        "Examples": [
          "Clear glass",
          "Water",
          "Air"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Absorption": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Absorption is the process by which the energy of light is converted into another form (usually heat) when interacting with matter.",
      "description": "It selectively removes certain wavelengths, determining the color of reflecting objects. Critical for understanding material appearance and spectral selectivity.",
      "properties": {
        "Goal": "Convert light energy into internal energy.",
        "Applications": [
          "Spectral imaging",
          "Color filtering",
          "Thermal imaging"
        ],
        "Methods": [
          "Absorption coefficient",
          "Beer-Lambert law"
        ],
        "Examples": [
          "Black surface absorbing all light",
          "Red filter absorbing blue/green"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "03_data.json"
        ]
      }
    },
    "Binary Image": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "A binary image is a digital image that has only two possible values for each pixel, typically represented as 0 (black) and 1 (white), representing background and foreground respectively.",
      "description": "It is the result of thresholding a grayscale image and serves as a simplified representation for segmentation, object detection, and morphological processing in computer vision pipelines.",
      "properties": {
        "Goal": "Separate objects of interest from the background using intensity thresholds.",
        "Applications": [
          "Object segmentation",
          "Document analysis",
          "Medical imaging",
          "Industrial inspection"
        ],
        "Methods": [
          "Thresholding",
          "Otsu's method",
          "Adaptive thresholding"
        ],
        "Examples": [
          "Scanned text documents",
          "Silhouettes",
          "Mask images"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Thresholding": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Thresholding is an image segmentation technique that converts a grayscale image into a binary image by assigning pixel values above a threshold to one class and below to another.",
      "description": "It is one of the simplest and most widely used segmentation methods, effective when objects and background have sufficiently different intensity distributions.",
      "properties": {
        "Goal": "Create a binary mask separating foreground from background.",
        "Applications": [
          "Document binarization",
          "Motion detection",
          "Edge-based segmentation"
        ],
        "Methods": [
          "Global thresholding",
          "Local/adaptive thresholding",
          "Otsu's method"
        ],
        "Examples": [
          "Converting medical scans to highlight tumors",
          "Isolating text in scanned pages"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision",
          "machine_vision_v1"
        ],
        "source_files": [
          "04_data.json",
          "initial.json"
        ]
      }
    },
    "Preprocessing": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Preprocessing refers to operations applied to an input image before the main analysis to enhance relevant features and suppress noise or unwanted variations.",
      "description": "In binary image analysis, it improves the quality of subsequent thresholding and segmentation by reducing noise, normalizing illumination, and enhancing contrast.",
      "properties": {
        "Goal": "Improve image quality for robust downstream processing.",
        "Applications": [
          "Noise reduction",
          "Contrast enhancement",
          "Illumination correction"
        ],
        "Methods": [
          "Smoothing filters",
          "Histogram equalization",
          "Bilateral filtering"
        ],
        "Examples": [
          "Applying Gaussian blur before thresholding",
          "Removing salt-and-pepper noise"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Segmentation": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Segmentation is the process of partitioning an image into meaningful regions or objects, often producing a binary mask where each region is labeled.",
      "description": "In binary vision, it typically results in a foreground/background separation and is a critical step before recognition or measurement.",
      "properties": {
        "Goal": "Isolate objects of interest from the background.",
        "Applications": [
          "Object counting",
          "Defect detection",
          "Character recognition"
        ],
        "Methods": [
          "Thresholding",
          "Edge detection",
          "Region growing"
        ],
        "Examples": [
          "Separating cells in microscopy",
          "Extracting text blocks"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Post-processing": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Post-processing involves refinement operations applied to segmented binary images to correct errors, remove noise, and improve object integrity.",
      "description": "Common operations include morphological filtering to close gaps, remove small islands, or smooth boundaries in binary masks.",
      "properties": {
        "Goal": "Clean and refine binary segmentation results.",
        "Applications": [
          "Hole filling",
          "Noise removal",
          "Boundary smoothing"
        ],
        "Methods": [
          "Morphological closing",
          "Opening",
          "Connected component filtering"
        ],
        "Examples": [
          "Removing small speckles after thresholding",
          "Filling holes in segmented objects"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Shape Representation": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Shape representation involves extracting compact and descriptive features from binary object silhouettes to enable recognition and classification.",
      "description": "It transforms raw pixel data into structural descriptors such as contours, moments, or geometric properties for higher-level analysis.",
      "properties": {
        "Goal": "Encode object geometry in a recognition-friendly format.",
        "Applications": [
          "Object classification",
          "Pose estimation",
          "Defect analysis"
        ],
        "Methods": [
          "Contour extraction",
          "Moment invariants",
          "Fourier descriptors"
        ],
        "Examples": [
          "Circularity measure",
          "Bounding box",
          "Convex hull"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Labeling": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Labeling is the process of assigning a unique identifier to each connected component in a binary image, enabling individual object analysis.",
      "description": "Also known as connected component labeling, it is essential for counting, measuring, and tracking distinct objects in a scene.",
      "properties": {
        "Goal": "Identify and enumerate separate objects in a binary mask.",
        "Applications": [
          "Particle counting",
          "Cell tracking",
          "Character segmentation"
        ],
        "Methods": [
          "Two-pass algorithm",
          "Union-find",
          "Recursive labeling"
        ],
        "Examples": [
          "Labeling individual coins in an image",
          "Numbering text lines"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Recognition": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Recognition in binary image analysis refers to classifying or identifying objects based on their shape, size, or extracted features from labeled regions.",
      "description": "It maps low-level binary patterns to semantic categories using prior knowledge, templates, or learned models.",
      "properties": {
        "Goal": "Assign meaningful labels to detected objects.",
        "Applications": [
          "OCR",
          "Symbol recognition",
          "Part identification"
        ],
        "Methods": [
          "Template matching",
          "Feature classification",
          "Statistical pattern recognition"
        ],
        "Examples": [
          "Identifying machine parts on a conveyor",
          "Reading license plates"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Binary Image Analysis Pipeline": {
      "type": "Framework",
      "domain": "Computer Vision",
      "definition": "The binary image analysis pipeline is a sequential workflow consisting of preprocessing, segmentation, post-processing, labeling, shape representation, and recognition to extract meaningful information from images.",
      "description": "It represents a classical, modular approach to vision tasks where images are progressively transformed from raw pixels to semantic understanding via binary intermediates.",
      "properties": {
        "Goal": "Achieve automatic object understanding through structured processing stages.",
        "Applications": [
          "Document processing",
          "Industrial automation",
          "Biomedical analysis"
        ],
        "Methods": [
          "Modular stage-wise processing",
          "Feedback loops (optional)"
        ],
        "Examples": [
          "OCR system pipeline",
          "Defect detection in manufacturing"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "04_data.json"
        ]
      }
    },
    "Texture": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Texture refers to the spatial arrangement and variation of intensity or color values in an image region, representing surface patterns independent of overall color or brightness.",
      "description": "It provides critical information about material properties and structural repetition. Texture analysis enables segmentation, classification, and synthesis in natural and artificial scenes.",
      "properties": {
        "Goal": "Capture local spatial patterns and repetitions in image intensity or color.",
        "Applications": [
          "Material classification",
          "Image segmentation",
          "Defect detection",
          "Content-based retrieval"
        ],
        "Methods": [
          "Statistical methods",
          "Filter banks",
          "Structural analysis"
        ],
        "Examples": [
          "Grass",
          "Sand",
          "Brick wall",
          "Checkerboard",
          "Striped fabric"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Texture Segmentation": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Texture segmentation is the process of partitioning an image into regions with homogeneous texture properties, separating areas of different surface patterns.",
      "description": "It extends intensity-based segmentation by incorporating local spatial statistics or frequency content to distinguish textured regions.",
      "properties": {
        "Goal": "Group pixels into regions sharing similar textural appearance.",
        "Applications": [
          "Medical imaging",
          "Remote sensing",
          "Fabric inspection",
          "Scene understanding"
        ],
        "Methods": [
          "Supervised classification",
          "Unsupervised clustering",
          "Filter response analysis"
        ],
        "Examples": [
          "Separating grass from sky",
          "Isolating wood grain from metal"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Texture Descriptors": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Texture descriptors are quantitative features extracted from image patches to characterize local texture properties in a compact, discriminative form.",
      "description": "They transform raw pixel neighborhoods into feature vectors suitable for classification, clustering, or similarity comparison.",
      "properties": {
        "Goal": "Encode texture appearance into robust, comparable feature representations.",
        "Applications": [
          "Texture classification",
          "Retrieval",
          "Synthesis",
          "Anomaly detection"
        ],
        "Methods": [
          "Gray-level co-occurrence matrix",
          "Local Binary Patterns",
          "Gabor filters",
          "Histogram of gradients"
        ],
        "Examples": [
          "GLCM contrast",
          "LBP codes",
          "Filter bank energy"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Local Binary Patterns": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Local Binary Patterns (LBP) is a texture descriptor that labels each pixel by thresholding its neighborhood and encoding the result as a binary pattern, forming a rotation-invariant texture spectrum.",
      "description": "It is computationally efficient and robust to monotonic illumination changes, widely used for face recognition and texture classification.",
      "properties": {
        "Goal": "Describe local texture via binary comparisons with center pixel.",
        "Applications": [
          "Face recognition",
          "Biomedical texture analysis",
          "Industrial surface inspection"
        ],
        "Methods": [
          "Circular neighborhood sampling",
          "Binary encoding",
          "Histogram aggregation"
        ],
        "Examples": [
          "LBP(8,1) code",
          "Uniform LBP",
          "Rotation-invariant LBP"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Statistical Texture Analysis": {
      "type": "Framework",
      "domain": "Computer Vision",
      "definition": "Statistical texture analysis models texture as the spatial distribution of gray-level values, using statistical measures to characterize local intensity variations.",
      "description": "It assumes texture arises from repeated patterns or random processes and uses first- or second-order statistics to describe regions.",
      "properties": {
        "Goal": "Quantify texture via intensity distribution and spatial relationships.",
        "Applications": [
          "Remote sensing",
          "Medical diagnostics",
          "Quality control"
        ],
        "Methods": [
          "Co-occurrence matrices",
          "Run-length encoding",
          "Autocorrelation"
        ],
        "Examples": [
          "GLCM energy",
          "Contrast",
          "Entropy"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Structural Texture Analysis": {
      "type": "Framework",
      "domain": "Computer Vision",
      "definition": "Structural texture analysis assumes texture is composed of repeated primitive elements (textons) arranged according to placement rules.",
      "description": "It focuses on identifying basic texture elements and their spatial organization, suitable for regular, man-made patterns.",
      "properties": {
        "Goal": "Decompose texture into primitives and syntactic rules.",
        "Applications": [
          "Fabric design",
          "Pattern recognition",
          "Synthetic texture generation"
        ],
        "Methods": [
          "Morphological operations",
          "Grammar-based modeling",
          "Texton mapping"
        ],
        "Examples": [
          "Brick wall = rectangle + grid",
          "Checkerboard = square + alternation"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "05_data.json"
        ]
      }
    },
    "Local Features": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Local features are distinctive image patterns that are detected at specific points (keypoints) and described in a way that enables reliable matching across different views, scales, or illumination conditions.",
      "description": "They serve as the foundation for tasks requiring correspondence between images, such as stitching, 3D reconstruction, object recognition, and tracking. A complete local feature pipeline includes detection, description, and matching.",
      "properties": {
        "Goal": "Establish robust point correspondences between images under geometric and photometric transformations.",
        "Applications": [
          "Image matching",
          "Panorama stitching",
          "Structure from Motion",
          "Object recognition"
        ],
        "Methods": [
          "Keypoint detection",
          "Descriptor extraction",
          "Feature matching"
        ],
        "Examples": [
          "Harris corners",
          "SIFT keypoints",
          "ORB features"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Keypoint Detection": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Keypoint detection is the process of identifying salient, repeatable locations in an image where local features can be reliably extracted and matched across different views.",
      "description": "Good keypoints are typically corners, blobs, or junctions that remain stable under small transformations. Detection is the first stage in the local feature pipeline.",
      "properties": {
        "Goal": "Locate stable points invariant to translation, rotation, and scale (to varying degrees).",
        "Applications": [
          "Feature-based alignment",
          "Visual odometry",
          "Augmented reality"
        ],
        "Methods": [
          "Harris",
          "FAST",
          "DoG (SIFT)",
          "MSER"
        ],
        "Examples": [
          "Corner points",
          "Blob centers",
          "Region extrema"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Feature Description": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Feature description involves computing a compact, discriminative vector (descriptor) from the image patch around a detected keypoint to enable robust matching.",
      "description": "The descriptor captures local appearance and is designed to be invariant to scale, rotation, and illumination changes. It is the second stage after keypoint detection.",
      "properties": {
        "Goal": "Encode local image appearance into a matching-friendly representation.",
        "Applications": [
          "Wide baseline matching",
          "Object retrieval",
          "Loop closure in SLAM"
        ],
        "Methods": [
          "SIFT",
          "SURF",
          "BRIEF",
          "ORB",
          "Histogram of gradients"
        ],
        "Examples": [
          "128D SIFT vector",
          "64D SURF",
          "256-bit BRIEF"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Feature Matching": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Feature matching is the process of finding correspondences between descriptors from two images by measuring similarity (e.g., Euclidean distance or Hamming distance).",
      "description": "It establishes pairwise associations between keypoints, enabling geometric verification (e.g., via RANSAC) to filter outliers. It is the final stage in local feature pipelines.",
      "properties": {
        "Goal": "Identify correct point-to-point correspondences across images.",
        "Applications": [
          "Image stitching",
          "3D reconstruction",
          "Visual tracking"
        ],
        "Methods": [
          "Nearest neighbor",
          "Ratio test",
          "Cross-checking",
          "FLANN"
        ],
        "Examples": [
          "Matching SIFT descriptors",
          "Hamming distance for binary ORB"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Image Gradient": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The image gradient is a vector field representing the directional change in intensity at each pixel, computed as the partial derivatives in x and y directions.",
      "description": "It is fundamental to edge and corner detection. The magnitude indicates edge strength, and the direction indicates edge orientation.",
      "properties": {
        "Goal": "Quantify local intensity changes for feature detection.",
        "Applications": [
          "Edge detection",
          "Corner detection",
          "Optical flow"
        ],
        "Methods": [
          "Sobel operator",
          "Prewitt",
          "Finite differences"
        ],
        "Examples": [
          "∇I = [∂I/∂x, ∂I/∂y]"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Structure Tensor": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The structure tensor (or second-moment matrix) is a 2x2 matrix that summarizes the predominant directions and strength of gradients in a local image neighborhood.",
      "description": "It is used in corner detection to distinguish corners (high variation in all directions), edges (variation in one direction), and flat regions (low variation).",
      "properties": {
        "Goal": "Analyze local gradient distribution for feature classification.",
        "Applications": [
          "Corner detection",
          "Motion estimation",
          "Texture analysis"
        ],
        "Methods": [
          "M = Σ w [Ix², IxIy; IxIy, Iy²]",
          "Eigenvalue analysis"
        ],
        "Examples": [
          "Harris response",
          "Shi-Tomasi corner measure"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "06_data.json"
        ]
      }
    },
    "Pattern Recognition": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Pattern recognition is the process of automatically detecting and classifying structured patterns or regularities in data, enabling machines to interpret complex signals such as images, speech, or text.",
      "description": "In computer vision, it involves identifying objects, faces, actions, or scenes by learning discriminative features from labeled data and applying statistical or structural models for decision-making.",
      "properties": {
        "Goal": "Assign meaningful labels to input data based on learned patterns.",
        "Applications": [
          "Object recognition",
          "Face detection",
          "Medical diagnosis",
          "Document analysis"
        ],
        "Methods": [
          "Statistical classification",
          "Template matching",
          "Neural networks",
          "Syntactic analysis"
        ],
        "Examples": [
          "Recognizing handwritten digits",
          "Identifying tumors in MRI scans"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Statistical Pattern Recognition": {
      "type": "Framework",
      "domain": "Computer Vision",
      "definition": "Statistical pattern recognition models patterns as random variables and uses probability theory to make decisions based on feature measurements and class-conditional densities.",
      "description": "It assumes patterns are represented by feature vectors in a high-dimensional space and applies Bayesian decision theory to minimize classification error.",
      "properties": {
        "Goal": "Minimize classification error using probabilistic models.",
        "Applications": [
          "Face recognition",
          "Character recognition",
          "Speech processing"
        ],
        "Methods": [
          "Bayes classifier",
          "Maximum likelihood",
          "Gaussian mixture models",
          "k-NN"
        ],
        "Examples": [
          "Classifying iris patterns",
          "Spam email filtering"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Structural Pattern Recognition": {
      "type": "Framework",
      "domain": "Computer Vision",
      "definition": "Structural pattern recognition represents patterns using symbolic data structures such as strings, trees, or graphs, emphasizing relational and syntactic organization.",
      "description": "It is suitable for patterns with explicit compositional structure, using grammars and parsing to recognize hierarchical relationships.",
      "properties": {
        "Goal": "Recognize patterns based on their compositional and relational structure.",
        "Applications": [
          "Syntactic OCR",
          "Fingerprint analysis",
          "Chemical structure recognition"
        ],
        "Methods": [
          "Formal grammars",
          "Graph matching",
          "Parsing algorithms"
        ],
        "Examples": [
          "Recognizing sentence structure",
          "Identifying molecular bonds"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Feature Vector": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "A feature vector is a fixed-length numerical representation of an input pattern, where each element corresponds to a measured or derived attribute.",
      "description": "It transforms raw data into a format suitable for statistical classifiers, enabling distance-based or probabilistic decision-making in pattern space.",
      "properties": {
        "Goal": "Map complex patterns into comparable numerical space.",
        "Applications": [
          "Classification",
          "Clustering",
          "Regression"
        ],
        "Methods": [
          "Feature extraction",
          "Dimensionality reduction",
          "Normalization"
        ],
        "Examples": [
          "HOG features for pedestrians",
          "MFCC for speech",
          "Pixel intensities"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Classifier": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "A classifier is a function that maps input feature vectors to discrete class labels, typically learned from labeled training examples.",
      "description": "It forms the decision stage in pattern recognition systems, using learned boundaries or probabilities to assign new observations to predefined categories.",
      "properties": {
        "Goal": "Assign input patterns to correct categories with minimal error.",
        "Applications": [
          "Object detection",
          "Face verification",
          "Anomaly detection"
        ],
        "Methods": [
          "Linear discriminant",
          "SVM",
          "Decision trees",
          "Neural networks"
        ],
        "Examples": [
          "k-NN",
          "Naive Bayes",
          "Random Forest"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Template Matching": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Template matching is a pattern recognition technique that searches for a known pattern (template) in an image by sliding it across all locations and measuring similarity.",
      "description": "It is simple and effective for rigid, well-defined patterns under controlled conditions but sensitive to scale, rotation, and illumination changes.",
      "properties": {
        "Goal": "Locate instances of a known pattern within an image.",
        "Applications": [
          "Character recognition",
          "Defect detection",
          "Logo identification"
        ],
        "Methods": [
          "Correlation",
          "Normalized cross-correlation",
          "SSD"
        ],
        "Examples": [
          "Finding 'STOP' signs",
          "Matching printed circuit patterns"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Bayes Classifier": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The Bayes classifier assigns an input pattern to the class with the highest posterior probability, computed using Bayes' theorem and class-conditional likelihoods.",
      "description": "It is statistically optimal when probability distributions are known, forming the theoretical foundation for many practical classifiers.",
      "properties": {
        "Goal": "Minimize expected classification error using probabilistic inference.",
        "Applications": [
          "Spam filtering",
          "Medical diagnosis",
          "Document classification"
        ],
        "Methods": [
          "P(class|features) = P(features|class) * P(class) / P(features)"
        ],
        "Examples": [
          "Naive Bayes",
          "Gaussian Bayes classifier"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Object Recognition": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Object recognition is the task of identifying and localizing specific objects within images or video, assigning them semantic class labels.",
      "description": "It combines detection (where) and classification (what) and is a core capability in autonomous systems, surveillance, and augmented reality.",
      "properties": {
        "Goal": "Detect and classify objects in visual scenes.",
        "Applications": [
          "Autonomous driving",
          "Robotics",
          "Image search",
          "Security"
        ],
        "Methods": [
          "Sliding window",
          "Region proposals",
          "Deep learning (CNNs)"
        ],
        "Examples": [
          "Detecting pedestrians",
          "Recognizing furniture in rooms"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "07_data.json"
        ]
      }
    },
    "Motion": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Motion in computer vision refers to the apparent displacement of image brightness patterns between consecutive frames in a video sequence, caused by relative movement between the camera and the scene.",
      "description": "It provides critical information about scene dynamics, enabling applications such as tracking, segmentation, and 3D reconstruction. Motion analysis distinguishes between camera motion, object motion, and complex non-rigid deformations.",
      "properties": {
        "Goal": "Estimate and interpret changes in image appearance over time.",
        "Applications": [
          "Video stabilization",
          "Object tracking",
          "Action recognition",
          "Autonomous navigation"
        ],
        "Methods": [
          "Optical flow",
          "Feature tracking",
          "Block matching",
          "Differential methods"
        ],
        "Examples": [
          "Car moving across frames",
          "Person walking",
          "Camera panning"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Optical Flow": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Optical flow is the dense 2D vector field representing the apparent motion of brightness patterns between two consecutive image frames, assigning a displacement vector to each pixel.",
      "description": "It models local image changes under the brightness constancy assumption and is used to estimate scene motion, segment moving objects, and support higher-level video analysis.",
      "properties": {
        "Goal": "Compute per-pixel motion vectors between image pairs.",
        "Applications": [
          "Motion compensation",
          "Video compression",
          "Robot navigation",
          "Medical imaging"
        ],
        "Methods": [
          "Lucas-Kanade (sparse)",
          "Horn-Schunck (dense)",
          "Farneback",
          "Deep learning-based flow"
        ],
        "Examples": [
          "Flow field around a moving car",
          "Expansion flow from camera zoom"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Brightness Constancy Assumption": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "The brightness constancy assumption states that the intensity of a moving point remains constant between consecutive frames, i.e., I(x, t) = I(x + d, t + 1), where d is the displacement.",
      "description": "It is the foundational constraint in differential optical flow methods, enabling the formulation of the optical flow constraint equation from spatiotemporal image gradients.",
      "properties": {
        "Goal": "Link pixel intensity across time for motion estimation.",
        "Applications": [
          "Optical flow computation",
          "Motion detection",
          "Tracking"
        ],
        "Methods": [
          "Gradient-based flow",
          "Aperture problem solving"
        ],
        "Examples": [
          "Valid for Lambertian surfaces under constant illumination"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Lucas-Kanade Method": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The Lucas-Kanade method is a sparse, differential technique for optical flow estimation that assumes local motion constancy within small spatial neighborhoods and solves for motion using weighted least squares.",
      "description": "It is efficient and robust when applied to textured regions or tracked features, commonly used in real-time tracking and sparse flow applications.",
      "properties": {
        "Goal": "Estimate local motion at feature points with sub-pixel accuracy.",
        "Applications": [
          "Feature tracking",
          "Visual odometry",
          "Augmented reality"
        ],
        "Methods": [
          "Local window analysis",
          "Weighted least squares",
          "Pyramidal refinement"
        ],
        "Examples": [
          "Tracking facial landmarks",
          "Corner point tracking in video"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Horn-Schunck Method": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The Horn-Schunck method is a dense optical flow algorithm that minimizes a global energy functional combining the brightness constancy error and a smoothness term across the entire image.",
      "description": "It produces smooth flow fields by enforcing spatial coherence, making it suitable for dense motion estimation even in textureless regions.",
      "properties": {
        "Goal": "Compute smooth, dense optical flow over the full image.",
        "Applications": [
          "Motion segmentation",
          "Video editing",
          "Fluid flow visualization"
        ],
        "Methods": [
          "Variational optimization",
          "Euler-Lagrange equations",
          "Iterative solvers"
        ],
        "Examples": [
          "Global flow in translating scenes",
          "Dense flow in medical ultrasound"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Motion Segmentation": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Motion segmentation is the process of partitioning a video into regions corresponding to independently moving objects based on their motion patterns.",
      "description": "It leverages optical flow or feature trajectories to group pixels with coherent motion, enabling object-level video analysis.",
      "properties": {
        "Goal": "Separate independently moving objects in dynamic scenes.",
        "Applications": [
          "Video surveillance",
          "Autonomous driving",
          "Action analysis"
        ],
        "Methods": [
          "Flow clustering",
          "Layered motion models",
          "Graph cuts"
        ],
        "Examples": [
          "Separating pedestrian from background",
          "Isolating multiple cars"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Feature Tracking": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Feature tracking involves detecting distinctive points in one frame and finding their corresponding locations in subsequent frames using local search and similarity metrics.",
      "description": "It is a sparse motion representation widely used in real-time systems due to low computational cost and robustness when features are well-distributed.",
      "properties": {
        "Goal": "Maintain persistent point correspondences across video frames.",
        "Applications": [
          "SLAM",
          "Structure from Motion",
          "Camera pose estimation"
        ],
        "Methods": [
          "KLT tracker",
          "Descriptor matching",
          "Sub-pixel refinement"
        ],
        "Examples": [
          "Tracking Harris corners over time",
          "Sparse trajectory bundles"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "08_data.json"
        ]
      }
    },
    "Image Matching": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Image matching is the process of establishing correspondences between two or more images of the same scene taken from different viewpoints, times, or sensors to determine geometric or photometric relationships.",
      "description": "It forms the foundation for applications requiring image alignment, such as panorama stitching, 3D reconstruction, and visual tracking. Matching can be sparse (feature-based) or dense (pixel-wise).",
      "properties": {
        "Goal": "Find reliable point or region correspondences across images.",
        "Applications": [
          "Structure from Motion",
          "Stereo vision",
          "Image retrieval",
          "Augmented reality"
        ],
        "Methods": [
          "Feature-based matching",
          "Area-based matching",
          "Direct methods",
          "Learning-based matching"
        ],
        "Examples": [
          "Matching SIFT features",
          "Dense optical flow",
          "Template correlation"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Epipolar Geometry": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Epipolar geometry describes the intrinsic projective relationship between two views of a 3D scene, constraining the location of corresponding points to lie on corresponding epipolar lines.",
      "description": "It reduces the search space for matching from 2D to 1D and is encapsulated by the fundamental matrix (for uncalibrated cameras) or essential matrix (for calibrated cameras).",
      "properties": {
        "Goal": "Constrain correspondence search using stereo geometry.",
        "Applications": [
          "Stereo matching",
          "Camera calibration",
          "Motion estimation"
        ],
        "Methods": [
          "Fundamental matrix estimation",
          "Essential matrix",
          "Epipolar line computation"
        ],
        "Examples": [
          "Left image point maps to line in right image"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Fundamental Matrix": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The fundamental matrix is a 3x3 rank-2 matrix that encapsulates the epipolar geometry between two uncalibrated images, mapping points in one image to epipolar lines in the other.",
      "description": "It is estimated from point correspondences and used to enforce geometric consistency in matching and reconstruction. F satisfies x'ᵀ F x = 0 for corresponding points x and x'.",
      "properties": {
        "Goal": "Encode projective geometry between two views without calibration.",
        "Applications": [
          "Correspondence validation",
          "Camera pose estimation",
          "Image rectification"
        ],
        "Methods": [
          "8-point algorithm",
          "7-point algorithm",
          "RANSAC estimation"
        ],
        "Examples": [
          "F computed from 8+ matches",
          "Used in guided matching"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Essential Matrix": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "The essential matrix is a 3x3 matrix that describes the epipolar geometry between two calibrated cameras, encoding relative rotation and translation (up to scale) between their coordinate systems.",
      "description": "It is a specialized version of the fundamental matrix when intrinsic parameters are known. E = K'ᵀ F K, and it has exactly two equal non-zero singular values.",
      "properties": {
        "Goal": "Recover relative pose from calibrated image pairs.",
        "Applications": [
          "5-point algorithm",
          "Visual odometry",
          "SLAM initialization"
        ],
        "Methods": [
          "5-point algorithm",
          "Nister's method",
          "Decomposition into R and t"
        ],
        "Examples": [
          "E from 5+ matches in calibrated stereo"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "RANSAC": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "RANSAC (Random Sample Consensus) is a robust estimation algorithm that fits a model to data containing outliers by iteratively selecting random subsets, estimating the model, and counting inliers.",
      "description": "It is widely used in geometric vision to estimate fundamental/essential matrices, homographies, or camera poses from noisy correspondences.",
      "properties": {
        "Goal": "Estimate model parameters robustly in the presence of outliers.",
        "Applications": [
          "Feature matching cleanup",
          "Pose estimation",
          "Image stitching"
        ],
        "Methods": [
          "Random sampling",
          "Model fitting",
          "Inlier counting",
          "Iteration until confidence"
        ],
        "Examples": [
          "Estimating F from 1000 matches with 60% outliers"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Stereo Matching": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Stereo matching is the process of finding corresponding pixels between a pair of rectified stereo images to compute depth via triangulation, producing a disparity map.",
      "description": "It exploits epipolar constraints to reduce search to 1D and uses similarity measures (e.g., SAD, NCC) with aggregation and optimization to handle ambiguity and occlusions.",
      "properties": {
        "Goal": "Compute dense depth from calibrated stereo pairs.",
        "Applications": [
          "3D reconstruction",
          "Robot navigation",
          "Autonomous driving"
        ],
        "Methods": [
          "Local block matching",
          "Semi-global matching",
          "Graph cuts",
          "Deep stereo"
        ],
        "Examples": [
          "Disparity map from left-right image pair"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Image Rectification": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Image rectification is the process of warping a stereo image pair so that epipolar lines become horizontal and conjugate points have the same y-coordinate, simplifying correspondence search.",
      "description": "It enables efficient scanline-based stereo algorithms and is typically performed using the fundamental matrix or known camera parameters.",
      "properties": {
        "Goal": "Align epipolar lines for efficient stereo matching.",
        "Applications": [
          "Dense stereo",
          "Disparity estimation",
          "Multi-view stereo"
        ],
        "Methods": [
          "Hartley’s method",
          "Polar rectification",
          "Calibrated rectification"
        ],
        "Examples": [
          "Transforming converging epipoles to infinity"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Homography": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "A homography is a 3x3 projective transformation that maps points from one plane to another (or between two views of the same plane), preserving collinearity but not distances or angles.",
      "description": "It is used to align images of planar scenes, remove perspective distortion, and enable direct pixel-wise comparison in matching.",
      "properties": {
        "Goal": "Map image coordinates between planar views.",
        "Applications": [
          "Panorama stitching",
          "Image mosaicking",
          "Augmented reality",
          "Document scanning"
        ],
        "Methods": [
          "DLT (Direct Linear Transformation)",
          "RANSAC estimation from 4+ points"
        ],
        "Examples": [
          "Warping book page to frontal view"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "09_data.json"
        ]
      }
    },
    "Depth from Stereo": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Depth from stereo is the recovery of 3D scene depth by triangulating corresponding points in a pair of images captured from slightly different viewpoints (baseline).",
      "description": "It relies on disparity—the horizontal shift of a point between left and right images—to compute depth inversely proportional to disparity via calibrated camera geometry.",
      "properties": {
        "Goal": "Compute per-pixel depth using binocular disparity.",
        "Applications": [
          "3D reconstruction",
          "Robot navigation",
          "Virtual reality"
        ],
        "Methods": [
          "Stereo matching",
          "Triangulation",
          "Rectification",
          "Disparity map refinement"
        ],
        "Examples": [
          "Z = (f * B) / d, where d is disparity"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "10_data.json"
        ]
      }
    },
    "Structure from Motion": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Structure from Motion (SfM) is the simultaneous estimation of 3D scene structure and camera motion from a set of 2D images taken from different viewpoints, typically using feature correspondences.",
      "description": "It solves for camera poses and sparse 3D points via bundle adjustment, forming the basis for dense reconstruction and large-scale 3D modeling.",
      "properties": {
        "Goal": "Reconstruct 3D scene and camera trajectory from unordered image collections.",
        "Applications": [
          "Cultural heritage",
          "Aerial mapping",
          "Film production",
          "SLAM"
        ],
        "Methods": [
          "Feature tracking",
          "Incremental SfM",
          "Global SfM",
          "Bundle adjustment"
        ],
        "Examples": [
          "3D model from tourist photos",
          "Point cloud from drone video"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "10_data.json"
        ]
      }
    },
    "Monocular Depth Cues": {
      "type": "Concept",
      "domain": "Computer Vision",
      "definition": "Monocular depth cues are visual signals in a single image that provide information about relative or absolute depth without requiring multiple viewpoints.",
      "description": "They mimic human perception mechanisms and include occlusion, perspective, shading, texture gradient, and known object size, enabling depth estimation from single images.",
      "properties": {
        "Goal": "Infer 3D layout from 2D image cues.",
        "Applications": [
          "Single-image depth estimation",
          "Image editing",
          "Autonomous navigation"
        ],
        "Methods": [
          "Shape from shading",
          "Depth from defocus",
          "Learning-based monocular depth"
        ],
        "Examples": [
          "Linear perspective in roads",
          "Occlusion of distant objects"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "10_data.json"
        ]
      }
    },
    "Shape from Shading": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Shape from shading recovers surface orientation (normals) by analyzing how light intensity varies across a surface under known illumination, assuming Lambertian reflectance.",
      "description": "It solves an inverse optics problem to estimate local surface tilt from image gradients, enabling 3D reconstruction from a single image.",
      "properties": {
        "Goal": "Estimate surface normals from intensity gradients.",
        "Applications": [
          "Planetary surface mapping",
          "Medical imaging",
          "Industrial inspection"
        ],
        "Methods": [
          "Reflectance map",
          "Variational optimization",
          "Photometric stereo"
        ],
        "Examples": [
          "Reconstructing face shape from one photo"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "10_data.json"
        ]
      }
    },
    "Triangulation": {
      "type": "Method",
      "domain": "Computer Vision",
      "definition": "Triangulation is the process of determining the 3D position of a point by intersecting rays from two or more calibrated cameras that observe the corresponding 2D image points.",
      "description": "It is the geometric foundation of passive 3D reconstruction, converting image correspondences into metric 3D coordinates using camera projection matrices.",
      "properties": {
        "Goal": "Compute 3D point from 2D correspondences and camera parameters.",
        "Applications": [
          "3D reconstruction",
          "Robot localization",
          "Augmented reality"
        ],
        "Methods": [
          "Linear triangulation",
          "Mid-point method",
          "Optimal triangulation"
        ],
        "Examples": [
          "X = intersection of two rays in space"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision"
        ],
        "source_files": [
          "10_data.json"
        ]
      }
    },
    "Deep Neural Networks (DNNs)": {
      "type": "Technique/Algorithm",
      "domain": "Machine Learning",
      "definition": "Popular machine learning techniques, including Convolutional Neural Networks (CNNs), which are often used for image recognition.",
      "description": "DNNs have enabled significant progress in image recognition, particularly when utilized with large datasets such as ImageNet.",
      "properties": {
        "Goal": "Combine both representation (feature extraction) and classification steps in an end-to-end approach, learning features directly from the data.",
        "Applications": [
          "Image Recognition"
        ],
        "Methods": [
          "Convolutional Neural Networks (CNNs)"
        ],
        "Examples": [
          "ImageNet Challenge winning systems"
        ]
      },
      "metadata": {
        "subjects": [
          "machine_vision_v1"
        ],
        "source_files": [
          "initial.json"
        ]
      }
    },
    "Structure from Motion (SfM)": {
      "type": "Geometric Recovery Technique",
      "domain": "Geometry & 3D Reconstruction",
      "definition": "The problem of estimating $m$ camera matrices ($\\mathbf{M}_i$) and $n$ 3D points ($\\mathbf{P}_j$) given $m$ images of $n$ fixed points and their correspondences.",
      "description": "SfM relies on minimizing the reprojection error between observed and predicted image points through nonlinear least-squares optimization (Bundle Adjustment).",
      "properties": {
        "Goal": [
          "Estimate camera poses and 3D scene structure from 2D image sequences."
        ],
        "Applications": [
          "3D surface reconstruction"
        ],
        "Methods": [
          "Bundle Adjustment",
          "Nonlinear least-squares minimization"
        ],
        "Examples": []
      },
      "metadata": {
        "subjects": [
          "machine_vision_v1"
        ],
        "source_files": [
          "initial.json"
        ]
      }
    },
    "Sparsity Structure": {
      "type": "Concept",
      "domain": "Matrix Computations",
      "definition": "The sparsity structure of a matrix or matrix subspace specifies the pattern of entries that may be nonzero, independent of the actual numerical values.",
      "description": "Sparsity structures capture the combinatorial pattern of allowed nonzeros in matrices, enabling algorithms that exploit memory efficiency and reduce computational cost. They are essential in large-scale problems such as PDE discretizations, graph Laplacians, and structured matrix factorizations.",
      "properties": {
        "Goal": "Identify and exploit zero-patterns for efficient computation.",
        "Applications": [
          "Sparse LU factorization",
          "Finite difference discretizations",
          "Graph-based matrix algorithms"
        ],
        "Methods": [
          "Pattern analysis",
          "Fill-in minimization",
          "Graph reordering"
        ],
        "Examples": [
          "Tridiagonal pattern",
          "Band matrix structure"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "00_extra_01.json"
        ]
      }
    },
    "Standard Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Computations",
      "definition": "A standard matrix subspace is a subspace that admits a basis consisting of standard matrices, each with exactly one entry equal to 1 and all other entries equal to 0.",
      "description": "Standard matrix subspaces represent matrix sets defined purely by sparsity patterns, without structural constraints such as symmetry. Orthogonal projection onto these subspaces is straightforward—simply zero out entries outside the allowed pattern.",
      "properties": {
        "Goal": "Model matrix sets defined solely by sparsity constraints.",
        "Applications": [
          "Sparse matrix approximations",
          "Projection methods",
          "Structured LU factorization"
        ],
        "Methods": [
          "Entrywise projection",
          "Basis construction from standard matrices"
        ],
        "Examples": [
          "Diagonal matrices",
          "Strictly lower triangular matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "00_extra_01.json"
        ]
      }
    },
    "Orthogonal Projector (Matrix Subspace)": {
      "type": "Operator",
      "domain": "Numerical Analysis",
      "definition": "An orthogonal projector onto a matrix subspace V is a linear operator P such that P² = P and the range of P is orthogonal to the nullspace of P.",
      "description": "Orthogonal projectors provide optimal approximations in least-squares and matrix approximation problems. Projection onto matrix subspaces is fundamental in algorithmic factoring, dimension reduction, and solving underdetermined systems.",
      "properties": {
        "Goal": "Project matrices onto a subspace with minimal error under the Frobenius inner product.",
        "Applications": [
          "Approximate factoring",
          "Subspace splitting",
          "Sylvester-type equations"
        ],
        "Methods": [
          "Construction via orthonormal basis",
          "Symmetrization operators"
        ],
        "Examples": [
          "P(A) = (A + A*)/2 for Hermitian matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "00_extra_01.json"
        ]
      }
    },
    "Invertible Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Theory",
      "definition": "A matrix subspace V is invertible if the set of inverses of its nonsingular elements forms another matrix subspace V⁻¹.",
      "description": "Invertible matrix subspaces allow algorithmic factorization because their inverses preserve linear structure. Classical examples include triangular matrices and Hermitian matrices, which maintain structure under inversion.",
      "properties": {
        "Goal": "Support structured factorization where both A and its factors belong to linear matrix families.",
        "Applications": [
          "LU factorization",
          "Symmetric factorizations",
          "Matrix subspace factoring"
        ],
        "Methods": [
          "Closure under inversion",
          "Polynomial relations"
        ],
        "Examples": [
          "Lower triangular matrices",
          "Hermitian matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "00_extra_01.json"
        ]
      }
    },
    "Singular Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Theory",
      "definition": "A matrix subspace is singular if every matrix within it is singular, i.e., no element has a nonzero determinant.",
      "description": "Singular matrix subspaces arise in low-rank approximations, special matrix pencils, and structured operator families. They often encode degenerate behavior and limit the applicability of classical factorizations.",
      "properties": {
        "Goal": "Characterize spaces where invertibility is impossible.",
        "Applications": [
          "Rank-k matrix sets",
          "SVD truncation analysis",
          "Generalized eigenvalue pencils"
        ],
        "Methods": [
          "Nullspace analysis",
          "Subspace closure"
        ],
        "Examples": [
          "Rank-k matrices for k < n"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "00_extra_01.json"
        ]
      }
    },
    "Polynomially Closed Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Theory",
      "definition": "A matrix subspace V is polynomially closed if p(A) ∈ V for every A ∈ V and every polynomial p with scalar coefficients.",
      "description": "Polynomial closure ensures that structural properties of matrices are preserved under algebraic operations such as exponentiation, inversion, or functional calculus. This property is essential when applying iterative or polynomial-based algorithms within the subspace.",
      "properties": {
        "Goal": "Guarantee closure under matrix polynomial transformations.",
        "Applications": [
          "Iterative methods",
          "Matrix functions",
          "Structured inversion"
        ],
        "Methods": [
          "Polynomial evaluation",
          "Minimal polynomial arguments"
        ],
        "Examples": [
          "Hermitian matrices",
          "Complex symmetric matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "00_extra_01.json"
        ]
      }
    },
    "Closure of V₁V₂": {
      "type": "Concept",
      "domain": "Matrix Subspaces",
      "definition": "The closure of the product set V₁V₂ consists of all matrices that can be approximated arbitrarily well by products of matrices from subspaces V₁ and V₂.",
      "description": "The closure of V₁V₂ determines whether approximate factorizations exist even when exact ones do not. This concept plays a central role in understanding numerical stability, perturbation behavior, and feasibility of approximate matrix factorizations.",
      "properties": {
        "Goal": "Characterize attainable approximate factorizations.",
        "Applications": [
          "Approximate LU",
          "Low-rank approximations",
          "Structured matrix decompositions"
        ],
        "Methods": [
          "Topology of matrix spaces",
          "Perturbation analysis"
        ],
        "Examples": [
          "Every matrix is arbitrarily close to one with an LU factorization"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "00_extra_01.json"
        ]
      }
    },
    "Matrix Computations": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "The study and development of algorithms for performing operations on matrices, including addition, multiplication, inversion, decomposition, and solving linear systems.",
      "description": "Matrix computations are central to numerical analysis, scientific computing, and computational science. They underpin applications in physics simulations, data analysis, machine learning, and computer graphics. The computational complexity of classical algorithms for matrix multiplication is O(n³), while storage is typically O(n²).",
      "properties": {
        "Goal": "Efficiently perform fundamental matrix operations with numerical stability and minimal computational cost.",
        "Applications": [
          "Scientific computing",
          "PDE discretization",
          "Data analysis",
          "Machine learning",
          "Computer graphics"
        ],
        "Methods": [
          "Gaussian elimination",
          "LU decomposition",
          "SVD",
          "Iterative methods"
        ],
        "Examples": [
          "Matrix multiplication",
          "Solving Ax=b",
          "Eigenvalue computation"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "01_introduction.json",
          "02_product_of_matrix_subspaces_in_factoring_matrices.json"
        ]
      }
    },
    "Inner Product": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A binary operation on two vectors in ℂⁿ that produces a scalar, defined as ⟨x, y⟩ = yᵀx̄ = ∑ⱼ xⱼȳⱼ, where x, y ∈ ℂⁿ.",
      "description": "The inner product (also known as dot product in real vector spaces) measures similarity between vectors and forms the foundation of orthogonality, norms, and projections. In complex spaces, it involves conjugation to ensure positive definiteness of the induced norm.",
      "properties": {
        "Goal": "Quantify angle and similarity between vectors; enable orthogonal decomposition.",
        "Applications": [
          "Gram-Schmidt orthogonalization",
          "Least squares",
          "Signal processing",
          "Quantum mechanics"
        ],
        "Methods": [
          "N/A"
        ],
        "Examples": [
          "⟨x, y⟩ = ∑ xⱼȳⱼ over j=1 to n"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "01_introduction.json",
          "extra_01.json"
        ]
      }
    },
    "Computational Complexity (Matrix Multiplication)": {
      "type": "Metric",
      "domain": "Algorithm Analysis",
      "definition": "The asymptotic resource requirement for matrix multiplication and related operations, classically O(n³) time and O(n²) space for n×n matrices.",
      "description": "Standard matrix multiplication of two n×n matrices requires O(n³) arithmetic operations using the naive algorithm. While theoretical improvements exist (e.g., Strassen’s O(n².⁸⁰⁷)), practical methods remain close to O(n³). Storage scales quadratically as O(n²).",
      "properties": {
        "Goal": "Assess efficiency and scalability of matrix algorithms.",
        "Applications": [
          "Performance prediction",
          "Algorithm selection",
          "Hardware design"
        ],
        "Methods": [
          "Big-O notation",
          "Arithmetic circuit complexity"
        ],
        "Examples": [
          "O(n³) time",
          "O(n²) space"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "01_introduction.json",
          "extra_01.json"
        ]
      }
    },
    "LU Factorization": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "LU factorization decomposes a matrix A into a lower triangular matrix L and an upper triangular matrix U such that A = LU, or with pivoting PA = LU, enabling efficient solution of linear systems.",
      "description": "It is a fundamental algorithm with O(n^3) complexity, used for solving Ax = b by forward and backward substitution, and approximated for large matrices using subspace products.",
      "properties": {
        "Goal": "Decompose matrices for efficient linear system solving.",
        "Applications": [
          "Numerical linear algebra",
          "PDE discretization",
          "Data processing"
        ],
        "Methods": [
          "Gaussian elimination",
          "Pivoting for stability",
          "Partial pivoting"
        ],
        "Examples": [
          "A = LU for square matrices",
          "PA = LU with permutation P"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "02_product_of_matrix_subspaces_in_factoring_matrices.json"
        ]
      }
    },
    "Singular Value Decomposition": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Singular Value Decomposition (SVD) factors a matrix A into A = U Σ V^*, where U and V are unitary matrices and Σ is diagonal with singular values, providing the best low-rank approximations.",
      "description": "It is a dominant paradigm for matrix factorizations with O(n^3) complexity, useful for eigenvalue problems and data compression, but alternatives like subspace products are explored for efficiency.",
      "properties": {
        "Goal": "Provide orthogonal factorization and low-rank approximations.",
        "Applications": [
          "Data compression",
          "Principal component analysis",
          "Pseudo-inverse"
        ],
        "Methods": [
          "Golub-Reinsch algorithm",
          "Divide-and-conquer",
          "Iterative methods"
        ],
        "Examples": [
          "A ≈ U_k Σ_k V_k^* for rank-k approximation"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "02_product_of_matrix_subspaces_in_factoring_matrices.json",
          "03_the_singular_value_decomposition.json"
        ]
      }
    },
    "Eigenvalue Problem": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "The eigenvalue problem involves finding scalars λ and vectors x such that Ax = λx for a matrix A, crucial for understanding matrix properties and stability in systems.",
      "description": "It arises in PDEs and vibrations, solved with O(n^3) methods like QR algorithm, but approximations using subspace products can reduce complexity for large n.",
      "properties": {
        "Goal": "Compute eigenvalues and eigenvectors numerically.",
        "Applications": [
          "Vibration analysis",
          "Quantum mechanics",
          "Stability analysis"
        ],
        "Methods": [
          "QR algorithm",
          "Power iteration",
          "Lanczos algorithm"
        ],
        "Examples": [
          "Symmetric matrix eigenvalues",
          "Generalized eigenvalue Ax = λBx"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "02_product_of_matrix_subspaces_in_factoring_matrices.json",
          "10_eigenvalue_problems_and_functions_of_matrices.json",
          "11_Iterative methods_for_eigenvalue_problems.json"
        ]
      }
    },
    "Product of Matrix Subspaces": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "The product of matrix subspaces V1 V2 is defined as {V1 V2 : V1 ∈ V1, V2 ∈ V2}, providing a framework for approximating matrices with fewer parameters than full rank.",
      "description": "It allows factoring matrices into low-complexity forms like sum of outer products, useful for large n with small k, achieving 2nk parameters and potential for lower computational costs.",
      "properties": {
        "Goal": "Approximate matrices efficiently using subspace products.",
        "Applications": [
          "Large matrix factorization",
          "PDE discretization",
          "Data storage"
        ],
        "Methods": [
          "Subspace multiplication",
          "Rank-k approximation",
          "Norm minimization"
        ],
        "Examples": [
          "A ≈ sum_{j=1}^k u_j v_j^*",
          "I + V1 V2 inversion"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "02_product_of_matrix_subspaces_in_factoring_matrices.json"
        ]
      }
    },
    "Gram-Schmidt Orthogonalization": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Gram-Schmidt orthogonalization transforms a set of linearly independent vectors into an orthonormal set using projections and normalization.",
      "description": "It is used in QR factorization and subspace computations, with classical and modified variants for numerical stability.",
      "properties": {
        "Goal": "Produce orthonormal bases from vector sets.",
        "Applications": [
          "QR decomposition",
          "Subspace orthogonalization",
          "Least squares"
        ],
        "Methods": [
          "Classical Gram-Schmidt",
          "Modified Gram-Schmidt",
          "Householder reflections"
        ],
        "Examples": [
          "Orthogonalizing columns of A"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "02_product_of_matrix_subspaces_in_factoring_matrices.json"
        ]
      }
    },
    "Low-Rank Approximation": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Low-rank approximation finds a matrix Fk of rank k that minimizes ||A - Fk|| for a given norm, often using SVD or subspace products for efficiency.",
      "description": "It reduces storage and computation for large matrices, with subspace products offering 2nk parameters for rank-k approximations.",
      "properties": {
        "Goal": "Approximate high-dimensional matrices with lower rank.",
        "Applications": [
          "Data compression",
          "Noise reduction",
          "Machine learning"
        ],
        "Methods": [
          "SVD truncation",
          "Subspace product",
          "Randomized algorithms"
        ],
        "Examples": [
          "Fk = sum u_j v_j^*",
          "min_{rank(F)=k} ||A - F||"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "02_product_of_matrix_subspaces_in_factoring_matrices.json"
        ]
      }
    },
    "Hermitian Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A Hermitian matrix M satisfies M^* = M, where * denotes conjugate transpose, with real eigenvalues and orthogonal eigenvectors.",
      "description": "It requires n^2 real parameters for storage, or fewer in subspace approximations, used in quantum mechanics and signal processing.",
      "properties": {
        "Goal": "Model self-adjoint operators in complex spaces.",
        "Applications": [
          "Quantum computing",
          "Covariance matrices",
          "Spectral analysis"
        ],
        "Methods": [
          "Eigen decomposition",
          "Cholesky factorization (positive definite)"
        ],
        "Examples": [
          "M with real diagonal and conjugate symmetric off-diagonals"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "02_product_of_matrix_subspaces_in_factoring_matrices.json"
        ]
      }
    },
    "Singular Value": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The singular values of a matrix A are the non-negative square roots of the eigenvalues of A^*A (or AA^*), ordered as σ₁ ≥ σ₂ ≥ ⋯ ≥ σ_r > 0, where r is the rank of A.",
      "description": "Singular values measure the 'importance' or 'strength' of each principal direction in the matrix. The largest singular value σ₁ equals the operator norm ||A||, and they uniquely determine the best low-rank approximations.",
      "properties": {
        "Goal": "Quantify the magnitude of principal components in matrix decomposition.",
        "Applications": [
          "Rank determination",
          "Condition number computation (σ₁/σ_r)",
          "Numerical stability analysis"
        ],
        "Methods": [
          "Eigenvalue decomposition of A^*A",
          "Square root of eigenvalues"
        ],
        "Examples": [
          "σ₁ = ||A|| = max_{||x||=1} ||Ax|| represents maximum stretch"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "03_the_singular_value_decomposition.json"
        ]
      }
    },
    "Unitary Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A unitary matrix Q ∈ ℂ^{n×n} satisfies Q^*Q = I, meaning its columns (and rows) form an orthonormal basis that preserves the Euclidean norm: ||Qx|| = ||x|| for all x ∈ ℂ^n.",
      "description": "Unitary matrices represent rotations and reflections in complex space. They appear in QR decomposition, SVD, and eigenvalue problems, preserving distances and angles during transformations.",
      "properties": {
        "Goal": "Preserve norms and inner products in linear transformations.",
        "Applications": [
          "QR decomposition",
          "Singular Value Decomposition",
          "Numerical linear algebra algorithms",
          "Quantum computing gates"
        ],
        "Methods": [
          "Gram-Schmidt process",
          "Householder reflections"
        ],
        "Examples": [
          "Q = [q₁ ⋯ qₙ] where {qⱼ} are orthonormal vectors"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "03_the_singular_value_decomposition.json"
        ]
      }
    },
    "Low Rank Approximation": {
      "type": "Method",
      "domain": "Linear Algebra",
      "definition": "The low-rank approximation problem finds a rank-k matrix F_k that minimizes ||A - F_k||_F among all rank-k matrices, solved uniquely by SVD: F_k = ∑_{j=1}^k σⱼ uⱼ vⱼ^*.",
      "description": "SVD provides the optimal rank-k approximation in Frobenius norm, equivalent to keeping the k largest singular values. This compresses data while minimizing reconstruction error.",
      "properties": {
        "Goal": "Approximate high-dimensional matrix with lower-rank version minimizing ||A - F_k||_F.",
        "Applications": [
          "Data compression",
          "Dimensionality reduction",
          "Image denoising",
          "Recommendation systems"
        ],
        "Methods": [
          "Truncated SVD",
          "Eckart-Young theorem"
        ],
        "Examples": [
          "F_k = U_k Σ_k V_k^* where error = ∑_{j=k+1}^r σⱼ²"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "03_the_singular_value_decomposition.json"
        ]
      }
    },
    "Frobenius Norm": {
      "type": "Metric",
      "domain": "Linear Algebra",
      "definition": "The Frobenius norm of A ∈ ℂ^{m×n} is ||A||_F = √(∑_{j=1}^m ∑_{k=1}^n |a_{jk}|²) = √trace(A^*A), measuring the Euclidean norm of the matrix treated as a vector in ℂ^{mn}.",
      "description": "Widely used in matrix approximation problems due to computational convenience and equivalence to SVD error. It's unitarily invariant: ||Q₁AQ₂||_F = ||A||_F for unitary Q₁, Q₂.",
      "properties": {
        "Goal": "Measure matrix 'size' as vector in high-dimensional space.",
        "Applications": [
          "Low-rank approximation error",
          "Matrix compression",
          "Least squares problems"
        ],
        "Methods": [
          "√(sum of squared entries)",
          "√trace(A^*A)"
        ],
        "Examples": [
          "||A||_F² = ∑ σⱼ² where σⱼ are singular values"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "03_the_singular_value_decomposition.json"
        ]
      }
    },
    "Operator Norm": {
      "type": "Metric",
      "domain": "Linear Algebra",
      "definition": "The operator norm ||A|| = max_{||x||=1} ||Ax|| measures the maximum stretch factor of the linear transformation A: ℂ^n → ℂ^m.",
      "description": "For SVD, ||A|| = σ₁, the largest singular value. It quantifies how much A can amplify unit vectors and determines numerical stability.",
      "properties": {
        "Goal": "Measure maximum amplification by linear transformation.",
        "Applications": [
          "Condition number (||A|| ⋅ ||A⁻¹||)",
          "Stability analysis",
          "Spectral radius bounds"
        ],
        "Methods": [
          "Largest singular value σ₁",
          "max_{||x||=1} ||Ax||"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "03_the_singular_value_decomposition.json"
        ]
      }
    },
    "Matrix Subspace": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A subset V ⊆ ℂ^{n×n} of matrices closed under addition and scalar multiplication, forming a vector space over ℂ.",
      "description": "Matrix subspaces are used to model families of matrices sharing structural properties (e.g., invertibility, symmetry, triangularity). They enable low-rank approximations, invariant subspaces in eigenvalue problems, and structured matrix factorizations such as LU or SVD within constrained sets.",
      "properties": {
        "Goal": "Group matrices with common algebraic or analytic traits to simplify computations and preserve structure in factorizations.",
        "Applications": [
          "Structured matrix factorization",
          "Low-rank approximation",
          "Krylov methods",
          "Invariant subspace computation"
        ],
        "Methods": [
          "Span construction",
          "Closure under operations",
          "Equivalence via similarity"
        ],
        "Examples": [
          "span{I, A}",
          "span{A, B}",
          "upper triangular matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "04_matrix_subspaces_for_factoring_matrices.json"
        ]
      }
    },
    "Nonsingular Matrix Subspace": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A matrix subspace V ⊆ ℂ^{n×n} that contains at least one invertible matrix (det V ≠ 0 for some V ∈ V).",
      "description": "Nonsingular subspaces guarantee the existence of invertible elements, enabling the definition of the set of inverses Inv(V) = {V⁻¹ : V ∈ V, det V ≠ 0}. Such subspaces are crucial for ensuring well-defined inverse-based factorizations (e.g., LU within V).",
      "properties": {
        "Goal": "Ensure invertibility within a structured family of matrices to support factorization algorithms.",
        "Applications": [
          "LU factorization",
          "Generalized inverse",
          "Structured preconditioning"
        ],
        "Methods": [
          "Perturbation arguments",
          "Open-dense property in finite dimensions"
        ],
        "Examples": [
          "V = span{I, A} for nonsingular A",
          "Upper triangular matrices with nonzero diagonals"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "04_matrix_subspaces_for_factoring_matrices.json"
        ]
      }
    },
    "Inv(V)": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The set of inverses of all invertible matrices in a matrix subspace V: Inv(V) = {W : W = V⁻¹, V ∈ V, det V ≠ 0}.",
      "description": "For nonsingular matrix subspaces, Inv(V) is itself a matrix subspace under mild conditions. This enables dual-space factorizations and ensures closure under inversion within structured sets, vital for algorithmic stability in LU-type methods.",
      "properties": {
        "Goal": "Construct a subspace of inverses to maintain structure across factorization steps.",
        "Applications": [
          "LU within subspace",
          "Preconditioner design",
          "Group-inverse problems"
        ],
        "Methods": [
          "Similarity transformation",
          "Closure proof via nonsingularity"
        ],
        "Examples": [
          "Inv(upper triangular) = lower triangular",
          "Inv(Hermitian) = Hermitian"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "04_matrix_subspaces_for_factoring_matrices.json"
        ]
      }
    },
    "LU Factorization within Subspace": {
      "type": "Method Gaussian elimination",
      "domain": "Numerical Linear Algebra",
      "definition": "Decomposition of a matrix A ∈ V into A = LU where L is lower triangular and U is upper triangular, both belonging to predefined matrix subspaces derived from V.",
      "description": "By identifying nonsingular matrix subspaces V and W = Inv(V), LU factorization can be confined within V × W, ensuring all intermediate matrices remain structured. This supports specialized algorithms for banded, symmetric positive-definite, or approximate low-rank problems.",
      "properties": {
        "Goal": "Perform Gaussian elimination while preserving membership in designated matrix subspaces.",
        "Applications": [
          "Banded LU",
          "SPD factorization",
          "Low-rank updates",
          "Krylov-based solvers"
        ],
        "Methods": [
          "Schur complement",
          "Pivot-free elimination",
          "Subspace projection"
        ],
        "Examples": [
          "A ∈ upper triangular → L = I, U = A",
          "A ∈ V, L ∈ V, U ∈ Inv(V)"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "04_matrix_subspaces_for_factoring_matrices.json"
        ]
      }
    },
    "Krylov Subspace": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "The matrix subspace K_j(A; I) = span{I, A, A², ..., A^{j-1}} generated by powers of a matrix A starting from the identity.",
      "description": "Krylov subspaces lie at the core of iterative methods for large-scale eigenvalue and linear system problems (GMRES, Lanczos, Arnoldi). Their dimension grows linearly until saturation at the degree of the minimal polynomial of A.",
      "properties": {
        "Goal": "Approximate eigenpairs or solve Ax = b using low-dimensional subspaces built from matrix powers.",
        "Applications": [
          "GMRES",
          "Lanczos algorithm",
          "Arnoldi iteration",
          "Model order reduction"
        ],
        "Methods": [
          "Arnoldi process",
          "Lanczos tridiagonalization",
          "Minimal polynomial degree"
        ],
        "Examples": [
          "K_3(A;I) = span{I, A, A²}",
          "K_n(A;I) = ℂ^{n×n} if minimal polynomial degree n"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "04_matrix_subspaces_for_factoring_matrices.json",
          "05_factoring_algorithmically.json",
          "08_iterative_methods_for_linear_systems.json",
          "extra_00_01.json"
        ]
      }
    },
    "Matrix Polynomials": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "Functions p(z) = ∑_{k=0}^m c_k z^k evaluated at matrices: p(A) = ∑_{k=0}^m c_k A^k for A ∈ ℂ^{n×n}.",
      "description": "Matrix polynomials map matrix subspaces to themselves if V is closed under powers of its members. They define minimal and characteristic polynomials, enable Cayley-Hamilton applications, and support function-based iterative methods.",
      "properties": {
        "Goal": "Extend scalar polynomial theory to matrices for spectral analysis and function approximation.",
        "Applications": [
          "Cayley-Hamilton theorem",
          "Matrix exponential",
          " preconditioning",
          "Spectral projectors"
        ],
        "Methods": [
          "Horner scheme",
          "Paterson-Stockmeyer",
          "Jordan form evaluation"
        ],
        "Examples": [
          "p(z) = z²−2z+1 → p(A) = A²−2A+I = 0",
          "q(z) = det(A−zI) → q(A)=0"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "04_matrix_subspaces_for_factoring_matrices.json"
        ]
      }
    },
    "Projection Operator": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A linear operator P on ℂ^n such that P² = P, projecting vectors onto a subspace R(P) with kernel R(I−P).",
      "description": "Projections decompose spaces into direct sums and are building blocks for oblique and orthogonal projections in iterative solvers. Orthogonal projections minimize least-squares error and appear in GMRES and conjugate gradients.",
      "properties": {
        "Goal": "Decompose space and approximate solutions within subspaces.",
        "Applications": [
          "GMRES",
          "Oblique projection",
          "Deflation",
          "Range/Hermite interpolation"
        ],
        "Methods": [
          "Orthogonal basis construction",
          "QR factorization",
          "SVD-based projection"
        ],
        "Examples": [
          "P = VV^H for orthonormal columns V",
          "I−P orthogonal complement"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "04_matrix_subspaces_for_factoring_matrices.json",
          "05_factoring_algorithmically.json",
          "extra_00_01.json"
        ]
      }
    },
    "Matrix Product V1 V2": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The matrix product V1 V2 represents the set of all matrices formed by multiplying elements from subspaces V1 and V2, i.e., {V1 V2 : V1 ∈ V1, V2 ∈ V2}.",
      "description": "This construct is used to approximate or factor large matrices A by finding subspaces such that A lies in V1 V2, enabling low-parameter representations.",
      "properties": {
        "Goal": "Represent matrices as products of subspace elements for factorization.",
        "Applications": [
          "Matrix approximation",
          "Low-rank factoring",
          "Compression"
        ],
        "Methods": [
          "Subspace identification",
          "Optimization over subspaces"
        ],
        "Examples": [
          "A ≈ V1 V2 for V1, V2 low-dimensional"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "05_factoring_algorithmically.json"
        ]
      }
    },
    "Range of Projection": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The range R(P) of a projection P is the set {y : y = Px for some x}, representing the subspace onto which P projects.",
      "description": "For orthogonal projections, R(P) is perpendicular to R(I - P), forming an orthogonal decomposition of the space.",
      "properties": {
        "Goal": "Define the projected subspace.",
        "Applications": [
          "Subspace identification",
          "Dimensionality reduction"
        ],
        "Methods": [
          "Column span of P",
          "Fixed points of P"
        ],
        "Examples": [
          "R(P) = span{columns of P}"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "05_factoring_algorithmically.json"
        ]
      }
    },
    "Orthogonal Complement": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The orthogonal complement of a subspace V is the set of vectors perpendicular to all elements in V, denoted V^⊥.",
      "description": "For projections, if P is orthogonal, then R(P) ⊥ R(I - P), ensuring the decomposition is orthogonal.",
      "properties": {
        "Goal": "Decompose space into perpendicular subspaces.",
        "Applications": [
          "Gram-Schmidt",
          "Least squares",
          "Spectral methods"
        ],
        "Methods": [
          "Dot product zero condition",
          "Null space of transpose"
        ],
        "Examples": [
          "R(I - P) as complement of R(P)"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "05_factoring_algorithmically.json",
          "extra_01.json"
        ]
      }
    },
    "Algorithmic Factoring": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Algorithmic factoring refers to computational methods for decomposing matrices into structured forms like products of subspaces or triangular factors.",
      "description": "It leverages Krylov subspaces and projections to factor matrices efficiently, especially for large-scale problems.",
      "properties": {
        "Goal": "Factor matrices using algorithmic techniques.",
        "Applications": [
          "Large linear systems",
          "Eigenproblems",
          "Approximations"
        ],
        "Methods": [
          "Subspace iteration",
          "Projection-based factoring"
        ],
        "Examples": [
          "A = V1 W^{-1} using subspaces"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "05_factoring_algorithmically.json"
        ]
      }
    },
    "Invariant Subspace": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "An invariant subspace W for matrix A satisfies A W ⊆ W, meaning A maps W into itself.",
      "description": "Invariant subspaces are key in spectral theory and factoring, often identified via Krylov methods or projections.",
      "properties": {
        "Goal": "Find subspaces stable under matrix action.",
        "Applications": [
          "Eigen decomposition",
          "Schur form",
          "Model reduction"
        ],
        "Methods": [
          "Subspace iteration",
          "Arnoldi process"
        ],
        "Examples": [
          "Eigenvector spans 1D invariant subspace"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "05_factoring_algorithmically.json"
        ]
      }
    },
    "Square Matrix A": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A square matrix A ∈ ℂ^{n×n} is a matrix with equal rows and columns, central to linear transformations and eigenvalue problems.",
      "description": "In factoring contexts, A is decomposed using subspaces, projections, or iterations for computational efficiency.",
      "properties": {
        "Goal": "Represent linear operators on finite-dimensional spaces.",
        "Applications": [
          "Systems of equations",
          "Transformations",
          "Spectral analysis"
        ],
        "Methods": [
          "Factorization",
          "Iteration",
          "Diagonalization"
        ],
        "Examples": [
          "A with complex entries"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "05_factoring_algorithmically.json"
        ]
      }
    },
    "LU Factorization with Partial Pivoting": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "LU factorization with partial pivoting decomposes a matrix A ∈ ℂ^{n×n} into PA = LU, where P is a permutation matrix, L is unit lower triangular with |l_{ij}| ≤ 1 for i > j, and U is upper triangular. Partial pivoting selects the largest absolute entry in the current column as the pivot to minimize numerical instability.",
      "description": "This algorithm enhances the stability of Gaussian elimination by row permutations to avoid small pivots. It is the standard method for solving linear systems Ax = b in practice, balancing computational cost (O(n³)) with robustness against round-off errors in floating-point arithmetic.",
      "properties": {
        "Goal": "Stable triangular factorization of a matrix for solving linear systems and computing inverses.",
        "Applications": [
          "Solving Ax = b",
          "Matrix inversion",
          "Determinant computation",
          "Condition number estimation"
        ],
        "Methods": [
          "Gaussian elimination with row pivoting",
          "In-place storage using single array",
          "Compact WY representation for L"
        ],
        "Examples": [
          "4×4 matrix example with pivots 8, 17/4, -6/7, 2 showing growth control"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "06_computing_the_LU_factorization_with_partial_pivoting.json"
        ]
      }
    },
    "Partial Pivoting": {
      "type": "Strategy",
      "domain": "Numerical Linear Algebra",
      "definition": "A pivoting strategy in Gaussian elimination that, at step k, permutes rows so that the entry of largest magnitude in column k (from row k to n) becomes the pivot, ensuring |pivot| = max_{i≥k} |a_{ik}|.",
      "description": "Partial pivoting prevents division by small pivots, reducing amplification of round-off errors. It guarantees that all subdiagonal entries in L satisfy |l_{ij}| ≤ 1, bounding the growth factor ρ ≤ 2^{n-1} in theory, though typically much smaller in practice.",
      "properties": {
        "Goal": "Minimize numerical error propagation during elimination by choosing largest available pivot.",
        "Applications": [
          "LU factorization",
          "Linear system solving",
          "Matrix decomposition in finite precision"
        ],
        "Methods": [
          "Row interchange before elimination step",
          "Column scanning for max absolute value"
        ],
        "Examples": [
          "P1 swaps rows to bring largest entry to diagonal position"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "06_computing_the_LU_factorization_with_partial_pivoting.json"
        ]
      }
    },
    "Permutation Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A permutation matrix P is a square matrix with exactly one 1 in each row and column and 0s elsewhere. Multiplying PA permutes the rows of A; AP permutes columns.",
      "description": "In LU with partial pivoting, P represents the cumulative row interchanges. Since P^{-1} = P^T = P^*, it preserves norms: ||Px|| = ||x||. The final factorization is PA = LU.",
      "properties": {
        "Goal": "Represent row or column reordering in matrix factorizations.",
        "Applications": [
          "Pivoting in LU",
          "Reordering for sparsity",
          "Graph relabeling"
        ],
        "Methods": [
          "Identity with swapped rows",
          "Product of elementary permutation matrices"
        ],
        "Examples": [
          "P = [0 1; 1 0] swaps rows 1 and 2"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "06_computing_the_LU_factorization_with_partial_pivoting.json"
        ]
      }
    },
    "Growth Factor": {
      "type": "Metric",
      "domain": "Numerical Linear Algebra",
      "definition": "The growth factor rho in LU factorization with partial pivoting is defined as rho = max_{i,j} |u_{ij}| / max_{i,j} |a_{ij}|, measuring the largest entry in U relative to the original matrix A.",
      "description": "Controls backward stability: computed factors satisfy L_hat U_hat = P A + delta A with ||delta A|| / ||A|| = O(rho epsilon_machine). Partial pivoting keeps rho moderate in practice, though worst-case rho = 2^{n-1} is possible.",
      "properties": {
        "Goal": "Quantify element growth during Gaussian elimination to assess numerical stability.",
        "Applications": [
          "Backward error analysis",
          "Condition estimation",
          "Pivoting strategy evaluation"
        ],
        "Methods": [
          "Ratio of max |u_{ij}| to max |a_{ij}|",
          "Monitored during factorization"
        ],
        "Examples": [
          "Wilkinson's matrix gives rho approx 2^{n-1}"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "06_computing_the_LU_factorization_with_partial_pivoting.json"
        ]
      }
    },
    "Condition Number": {
      "type": "Metric",
      "domain": "Numerical Linear Algebra",
      "definition": "The condition number κ(W) of a nonsingular matrix W is κ(W) = ||W|| ⋅ ||W^{-1}||, with respect to any consistent matrix norm. For the 1-norm or ∞-norm, κ₁(W) = σ₁/σₙ where σ are singular values.",
      "description": "Measures sensitivity of linear system solution to perturbations. Large κ implies ill-conditioned system: small changes in input cause large output changes. In LU context, related to pivot size and growth.",
      "properties": {
        "Goal": "Quantify sensitivity of Ax = b to perturbations in A or b.",
        "Applications": [
          "Error bounds in linear solvers",
          "Preconditioning design",
          "Numerical stability analysis"
        ],
        "Methods": [
          "SVD-based: κ₂ = σ_max / σ_min",
          "1-norm estimation via Hager's method"
        ],
        "Examples": [
          "κ(A) ≈ 10^k ⇒ lose k digits of accuracy"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "06_computing_the_LU_factorization_with_partial_pivoting.json"
        ]
      }
    },
    "Gaussian Elimination": {
      "type": "Method",
      "domain": "Linear Algebra",
      "definition": "Gaussian elimination transforms a linear system Ax = b into upper triangular form Ux = c via row operations: adding multiples of one row to another. With pivoting, it forms the basis of LU factorization.",
      "description": "Core algorithm for solving linear systems. Without pivoting, unstable for small pivots. With partial pivoting, becomes robust standard method. Can be expressed as sequence of rank-1 updates or multiplier storage in L.",
      "properties": {
        "Goal": "Reduce system to triangular form for back substitution.",
        "Applications": [
          "Linear system solving",
          "Matrix factorization",
          "Determinant via product of diagonals"
        ],
        "Methods": [
          "Forward elimination",
          "Back substitution",
          "Pivoting variants"
        ],
        "Examples": [
          "4×4 system reduced step-by-step with P, L, U shown"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "06_computing_the_LU_factorization_with_partial_pivoting.json"
        ]
      }
    },
    "Cholesky Factorization": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "A decomposition of a Hermitian positive definite matrix A into A = R*R, where R is an upper triangular matrix with positive diagonal entries.",
      "description": "Cholesky factorization exploits symmetry and positive definiteness to reduce computational cost from O(n³) for general LU to approximately n³/3 flops while requiring only n²/2 storage. It is widely used in optimization, Monte Carlo simulations, and solving normal equations.",
      "properties": {
        "Goal": "Efficiently factorize Hermitian positive definite matrices with half the storage and one-third the operations of LU.",
        "Applications": [
          "Quadratic programming",
          "Kalman filtering",
          "Covariance decomposition",
          "Monte Carlo methods"
        ],
        "Methods": [
          "Block elimination",
          "Outer product form",
          "Inner product form"
        ],
        "Examples": [
          "A = R*R with R upper triangular and diag(R) > 0"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.json"
        ]
      }
    },
    "Positive Definite Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A Hermitian matrix A ∈ ℂ^{n×n} such that (Ax, x) > 0 for all nonzero x ∈ ℂ^n.",
      "description": "Positive definite matrices arise in energy minimization, covariance modeling, and elliptic PDEs. They guarantee unique Cholesky factors, stable inverses, and real positive eigenvalues. The property is preserved under congruence: M*A*M is positive definite if M is invertible.",
      "properties": {
        "Goal": "Model strictly convex quadratic forms and ensure numerical stability in factorizations.",
        "Applications": [
          "Optimization",
          "Statistics",
          "Physics simulations",
          "Control theory"
        ],
        "Methods": [
          "Sylvester's criterion",
          "Cholesky test",
          "Eigenvalue analysis"
        ],
        "Examples": [
          "Covariance matrices",
          "Hessians of convex functions",
          "A = R*R"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.json"
        ]
      }
    },
    "Sylvester Equation": {
      "type": "Concept",
      "domain": "Control Theory",
      "definition": "A matrix equation of the form AX − XB = C, where A, B, C are given matrices and X is unknown.",
      "description": "The Sylvester equation models linear system interconnections and appears in control design, model reduction, and eigenvalue assignment. When σ(A) ∩ σ(B) = ∅, it has a unique solution solvable in O(n³) via Schur triangulation or vectorization (kronecker form).",
      "properties": {
        "Goal": "Solve for coupling matrix X in interconnected linear systems or compute Lyapunov functions.",
        "Applications": [
          "Stability analysis",
          "Model order reduction",
          "Riccati equations",
          "Pole placement"
        ],
        "Methods": [
          "Schur method",
          "Hessenberg-Schur algorithm",
          "Bartels-Stewart",
          "Vectorization"
        ],
        "Examples": [
          "AX − XA* = −BB* (Lyapunov)",
          "AX − XB = C with diagonal A, B"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.json"
        ]
      }
    },
    "Discrete Fourier Transform (DFT)": {
      "type": "Method",
      "domain": "Signal Processing",
      "definition": "A linear transformation mapping a sequence x₀, ..., x_{n-1} to coefficients cⱼ = ∑ₖ xₖ ω^{jk}, where ω = e^{-2πi/n}, represented by the Vandermonde matrix Fₙ.",
      "description": "The DFT diagonalizes circulant matrices and enables fast convolution, filtering, and spectral analysis. The Fast Fourier Transform (FFT) computes it in O(n log n) using divide-and-conquer on power-of-two sizes.",
      "properties": {
        "Goal": "Decompose signals into frequency components and accelerate convolution/correlation.",
        "Applications": [
          "Audio processing",
          "Image compression",
          "PDE solvers",
          "Polynomial multiplication"
        ],
        "Methods": [
          "Cooley-Tukey FFT",
          "Radix-2",
          "Split-radix",
          "Bluestein"
        ],
        "Examples": [
          "F₄ = [[1,1,1,1], [1,-1,1,-1], ...]",
          "FFT of length 2^l"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.json",
          "extra_01.json"
        ]
      }
    },
    "Fast Fourier Transform (FFT)": {
      "type": "Algorithm",
      "domain": "Numerical Algorithms",
      "definition": "An efficient algorithm for computing the DFT in O(n log n) operations by recursively splitting into even/odd indices when n is a power of two.",
      "description": "The Cooley-Tukey FFT reduces DFT complexity from O(n²) to O(n log n) using butterfly operations and twiddle factors. It is foundational in digital signal processing and enables real-time spectral analysis.",
      "properties": {
        "Goal": "Compute DFT with minimal arithmetic operations using recursive decomposition.",
        "Applications": [
          "Spectral methods",
          "FFT-based convolution",
          "MRI reconstruction",
          "Audio synthesis"
        ],
        "Methods": [
          "Decimation-in-time",
          "Decimation-in-frequency",
          "Bit-reversal",
          "In-place computation"
        ],
        "Examples": [
          "Radix-2 butterfly: yⱼ = x_{2j} + ω^j x_{2j+1}"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.json",
          "extra_00_04.json",
          "extra_01.json"
        ]
      }
    },
    "Schur Triangulation": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "A similarity transformation A = Q T Q* where T is upper triangular and Q is unitary, revealing eigenvalues on the diagonal of T.",
      "description": "Schur form is the foundation for robust eigenvalue computation and solving Sylvester equations. The QR-based Francis algorithm computes it in O(n³) with high backward stability. Real Schur form handles complex conjugate pairs.",
      "properties": {
        "Goal": "Reduce matrix to triangular form under unitary similarity to expose eigenvalues and enable block algorithms.",
        "Applications": [
          "Eigenvalue problems",
          "Sylvester solvers",
          "Matrix functions",
          "Control theory"
        ],
        "Methods": [
          "Francis QR algorithm",
          "Hessenberg reduction",
          "Double shift"
        ],
        "Examples": [
          "A = Q T Q* with T upper triangular, diag(T) = eigenvalues"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.json"
        ]
      }
    },
    "Iterative Methods": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Iterative methods are algorithms that generate a sequence of approximations x_j to the solution x of a linear system Ax = b, starting from an initial guess and refining it until convergence.",
      "description": "They are preferred for large sparse systems where direct methods like Gaussian elimination are O(n^3) and computationally expensive, with per-iteration costs often O(n^2) or less, such as O(n) for sparse or O(n log n) with structured matrices.",
      "properties": {
        "Goal": "Solve large linear systems efficiently without full factorization.",
        "Applications": [
          "PDE discretizations",
          "Optimization",
          "Eigenproblems"
        ],
        "Methods": [
          "Krylov subspace methods",
          "Conjugate gradient",
          "GMRES",
          "Preconditioned iterations"
        ],
        "Examples": [
          "x_{j+1} = x_j + correction",
          "Convergence when ||r_j|| small"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "08_iterative_methods_for_linear_systems.json"
        ]
      }
    },
    "Linear System": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "A linear system is an equation of the form Ax = b where A is an n x n matrix, x is the unknown vector, and b is the right-hand side vector.",
      "description": "For large n (e.g., 10^4 to 10^8), iterative methods are used due to high O(n^3) cost of direct solvers, especially when A is sparse or structured.",
      "properties": {
        "Goal": "Find x such that Ax = b.",
        "Applications": [
          "Scientific simulations",
          "Machine learning",
          "Engineering"
        ],
        "Methods": [
          "Direct (LU, QR)",
          "Iterative (CG, GMRES)"
        ],
        "Examples": [
          "A in C^{n x n}, b in C^n"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "08_iterative_methods_for_linear_systems.json"
        ]
      }
    },
    "Arnoldi Process": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "The Arnoldi process builds an orthonormal basis Qⱼ for the Krylov subspace Kⱼ(A; b) and a Hessenberg matrix Hⱼ such that A Qⱼ = Qⱼ₊₁ Ĥⱼ.",
      "description": "It uses Gram–Schmidt-like orthogonalisation to compute basis vectors qₖ recursively, enabling reduced-order projections for solving systems or eigenvalues.",
      "properties": {
        "Goal": "Orthogonalise Krylov basis for stable computations.",
        "Applications": [
          "GMRES",
          "Eigenvalue solvers",
          "Matrix functions"
        ],
        "Methods": [
          "Recursive computation: hₖ,ₖ₋₁ qₖ = A qₖ₋₁ − Σ hₗ,ₖ₋₁ qₗ"
        ],
        "Examples": [
          "Qⱼ₊₁ Ĥⱼ = A Qⱼ"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "08_iterative_methods_for_linear_systems.json"
        ]
      }
    },
    "GMRES": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Generalised Minimal Residual (GMRES) is an iterative method that finds xⱼ in x₀ + Kⱼ(A; r₀) minimising ‖b − A xⱼ‖₂ for nonsymmetric systems.",
      "description": "It uses Arnoldi to build the basis and solves a least-squares problem with the Hessenberg matrix at each step, restarting when j is large.",
      "properties": {
        "Goal": "Minimise residual norm over Krylov subspace.",
        "Applications": [
          "Nonsymmetric linear systems",
          "PDE solvers"
        ],
        "Methods": [
          "Arnoldi orthogonalisation",
          "Least squares on Ĥⱼ y − α e₁"
        ],
        "Examples": [
          "xⱼ = Qⱼ yⱼ where yⱼ minimises ‖Ĥⱼ y − α e₁‖"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "08_iterative_methods_for_linear_systems.json"
        ]
      }
    },
    "Conjugate Gradient Method": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "The Conjugate Gradient (CG) method solves symmetric positive definite systems Ax = b by minimising the A-norm error ‖x − xⱼ‖ₐ over the Krylov subspace.",
      "description": "It generates A-conjugate search directions implicitly via a three-term recurrence, equivalent to Lanczos for symmetric matrices, with optimal polynomial approximation properties.",
      "properties": {
        "Goal": "Minimise quadratic form (x, A x)/2 − (b, x).",
        "Applications": [
          "SPD linear systems",
          "Optimisation (Newton-CG)"
        ],
        "Methods": [
          "Conjugate directions",
          "Residual orthogonalisation"
        ],
        "Examples": [
          "xⱼ₊₁ = xⱼ + αⱼ pⱼ with pⱼ₊₁ = rⱼ₊₁ + βⱼ pⱼ"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "08_iterative_methods_for_linear_systems.json",
          "extra_00_01.json"
        ]
      }
    },
    "Preconditioning": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Preconditioning transforms the system Ax = b into an equivalent form M⁻¹ A x = M⁻¹ b where M approximates A, improving conditioning and convergence speed.",
      "description": "Good preconditioners M are cheap to invert and make M⁻¹ A close to the identity or with clustered eigenvalues.",
      "properties": {
        "Goal": "Accelerate iterative solver convergence.",
        "Applications": [
          "Large sparse systems",
          "PDE solvers"
        ],
        "Methods": [
          "Incomplete LU",
          "Jacobi",
          "Multigrid",
          "Domain decomposition"
        ],
        "Examples": [
          "Left preconditioning: solve M y = c then A x = M y"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "08_iterative_methods_for_linear_systems.json",
          "09_preconditioning.json"
        ]
      }
    },
    "Residual Norm": {
      "type": "Metric",
      "domain": "Numerical Analysis",
      "definition": "The residual norm ‖rⱼ‖ = ‖b − A xⱼ‖ measures how well the approximate solution xⱼ satisfies the system Ax = b.",
      "description": "In iterative methods, residuals decrease monotonically in GMRES, and are used to test convergence.",
      "properties": {
        "Goal": "Quantify approximation error in equation satisfaction.",
        "Applications": [
          "Convergence testing",
          "Stopping criteria"
        ],
        "Methods": [
          "Euclidean norm",
          "Relative residual"
        ],
        "Examples": [
          "‖rⱼ₊₁‖ ≤ ‖rⱼ‖ in GMRES"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "08_iterative_methods_for_linear_systems.json"
        ]
      }
    },
    "A-Norm Error": {
      "type": "Metric",
      "domain": "Numerical Analysis",
      "definition": "The A-norm error ‖x − xⱼ‖ₐ = ( (x − xⱼ), A (x − xⱼ) )^{1/2} measures the error in the energy norm for SPD A.",
      "description": "CG minimises this norm over the Krylov subspace, relating to the quadratic form minimised in the system.",
      "properties": {
        "Goal": "Quantify solution error in energy sense.",
        "Applications": [
          "CG convergence analysis",
          "Variational problems"
        ],
        "Methods": [
          "Defined via inner product (x, A y)"
        ],
        "Examples": [
          "Min ‖x − xⱼ‖ₐ in CG"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "08_iterative_methods_for_linear_systems.json"
        ]
      }
    },
    "Polynomial Approximation": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "Polynomial approximation in iterative methods views xⱼ = pⱼ₋₁(A) b as a polynomial in A applied to b, minimising residuals or errors via min-max problems over polynomials.",
      "description": "Convergence bounds use Chebyshev or other polynomials to estimate rates based on eigenvalue distribution.",
      "properties": {
        "Goal": "Approximate A⁻¹ b via polynomials in A.",
        "Applications": [
          "Convergence analysis",
          "Accelerated methods"
        ],
        "Methods": [
          "Min-max over deg ≤ j-1",
          "Chebyshev acceleration"
        ],
        "Examples": [
          "min_{deg p ≤ j-1, p(0)=1} max_λ |p(λ)|"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "08_iterative_methods_for_linear_systems.json"
        ]
      }
    },
    "Hessenberg Matrix": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "A Hessenberg matrix H is upper triangular except for the subdiagonal, arising in Arnoldi as the projection of A onto the Krylov basis.",
      "description": "It simplifies least-squares solves in GMRES and eigenvalue computations.",
      "properties": {
        "Goal": "Reduce matrix for efficient projections.",
        "Applications": [
          "GMRES minimisation",
          "QR algorithm"
        ],
        "Methods": [
          "From Arnoldi: Ĥⱼ with subdiagonal"
        ],
        "Examples": [
          "Hⱼ tridiagonal in Lanczos"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "08_iterative_methods_for_linear_systems.json"
        ]
      }
    },
    "Jacobi Preconditioner": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "The Jacobi preconditioner is M = diag(A) + ωI, where diag(A) contains the diagonal entries of A and ω ∈ ℂ is an optional damping parameter (often ω = 0). It is the simplest splitting M = D, N = A - D.",
      "description": "Extremely cheap to apply (O(n) scaling) and parallelizable. Effective when A is diagonally dominant. Corresponds to the classical Jacobi iterative method. Often used as a baseline or building block in more sophisticated preconditioners.",
      "properties": {
        "Goal": "Damp high-frequency error components using only diagonal information.",
        "Applications": [
          "Smooth initial guess for multigrid",
          "Baseline for preconditioner comparison",
          "Diagonally dominant systems"
        ],
        "Methods": [
          "Extract diagonal",
          "Optional damping ω",
          "Inverse is element-wise division"
        ],
        "Examples": [
          "M_i = diag(A) + ωI with ω = 0 for standard Jacobi"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "09_preconditioning.json"
        ]
      }
    },
    "Incomplete LU Factorization": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "Incomplete LU (ILU) computes factors L̂ and Û such that L̂Û ≈ A with the same sparsity pattern as A (or a prescribed superset), dropping fill-in during Gaussian elimination to control memory and cost.",
      "description": "Widely used for general sparse matrices. Variants include ILU(0) (no fill), ILU(k) (level-k fill), and ILUT (threshold dropping). Provides a trade-off between robustness and efficiency. Often combined with reordering (e.g., RCM, nested dissection).",
      "properties": {
        "Goal": "Approximate LU factorization while preserving sparsity for use as preconditioner.",
        "Applications": [
          "Finite element systems",
          "CFD problems",
          "Circuit simulation"
        ],
        "Methods": [
          "Modified Gaussian elimination with drop tolerance",
          "Dual-threshold ILUT",
          "Level-of-fill ILU(k)"
        ],
        "Examples": [
          "ILU(0): drop all fill-in outside original nonzero pattern"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "09_preconditioning.json",
          "extra_00_01.json"
        ]
      }
    },
    "Sparse Approximate Inverse": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "A sparse approximate inverse (SAI) preconditioner computes a sparse matrix M such that ||I - MA||_F or ||I - AW||_F is minimized over all matrices W with a prescribed sparsity pattern, effectively approximating A^{-1} directly.",
      "description": "Application cost is O(nnz(M)) matrix-vector products. Excellent parallel performance due to explicit form. Often constructed via Frobenius norm minimization on independent columns or using QR factorizations of local submatrices.",
      "properties": {
        "Goal": "Construct explicit sparse approximation to A^{-1} for fast matrix-vector products.",
        "Applications": [
          "Highly parallel architectures (GPU, many-core)",
          "Unstructured grids",
          "High-performance computing"
        ],
        "Methods": [
          "Frobenius norm minimization per column",
          "SPAID (sparse approximate inverse by distance)",
          "FSAI (factorized sparse approximate inverse)"
        ],
        "Examples": [
          "min_W ||AW - I||_F with W constrained to sparsity pattern of A^k"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "09_preconditioning.json",
          "extra_01.json"
        ]
      }
    },
    "Left Preconditioning": {
      "type": "Technique",
      "domain": "Numerical Linear Algebra",
      "definition": "Left preconditioning transforms Ax = b into M^{-1}Ax = M^{-1}b, preserving the solution x but altering the residual norm to ||M^{-1}(b - Ax)||.",
      "description": "Common in practice because it directly improves the convergence diagnostics used by Krylov methods (residual-based stopping criteria). Does not change the right-hand side in a way that affects eigenvalue clustering as strongly as right preconditioning.",
      "properties": {
        "Goal": "Improve convergence while keeping solution unchanged and monitoring preconditioned residuals.",
        "Applications": [
          "Standard choice in most libraries (PETSc, Trilinos)",
          "GMRES with ILU"
        ],
        "Methods": [
          "Apply M^{-1} to system matrix and RHS"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "09_preconditioning.json"
        ]
      }
    },
    "Right Preconditioning": {
      "type": "Technique",
      "domain": "Numerical Linear Algebra",
      "definition": "Right preconditioning transforms Ax = b into AM^{-1}y = b with x = M^{-1}y, preserving the residual norm ||b - Ax|| but solving for an intermediate variable y.",
      "description": "Often preferred when the preconditioner naturally approximates the inverse action from the right. Common in domain decomposition and multigrid methods. Requires extra step to recover x.",
      "properties": {
        "Goal": "Cluster eigenvalues of AM^{-1} while monitoring true residuals.",
        "Applications": [
          "Additive Schwarz",
          "Algebraic multigrid (AMG)",
          "When M approximates A from the right"
        ],
        "Methods": [
          "Solve AM^{-1}y = b, then x = M^{-1}y"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "09_preconditioning.json"
        ]
      }
    },
    "Splitting Methods": {
      "type": "Framework",
      "domain": "Iterative Methods",
      "definition": "A matrix splitting decomposes A = M - N where M is nonsingular and easy to invert. The iteration x^{k+1} = M^{-1}Nx^k + M^{-1}b converges if ρ(M^{-1}N) < 1.",
      "description": "Foundation of classical iterative methods (Jacobi, Gauss-Seidel, SOR) and modern preconditioning. The spectral radius of the iteration matrix B = M^{-1}N determines convergence rate.",
      "properties": {
        "Goal": "Construct fixed-point iterations via A = M - N with ρ(M^{-1}N) < 1.",
        "Applications": [
          "Stationary iterative methods",
          "Preconditioner design",
          "Convergence theory"
        ],
        "Methods": [
          "M = D (Jacobi)",
          "M = D+L (Gauss-Seidel)",
          "M = (D+ωL)/ω (SOR)"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "09_preconditioning.json"
        ]
      }
    },
    "Gershgorin Circle Theorem": {
      "type": "Theorem",
      "domain": "Matrix Analysis",
      "definition": "Every eigenvalue of A ∈ ℂ^{n×n} lies in at least one disk Dⱼ = {z : |z − aⱼⱼ| ≤ Rⱼ}, where Rⱼ = Σ_{l≠j} |aⱼₗ|}.",
      "description": "Provides eigenvalue localization without computation. Disks are centered at diagonal entries with radii equal to off-diagonal row sums. Useful for bounding spectral radius and detecting diagonal dominance.",
      "properties": {
        "Goal": "Locate eigenvalues in complex plane using only matrix entries.",
        "Applications": [
          "Convergence analysis",
          "Error bounding",
          "Preconditioning design"
        ],
        "Methods": [
          "Row-sum computation",
          "Union of disks",
          "Refinement via similarity"
        ],
        "Examples": [
          "Strictly diagonally dominant → eigenvalues in disjoint disks"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "10_eigenvalue_problems_and_functions_of_matrices.json"
        ]
      }
    },
    "Power Iteration": {
      "type": "Method",
      "domain": "Eigenvalue Algorithms",
      "definition": "An iterative method that computes the dominant eigenvalue and eigenvector by repeatedly applying A to a vector and normalizing: q^{(k)} = A q^{(k-1)} / ||A q^{(k-1)}||, λ^{(k)} = (A q^{(k)}, q^{(k)}).",
      "description": "Converges linearly to the eigenvector corresponding to the largest-magnitude eigenvalue if |λ₁| > |λ₂| ≥ ... ≥ |λₙ|. Convergence rate is |λ₂/λ₁|. Inverse iteration targets smallest or shifted eigenvalues.",
      "properties": {
        "Goal": "Find dominant eigenpair with minimal storage and simple operations.",
        "Applications": [
          "PageRank",
          "Principal Component Analysis",
          "Vibration modes"
        ],
        "Methods": [
          "Rayleigh quotient",
          "Normalization",
          "Deflation",
          "Shift-and-invert"
        ],
        "Examples": [
          "q^{(k)} ≈ x₁ + O((λ₂/λ₁)^k)"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "10_eigenvalue_problems_and_functions_of_matrices.json"
        ]
      }
    },
    "Householder Reflection": {
      "type": "Transformation",
      "domain": "Numerical Linear Algebra",
      "definition": "A unitary matrix H = I − 2vv*/(v*v) that reflects a vector x across the hyperplane perpendicular to v, mapping x to σe₁.",
      "description": "Used in QR factorization and Hessenberg reduction to introduce zeros below the subdiagonal. Numerically stable and requires O(n) operations per reflection. Essential for implicit QR algorithm.",
      "properties": {
        "Goal": "Zero out selected entries via unitary similarity while preserving eigenvalues.",
        "Applications": [
          "QR decomposition",
          "Hessenberg form",
          "Tridiagonalization"
        ],
        "Methods": [
          "Sign choice for stability",
          "WY representation",
          "Blocked Householder"
        ],
        "Examples": [
          "H x = −sign(x₁) ||x|| e₁"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "10_eigenvalue_problems_and_functions_of_matrices.json"
        ]
      }
    },
    "QR Algorithm": {
      "type": "Algorithm",
      "domain": "Eigenvalue Computation",
      "definition": "An iterative method that computes the Schur form via repeated QR decompositions: A_{k+1} = R_k Q_k, with A_0 = A. With shifts, converges cubically to upper triangular form.",
      "description": "The de facto standard for dense eigenvalue problems. Implicit version uses Householder/Bulge chasing to reduce cost from O(n³) per iteration to O(n). Francis shift accelerates convergence.",
      "properties": {
        "Goal": "Compute all eigenvalues (and optionally eigenvectors) via unitary similarity to triangular form.",
        "Applications": [
          "MATLAB eig",
          "LAPACK",
          "Control theory",
          "PDE solvers"
        ],
        "Methods": [
          "Hessenberg reduction",
          "Francis double shift",
          "Deflation",
          "Balancing"
        ],
        "Examples": [
          "A_{k+1} = Q_k* A_k Q_k"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "10_eigenvalue_problems_and_functions_of_matrices.json"
        ]
      }
    },
    "Generalized Eigenvalue Problem": {
      "type": "Problem",
      "domain": "Numerical Linear Algebra",
      "definition": "Find λ ∈ ℂ and nonzero x ∈ ℂⁿ such that Ax = λBx, with A, B ∈ ℂ^{n×n}.",
      "description": "Arises in structural dynamics, control, and Markov chains. Reduced to standard form if B invertible (M = B⁻¹A). QZ algorithm generalizes QR using unitary transformations to triangularize both matrices.",
      "properties": {
        "Goal": "Solve coupled systems or weighted eigenvalue problems.",
        "Applications": [
          "Vibration with constraints",
          "Markov chain stationary distribution",
          "Optimal control"
        ],
        "Methods": [
          "QZ algorithm",
          "Cholesky + standard EVP",
          "Shift-and-invert"
        ],
        "Examples": [
          "Ax = λBx with B positive definite"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "10_eigenvalue_problems_and_functions_of_matrices.json"
        ]
      }
    },
    "Matrix Function": {
      "type": "Concept",
      "domain": "Matrix Analysis",
      "definition": "A function f applied to matrix A such that f(A) respects the spectral mapping: if A = XΛX⁻¹, then f(A) = X f(Λ) X⁻¹, with f(Λ) = diag(f(λⱼ)).",
      "description": "Defined via Jordan form or Cauchy integral. Preserves eigenvalues: σ(f(A)) = f(σ(A)). Computed via Schur form, Padé approximants, or scaling-and-squaring (for exp).",
      "properties": {
        "Goal": "Extend scalar functions to matrices while preserving algebraic structure.",
        "Applications": [
          "Matrix exponential in ODEs",
          "Matrix logarithm in geometry",
          "Sign function in control"
        ],
        "Methods": [
          "Schur-Parlett",
          "Scaling-and-squaring",
          "Padé approximation",
          "Contour integral"
        ],
        "Examples": [
          "exp(A)",
          "A^{1/2}",
          "sign(A)"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "10_eigenvalue_problems_and_functions_of_matrices.json",
          "11_Iterative methods_for_eigenvalue_problems.json"
        ]
      }
    },
    "Field of Values": {
      "type": "Concept",
      "domain": "Matrix Analysis",
      "definition": "The set F(A) = {x*Ax : x ∈ ℂⁿ, ||x||=1}, also known as the numerical range.",
      "description": "Convex, compact set containing all eigenvalues. Bounds spectral radius and condition number. For normal matrices, F(A) is the convex hull of eigenvalues.",
      "properties": {
        "Goal": "Characterize operator behavior beyond eigenvalues.",
        "Applications": [
          "Stability analysis",
          "Convergence of iterations",
          "Pseudospectrum approximation"
        ],
        "Methods": [
          "Rayleigh quotient",
          "Hausdorff-Toeplitz theorem",
          "Discretization"
        ],
        "Examples": [
          "F(A) = conv(σ(A)) if A normal"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "10_eigenvalue_problems_and_functions_of_matrices.json"
        ]
      }
    },
    "Matrix Square Root": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "The matrix square root X satisfies X^2 = A for a matrix A, with the principal square root being positive definite if A is.",
      "description": "It is computed iteratively, e.g., via Newton's method, for applications in statistics, control, and geometry.",
      "properties": {
        "Goal": "Find X such that X^2 = A.",
        "Applications": [
          "Covariance matrices",
          "Riemannian metrics",
          "Polar decomposition"
        ],
        "Methods": [
          "Newton iteration",
          "Denman-Beavers",
          "Schur method"
        ],
        "Examples": [
          "X = sqrt(A) for SPD A"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "11_Iterative methods_for_eigenvalue_problems.json"
        ]
      }
    },
    "Newton's Iteration for Square Root": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Newton's iteration for the matrix square root updates X_{k+1} = (X_k + A X_k^{-1}) / 2, starting from X_0 = I or other, converging quadratically to sqrt(A).",
      "description": "It is stable for positive definite A, with safeguards for convergence, used in large-scale computations via iterative solvers.",
      "properties": {
        "Goal": "Compute sqrt(A) iteratively.",
        "Applications": [
          "Matrix sign function",
          "Algebraic Riccati equations"
        ],
        "Methods": [
          "Matrix inversion at each step",
          "Quadratic convergence"
        ],
        "Examples": [
          "X_{k+1} = (X_k + A X_k^{-1}) / 2"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "11_Iterative methods_for_eigenvalue_problems.json"
        ]
      }
    },
    "Spectral Shift": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Spectral shift transforms the eigenvalue problem to (A - μ I) x = (λ - μ) x, shifting eigenvalues by μ to target specific parts of the spectrum or improve conditioning.",
      "description": "Choosing μ near a target eigenvalue accelerates convergence in iterative methods; for real μ, it can make the matrix positive definite.",
      "properties": {
        "Goal": "Adjust spectrum for better numerical properties.",
        "Applications": [
          "Interior eigenvalues",
          "Deflation",
          "Preconditioning"
        ],
        "Methods": [
          "μ close to target λ",
          "μ = (λ_min + λ_max)/2"
        ],
        "Examples": [
          "Λ(A - μ I) = Λ(A) - μ"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "11_Iterative methods_for_eigenvalue_problems.json"
        ]
      }
    },
    "Positive Definite Shift": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Positive definite shift chooses μ such that A - μ I is positive definite, e.g., μ < λ_min(A) for symmetric A, enabling use of CG or other SPD methods.",
      "description": "It stabilizes iterations and bounds condition number; for estimated ˆμ ≈ λ_min, adjust with a > 0 to ensure positivity.",
      "properties": {
        "Goal": "Make shifted matrix SPD for efficient solving.",
        "Applications": [
          "Shift-and-invert",
          "Preconditioned eigensolvers"
        ],
        "Methods": [
          "μ = ˆμ - a, a > 0",
          "Rayleigh quotient estimate"
        ],
        "Examples": [
          "A - μ I with μ = ˆμ - a"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "11_Iterative methods_for_eigenvalue_problems.json"
        ]
      }
    },
    "Spectrum Λ(A)": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The spectrum Λ(A) is the set of all eigenvalues of matrix A.",
      "description": "Shifting modifies the spectrum as Λ(A - μ I) = Λ(A) - μ, used to isolate eigenvalues or improve numerical properties.",
      "properties": {
        "Goal": "Characterize matrix via its eigenvalues.",
        "Applications": [
          "Spectral radius",
          "Conditioning",
          "Stability"
        ],
        "Methods": [
          "Eigen decomposition",
          "Characteristic polynomial"
        ],
        "Examples": [
          "Λ(A) subset C for A in C^{n x n}"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "11_Iterative methods_for_eigenvalue_problems.json",
          "extra_00_01.json"
        ]
      }
    },
    "Preconditioned Conjugate Gradient": {
      "type": "Algorithm",
      "domain": "Numerical Linear Algebra",
      "definition": "A variant of the Conjugate Gradient method that applies a preconditioner M to cluster eigenvalues of A and accelerate convergence.",
      "description": "PCG solves M^{-1}Ax = M^{-1}b using CG on the transformed system. Proper preconditioning reduces iteration count and improves numerical stability.",
      "properties": {
        "Goal": "Improve convergence of CG via spectral conditioning.",
        "Applications": [
          "PDE solvers",
          "Large-scale sparse systems",
          "Domain decomposition"
        ],
        "Methods": [
          "Left/right preconditioning",
          "Incomplete factorizations",
          "Spectral clustering"
        ],
        "Examples": [
          "IC(0) preconditioning",
          "AMG-preconditioned CG"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_01.json"
        ]
      }
    },
    "Krylov Subspace Methods": {
      "type": "Algorithm Family",
      "domain": "Numerical Linear Algebra",
      "definition": "A class of iterative algorithms that approximate solutions to Ax = b or eigenvalue problems by projecting them onto progressively larger Krylov subspaces.",
      "description": "Includes CG, GMRES, MINRES, BiCG, QMR, and others. These methods rely on polynomial approximations to A and require only matrix-vector products, making them ideal for large sparse systems.",
      "properties": {
        "Goal": "Solve large-scale problems using matrix-free operations.",
        "Applications": [
          "Sparse linear systems",
          "Eigenvalue problems",
          "PDE discretizations"
        ],
        "Methods": [
          "Arnoldi iteration",
          "Lanczos iteration",
          "Restarting",
          "Preconditioning"
        ],
        "Examples": [
          "GMRES",
          "CG",
          "MINRES",
          "BiCGSTAB"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_01.json"
        ]
      }
    },
    "Matrix Factorization": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "The process of decomposing a matrix into a product of structured factors such as triangular, orthogonal, or diagonal matrices.",
      "description": "Matrix factorizations enable efficient solving of linear systems, eigenvalue problems, and optimization tasks. Examples include LU, QR, Cholesky, Schur, and SVD.",
      "properties": {
        "Goal": "Reduce computational complexity by decomposing matrices into simpler components.",
        "Applications": [
          "Solving Ax = b",
          "Eigenvalue algorithms",
          "Least-squares problems",
          "Preconditioning"
        ],
        "Methods": [
          "Pivoting",
          "Orthogonal transformations",
          "Triangularization"
        ],
        "Examples": [
          "LU",
          "QR",
          "SVD",
          "Cholesky",
          "Schur"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_01.json"
        ]
      }
    },
    "A - μ I": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A shifted matrix operator formed by subtracting a scalar multiple of the identity matrix from A.",
      "description": "The matrix A - μI is central to spectral shift techniques, inverse iteration, Rayleigh Quotient Iteration, and many eigenvalue algorithms. Shifting by μ changes the eigenvalues to λ_i - μ, allowing selective amplification of eigen-components.",
      "properties": {
        "Goal": "Shift the spectrum of A so selected eigenvalues become easier to isolate or invert.",
        "Applications": [
          "Inverse Iteration",
          "Rayleigh Quotient Iteration",
          "Spectral Shift",
          "Shift-Invert Methods"
        ],
        "Methods": [
          "Shift-and-invert transformation",
          "Spectral manipulation"
        ],
        "Examples": [
          "(A - λ_min I) is used to emphasize smallest eigenvalues"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_02.json"
        ]
      }
    },
    "Backward Stability": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "A property of an algorithm where the computed solution is the exact solution to a slightly perturbed input problem.",
      "description": "Backward stability ensures that numerical errors arise only from small perturbations in the input, making the algorithm robust. LU with partial pivoting, QR factorization, and SVD-based algorithms are typical examples.",
      "properties": {
        "Goal": "Guarantee high numerical reliability even under rounding error.",
        "Applications": [
          "Linear system solving",
          "Eigenvalue computations",
          "Least squares"
        ],
        "Methods": [
          "Perturbation analysis",
          "Stability theory"
        ],
        "Examples": [
          "LU with partial pivoting is backward stable for many matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_02.json"
        ]
      }
    },
    "Brauer Cassini Ovals": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "A set of regions in the complex plane providing eigenvalue inclusion bounds based on a partition of the matrix.",
      "description": "Brauer’s theorem partitions the matrix into blocks and constructs Cassini ovals that enclose all eigenvalues. They generalize Gershgorin’s disks and often produce tighter bounds.",
      "properties": {
        "Goal": "Provide eigenvalue inclusion regions sharper than Gershgorin circles.",
        "Applications": [
          "Eigenvalue estimation",
          "Spectral bounds",
          "Iterative method convergence analysis"
        ],
        "Methods": [
          "Block partitioning",
          "Cassini oval construction"
        ],
        "Examples": [
          "Used to bound eigenvalues of non-normal matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_02.json"
        ]
      }
    },
    "Cauchy Integral Formula": {
      "type": "Concept",
      "domain": "Complex Analysis / Matrix Functions",
      "definition": "A contour integral formula that represents analytic functions inside a closed curve using boundary values.",
      "description": "In matrix computations, the Cauchy integral formula is used to define analytic matrix functions such as f(A). It allows representing functions of matrices using contour integrals around the spectrum.",
      "properties": {
        "Goal": "Define analytic functions of matrices using contour integration.",
        "Applications": [
          "Matrix exponential",
          "Matrix sign function",
          "Matrix square root"
        ],
        "Methods": [
          "Contour integration",
          "Residue calculus"
        ],
        "Examples": [
          "f(A) = (1/(2πi)) ∮ f(z)(zI - A)⁻¹ dz"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_02.json"
        ]
      }
    },
    "Chebyshev Polynomials": {
      "type": "Concept",
      "domain": "Approximation Theory",
      "definition": "A family of orthogonal polynomials minimizing the maximum error over [-1,1].",
      "description": "Chebyshev polynomials play a key role in iterative methods, eigenvalue solvers, spectral methods, and approximation theory. They provide optimal polynomial approximants and reduce oscillations.",
      "properties": {
        "Goal": "Achieve near-minimax polynomial approximation.",
        "Applications": [
          "Polynomial approximation",
          "Iterative methods acceleration",
          "Spectral discretization"
        ],
        "Methods": [
          "Orthogonal polynomials",
          "Clenshaw recurrence"
        ],
        "Examples": [
          "Chebyshev semi-iterative method"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_02.json"
        ]
      }
    },
    "Circulant Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A structured matrix where each row is a cyclic shift of the previous row.",
      "description": "Circulant matrices are diagonalizable by the discrete Fourier transform (DFT), making them computationally efficient for convolution, filtering, and spectral analysis. Fast multiplication using FFT leads to O(n log n) operations.",
      "properties": {
        "Goal": "Exploit cyclic structure for fast computations.",
        "Applications": [
          "Signal processing",
          "Fast convolution",
          "Spectral graph theory"
        ],
        "Methods": [
          "DFT diagonalization",
          "FFT-based multiplication"
        ],
        "Examples": [
          "Toeplitz-circulant approximation",
          "Convolution as circulant matrix-vector product"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_03.json"
        ]
      }
    },
    "Complete Pivoting": {
      "type": "Algorithm",
      "domain": "Numerical Linear Algebra",
      "definition": "A pivoting strategy for Gaussian elimination where the largest element in the remaining submatrix is chosen as the pivot.",
      "description": "Complete pivoting reduces numerical instability by selecting globally maximal pivots. Although more stable than partial pivoting, it is computationally more expensive and therefore rarely used in large-scale problems.",
      "properties": {
        "Goal": "Improve numerical stability of Gaussian elimination.",
        "Applications": [
          "Gaussian elimination",
          "LU factorization"
        ],
        "Methods": [
          "Global pivot search",
          "Row and column permutations"
        ],
        "Examples": [
          "Used when numerical precision is critical in small matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_03.json"
        ]
      }
    },
    "Congruence Transformation": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A transformation of the form A ↦ PᵀAP where P is nonsingular, often used to analyze symmetric matrices.",
      "description": "Congruence transformations preserve definiteness properties of matrices. They are commonly used in quadratic forms, inertia theory, and Lyapunov stability analysis.",
      "properties": {
        "Goal": "Preserve quadratic form properties while transforming matrices.",
        "Applications": [
          "Quadratic forms",
          "Lyapunov equations",
          "Symmetric matrix analysis"
        ],
        "Methods": [
          "Matrix decomposition",
          "Equivalence transformations"
        ],
        "Examples": [
          "Sylvester’s law of inertia"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_03.json"
        ]
      }
    },
    "Control Synthesis": {
      "type": "Concept",
      "domain": "Control Theory",
      "definition": "The process of designing controllers that achieve desired system performance using mathematical models.",
      "description": "Control synthesis includes optimal control, state feedback, LQR design, and H∞ control. Many linear algebraic problems such as Lyapunov and Riccati equations appear in control synthesis.",
      "properties": {
        "Goal": "Design controllers that stabilize and optimize dynamic systems.",
        "Applications": [
          "Robotics",
          "Aerospace",
          "Feedback systems"
        ],
        "Methods": [
          "State-space design",
          "Linear algebraic equations",
          "Optimization"
        ],
        "Examples": [
          "LQR controller synthesis",
          "Pole placement"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_03.json"
        ]
      }
    },
    "Direct Sum": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A decomposition V = U ⊕ W where every vector in V has a unique representation as u + w with u ∈ U and w ∈ W.",
      "description": "Direct sum decompositions are fundamental in structure theory, invariant subspaces, and block matrix representations. They provide a clean separation of components across orthogonal or complementary subspaces.",
      "properties": {
        "Goal": "Split a vector space into independent, non-overlapping components.",
        "Applications": [
          "Invariant subspaces",
          "Block diagonalization",
          "Decomposing matrix subspaces"
        ],
        "Methods": [
          "Projection operators",
          "Complementary subspaces"
        ],
        "Examples": [
          "V = span(e1) ⊕ span(e2,e3)"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_03.json"
        ]
      }
    },
    "Domain Decomposition": {
      "type": "Algorithm",
      "domain": "Numerical PDEs / Scientific Computing",
      "definition": "A method for solving PDEs by dividing the computational domain into smaller subdomains and solving local problems.",
      "description": "Domain decomposition improves parallelism, reduces communication, and allows localized problem-solving. It forms the basis for Schwarz methods, additive and multiplicative preconditioners.",
      "properties": {
        "Goal": "Solve large PDE systems efficiently using parallel local solves.",
        "Applications": [
          "Elliptic PDEs",
          "Finite element methods",
          "Parallel solvers"
        ],
        "Methods": [
          "Schwarz iteration",
          "Overlapping and non-overlapping decompositions"
        ],
        "Examples": [
          "Additive Schwarz method"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_03.json"
        ]
      }
    },
    "Elliptic PDE": {
      "type": "Concept",
      "domain": "Partial Differential Equations",
      "definition": "A class of PDEs characterized by positive-definite differential operators, often requiring solution of large sparse systems.",
      "description": "Elliptic PDEs frequently arise in steady-state physical systems. Their discretizations lead to SPD linear systems that are ideal for conjugate gradients and multigrid solvers.",
      "properties": {
        "Goal": "Model steady-state physical processes such as diffusion, elasticity, and electrostatics.",
        "Applications": [
          "Finite element analysis",
          "Heat conduction",
          "Electrostatics"
        ],
        "Methods": [
          "Discretization",
          "Iterative solvers"
        ],
        "Examples": [
          "Poisson equation: −Δu = f"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_03.json"
        ]
      }
    },
    "Eigenvalue Decomposition": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "A decomposition A = VΛV⁻¹ where Λ is diagonal and V contains eigenvectors of A.",
      "description": "Eigenvalue decomposition exists for diagonalizable matrices and plays a central role in spectral analysis, diagonalization, and understanding matrix dynamics.",
      "properties": {
        "Goal": "Represent a matrix in terms of its eigenvalues and eigenvectors.",
        "Applications": [
          "Diagonalization",
          "Matrix functions",
          "Stability analysis"
        ],
        "Methods": [
          "Similarity transformation"
        ],
        "Examples": [
          "A diagonalizable matrix with distinct eigenvalues"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_03.json"
        ]
      }
    },
    "Exponential Integrators": {
      "type": "Algorithm",
      "domain": "Numerical Differential Equations",
      "definition": "A class of time-stepping methods that use the matrix exponential to solve stiff differential equations.",
      "description": "By explicitly integrating the linear part of the system, exponential integrators reduce stiffness and enable stable large time steps. They use matrix exponential actions such as exp(A)v.",
      "properties": {
        "Goal": "Solve stiff ODEs efficiently with large stable time steps.",
        "Applications": [
          "Stiff ODEs",
          "Schrödinger equation",
          "Fluid dynamics"
        ],
        "Methods": [
          "Krylov subspace exponential actions",
          "φ-functions"
        ],
        "Examples": [
          "Exponential Euler method",
          "ETD Runge–Kutta"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_03.json"
        ]
      }
    },
    "Fill-in": {
      "type": "Concept",
      "domain": "Sparse Linear Algebra",
      "definition": "New nonzero entries created during matrix factorizations such as LU, Cholesky, or QR.",
      "description": "Fill-in affects storage requirements and computational cost. Minimizing fill-in is essential for sparse solvers and preconditioners. Ordering strategies such as minimum degree or nested dissection aim to reduce fill-in.",
      "properties": {
        "Goal": "Quantify and manage new nonzeros generated during factorization.",
        "Applications": [
          "Sparse LU factorization",
          "Cholesky factorization",
          "Preconditioner construction"
        ],
        "Methods": [
          "Graph-based ordering",
          "Approximate minimum degree",
          "Nested dissection"
        ],
        "Examples": [
          "Cholesky of 2D Laplacian grid matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_04.json"
        ]
      }
    },
    "Francis Shift": {
      "type": "Concept",
      "domain": "Eigenvalue Algorithms",
      "definition": "A shift strategy used in the QR algorithm to accelerate convergence to eigenvalues.",
      "description": "The implicit double-shift QR method introduced by Francis uses complex conjugate shifts to target eigenvalues more rapidly. This is a key technique in modern dense eigenvalue solvers.",
      "properties": {
        "Goal": "Accelerate QR eigenvalue algorithm convergence.",
        "Applications": [
          "QR algorithm",
          "Hessenberg QR iteration"
        ],
        "Methods": [
          "Implicit double shift",
          "Bulge-chasing"
        ],
        "Examples": [
          "Francis double-shift QR iteration"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_04.json"
        ]
      }
    },
    "Givens Rotation": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "A plane rotation used to introduce zeros in matrices, especially in QR factorization.",
      "description": "Givens rotations apply a rotation in a 2D coordinate plane to eliminate specific matrix elements. They are ideal for sparse matrices because they introduce minimal fill-in.",
      "properties": {
        "Goal": "Eliminate matrix entries while preserving orthogonality.",
        "Applications": [
          "QR factorization",
          "Least squares",
          "Hessenberg reduction"
        ],
        "Methods": [
          "Orthogonal transformations",
          "Sparse QR algorithms"
        ],
        "Examples": [
          "Givens QR for sparse least-squares problems"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_04.json"
        ]
      }
    },
    "GL(n, ℂ)": {
      "type": "Concept",
      "domain": "Linear Algebra / Group Theory",
      "definition": "The group of all invertible n×n matrices over the complex numbers.",
      "description": "GL(n,ℂ) is the fundamental group representing all linear automorphisms of ℂⁿ. Many matrix algorithms rely on structure and properties preserved under GL(n,ℂ) transformations.",
      "properties": {
        "Goal": "Describe the set of all invertible linear transformations.",
        "Applications": [
          "Matrix decomposition theory",
          "Similarity transformations",
          "Lie groups"
        ],
        "Methods": [
          "Determinant analysis",
          "Group operations"
        ],
        "Examples": [
          "A ∈ GL(n,ℂ) iff det(A) ≠ 0"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_04.json"
        ]
      }
    },
    "Hessenberg Form": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "A nearly triangular matrix where all entries below the first subdiagonal are zero.",
      "description": "Hessenberg form is the target of reduction prior to running the QR algorithm. Transforming a matrix into Hessenberg form preserves eigenvalues and reduces computational cost.",
      "properties": {
        "Goal": "Reduce general matrices to a structured form suitable for eigenvalue algorithms.",
        "Applications": [
          "QR algorithm",
          "Iterative eigenvalue methods"
        ],
        "Methods": [
          "Householder transformations"
        ],
        "Examples": [
          "Upper Hessenberg reduction of dense matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_04.json"
        ]
      }
    },
    "Hessenberg Reduction": {
      "type": "Algorithm",
      "domain": "Numerical Linear Algebra",
      "definition": "An algorithm that transforms a matrix into Hessenberg form using orthogonal transformations.",
      "description": "Reduction to Hessenberg form is a standard preprocessing step in dense eigenvalue computations. It reduces complexity in subsequent QR iterations while preserving eigenvalues.",
      "properties": {
        "Goal": "Efficiently produce a Hessenberg matrix from a general matrix.",
        "Applications": [
          "Eigenvalue computations",
          "QR algorithm initialization"
        ],
        "Methods": [
          "Householder reflections"
        ],
        "Examples": [
          "MATLAB's hess(A)"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_04.json"
        ]
      }
    },
    "High Parallelism": {
      "type": "Concept",
      "domain": "High-Performance Computing",
      "definition": "The property of an algorithm being able to exploit many processors simultaneously with minimal communication.",
      "description": "High parallelism is crucial for modern numerical algorithms on GPUs, clusters, and distributed systems. FFT, matrix multiplication, and multigrid methods often exhibit strong parallel characteristics.",
      "properties": {
        "Goal": "Increase performance by reducing sequential bottlenecks.",
        "Applications": [
          "FFT",
          "Multigrid",
          "Parallel linear solvers"
        ],
        "Methods": [
          "Task decomposition",
          "Domain decomposition"
        ],
        "Examples": [
          "FFT butterfly operations executed in parallel"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_04.json"
        ]
      }
    },
    "Idempotency": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A property of an operator P satisfying P² = P.",
      "description": "Idempotent operators correspond to projection operators onto subspaces. They play a central role in matrix subspace theory, direct sum decompositions, and oblique projections.",
      "properties": {
        "Goal": "Characterize projection-like operators with stable repeated action.",
        "Applications": [
          "Projection operators",
          "Invariant subspaces"
        ],
        "Methods": [
          "Spectral decomposition",
          "Subspace projections"
        ],
        "Examples": [
          "Orthogonal projection matrix"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_04.json"
        ]
      }
    },
    "Imaginary Unit j": {
      "type": "Concept",
      "domain": "Complex Analysis",
      "definition": "The imaginary unit j satisfying j² = −1, commonly used in electrical engineering notation.",
      "description": "In many numerical algorithms involving complex arithmetic, j corresponds to the imaginary axis. It is mathematically equivalent to the complex unit i in mathematics.",
      "properties": {
        "Goal": "Represent the imaginary axis in complex numbers.",
        "Applications": [
          "Fourier analysis",
          "Complex matrix computations"
        ],
        "Methods": [
          "Complex arithmetic"
        ],
        "Examples": [
          "e^{jθ} representation in FFT"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_04.json"
        ]
      }
    },
    "Inverse Iteration": {
      "type": "Algorithm",
      "domain": "Eigenvalue Algorithms",
      "definition": "An iterative method for approximating eigenvectors by repeatedly solving shifted linear systems.",
      "description": "Inverse iteration computes eigenvectors associated with a selected eigenvalue by repeatedly solving (A - μI)x_{k+1} = x_k. When μ is close to an eigenvalue, convergence is rapid.",
      "properties": {
        "Goal": "Compute eigenvectors accurately for a given eigenvalue shift.",
        "Applications": [
          "Eigenvalue refinement",
          "Rayleigh Quotient Iteration"
        ],
        "Methods": [
          "Shifted linear solves",
          "Spectral shift"
        ],
        "Examples": [
          "Inverse iteration with Rayleigh quotient shift"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_04.json"
        ]
      }
    },
    "Jordan Canonical Form": {
      "type": "Concept",
      "domain": "Linear Algebra / Spectral Theory",
      "definition": "A block-diagonal matrix consisting of Jordan blocks that represent the structure of a linear operator up to similarity transformations.",
      "description": "The Jordan Canonical Form reveals eigenvalues, geometric multiplicities, and algebraic multiplicities of a matrix. It classifies matrices up to similarity and provides insight into non-diagonalizable operators.",
      "properties": {
        "Goal": "Classify matrices under similarity and reveal spectral structure.",
        "Applications": [
          "Differential equations",
          "Matrix functions",
          "Control theory"
        ],
        "Methods": [
          "Similarity transformations",
          "Generalized eigenvectors"
        ],
        "Examples": [
          "Jordan blocks for repeated eigenvalues"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_05.json"
        ]
      }
    },
    "Kronecker Product": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "An operation on two matrices that produces a block matrix, defined as A ⊗ B = [a_ij B].",
      "description": "Kronecker products encode tensor product structures and appear in matrix equations, vectorization identities, and discretizations of PDEs. They are a foundation of fast algorithms for large structured systems.",
      "properties": {
        "Goal": "Represent tensor products and structured matrix operations.",
        "Applications": [
          "Sylvester equations",
          "Quantum computing",
          "Tensor calculus"
        ],
        "Methods": [
          "Block matrix construction",
          "Vectorization identities"
        ],
        "Examples": [
          "vec(AXB) = (Bᵀ ⊗ A) vec(X)"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_05.json"
        ]
      }
    },
    "Lower Triangular Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A matrix whose entries above the main diagonal are all zero.",
      "description": "Lower triangular matrices appear naturally in LU factorization and recursive matrix algorithms. They support fast forward substitution for solving linear systems.",
      "properties": {
        "Goal": "Provide a structured matrix enabling efficient solves.",
        "Applications": [
          "LU factorization",
          "Forward substitution"
        ],
        "Methods": [
          "Matrix decomposition"
        ],
        "Examples": [
          "L in LU = L U"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_05.json"
        ]
      }
    },
    "Matrix Pencil": {
      "type": "Concept",
      "domain": "Generalized Eigenvalue Theory",
      "definition": "A parametric family of matrices of the form A - λB representing a generalized eigenvalue problem.",
      "description": "Matrix pencils are central to generalized eigenvalue problems, QZ algorithms, and control theory. They allow describing systems with singular B or differential-algebraic structure.",
      "properties": {
        "Goal": "Represent generalized eigenvalue problems.",
        "Applications": [
          "QZ algorithm",
          "Control synthesis",
          "DAEs"
        ],
        "Methods": [
          "Generalized Schur decomposition"
        ],
        "Examples": [
          "det(A - λB) = 0 gives generalized eigenvalues"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_05.json"
        ]
      }
    },
    "Matrix Rank": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The number of linearly independent rows or columns of a matrix.",
      "description": "Matrix rank reveals fundamental properties of a matrix such as invertibility, nullity, and the dimension of its image. Rank plays a core role in solving linear systems, low-rank approximation, and SVD.",
      "properties": {
        "Goal": "Quantify the dimension of the range of a matrix.",
        "Applications": [
          "Linear systems",
          "Low-rank approximation",
          "SVD"
        ],
        "Methods": [
          "Row reduction",
          "Singular value decomposition"
        ],
        "Examples": [
          "Rank-deficient least squares problems"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_05.json"
        ]
      }
    },
    "Minimal Polynomial": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "The monic polynomial of least degree such that p(A) = 0.",
      "description": "The minimal polynomial characterizes the algebraic structure of a matrix. Its degree determines the size of Jordan blocks and governs convergence of polynomial iterative methods.",
      "properties": {
        "Goal": "Capture the smallest polynomial annihilating a matrix.",
        "Applications": [
          "Jordan form",
          "Krylov methods",
          "Matrix functions"
        ],
        "Methods": [
          "Cayley-Hamilton theorem",
          "Companion matrices"
        ],
        "Examples": [
          "Diagonalizable matrices have minimal polynomial with simple roots"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_05.json"
        ]
      }
    },
    "Normal Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra / Spectral Theory",
      "definition": "A matrix that commutes with its conjugate transpose, satisfying AA* = A*A.",
      "description": "Normal matrices include Hermitian, unitary, and orthogonal matrices as special cases. They are diagonalizable by a unitary matrix and enjoy well-conditioned eigenvalue problems.",
      "properties": {
        "Goal": "Generalize diagonalizable and orthogonally diagonalizable matrices.",
        "Applications": [
          "Quantum mechanics",
          "Spectral analysis",
          "Matrix functions"
        ],
        "Methods": [
          "Unitary diagonalization"
        ],
        "Examples": [
          "Unitary matrices, Hermitian matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_05.json"
        ]
      }
    },
    "Numerical Radius": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "The quantity w(A) = max_{‖x‖=1} |x*Ax|, representing the radius of the smallest disk containing the field of values.",
      "description": "The numerical radius provides a tighter bound than the spectral radius for non-normal matrices. It is related to operator norms and is used in stability analysis.",
      "properties": {
        "Goal": "Bound eigenvalues and assess stability.",
        "Applications": [
          "Operator theory",
          "Stability",
          "Non-normal matrices"
        ],
        "Methods": [
          "Field of values analysis"
        ],
        "Examples": [
          "w(A) ≤ ‖A‖ ≤ 2 w(A)"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_05.json"
        ]
      }
    },
    "Pseudospectrum": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "The ε-pseudospectrum of A is the set of complex numbers z for which ‖(A - zI)⁻¹‖ > 1/ε or z is an eigenvalue of a nearby matrix.",
      "description": "Pseudospectra describe sensitivity of eigenvalues to perturbations and are crucial in understanding non-normal behavior, transient growth, and numerical stability.",
      "properties": {
        "Goal": "Assess robustness of eigenvalues under perturbations.",
        "Applications": [
          "Non-normal matrices",
          "Stability analysis",
          "Control theory"
        ],
        "Methods": [
          "Resolvent norm computation"
        ],
        "Examples": [
          "Highly non-normal matrices have large pseudospectra"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_05.json"
        ]
      }
    },
    "Quadratic Form": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A function of the form q(x) = xᵀAx for a symmetric matrix A.",
      "description": "Quadratic forms relate directly to eigenvalues, definiteness, and optimization landscapes. They appear in stability theory, energy minimization, and classification of matrices.",
      "properties": {
        "Goal": "Model energy-like quantities through symmetric matrices.",
        "Applications": [
          "Optimization",
          "Stability",
          "Statistics"
        ],
        "Methods": [
          "Spectral decomposition"
        ],
        "Examples": [
          "Second-order Taylor approximations"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_05.json"
        ]
      }
    },
    "QZ Algorithm": {
      "type": "Algorithm",
      "domain": "Generalized Eigenvalue Problems",
      "definition": "An algorithm that reduces a matrix pencil (A, B) to generalized Schur form using unitary transformations.",
      "description": "The QZ algorithm generalizes the QR algorithm to solve A x = λ B x problems. It produces upper triangular matrices S and T such that Q* A Z = S and Q* B Z = T. It is the standard dense solver for generalized eigenvalues.",
      "properties": {
        "Goal": "Compute generalized eigenvalues reliably for matrix pencils.",
        "Applications": [
          "DAE systems",
          "Control theory",
          "Stability analysis"
        ],
        "Methods": [
          "Generalized Hessenberg reduction",
          "Unitary similarity"
        ],
        "Examples": [
          "MATLAB's eig(A, B)"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Row Reduction": {
      "type": "Algorithm",
      "domain": "Linear Algebra",
      "definition": "A sequence of elementary row operations used to simplify matrices to row echelon or reduced row echelon form.",
      "description": "Row reduction is fundamental for solving linear systems, computing ranks, bases, and understanding matrix structure. It underpins Gaussian elimination and many algebraic manipulations.",
      "properties": {
        "Goal": "Transform matrices into simplified forms revealing rank and solution structure.",
        "Applications": [
          "Solving linear systems",
          "Rank computation",
          "Null space"
        ],
        "Methods": [
          "Elementary row operations",
          "Pivoting"
        ],
        "Examples": [
          "RREF calculation for Ax = b"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Schur Complement": {
      "type": "Concept",
      "domain": "Matrix Theory",
      "definition": "For a block matrix, the Schur complement of A in M = [[A, B], [C, D]] is S = D - C A⁻¹ B.",
      "description": "The Schur complement appears in block Gaussian elimination, matrix inversion, optimization, and statistics. It captures conditional behavior and plays a crucial role in SPD testing.",
      "properties": {
        "Goal": "Reduce block-matrix operations to smaller components.",
        "Applications": [
          "Optimization",
          "Block LU",
          "Covariance matrices",
          "Linear systems"
        ],
        "Methods": [
          "Block elimination",
          "Matrix inversion"
        ],
        "Examples": [
          "Used in Kalman filter equations"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Search Directions (in CG)": {
      "type": "Concept",
      "domain": "Iterative Methods",
      "definition": "The sequence of A-conjugate directions generated by the Conjugate Gradient method.",
      "description": "Search directions determine the efficiency and convergence of CG. Each direction is constructed to be A-orthogonal to all previous ones, ensuring optimality in SPD problems.",
      "properties": {
        "Goal": "Define the sequence of directions used to minimize the error over Krylov subspaces.",
        "Applications": [
          "CG iterations",
          "Krylov subspace minimization"
        ],
        "Methods": [
          "A-orthogonalization",
          "Residual-based direction updates"
        ],
        "Examples": [
          "pₖ = rₖ + βₖ pₖ₋₁"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Similarity Transformation": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A transformation of the form A → S⁻¹ A S with S invertible, preserving eigenvalues.",
      "description": "Similarity transformations classify matrices into equivalence classes and preserve spectral properties. They form the basis for diagonalization, Jordan forms, and spectral algorithms.",
      "properties": {
        "Goal": "Transform matrices while preserving eigenvalues and spectral characteristics.",
        "Applications": [
          "Diagonalization",
          "Jordan canonical form",
          "Invariant subspaces"
        ],
        "Methods": [
          "Change of basis",
          "Matrix equivalence"
        ],
        "Examples": [
          "S⁻¹ A S is diagonalizable"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Spectral Gap": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "The difference between two adjacent eigenvalues, often used to characterize convergence rates.",
      "description": "A larger spectral gap leads to faster convergence of iterative methods such as power iteration and CG. Spectral gap also governs mixing times in Markov chains.",
      "properties": {
        "Goal": "Measure separation between key eigenvalues.",
        "Applications": [
          "Power iteration",
          "Graph Laplacians",
          "Markov chains"
        ],
        "Methods": [
          "Eigenvalue analysis"
        ],
        "Examples": [
          "Gap = λ₂ - λ₁ for stochastic matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Spectral Mapping Theorem": {
      "type": "Concept",
      "domain": "Matrix Functions",
      "definition": "The theorem stating that for analytic functions f, the spectrum satisfies Λ(f(A)) = f(Λ(A)).",
      "description": "Spectral mapping allows computation of matrix functions by applying the scalar function to eigenvalues. It is fundamental to understanding behaviors of matrix exponentials and polynomials.",
      "properties": {
        "Goal": "Relate eigenvalues of functions of matrices to functions of eigenvalues.",
        "Applications": [
          "Matrix exponential",
          "Matrix square root",
          "Krylov methods"
        ],
        "Methods": [
          "Analytic functions",
          "Contour integrals"
        ],
        "Examples": [
          "Λ(e^A) = e^{Λ(A)}"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Spectral Radius": {
      "type": "Concept",
      "domain": "Spectral Theory",
      "definition": "The quantity ρ(A) = max |λᵢ|, the maximum magnitude of eigenvalues of A.",
      "description": "Spectral radius determines convergence of matrix iterations, stability, and power method behavior. It is a core concept in numerical linear algebra and operator theory.",
      "properties": {
        "Goal": "Measure the largest eigenvalue magnitude.",
        "Applications": [
          "Stability analysis",
          "Power iteration",
          "Matrix norms"
        ],
        "Methods": [
          "Eigenvalue computation"
        ],
        "Examples": [
          "ρ(A) < 1 ensures convergence of Neumann series"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Strassen Algorithm": {
      "type": "Algorithm",
      "domain": "Matrix Computations",
      "definition": "A subcubic algorithm for matrix multiplication with complexity O(n^{log₂7}) ≈ O(n^{2.807}).",
      "description": "Strassen’s method reduces the number of multiplications required for matrix multiplication. It forms the basis for fast algebraic algorithms and motivates subcubic research.",
      "properties": {
        "Goal": "Multiply matrices faster than classical O(n³) time.",
        "Applications": [
          "Large dense matrices",
          "Parallel computing"
        ],
        "Methods": [
          "Divide-and-conquer",
          "Bilinear algorithms"
        ],
        "Examples": [
          "7 multiplications instead of 8 for 2x2 blocks"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Subcubic Algorithms": {
      "type": "Concept",
      "domain": "Matrix Computations",
      "definition": "Algorithms for matrix multiplication with asymptotic complexity below O(n³).",
      "description": "Subcubic algorithms include Strassen, Coppersmith–Winograd, and subsequent improvements. They are foundational in algebraic complexity theory.",
      "properties": {
        "Goal": "Push theoretical limits of fast matrix multiplication.",
        "Applications": [
          "Computational complexity",
          "Large-scale algebraic computations"
        ],
        "Methods": [
          "Tensor decomposition",
          "Bilinear complexity"
        ],
        "Examples": [
          "Coppersmith–Winograd algorithm"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Subspace Decomposition": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A decomposition of a vector or matrix space into multiple complementary subspaces.",
      "description": "Subspace decompositions underpin direct-sum representations, block structure in matrices, and multilevel numerical algorithms. They enable modularization of linear algebraic problems.",
      "properties": {
        "Goal": "Split spaces into lower-dimensional components with structured relationships.",
        "Applications": [
          "Multigrid",
          "Block matrices",
          "Direct sum"
        ],
        "Methods": [
          "Orthogonal complement",
          "Projection operators"
        ],
        "Examples": [
          "V = U1 ⊕ U2 ⊕ … ⊕ Uk"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Subspace Iteration": {
      "type": "Algorithm",
      "domain": "Eigenvalue Algorithms",
      "definition": "An iterative method that simultaneously computes multiple dominant eigenvectors by repeatedly applying A to a subspace.",
      "description": "Subspace iteration generalizes the power method to multiple vectors. It forms the basis of modern eigensolvers such as LOBPCG and block Krylov methods.",
      "properties": {
        "Goal": "Approximate several eigenpairs by iterating on subspaces.",
        "Applications": [
          "Large-scale eigenvalue problems",
          "PDE solvers",
          "Block Krylov methods"
        ],
        "Methods": [
          "Orthogonalization",
          "Rayleigh–Ritz projection"
        ],
        "Examples": [
          "Block power iteration"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_06.json"
        ]
      }
    },
    "Sylvester Equation Solution": {
      "type": "Algorithm",
      "domain": "Matrix Equations",
      "definition": "The process of solving the matrix equation AX + XB = C for X, typically when spectra of A and B do not overlap.",
      "description": "Sylvester equation solvers rely on Schur decompositions and Bartels–Stewart algorithms. They play a central role in control theory, Lyapunov equations, and model reduction.",
      "properties": {
        "Goal": "Compute X efficiently when AX + XB = C.",
        "Applications": [
          "Control theory",
          "Lyapunov equations",
          "Model reduction"
        ],
        "Methods": [
          "Bartels–Stewart algorithm",
          "Schur decomposition"
        ],
        "Examples": [
          "Solving AX + XB = Q for stability analysis"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Symmetric Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A real matrix satisfying Aᵀ = A.",
      "description": "Symmetric matrices enjoy real eigenvalues, orthogonal diagonalization, and strong numerical stability. They form the real counterpart of Hermitian matrices.",
      "properties": {
        "Goal": "Represent self-adjoint real operators.",
        "Applications": [
          "Optimization",
          "Eigenvalue problems",
          "PDE discretizations"
        ],
        "Methods": [
          "Orthogonal diagonalization"
        ],
        "Examples": [
          "Graph Laplacian matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Symmetric Positive Definite Matrices": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "Real symmetric matrices A for which xᵀAx > 0 for all nonzero x.",
      "description": "SPD matrices arise in PDEs, covariance analysis, optimization, and CG methods. They guarantee unique Cholesky factorizations and fast iterative convergence.",
      "properties": {
        "Goal": "Characterize strictly positive curvature of quadratic forms.",
        "Applications": [
          "Conjugate Gradient",
          "Cholesky factorization",
          "Machine learning"
        ],
        "Methods": [
          "Eigenvalue analysis",
          "Quadratic forms"
        ],
        "Examples": [
          "Graph Laplacian + εI"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Triangular Matrices": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "Matrices that are either upper or lower triangular.",
      "description": "Triangular matrices form the basis of LU factorization and back/forward substitution. They preserve structure in matrix decompositions and support fast algorithms.",
      "properties": {
        "Goal": "Define structured matrices enabling efficient solving.",
        "Applications": [
          "LU factorization",
          "QR reduction",
          "Matrix decomposition"
        ],
        "Methods": [
          "Back substitution",
          "Forward substitution"
        ],
        "Examples": [
          "Upper and lower triangular matrices"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Triangular Solves": {
      "type": "Algorithm",
      "domain": "Numerical Linear Algebra",
      "definition": "Algorithms that solve triangular linear systems via forward or backward substitution.",
      "description": "Triangular solves are low-cost O(n²) operations widely used inside LU, QR, Cholesky, and Krylov methods. They involve recursive elimination without pivoting.",
      "properties": {
        "Goal": "Solve triangular systems efficiently.",
        "Applications": [
          "LU solve",
          "QR solve",
          "Block triangular systems"
        ],
        "Methods": [
          "Forward substitution",
          "Backward substitution"
        ],
        "Examples": [
          "Solving Lx = b in O(n²) time"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Tridiagonal Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A matrix whose nonzero entries lie on the main diagonal and the two adjacent diagonals.",
      "description": "Tridiagonal matrices arise in 1D PDE discretizations, eigenvalue algorithms, and Lanczos methods. They enable fast O(n) algorithms for solving linear systems.",
      "properties": {
        "Goal": "Represent sparse structured operators with minimal bandwidth.",
        "Applications": [
          "Lanczos algorithm",
          "Thomas algorithm",
          "Finite differences"
        ],
        "Methods": [
          "Specialized LU",
          "Orthogonal reduction"
        ],
        "Examples": [
          "1D Poisson matrix"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Unitary Similarity": {
      "type": "Concept",
      "domain": "Linear Algebra / Spectral Theory",
      "definition": "A similarity transformation of the form A → U*AU where U is unitary.",
      "description": "Unitary similarity preserves norms, eigenvalues, and stability, making it essential in Schur decomposition, unitarily diagonalizable matrices, and spectral algorithms.",
      "properties": {
        "Goal": "Transform matrices while preserving numerical stability and eigenvalues.",
        "Applications": [
          "Schur decomposition",
          "Matrix diagonalization"
        ],
        "Methods": [
          "Unitary transformations"
        ],
        "Examples": [
          "Hessenberg reduction via unitary similarity"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Unitary Transformation": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A transformation represented by a unitary matrix U such that U*U = I.",
      "description": "Unitary transformations preserve norms, orthogonality, and conditioning. They are essential for stable QR factorization, Hessenberg reduction, and SVD.",
      "properties": {
        "Goal": "Apply stable norm-preserving transformations.",
        "Applications": [
          "QR factorization",
          "Eigenvalue methods",
          "Matrix reductions"
        ],
        "Methods": [
          "Givens rotations",
          "Householder reflections"
        ],
        "Examples": [
          "Q in QR decomposition"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Upper Triangular Factor": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "The U matrix in LU decomposition, where all entries below the diagonal are zero.",
      "description": "The upper triangular factor U stores the result of Gaussian elimination. It is used in backward substitution and forms half of the LU factorization.",
      "properties": {
        "Goal": "Store eliminated system structure for solving Ax = b.",
        "Applications": [
          "LU factorization",
          "Backward substitution"
        ],
        "Methods": [
          "Gaussian elimination"
        ],
        "Examples": [
          "LU = L U with U triangular"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Upper Triangular Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A matrix whose entries below the diagonal are all zero.",
      "description": "Upper triangular matrices naturally arise in QR and LU factorizations. They support fast backward substitution and reveal eigenvalues directly on the diagonal.",
      "properties": {
        "Goal": "Provide structure for efficient solves and spectral access.",
        "Applications": [
          "Schur decomposition",
          "LU factorization"
        ],
        "Methods": [
          "Back substitution"
        ],
        "Examples": [
          "R in QR factorization"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Vandermonde Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra / Approximation Theory",
      "definition": "A matrix whose rows or columns follow geometric progressions based on input nodes.",
      "description": "Vandermonde matrices appear in polynomial interpolation, Prony methods, and spectral approximations. They are often ill-conditioned, especially for large degrees.",
      "properties": {
        "Goal": "Represent polynomial basis evaluations compactly.",
        "Applications": [
          "Interpolation",
          "Polynomial fitting",
          "Signal processing"
        ],
        "Methods": [
          "Lagrange interpolation",
          "Basis evaluation"
        ],
        "Examples": [
          "V(i,j) = x_i^{j-1}"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Vector Subspace": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A subset of a vector space closed under addition and scalar multiplication.",
      "description": "Vector subspaces are foundational structures of linear algebra. They define solution sets, null spaces, ranges, invariant subspaces, and decomposition structures.",
      "properties": {
        "Goal": "Define linear structure within a larger vector space.",
        "Applications": [
          "Null space",
          "Range",
          "Direct sum decompositions"
        ],
        "Methods": [
          "Linear span",
          "Projection operators"
        ],
        "Examples": [
          "Null(A)",
          "Range(A)"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Residual r = b - Ax": {
      "type": "Concept",
      "domain": "Iterative Methods",
      "definition": "The difference between the right-hand side and the current approximation’s predicted value in a linear system.",
      "description": "Residuals determine convergence, stopping criteria, and direction generation in iterative methods such as CG, GMRES, and Richardson iteration.",
      "properties": {
        "Goal": "Measure progress of iterative solvers.",
        "Applications": [
          "CG",
          "GMRES",
          "Jacobi",
          "Richardson iteration"
        ],
        "Methods": [
          "Residual norm minimization"
        ],
        "Examples": [
          "||r_k|| used as stopping criterion"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Matrix Square Root (Alternative Notation)": {
      "type": "Concept",
      "domain": "Matrix Functions",
      "definition": "A matrix X satisfying X² = A. This entry covers the symbol-level term sqrt(A).",
      "description": "Matrix square roots arise in diffusion processes, covariance models, and analytic matrix functions. Many algorithms compute √A using Newton iterations or spectral decompositions.",
      "properties": {
        "Goal": "Compute a matrix whose square equals A.",
        "Applications": [
          "Diffusion equations",
          "Covariance analysis"
        ],
        "Methods": [
          "Newton’s iteration",
          "Schur decomposition"
        ],
        "Examples": [
          "Principal square root from spectral decomposition"
        ]
      },
      "metadata": {
        "subjects": [
          "numerical_matrix"
        ],
        "source_files": [
          "extra_00_07.json"
        ]
      }
    },
    "Numerosity Reduction": {
      "type": "Technique",
      "domain": "Data Pre-processing",
      "definition": "A data reduction technique that reduces the volume of data by choosing smaller, more compact representations without significantly sacrificing the integrity of analytical results.",
      "description": "Numerosity reduction aims to improve computational efficiency by replacing the original data with smaller parametric or non-parametric representations. Instead of storing the full dataset, only the model parameters or simplified summaries are kept. This enables faster querying, analysis, and modeling while preserving most essential patterns.",
      "properties": {
        "Goal": "Reduce data volume while maintaining the ability to perform accurate analysis.",
        "Categories": [
          "Parametric methods",
          "Non-parametric methods"
        ],
        "Parametric Methods": [
          "Regression models",
          "Log-linear models",
          "Other statistical modeling approaches where only model parameters are stored"
        ],
        "Non-parametric Methods": [
          "Histograms",
          "Clustering",
          "Sampling"
        ],
        "Advantages": [
          "Reduces storage space",
          "Improves algorithm performance",
          "Enables faster data processing"
        ],
        "Limitations": [
          "Potential loss of detail",
          "Quality depends on model or sampling assumptions"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_00.json"
        ]
      }
    },
    "Accuracy Paradox": {
      "type": "Concept",
      "domain": "Model Evaluation",
      "definition": "A phenomenon where a model with high accuracy performs poorly on the actual predictive task, especially when the dataset is imbalanced.",
      "description": "Accuracy can give a misleading impression of model performance because it does not account for class imbalance. In problems where one class dominates, models can achieve high accuracy by simply predicting the majority class, despite having little real predictive power. Therefore, additional evaluation metrics such as balanced accuracy, precision, recall, F1 score, or Cohen’s kappa should be used to correctly assess performance.",
      "properties": {
        "Problem": "Accuracy ignores the relative distribution of classes.",
        "Occurs_When": [
          "The dataset is imbalanced",
          "The majority class dominates predictions",
          "Accuracy does not reflect predictive power"
        ],
        "Better_Metrics": [
          "Balanced accuracy",
          "Precision",
          "Recall",
          "F1 score",
          "Cohen's kappa"
        ],
        "Example": "In a dataset with 275 healthy and 25 cancer patients, predicting everyone as healthy yields 91.7% accuracy but 0% predictive power for detecting cancer."
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_00.json"
        ]
      }
    },
    "Validation Set": {
      "type": "Dataset Partition",
      "domain": "Model Evaluation",
      "definition": "A subset of data used to tune model hyperparameters or select among competing models before final evaluation on the test set.",
      "description": "The validation set is used during model selection to ensure that the chosen model is not overfitting the training data. After models are compared based on validation performance, the final selected model is evaluated on a separate test set for unbiased performance estimation. This partitioning improves generalizability and prevents overly optimistic results.",
      "properties": {
        "Purpose": "Model selection, hyperparameter tuning, preventing overfitting.",
        "Process": [
          "Train model on training set",
          "Validate model and compare candidates",
          "Select optimal model before final testing"
        ],
        "When_Used": [
          "Neural network iteration selection",
          "Hyperparameter optimization",
          "Model comparison"
        ],
        "Related_Concepts": [
          "Training Set",
          "Test Set",
          "Cross-validation"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_00.json"
        ]
      }
    },
    "Artificial Data Generation": {
      "type": "Technique",
      "domain": "Data Pre-processing",
      "definition": "A set of methods used to create additional synthetic samples when insufficient data is available, helping balance datasets or increase training coverage.",
      "description": "Artificial data generation expands a dataset by creating new synthetic samples derived from observed data properties. It includes multiple methods such as noise injection, distribution-based artificial data creation, and SMOTE. These approaches are useful especially when some classes lack sufficient samples or when expanding feature coverage is required.",
      "properties": {
        "Goal": "Increase dataset size, improve class balance, expand training coverage.",
        "Methods": [
          "Artificial data (distribution-based sampling)",
          "Noise injection",
          "SMOTE"
        ],
        "Advantages": [
          "Improves model generalization",
          "Addresses class imbalance",
          "Expands data in a controlled manner"
        ],
        "Limitations": [
          "Can introduce unrealistic samples if assumptions are incorrect",
          "May increase risk of overfitting if not applied carefully"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_00.json"
        ]
      }
    },
    "Artificial Data": {
      "type": "Data Type",
      "domain": "Data Pre-processing",
      "definition": "Synthetic data generated by sampling from a statistical distribution that approximates the original dataset.",
      "description": "Artificial data is created by modeling the original dataset using parameters such as mean (μ) and standard deviation (σ). Each class is modeled separately, and the distribution must be known or estimated. New samples are then drawn from these distributions to form a larger dataset. This method is especially useful when the original dataset is too small for effective model training.",
      "properties": {
        "Goal": "Create a larger dataset based on estimated population characteristics.",
        "Process": [
          "Estimate distribution parameters for each class",
          "Model the population",
          "Draw m synthetic samples where m > N"
        ],
        "Advantages": [
          "Helpful for rare classes",
          "Simple to generate if distribution is known",
          "Supports better model generalization"
        ],
        "Limitations": [
          "Requires known or well-estimated distributions",
          "May oversimplify real-world complexity"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_00.json"
        ]
      }
    },
    "Personal Data": {
      "type": "concept",
      "domain": "Privacy",
      "definition": "Any information relating to an identified or identifiable natural person.",
      "description": "Covers both direct identifiers and quasi-identifiers that can re-identify individuals when combined.",
      "properties": {
        "examples": [
          "names",
          "addresses",
          "location traces",
          "online identifiers"
        ],
        "legal_status": "Protected under GDPR"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "Pseudonymisation": {
      "type": "method",
      "domain": "Privacy",
      "definition": "Replacing identifying attributes with artificial identifiers while keeping a mapping for re-identification.",
      "description": "Provides partial privacy but is reversible when the mapping is available.",
      "properties": {
        "characteristics": [
          "mapping retained separately",
          "reversible"
        ],
        "use_cases": [
          "research follow-ups",
          "risk-controlled data release"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "k-Anonymity": {
      "type": "privacy_property",
      "domain": "Privacy",
      "definition": "A property ensuring each record is indistinguishable from at least k-1 others based on quasi-identifiers.",
      "description": "Achieved through generalisation or suppression; vulnerable to attribute disclosure attacks.",
      "properties": {
        "parameter": "k",
        "methods": [
          "suppression",
          "generalisation"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "Advanced Data Modification": {
      "type": "method",
      "domain": "Privacy",
      "definition": "Heuristic or algorithmic methods for modifying datasets to reduce privacy risk while retaining analytical value.",
      "description": "Includes perturbation, synthetic data generation, and suppression heuristics.",
      "properties": {
        "approaches": [
          "perturbation",
          "synthetic data",
          "attribute suppression heuristics"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "Relational Database": {
      "type": "data_structure",
      "domain": "Databases",
      "definition": "A structured collection of relations (tables) organized according to the relational model.",
      "description": "Implements schemas, supports SQL, and typically enforces ACID properties.",
      "properties": {
        "features": [
          "tables",
          "schemas",
          "ACID transactions"
        ],
        "examples": [
          "PostgreSQL",
          "MySQL",
          "Oracle"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "NoSQL Databases": {
      "type": "category",
      "domain": "Databases",
      "definition": "A broad category of database systems that do not use the relational model.",
      "description": "Includes document stores, key-value stores, columnar stores, and graph databases.",
      "properties": {
        "advantages": [
          "schema flexibility",
          "horizontal scaling"
        ],
        "examples": [
          "MongoDB"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "MongoDB": {
      "type": "software_system",
      "domain": "Databases",
      "definition": "A document-oriented NoSQL database storing data in BSON format.",
      "description": "Allows flexible schema design and uses a hierarchical document model.",
      "properties": {
        "storage_format": "BSON",
        "model": "Document Store"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "Entity-Relationship Modeling": {
      "type": "method",
      "domain": "Databases",
      "definition": "A conceptual modeling method describing entities, attributes, and relationships.",
      "description": "Used to design relational schemas and understand domain structure.",
      "properties": {
        "components": [
          "entities",
          "attributes",
          "relationships"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "Relational Algebra": {
      "type": "theory",
      "domain": "Databases",
      "definition": "A formal set of operations on relations forming the foundation of SQL.",
      "description": "Core operators include projection, selection, and natural join.",
      "properties": {
        "operators": [
          "projection",
          "selection",
          "join"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "SQL": {
      "type": "language",
      "domain": "Databases",
      "definition": "A declarative language for managing and querying relational databases.",
      "description": "Supports SELECT, JOIN, INSERT, UPDATE, DELETE, and schema definition.",
      "properties": {
        "categories": [
          "DDL",
          "DML",
          "DQL"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "CIA Triad": {
      "type": "security_model",
      "domain": "Security",
      "definition": "A model defining three core principles of information security: Confidentiality, Integrity, Availability.",
      "description": "Used to categorize threats and security controls.",
      "properties": {
        "principles": [
          "Confidentiality",
          "Integrity",
          "Availability"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "Open Data Licences": {
      "type": "category",
      "domain": "Licensing",
      "definition": "Licenses governing how datasets may be used, shared, and modified.",
      "description": "Includes CC0, CC BY, and ODbL with differing attribution and sharing requirements.",
      "properties": {
        "examples": [
          "CC0",
          "CC BY",
          "ODbL"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "Metadata": {
      "type": "concept",
      "domain": "Data Management",
      "definition": "Structured information describing a dataset’s content, structure, provenance, and semantics.",
      "description": "Supports FAIR principles by enabling discovery and reuse.",
      "properties": {
        "components": [
          "variable descriptions",
          "units",
          "provenance",
          "versioning"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "ActivityMeasurement": {
      "type": "data_entity",
      "domain": "Example Schema",
      "definition": "An example entity representing individual activity measurements.",
      "description": "Used to illustrate relational and document database modeling.",
      "properties": {
        "attributes": [
          "measurementId",
          "personId",
          "monitorId",
          "measurementDate",
          "activeMinutes",
          "steps",
          "calories"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_01.json"
        ]
      }
    },
    "Data Collection Procedure": {
      "type": "process",
      "domain": "Data Collection",
      "definition": "A structured sequence of steps used to design, execute, and refine the process of gathering data for research or analysis.",
      "description": "Includes planning, initial pilot collection, iteration, and final data collection, followed by problem reporting.",
      "properties": {
        "steps": [
          "planning",
          "pilot collection",
          "iteration",
          "final collection",
          "problem reporting"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Data Collection Failure Modes": {
      "type": "concept",
      "domain": "Data Collection",
      "definition": "Common ways in which data collection systems or processes fail, producing incomplete, biased, or invalid data.",
      "description": "Includes sensor errors, unit conversion errors, device malfunction, filtering errors, and human mistakes.",
      "properties": {
        "examples": [
          "sensor placement error",
          "incorrect filtering",
          "signal saturation",
          "unit mismatch"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Signal Filtering Errors": {
      "type": "error_type",
      "domain": "Data Collection",
      "definition": "Errors caused by incorrect or overly aggressive filtering that removes legitimate data patterns.",
      "description": "A famous example is the ozone-hole data being removed due to filters discarding large drops as errors.",
      "properties": {
        "common_causes": [
          "thresholding errors",
          "misconfigured filters",
          "incorrect assumptions"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Sensor Placement Error": {
      "type": "error_type",
      "domain": "Sensor Data",
      "definition": "Incorrect attachment or orientation of sensors leading to inaccurate or unusable data.",
      "description": "Examples include reversing coils in welding studies or misplacing wearable devices.",
      "properties": {
        "effects": [
          "signal distortion",
          "activity mislabeling"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Battery Failure": {
      "type": "error_type",
      "domain": "Data Collection",
      "definition": "Data loss caused by device power depletion during recording sessions.",
      "description": "A common cause of missing data in field sensors, wearable devices, and mobile data collection.",
      "properties": {
        "impact": [
          "incomplete sessions",
          "loss of temporal continuity"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Repeatability": {
      "type": "quality_criterion",
      "domain": "Data Quality",
      "definition": "The ability to repeat a data collection procedure under identical conditions and obtain similar results.",
      "description": "A key criterion in experimental design and sensor-based studies.",
      "properties": {
        "requirements": [
          "controlled conditions",
          "stable instruments"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Reproducibility": {
      "type": "quality_criterion",
      "domain": "Data Quality",
      "definition": "The ability for independent researchers to reproduce results using the same methodology and data.",
      "description": "Requires documentation, transparent procedures, and stable data collection methods.",
      "properties": {
        "dependencies": [
          "documentation",
          "metadata",
          "repeatability"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Stability": {
      "type": "property",
      "domain": "Data Quality",
      "definition": "The extent to which measurement systems remain consistent over time.",
      "description": "Sensitive to environmental changes, sensor degradation, drift, and human behavior.",
      "properties": {
        "related_factors": [
          "drift",
          "non-stationarity",
          "sensor aging"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Margin of Error": {
      "type": "statistical_measure",
      "domain": "Statistics",
      "definition": "An expression of the amount of random sampling error in a survey's results.",
      "description": "Often expressed as a percentage and tied to confidence intervals.",
      "properties": {
        "inputs": [
          "sample size",
          "confidence level"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Confidence Interval": {
      "type": "statistical_concept",
      "domain": "Statistics",
      "definition": "A range of values derived from sample data that is likely to contain the true population value.",
      "description": "Determined using margin of error and Z-scores.",
      "properties": {
        "components": [
          "sample statistic",
          "margin of error"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Z-Score (Statistical)": {
      "type": "statistical_measure",
      "domain": "Statistics",
      "definition": "A normalized value expressing how many standard deviations a measurement is from the mean.",
      "description": "Used to compute confidence intervals and determine significance.",
      "properties": {
        "formula": "z = (x - μ) / σ"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Dropout Rate": {
      "type": "statistical_measure",
      "domain": "Sampling",
      "definition": "The proportion of participants who fail to complete a study or data collection procedure.",
      "description": "Critical for determining effective sample size.",
      "properties": {
        "applications": [
          "study planning",
          "survey design"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Stationary Elements": {
      "type": "concept",
      "domain": "Time Series",
      "definition": "Entities or systems whose properties do not change significantly over time.",
      "description": "Examples include stable mechanical components or controlled environments.",
      "properties": {
        "characteristics": [
          "constant mean",
          "constant variance"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Non-Stationary Elements": {
      "type": "concept",
      "domain": "Time Series",
      "definition": "Entities whose statistical properties change over time.",
      "description": "Typical for environmental data, human behavior, sensor drift, or seasonal changes.",
      "properties": {
        "examples": [
          "humans",
          "ecosystems",
          "aging machinery"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Temporal Drift": {
      "type": "phenomenon",
      "domain": "Time Series",
      "definition": "A gradual change in system behavior or measurement statistics over time.",
      "description": "One of the main challenges when gathering long-term sensor or human data.",
      "properties": {
        "types": [
          "sensor drift",
          "behavioral drift"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Seasonal Variability": {
      "type": "phenomenon",
      "domain": "Time Series",
      "definition": "Regular periodic fluctuations associated with seasons or cyclic processes.",
      "description": "A non-stationary factor influencing environmental and human measurements.",
      "properties": {
        "cycle_length": "annual or periodic"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Time Series Data Collection": {
      "type": "method",
      "domain": "Time Series",
      "definition": "Collecting data at successive time intervals to observe changes or patterns.",
      "description": "Important for avoiding leakage between training and testing windows.",
      "properties": {
        "design_features": [
          "distinct time windows",
          "sampling frequency"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Respect for Persons": {
      "type": "ethical_principle",
      "domain": "Human Research Ethics",
      "definition": "An ethical principle requiring that individuals be treated as autonomous agents.",
      "description": "Includes additional protections for individuals with diminished autonomy.",
      "properties": {
        "components": [
          "autonomy",
          "protection of vulnerable groups"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Beneficence": {
      "type": "ethical_principle",
      "domain": "Human Research Ethics",
      "definition": "An obligation to maximize possible benefits and minimize possible harms to participants.",
      "description": "A foundational principle in ethical human-subject research.",
      "properties": {
        "requirements": [
          "risk-benefit analysis",
          "avoidance of harm"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Justice": {
      "type": "ethical_principle",
      "domain": "Human Research Ethics",
      "definition": "The fair and equitable distribution of the burdens and benefits of research.",
      "description": "Ensures vulnerable or marginalized groups are not exploited.",
      "properties": {
        "considerations": [
          "equity",
          "fair selection of subjects"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_02.json"
        ]
      }
    },
    "Data Format Differences": {
      "type": "concept",
      "domain": "Data Integration",
      "definition": "Differences in structural or syntactic representation of data originating from multiple sources.",
      "description": "Common variations include CSV vs TXT, comma vs tab separators, and presence or absence of header rows.",
      "properties": {
        "examples": [
          "csv",
          "txt",
          "tab-separated",
          "comma-separated"
        ],
        "challenges": [
          "inconsistent parsing",
          "schema mismatch"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Cultural Formatting Differences": {
      "type": "concept",
      "domain": "Data Integration",
      "definition": "Variations in numeric and textual conventions across regions that affect data interpretation.",
      "description": "Typical issues include decimal comma versus decimal point and different date formats.",
      "properties": {
        "examples": [
          "1,23 vs 1.23",
          "DD/MM/YYYY vs MM/DD/YYYY"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Unit Mismatch": {
      "type": "error_type",
      "domain": "Data Integration",
      "definition": "A discrepancy where multiple data sources represent the same measurement using different units.",
      "description": "Examples include centimeters vs inches or acceleration measured in g versus m/s^2.",
      "properties": {
        "consequences": [
          "incorrect model inputs",
          "distorted feature magnitudes"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Timestamp Mismatch": {
      "type": "error_type",
      "domain": "Temporal Data",
      "definition": "Differences in the timestamp formats or reference scales used by multiple sensors or data sources.",
      "description": "Examples include Unix timestamps versus human-readable dates.",
      "properties": {
        "examples": [
          "Unix epoch",
          "ISO date strings"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Timestamp Synchronization": {
      "type": "method",
      "domain": "Temporal Data",
      "definition": "The process of aligning timestamps from multiple sources to a common reference scale.",
      "description": "Necessary when merging data from different devices or modalities.",
      "properties": {
        "methods": [
          "clock alignment",
          "interpolation",
          "offset correction"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Label Inconsistency": {
      "type": "error_type",
      "domain": "Data Integration",
      "definition": "A mismatch in semantic meaning or granularity of labels across datasets.",
      "description": "Example: one dataset defines 'walking' as flat-ground walking while another includes stairs.",
      "properties": {
        "effects": [
          "ambiguity",
          "inconsistent supervision"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Sensor Placement Variability": {
      "type": "concept",
      "domain": "Sensor Data",
      "definition": "Differences in sensor orientation or placement on the body or environment.",
      "description": "Placement differences significantly alter the recorded signals even for identical activities.",
      "properties": {
        "examples": [
          "upper arm vs wrist",
          "left vs right side"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Environmental Condition Variability": {
      "type": "concept",
      "domain": "Data Collection",
      "definition": "Changes in external conditions that affect the comparability of collected data.",
      "description": "Examples include season, terrain, indoor versus outdoor conditions, and environmental noise.",
      "properties": {
        "variables": [
          "temperature",
          "surface gradient",
          "lighting"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Calibration Differences": {
      "type": "error_type",
      "domain": "Sensor Data",
      "definition": "Variability in sensor output caused by imperfect or inconsistent calibration between devices.",
      "description": "Occurs even when sensors are of the same model and manufacturer.",
      "properties": {
        "causes": [
          "manufacturing tolerance",
          "wear",
          "environmental effects"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Incompatible Data Collection Protocols": {
      "type": "concept",
      "domain": "Data Collection",
      "definition": "A situation where datasets cannot be merged due to incompatible procedures, sampling rates, or measurement rules.",
      "description": "Arises when two studies follow fundamentally different data-collection methodologies.",
      "properties": {
        "reasons": [
          "different sampling rates",
          "non-matching units",
          "inconsistent label definitions"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Sampling Rate Mismatch": {
      "type": "error_type",
      "domain": "Sampling",
      "definition": "A discrepancy in sampling frequencies across sensors or data sources.",
      "description": "Makes direct merging difficult without resampling.",
      "properties": {
        "examples": [
          "50 Hz vs 100 Hz"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Battery–Sampling Trade-Off": {
      "type": "concept",
      "domain": "Sensor Data",
      "definition": "A trade-off between sampling rate and device battery consumption.",
      "description": "Higher sampling frequencies drain battery faster, limiting experiment duration.",
      "properties": {
        "factors": [
          "sampling rate",
          "battery capacity"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "GCD-Based Downsampling": {
      "type": "method",
      "domain": "Sampling",
      "definition": "Downsampling multiple signals to a shared sampling frequency derived from their greatest common divisor.",
      "description": "Can reduce data quantity but aligns signals to a common time base.",
      "properties": {
        "effect": "information loss"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "LCM-Based Oversampling": {
      "type": "method",
      "domain": "Sampling",
      "definition": "Oversampling multiple signals to a shared sampling frequency derived from their least common multiple.",
      "description": "Increases data size but avoids discarding information.",
      "properties": {
        "effect": "increased computational cost"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Confusion Matrix": {
      "type": "data_structure",
      "domain": "Performance Metrics",
      "definition": "A contingency table summarizing prediction results in classification tasks.",
      "description": "Captures true positives, false positives, true negatives, and false negatives.",
      "properties": {
        "entries": [
          "TP",
          "FP",
          "TN",
          "FN"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Balanced Accuracy": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "The average of recall obtained on each class.",
      "description": "Designed for imbalanced datasets where standard accuracy is misleading.",
      "properties": {
        "formula": "(TP/(TP+FN) + TN/(TN+FP)) / 2"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Precision": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "The proportion of predicted positives that are true positives.",
      "description": "Indicates how many positive predictions are correct.",
      "properties": {
        "formula": "TP / (TP + FP)"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Recall": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "The proportion of actual positives that are correctly identified.",
      "description": "Also called sensitivity; important in medical and anomaly detection tasks.",
      "properties": {
        "formula": "TP / (TP + FN)"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Specificity": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "The proportion of actual negatives that are correctly identified.",
      "description": "Complementary to recall, especially important for false-positive control.",
      "properties": {
        "formula": "TN / (TN + FP)"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "F1 Score": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "The harmonic mean of precision and recall.",
      "description": "Balances false positives and false negatives and is robust to imbalance.",
      "properties": {
        "formula": "2 * (precision * recall) / (precision + recall)"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Cohen’s Kappa": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "A statistic measuring inter-rater or model–true-label agreement adjusted for chance.",
      "description": "Useful for assessing classifier reliability beyond simple accuracy.",
      "properties": {
        "formula": "kappa = (p0 - pe) / (1 - pe)"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_03.json"
        ]
      }
    },
    "Available-Case Analysis": {
      "type": "method",
      "domain": "Missing Data Handling",
      "definition": "A deletion method where all available observations for each variable or pair of variables are used without requiring complete records.",
      "description": "Also known as pairwise deletion. It uses all observed values for each statistical computation, enabling retention of more data than listwise deletion.",
      "properties": {
        "advantages": [
          "retains more data",
          "useful for covariance estimation"
        ],
        "disadvantages": [
          "inconsistent sample sizes",
          "may bias associations under MAR or MNAR"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Complete-Case Analysis": {
      "type": "method",
      "domain": "Missing Data Handling",
      "definition": "A deletion method where entire rows containing any missing values are removed from analysis.",
      "description": "Also known as listwise deletion. Many tools do this automatically, which may reduce sample size and introduce bias.",
      "properties": {
        "advantages": [
          "simple to implement"
        ],
        "disadvantages": [
          "loss of data",
          "biased estimates unless MCAR"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Hot Deck Imputation": {
      "type": "method",
      "domain": "Missing Data Handling",
      "definition": "An imputation technique where missing values are replaced with values drawn from similar cases within the same dataset.",
      "description": "Values are selected from cases that are most similar to the incomplete record; appropriate when missingness is rare.",
      "properties": {
        "source": "same dataset",
        "requirements": [
          "sufficient similar donors"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Cold Deck Imputation": {
      "type": "method",
      "domain": "Missing Data Handling",
      "definition": "An imputation technique where missing values are replaced with values drawn from an external but similar dataset.",
      "description": "Used when in-dataset donor values are insufficient or not representative.",
      "properties": {
        "source": "external dataset",
        "risk": [
          "dataset mismatch bias"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Last Observation Carried Forward": {
      "type": "method",
      "domain": "Time-Series Missing Data",
      "definition": "A method that fills missing time-series values using the last observed measurement in the sequence.",
      "description": "Often used in clinical trials; can introduce bias even under MCAR.",
      "properties": {
        "assumption": "stability of observation over time",
        "problems": [
          "bias",
          "distorted autocorrelation"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Baseline Observation Carried Forward": {
      "type": "method",
      "domain": "Time-Series Missing Data",
      "definition": "A method where all missing values are replaced with the baseline measurement.",
      "description": "Simplifies analysis but can severely distort temporal trends.",
      "properties": {
        "strength": "preserves baseline values",
        "weakness": [
          "bias",
          "underestimates change"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Full Information Maximum Likelihood": {
      "type": "method",
      "domain": "Missing Data Modeling",
      "definition": "A maximum likelihood estimation approach that uses all available data to estimate parameters without explicitly imputing missing values.",
      "description": "Relies on likelihood contributions from incomplete cases; improves overall parameter accuracy.",
      "properties": {
        "advantages": [
          "uses all available data",
          "works under MAR"
        ],
        "limitations": [
          "underestimates standard errors",
          "requires correct model specification"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Expectation-Maximization Algorithm": {
      "type": "algorithm",
      "domain": "Statistical Estimation",
      "definition": "An iterative algorithm for maximum likelihood estimation in the presence of missing or latent variables.",
      "description": "Alternates between estimating expected log-likelihood (E-step) and maximizing it (M-step) until convergence.",
      "properties": {
        "steps": [
          "E-step",
          "M-step"
        ],
        "requirements": [
          "numerical data",
          "convergence criteria"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Imputation Pile-Up": {
      "type": "phenomenon",
      "domain": "Imputation Issues",
      "definition": "The tendency of imputed values to cluster around central tendencies such as the mean.",
      "description": "Common in mean imputation, reducing variability and distorting distributions.",
      "properties": {
        "effects": [
          "loss of variance",
          "biased correlations"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Loss of Variability in Imputed Data": {
      "type": "phenomenon",
      "domain": "Imputation Issues",
      "definition": "Reduction in variance of the dataset caused by imputation methods that fail to incorporate uncertainty.",
      "description": "Occurs in mean and regression imputation, affecting model estimates.",
      "properties": {
        "consequences": [
          "biased standard errors",
          "overconfident estimates"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Impossible Imputation Values": {
      "type": "error_type",
      "domain": "Imputation Issues",
      "definition": "Imputed values that are logically or physically impossible within the domain of the variable.",
      "description": "Examples include negative ozone values or impossible categorical combinations.",
      "properties": {
        "causes": [
          "regression imputation",
          "improper model constraints"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Within-Imputation Variance": {
      "type": "statistical_measure",
      "domain": "Multiple Imputation",
      "definition": "The component of variance resulting from sampling uncertainty within each imputed dataset.",
      "description": "Represents traditional sampling variance from standard complete-data analyses.",
      "properties": {
        "symbol": "Ū"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Between-Imputation Variance": {
      "type": "statistical_measure",
      "domain": "Multiple Imputation",
      "definition": "The variance arising from differences among the multiple imputed datasets.",
      "description": "Reflects uncertainty about the correct value to impute.",
      "properties": {
        "symbol": "B"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Total Variance in Multiple Imputation": {
      "type": "statistical_measure",
      "domain": "Multiple Imputation",
      "definition": "The combined variance estimate incorporating both within- and between-imputation variance.",
      "description": "Ensures correct standard errors and confidence intervals under MI.",
      "properties": {
        "formula": "T = Ū + (1 + 1/M) * B"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_04.json"
        ]
      }
    },
    "Attribute Noise": {
      "type": "error_type",
      "domain": "Data Quality",
      "definition": "Noise arising from erroneous, missing, or corrupted feature values in data.",
      "description": "Attribute noise distorts the representation of instances and can weaken classifiers by shifting feature distributions.",
      "properties": {
        "examples": [
          "typos",
          "invalid numeric values",
          "sensor failures"
        ],
        "effects": [
          "class boundary distortion",
          "small erroneous clusters"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Label Noise": {
      "type": "error_type",
      "domain": "Data Quality",
      "definition": "Noise caused by incorrect, inconsistent, or subjective class labels.",
      "description": "Label noise is particularly harmful for supervised learning, as it directly corrupts ground truth.",
      "properties": {
        "sources": [
          "human mistakes",
          "ambiguous labeling",
          "subjective judgments"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Noise Frequency Filtering": {
      "type": "method",
      "domain": "Signal Processing",
      "definition": "The removal of specific high- or low-frequency noise components from continuous signals.",
      "description": "Includes removing characteristic electrical interference, such as 50 Hz noise from power lines.",
      "properties": {
        "examples": [
          "notch filter",
          "band-pass filter"
        ],
        "risks": [
          "loss of useful information"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Equipment Error": {
      "type": "error_type",
      "domain": "Data Quality",
      "definition": "Noise or erroneous values caused by malfunctioning or poorly calibrated measurement devices.",
      "description": "Common in sensor-based data collection, producing sudden spikes or unrealistic readings.",
      "properties": {
        "causes": [
          "device failure",
          "poor calibration"
        ],
        "effects": [
          "outliers",
          "attribute noise"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Garbage Values": {
      "type": "error_type",
      "domain": "Data Pollution",
      "definition": "Invalid or meaningless values inserted into data fields due to errors, incompatibilities, or manual mistakes.",
      "description": "Garbage values may originate from free text fields, broken encodings, or copy-paste errors.",
      "properties": {
        "examples": [
          "nonsense strings",
          "unexpected tokens"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Delimiter Contamination": {
      "type": "error_type",
      "domain": "Data Pollution",
      "definition": "A data corruption issue where field values include delimiter characters, causing misalignment in structured formats.",
      "description": "Occurs in CSV or TSV files when text fields contain commas, tabs, or semicolons.",
      "properties": {
        "effects": [
          "row fragmentation",
          "column shifts"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Free-Text Field Contamination": {
      "type": "error_type",
      "domain": "Data Pollution",
      "definition": "Contamination caused by unstructured free-text fields containing unexpected patterns that break parsing.",
      "description": "Includes unescaped punctuation, symbols, or embedded delimiters.",
      "properties": {
        "sources": [
          "manual entry",
          "copy-paste"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Thousand-Separator Contamination": {
      "type": "error_type",
      "domain": "Data Pollution",
      "definition": "The presence of thousands separators in numeric fields causing parsing issues or incorrect numerical conversion.",
      "description": "Common when merging data across locales with different numeric conventions.",
      "properties": {
        "examples": [
          "1,000.50",
          "1.000,50"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Domain Drift": {
      "type": "phenomenon",
      "domain": "Data Quality",
      "definition": "A mismatch between the original data schema design and current data usage, leading to invalid category values.",
      "description": "Occurs when systems are repurposed, such as a 'Gender' field receiving business-related labels.",
      "properties": {
        "effects": [
          "category misalignment",
          "data pollution"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Global Outliers": {
      "type": "outlier_type",
      "domain": "Outlier Detection",
      "definition": "Data points that deviate significantly from the overall distribution of the dataset.",
      "description": "Most classical outlier detection methods target global outliers.",
      "properties": {
        "examples": [
          "extreme sensor readings"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Contextual Outliers": {
      "type": "outlier_type",
      "domain": "Outlier Detection",
      "definition": "Values that are only outliers within a specific contextual setting or subset of conditions.",
      "description": "A temperature of +25°C is an outlier in Oulu in January but not in July.",
      "properties": {
        "requires": [
          "contextual attributes"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Collective Outliers": {
      "type": "outlier_type",
      "domain": "Outlier Detection",
      "definition": "Groups of observations that together deviate significantly from the dataset’s normal behavior.",
      "description": "Individually normal points form an abnormal cluster, such as suspicious block transactions.",
      "properties": {
        "examples": [
          "market manipulation patterns"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Outlier Score": {
      "type": "metric",
      "domain": "Outlier Detection",
      "definition": "A continuous measure estimating how strongly an instance differs from expected normal behavior.",
      "description": "Used by many unsupervised and distance-based outlier detection models.",
      "properties": {
        "scale": "continuous"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Binary Outlier Classification": {
      "type": "metric",
      "domain": "Outlier Detection",
      "definition": "A binary decision mechanism indicating whether an instance is labeled as outlier or normal.",
      "description": "Often derived from thresholding outlier scores.",
      "properties": {
        "outputs": [
          "outlier",
          "non-outlier"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Subjective Outlier Thresholding": {
      "type": "method",
      "domain": "Outlier Detection",
      "definition": "Human-defined thresholding used to determine whether deviation magnitude is sufficient to consider a point an outlier.",
      "description": "Cutoff values depend on domain knowledge and application-specific tolerances.",
      "properties": {
        "difficulty": "non-universal thresholds"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Supervised Outlier Detection": {
      "type": "method",
      "domain": "Outlier Detection",
      "definition": "Outlier detection using labeled datasets, typically training classifiers to distinguish normal and abnormal instances.",
      "description": "Challenges include extreme class imbalance and insufficient outlier examples.",
      "properties": {
        "models": [
          "binary classifier",
          "one-class models"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Semi-Supervised Outlier Detection": {
      "type": "method",
      "domain": "Outlier Detection",
      "definition": "A hybrid outlier detection method using both labeled and unlabeled data.",
      "description": "Most useful for modeling normal behavior rather than rare anomalies.",
      "properties": {
        "strength": "leverages unlabeled data"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Unsupervised Outlier Detection": {
      "type": "method",
      "domain": "Outlier Detection",
      "definition": "Outlier detection assuming that normal instances form clusters, while outliers lie far from these clusters.",
      "description": "Useful when labels are unavailable; many clustering and density-based techniques apply.",
      "properties": {
        "assumptions": [
          "normal data clustered"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Parametric Outlier Detection": {
      "type": "method",
      "domain": "Statistical Outlier Detection",
      "definition": "Outlier detection assuming that normal data follow a known parametric distribution.",
      "description": "Includes Gaussian modeling, where low-probability instances are flagged as outliers.",
      "properties": {
        "examples": [
          "Gaussian distribution"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Nonparametric Outlier Detection": {
      "type": "method",
      "domain": "Statistical Outlier Detection",
      "definition": "Outlier detection without assuming any prior data distribution, relying instead on data-driven density estimation.",
      "description": "Often uses kernel density estimation to identify low-density regions.",
      "properties": {
        "examples": [
          "kernel density estimation"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Gaussian Outlier Modeling": {
      "type": "method",
      "domain": "Statistical Outlier Detection",
      "definition": "A parametric approach modeling normal data using a Gaussian distribution and identifying low-likelihood instances as outliers.",
      "description": "Effective when data approximately follow normal distributions.",
      "properties": {
        "requirements": [
          "mean and covariance estimation"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Kernel Density Estimation for Outlier Detection": {
      "type": "method",
      "domain": "Statistical Outlier Detection",
      "definition": "Outlier detection using a nonparametric density estimate of the normal data distribution.",
      "description": "Instances in low-density areas under the estimated kernel distribution are considered outliers.",
      "properties": {
        "techniques": [
          "Gaussian kernels",
          "Epanechnikov kernels"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Distance-Based Outlier Detection": {
      "type": "method",
      "domain": "Proximity-Based Methods",
      "definition": "Outlier detection based on distances to neighbors using global parameters such as radius and neighbor threshold.",
      "description": "Points with too few neighbors within a specified distance are flagged as outliers.",
      "properties": {
        "parameters": [
          "radius r",
          "threshold t"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Density-Based Outlier Detection": {
      "type": "method",
      "domain": "Proximity-Based Methods",
      "definition": "Outlier detection comparing local density of a point to the densities of its neighbors.",
      "description": "Low-density points relative to nearby points are considered outliers; LOF is a common example.",
      "properties": {
        "examples": [
          "Local Outlier Factor (LOF)"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Clustering-Based Outlier Detection": {
      "type": "method",
      "domain": "Outlier Detection",
      "definition": "Outlier detection assuming normal data form large dense clusters and outliers form small, sparse clusters.",
      "description": "Methods include K-Means and Gaussian Mixture Models.",
      "properties": {
        "weakness": [
          "sensitive to noise"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Sparse-Cluster Outliers": {
      "type": "outlier_type",
      "domain": "Clustering-Based Outlier Detection",
      "definition": "Outliers that belong to clusters with very low density or membership count.",
      "description": "Common in clustering-based detection where small clusters are treated as abnormal.",
      "properties": {
        "identification": [
          "cluster size threshold"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Gaussian Mixture Model Outlier Detection": {
      "type": "method",
      "domain": "Clustering-Based Outlier Detection",
      "definition": "Outlier detection using mixture models where low-probability points under the model are flagged as outliers.",
      "description": "Each Gaussian component models a cluster; outliers lie outside high-likelihood areas.",
      "properties": {
        "technique": "Expectation-Maximization for model fitting"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "One-Class Classification": {
      "type": "method",
      "domain": "Classification-Based Outlier Detection",
      "definition": "A classification method modeling only the normal class, treating deviations as outliers.",
      "description": "Widely used for anomaly detection when outliers are scarce or unknown.",
      "properties": {
        "algorithms": [
          "One-Class SVM",
          "Autoencoders"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Outlier Detection via SVM": {
      "type": "method",
      "domain": "Classification-Based Outlier Detection",
      "definition": "Use of Support Vector Machines to identify outliers based on deviation from the learned normal boundary.",
      "description": "Includes both binary SVM approaches and one-class SVMs for anomaly detection.",
      "properties": {
        "advantages": [
          "robust boundaries",
          "works in high dimensions"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Outlier Detection via Random Forest": {
      "type": "method",
      "domain": "Classification-Based Outlier Detection",
      "definition": "Outlier detection using ensemble tree models to identify points poorly represented by learned structure.",
      "description": "Random forests can identify unusual patterns based on path length or leaf rarity.",
      "properties": {
        "mechanism": [
          "rare-leaf detection"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Predictive Outlier Detection": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Outlier detection in streaming data using predictive models such as autoregressive or multivariate regressors.",
      "description": "Points are anomalous when observed values deviate significantly from predictions.",
      "properties": {
        "models": [
          "AR models",
          "regression models"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Streaming Outlier Detection": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Methods for detecting outliers in continuous data streams where time-order and latency constraints apply.",
      "description": "Includes monitoring individual points, shape irregularities, and aggregated change points.",
      "properties": {
        "challenges": [
          "real-time constraints",
          "concept drift"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Unusual-Shape Outlier Detection": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Identification of anomalous temporal patterns based on shape irregularities rather than pointwise deviations.",
      "description": "Useful in detecting unusual motion patterns or abnormal physiological waveforms.",
      "properties": {
        "techniques": [
          "transform-based",
          "distance-based"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Multidimensional Streaming Outlier Detection": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Outlier detection extended to multivariate streaming systems, considering correlations across multiple dimensions.",
      "description": "Detects single-point anomalies or aggregated shifts across sensors or modalities.",
      "properties": {
        "outputs": [
          "point anomalies",
          "change points",
          "rare classes"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Change-Point Outlier Detection": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Detection of abrupt shifts in the mean, variance, or structural properties of a time series.",
      "description": "Change points indicate transitions to abnormal system states.",
      "properties": {
        "effects": [
          "distributional shift",
          "anomaly onset"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Rare-Class Detection in Time Series": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Detection of rare or unusual event types in time-series streams.",
      "description": "Targets low-frequency events that have significant domain relevance, such as faults or intrusions.",
      "properties": {
        "challenges": [
          "class imbalance",
          "concept drift"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_05.json"
        ]
      }
    },
    "Scaling to Arbitrary Range": {
      "type": "method",
      "domain": "Data Transformation",
      "definition": "A normalization technique that linearly maps values to any target interval [a, b].",
      "description": "Generalization of min–max normalization where the target range is not limited to [0, 1], enabling flexible rescaling.",
      "properties": {
        "formula": "x' = (x - min) / (max - min) * (b - a) + a",
        "requirements": [
          "known min",
          "known max"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Storing Normalization Parameters": {
      "type": "process",
      "domain": "Data Transformation",
      "definition": "The practice of saving transformation parameters such as means, standard deviations, and min–max values for use in future data preprocessing.",
      "description": "Essential to avoid data leakage and ensure consistent scaling between training and test sets.",
      "properties": {
        "parameters": [
          "mean",
          "standard deviation",
          "min",
          "max"
        ],
        "purpose": [
          "reproducibility",
          "consistency"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Extreme Z-Score Threshold Rule": {
      "type": "heuristic",
      "domain": "Outlier Detection",
      "definition": "A method for identifying extreme values using high absolute Z-score cutoffs, typically 6–7, instead of the conventional 3.",
      "description": "Used to avoid misclassifying valid but rare signals as outliers, particularly in highly variable datasets.",
      "properties": {
        "thresholds": [
          "|z| > 6",
          "|z| > 7"
        ],
        "purpose": "avoid over-removal of valid extreme values"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Measurement Error Visualization": {
      "type": "method",
      "domain": "Data Quality",
      "definition": "Visualization techniques used to detect measurement errors through plots and distribution analysis.",
      "description": "Includes visual detection of extreme Z-scores, artifacts, or sudden jumps in sensor data.",
      "properties": {
        "tools": [
          "histograms",
          "box plots",
          "time-series plots"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Median Split": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A discretization method that divides a continuous variable into two groups using the median value as the threshold.",
      "description": "Commonly used for creating binary categories but may lead to information loss.",
      "properties": {
        "output": "two categories",
        "risk": [
          "loss of variance",
          "misclassification"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Distinctive Grouping": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A discretization method that forms groups based on meaningful or domain-specific distinctions rather than uniform intervals.",
      "description": "Useful when natural categories exist, such as age groups or educational stages.",
      "properties": {
        "basis": [
          "domain knowledge",
          "semantic categories"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Equal-Length Binning": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A discretization method dividing the range of a variable into intervals of equal width.",
      "description": "Bin boundaries are uniformly spaced, which may result in uneven sample counts per bin in skewed distributions.",
      "properties": {
        "bin_type": "fixed-width",
        "risks": [
          "unbalanced bins"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Equal-Size Binning": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A discretization method dividing data into bins containing approximately equal numbers of observations.",
      "description": "Also known as equal-frequency binning; adjusts bin boundaries based on data distribution.",
      "properties": {
        "bin_type": "equal-frequency",
        "risks": [
          "variable bin widths"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Natural Cut Points": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A discretization approach that chooses cut points based on evident gaps or changes in the data distribution.",
      "description": "Often identified visually or via distribution-based heuristics.",
      "properties": {
        "identification": [
          "visual inspection",
          "distribution jumps"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Concept Hierarchies": {
      "type": "structure",
      "domain": "Discretization",
      "definition": "Hierarchical structures that represent data from low-level granular categories to increasingly general abstractions.",
      "description": "Used for discretization and data generalization, enabling multi-level analysis.",
      "properties": {
        "levels": [
          "fine-grained",
          "intermediate",
          "coarse"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Dichotomization": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A method that converts a continuous variable into two groups based on a threshold.",
      "description": "Simplifies analysis but leads to substantial information loss.",
      "properties": {
        "risks": [
          "information loss",
          "increased misclassification"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Problems of Discretization": {
      "type": "concept",
      "domain": "Discretization",
      "definition": "The set of issues created when transforming continuous variables into discrete categories.",
      "description": "Includes information loss, artificial boundaries, and increased risk of misclassification.",
      "properties": {
        "effects": [
          "reduced statistical power",
          "loss of granularity"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Information Loss Through Dichotomization": {
      "type": "phenomenon",
      "domain": "Discretization",
      "definition": "The reduction of statistical information caused by collapsing continuous variables into binary categories.",
      "description": "Leads to loss of variability and weaker associations.",
      "properties": {
        "causes": [
          "thresholding",
          "binary splitting"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Misclassification Risk at Cut Points": {
      "type": "phenomenon",
      "domain": "Discretization",
      "definition": "The risk of mislabeling cases that fall near bin boundaries when discretizing continuous variables.",
      "description": "Boundary decisions may not reflect underlying data continuity.",
      "properties": {
        "causes": [
          "abrupt thresholds"
        ],
        "effects": [
          "noisy categorical labels"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Factor Variables": {
      "type": "datatype",
      "domain": "Categorical Data",
      "definition": "Variables that represent categories encoded as discrete levels rather than numeric values.",
      "description": "Used in statistical modeling to encode nominal or ordinal categories.",
      "properties": {
        "levels": "categorical labels"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Frequency Tables": {
      "type": "method",
      "domain": "Categorical Data",
      "definition": "Tabular summaries of categorical variables showing counts per category.",
      "description": "Useful for summarizing distributions of factor variables.",
      "properties": {
        "outputs": [
          "counts",
          "proportions"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Correspondence Tests": {
      "type": "method",
      "domain": "Categorical Data",
      "definition": "Statistical tests evaluating the association between two categorical variables.",
      "description": "Includes chi-square–type methods applicable to contingency tables.",
      "properties": {
        "inputs": [
          "contingency table"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Categorical vs Continuous Visualization": {
      "type": "method",
      "domain": "Categorical Data",
      "definition": "Visualization techniques that compare categorical groups against continuous measurements.",
      "description": "Examples include grouped boxplots for continuous outcomes across category levels.",
      "properties": {
        "tools": [
          "boxplots",
          "violin plots"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Data Aggregation": {
      "type": "method",
      "domain": "Data Reduction",
      "definition": "A data reduction method that summarizes groups of data points into aggregate values.",
      "description": "Common aggregates include mean, sum, and group-level statistics.",
      "properties": {
        "aggregates": [
          "mean",
          "sum",
          "count"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Data Generalization": {
      "type": "method",
      "domain": "Data Reduction",
      "definition": "A reduction method where data is replaced by more general forms using hierarchical abstraction.",
      "description": "Often implemented using concept hierarchies to reduce granularity.",
      "properties": {
        "approach": [
          "hierarchical abstraction"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Variable Construction": {
      "type": "process",
      "domain": "Data Reduction",
      "definition": "Constructing new variables from existing ones to improve predictive performance or simplify data representation.",
      "description": "Includes building composite indicators or derived attributes.",
      "properties": {
        "examples": [
          "interaction terms",
          "index scores"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Nonparametric Data Reduction Methods": {
      "type": "method",
      "domain": "Data Reduction",
      "definition": "Reduction methods that do not assume specific parametric forms and rely on data-driven transformations.",
      "description": "Useful when normality or linearity assumptions do not hold.",
      "properties": {
        "examples": [
          "binning",
          "wavelet transforms"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Transformation to Normality": {
      "type": "method",
      "domain": "Data Transformation",
      "definition": "Techniques aimed at making a variable's distribution more Gaussian.",
      "description": "Used to satisfy model assumptions; includes Box–Cox and other power transformations.",
      "properties": {
        "goal": "approximate normality"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Normality Tests": {
      "type": "method",
      "domain": "Statistical Diagnostics",
      "definition": "Statistical methods used to evaluate whether data follow a normal distribution.",
      "description": "Although not named explicitly, includes families of tests such as Shapiro–Wilk and KS tests.",
      "properties": {
        "outputs": [
          "p-value",
          "decision on normality"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Solutions for Non-Normality": {
      "type": "strategy",
      "domain": "Statistical Diagnostics",
      "definition": "Approaches to address non-normal data, including transformations or use of nonparametric models.",
      "description": "Ensures suitability of statistical models when assumptions fail.",
      "properties": {
        "strategies": [
          "transform variables",
          "apply nonparametric models"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_06.json"
        ]
      }
    },
    "Generalizability": {
      "type": "concept",
      "domain": "Model Evaluation",
      "definition": "The degree to which a model's findings or predictions apply beyond the data on which it was trained.",
      "description": "Generalizability reflects how well a model captures patterns that extend to new individuals, settings, or datasets.",
      "properties": {
        "aspects": [
          "population generalizability",
          "dataset generalizability"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Generalization Error": {
      "type": "metric",
      "domain": "Model Evaluation",
      "definition": "The expected error of a model when applied to new, unseen data.",
      "description": "It captures the discrepancy between training performance and real-world prediction accuracy.",
      "properties": {
        "components": [
          "bias",
          "variance"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Generalization Gap": {
      "type": "metric",
      "domain": "Model Evaluation",
      "definition": "The difference between training error and test error for a model.",
      "description": "A large gap indicates overfitting, while a small gap suggests good generalizability.",
      "properties": {
        "formula": "generalization_gap = test_error - training_error"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Dataset Generalization": {
      "type": "concept",
      "domain": "Model Evaluation",
      "definition": "The ability of a model to perform well on datasets collected under conditions different from the training dataset.",
      "description": "Important for real-world deployment, where data collection conditions vary.",
      "properties": {
        "sources_of_variation": [
          "different populations",
          "different sensors",
          "different contexts"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Retrospective Models": {
      "type": "model_type",
      "domain": "Descriptive Modeling",
      "definition": "Models that analyze past data to understand patterns, associations, or structures.",
      "description": "Used for insight generation and explanation rather than future prediction.",
      "properties": {
        "primary_goal": "understanding past phenomena"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Prospective Models": {
      "type": "model_type",
      "domain": "Predictive Modeling",
      "definition": "Models designed to predict future outcomes based on current or historical data.",
      "description": "Used in forecasting, risk assessment, and decision automation.",
      "properties": {
        "primary_goal": "predicting future events"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Theory-Driven Models": {
      "type": "model_type",
      "domain": "Modeling Approaches",
      "definition": "Models constructed primarily from theoretical principles rather than empirical patterns.",
      "description": "Often focuses on causal explanation and interpretable structures.",
      "properties": {
        "advantages": [
          "interpretability",
          "causal validity"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Data-Driven Models": {
      "type": "model_type",
      "domain": "Modeling Approaches",
      "definition": "Models built primarily from observed data, with minimal theoretical assumptions.",
      "description": "Typical in machine learning, where predictive accuracy is prioritized.",
      "properties": {
        "advantages": [
          "high predictive power"
        ],
        "risks": [
          "overfitting"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Explanatory Power": {
      "type": "property",
      "domain": "Model Interpretation",
      "definition": "The ability of a model to explain underlying relationships and mechanisms within data.",
      "description": "Prioritized in scientific modeling where interpretability and causal understanding are necessary.",
      "properties": {
        "factors": [
          "model structure",
          "variable relationships"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Predictive Power": {
      "type": "property",
      "domain": "Model Performance",
      "definition": "The ability of a model to accurately predict new observations.",
      "description": "Often maximized using data-driven machine learning approaches.",
      "properties": {
        "metrics": [
          "RMSE",
          "accuracy",
          "AUC"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Minimum Sample Size for Statistical Inference": {
      "type": "requirement",
      "domain": "Sampling",
      "definition": "The smallest number of observations needed to perform statistically valid inference.",
      "description": "Depends on model complexity, effect sizes, and desired confidence levels.",
      "properties": {
        "influenced_by": [
          "variance",
          "population size",
          "test type"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Sample Size for Predictive Modeling": {
      "type": "requirement",
      "domain": "Sampling",
      "definition": "The amount of data required for building predictive models that generalize well.",
      "description": "Larger sample sizes reduce variance and improve generalization.",
      "properties": {
        "depends_on": [
          "model complexity",
          "noise level",
          "feature dimensionality"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Precision of Parameter Estimation": {
      "type": "metric",
      "domain": "Statistical Inference",
      "definition": "The degree of certainty in estimated model parameters.",
      "description": "Higher sample sizes typically increase precision and reduce standard errors.",
      "properties": {
        "improved_by": [
          "larger datasets"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Population Representativeness": {
      "type": "concept",
      "domain": "Sampling",
      "definition": "The extent to which a sample reflects the target population.",
      "description": "Critical for valid generalization of study results.",
      "properties": {
        "risks_of_failure": [
          "sampling bias",
          "non-response bias"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Access to Data": {
      "type": "constraint",
      "domain": "Data Collection",
      "definition": "The availability of suitable and sufficient data for training and evaluating models.",
      "description": "Practical and ethical challenges may restrict data collection and usage.",
      "properties": {
        "limitations": [
          "privacy",
          "cost",
          "logistics"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Training Set": {
      "type": "data_partition",
      "domain": "Model Evaluation",
      "definition": "The subset of data used to estimate model parameters.",
      "description": "Forms the basis of the model's learned patterns.",
      "properties": {
        "purpose": [
          "parameter estimation"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Test Set": {
      "type": "data_partition",
      "domain": "Model Evaluation",
      "definition": "The subset of data reserved for evaluating a model's predictive performance.",
      "description": "Used only after model training and selection.",
      "properties": {
        "purpose": [
          "generalization assessment"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Hold-Out Method": {
      "type": "method",
      "domain": "Model Evaluation",
      "definition": "A data partitioning approach where the dataset is split into training and testing subsets.",
      "description": "Common simple method for estimating predictive performance.",
      "properties": {
        "common_splits": [
          "80-20",
          "2/3-1/3"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Eighty-Twenty Split": {
      "type": "data_partition",
      "domain": "Model Evaluation",
      "definition": "A partitioning approach where 80% of data is used for training and 20% for testing.",
      "description": "A common default in machine learning experiments.",
      "properties": {
        "ratio": "80-20"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Two-Thirds One-Third Split": {
      "type": "data_partition",
      "domain": "Model Evaluation",
      "definition": "A data partitioning scheme where two-thirds of the data is used for training and one-third for testing.",
      "description": "Alternative to the 80-20 split.",
      "properties": {
        "ratio": "2/3-1/3"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Training-Validation-Test Split": {
      "type": "data_partition",
      "domain": "Model Evaluation",
      "definition": "A three-way data split used to train, tune, and evaluate machine learning models.",
      "description": "Enables both hyperparameter tuning and final unbiased model evaluation.",
      "properties": {
        "components": [
          "training",
          "validation",
          "test"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Participant-Wise Cross-Validation": {
      "type": "method",
      "domain": "Model Evaluation",
      "definition": "A cross-validation technique where data from each participant forms a separate fold.",
      "description": "Used when data from the same participant is highly correlated.",
      "properties": {
        "use_case": [
          "sensor-based human activity recognition"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Leave-One-Out Validation": {
      "type": "method",
      "domain": "Model Evaluation",
      "definition": "A cross-validation technique where each instance is used as its own test set.",
      "description": "Computationally expensive but maximally uses available data.",
      "properties": {
        "folds": "n"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Temporal Dependency in Data": {
      "type": "phenomenon",
      "domain": "Time-Series Data",
      "definition": "The property that observations close in time are correlated rather than independent.",
      "description": "Invalidates random sampling and certain cross-validation methods.",
      "properties": {
        "causes": [
          "sliding windows",
          "sensor signals"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "No Random Sampling Under Temporal Dependence": {
      "type": "constraint",
      "domain": "Time-Series Data",
      "definition": "A restriction that prevents using random train-test splits when data points are temporally correlated.",
      "description": "Random splits cause leakage because training and test sets contain nearly identical patterns.",
      "properties": {
        "effects": [
          "data leakage",
          "inflated accuracy"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Day-Based Partitioning": {
      "type": "method",
      "domain": "Time-Series Data",
      "definition": "A data-splitting strategy where entire days are used as units to avoid temporal leakage.",
      "description": "Ensures that temporally adjacent observations are not split across training and test sets.",
      "properties": {
        "unit": "day"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Temporal Leakage": {
      "type": "error_type",
      "domain": "Model Evaluation",
      "definition": "Inadvertent inclusion of temporally correlated information in both training and test sets.",
      "description": "Causes overestimation of model performance.",
      "properties": {
        "sources": [
          "random splitting",
          "overlapping windows"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Sliding Windows": {
      "type": "method",
      "domain": "Feature Extraction",
      "definition": "A method for extracting features from sequential data by moving a fixed-size window across time.",
      "description": "Used in time-series classification and signal processing.",
      "properties": {
        "parameters": [
          "window size",
          "step size"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Overlapping Windows": {
      "type": "method",
      "domain": "Feature Extraction",
      "definition": "Sliding windows that partially overlap to create highly correlated feature vectors.",
      "description": "Leads to temporal dependency between training instances.",
      "properties": {
        "effect": "induces correlation"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Window-Based Feature Extraction": {
      "type": "method",
      "domain": "Feature Extraction",
      "definition": "Feature extraction performed on short, fixed-length windows of time-series data.",
      "description": "Generates feature vectors for classification tasks such as activity recognition.",
      "properties": {
        "common_features": [
          "mean",
          "variance",
          "energy"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Individual Models": {
      "type": "model_type",
      "domain": "Modeling Approaches",
      "definition": "Models trained on data from a single individual for personalized prediction.",
      "description": "Performs well on the specific individual but generalizes poorly to others.",
      "properties": {
        "strength": "high personalization",
        "weakness": "poor population generalization"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Population Models": {
      "type": "model_type",
      "domain": "Modeling Approaches",
      "definition": "Models trained on data from multiple individuals to generalize across users.",
      "description": "More robust in diverse real-world applications.",
      "properties": {
        "strength": "better generalization"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Activity Recognition": {
      "type": "application",
      "domain": "Machine Learning Applications",
      "definition": "The task of identifying human activities from sensor or contextual data.",
      "description": "Commonly involves using wearable sensor data like accelerometers and temperature sensors.",
      "properties": {
        "typical_inputs": [
          "accelerometer",
          "gyroscope",
          "temperature"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Sensor-Based Activity Detection": {
      "type": "application",
      "domain": "Machine Learning Applications",
      "definition": "Detecting human activities using sensor data collected from wearable or ambient devices.",
      "description": "Depends heavily on feature extraction and handling temporal correlations.",
      "properties": {
        "sensors": [
          "accelerometers",
          "temperature sensors"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Multi-Modal Feature Inputs": {
      "type": "concept",
      "domain": "Feature Engineering",
      "definition": "Combining features from multiple sensor modalities for improved model performance.",
      "description": "For example, integrating accelerometer features with body temperature signals.",
      "properties": {
        "modalities": [
          "accelerometer",
          "temperature"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Model Complexity–Generalization Tradeoff": {
      "type": "concept",
      "domain": "Model Selection",
      "definition": "The tradeoff in which more complex models fit training data better but often generalize worse.",
      "description": "Central to selecting models that balance expressiveness with robustness.",
      "properties": {
        "risk": "overfitting"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Simpler Models Generalize Better": {
      "type": "principle",
      "domain": "Model Selection",
      "definition": "The principle that models with lower complexity often perform better on new data.",
      "description": "Reflects Occam's razor in predictive modeling.",
      "properties": {
        "benefits": [
          "robustness",
          "lower variance"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Overcomplex Models": {
      "type": "model_issue",
      "domain": "Model Selection",
      "definition": "Models that are excessively flexible and overfit training data.",
      "description": "Characterized by small training error and large generalization error.",
      "properties": {
        "symptom": "large generalization gap"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Hyperparameter Selection": {
      "type": "process",
      "domain": "Model Selection",
      "definition": "The process of choosing optimal hyperparameters for a learning algorithm.",
      "description": "Typically done using validation sets or cross-validation.",
      "properties": {
        "examples": [
          "regularization strength",
          "learning rate",
          "tree depth"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Feature Selection": {
      "type": "process",
      "domain": "Model Selection",
      "definition": "The process of identifying the most relevant features for a predictive model.",
      "description": "Reduces overfitting, improves generalization, and simplifies models.",
      "properties": {
        "methods": [
          "filter methods",
          "wrapper methods",
          "embedded methods"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_07.json"
        ]
      }
    },
    "Machine Learning": {
      "type": "field",
      "domain": "Machine Learning",
      "definition": "A field of study and practice focused on algorithms that improve performance at tasks through experience (data).",
      "description": "Encompasses supervised, unsupervised, and reinforcement approaches and emphasizes predictive performance and automation.",
      "properties": {
        "subfields": [
          "Supervised Learning",
          "Unsupervised Learning",
          "Deep Learning"
        ],
        "typical_outputs": [
          "classifiers",
          "regressors",
          "clusterings"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Machine Learning Models": {
      "type": "category",
      "domain": "Machine Learning",
      "definition": "Parametric or non-parametric algorithm instantiations (models) learned from data to perform tasks such as classification or regression.",
      "description": "Examples include decision trees, SVMs, neural networks, ensemble models and one-class models used for anomaly detection.",
      "properties": {
        "examples": [
          "SVM",
          "Random Forest",
          "Neural Network",
          "One-Class Classification"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Supervised Learning": {
      "type": "method_category",
      "domain": "Machine Learning",
      "definition": "Learning where models are trained on input–output pairs (X, Y) to predict outputs from new inputs.",
      "description": "Requires labelled data and is sensitive to label noise and class imbalance.",
      "properties": {
        "common_algorithms": [
          "Logistic Regression",
          "SVM",
          "Random Forest",
          "Neural Networks"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Avoiding Overfitting": {
      "type": "practice",
      "domain": "Modeling Practice",
      "definition": "Techniques and design choices intended to reduce overfitting and improve model generalization.",
      "description": "Includes cross-validation, regularization, simpler models, early stopping, and appropriate data partitioning.",
      "properties": {
        "techniques": [
          "cross-validation",
          "regularization",
          "feature selection"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Bias-Variance Tradeoff": {
      "type": "concept",
      "domain": "Modeling Theory",
      "definition": "A conceptual decomposition of prediction error into bias (systematic error) and variance (estimation variability).",
      "description": "Guides model complexity choices: higher complexity reduces bias but increases variance and vice versa.",
      "properties": {
        "components": [
          "bias",
          "variance",
          "irreducible error"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Regularization": {
      "type": "method",
      "domain": "Modeling Practice",
      "definition": "Techniques that penalize model complexity during training to prevent overfitting (e.g., L1/L2 penalties, dropout).",
      "description": "Reduces variance at the cost of potentially increasing bias, improving generalization when tuned properly.",
      "properties": {
        "examples": [
          "L1 (lasso)",
          "L2 (ridge)",
          "dropout",
          "early stopping"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Model Bias": {
      "type": "issue",
      "domain": "Modeling",
      "definition": "Systematic errors in model outputs caused by incorrect model assumptions, biased training data, or simplistic model structures.",
      "description": "Closely related to societal bias: biased training data or labels can propagate harmful outcomes.",
      "properties": {
        "sources": [
          "sampling bias",
          "label bias",
          "feature omission"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Accuracy": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "Proportion of correct predictions over all predictions for classification tasks.",
      "description": "Simple metric but can be misleading with imbalanced classes (see Balanced Accuracy, F1 Score).",
      "properties": {
        "formula": "accuracy = (TP + TN) / (TP + TN + FP + FN)"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Training Error": {
      "type": "metric",
      "domain": "Model Evaluation",
      "definition": "Error measured on the training dataset after model fitting.",
      "description": "Used together with test error to compute the generalization gap; overly low training error may signal overfitting.",
      "properties": {
        "used_for": [
          "monitoring fit",
          "early stopping"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Statistical Methods": {
      "type": "category",
      "domain": "Statistics",
      "definition": "Formal techniques for describing data and drawing inferences about populations from samples.",
      "description": "Encompasses hypothesis testing, estimation, regression, likelihood methods and nonparametric alternatives.",
      "properties": {
        "families": [
          "parametric",
          "nonparametric",
          "Bayesian",
          "frequentist"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Specialized Statistical Modeling": {
      "type": "category",
      "domain": "Statistics",
      "definition": "Domain-specific statistical models (e.g., survival models, mixed-effects models, time-series models) used when standard models are inadequate.",
      "description": "Often required for hierarchical data, longitudinal measurements, or nonstandard error structures.",
      "properties": {
        "examples": [
          "mixed-effects",
          "survival analysis",
          "state-space models"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Regression Modeling": {
      "type": "method",
      "domain": "Statistics",
      "definition": "Models that estimate relationships between dependent variables and one or more explanatory variables (linear and nonlinear forms).",
      "description": "Includes linear regression, generalized linear models, and specialized regression variants for complex data.",
      "properties": {
        "variants": [
          "linear",
          "logistic",
          "Poisson",
          "survival"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Causal Hypothesis": {
      "type": "concept",
      "domain": "Inference",
      "definition": "A proposed causal relationship between variables that is typically tested using explanatory models and experimental design.",
      "description": "Causal claims require careful design, confounding control and sometimes randomized interventions.",
      "properties": {
        "requires": [
          "counterfactual reasoning",
          "control of confounders"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Statistical Power": {
      "type": "metric",
      "domain": "Statistics",
      "definition": "The probability that a test will correctly reject a false null hypothesis (1 - type II error rate).",
      "description": "Determines required sample sizes for hypothesis-driven (explanatory) studies.",
      "properties": {
        "depends_on": [
          "effect_size",
          "sample_size",
          "alpha"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Statistical Significance": {
      "type": "concept",
      "domain": "Statistics",
      "definition": "A conclusion that an observed effect is unlikely to be due to chance under a specified null model, typically operationalized by p-values.",
      "description": "Statistical significance does not imply practical importance.",
      "properties": {
        "common_thresholds": [
          "p < 0.05",
          "p < 0.01"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Nonlinear PCA": {
      "type": "method",
      "domain": "Dimensionality Reduction",
      "definition": "Extensions of PCA that capture nonlinear relationships (e.g., kernel PCA, autoencoder-based approaches).",
      "description": "Useful when principal directions of variation are curved or manifold-structured.",
      "properties": {
        "examples": [
          "kernel PCA",
          "autoencoder PCA",
          "Isomap variants"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Statistical Outlier Detection": {
      "type": "category",
      "domain": "Outlier Detection",
      "definition": "Methods that identify outliers using statistical modelling — either parametric (distributional) or nonparametric (density estimation).",
      "description": "Serves as a parent for Parametric Outlier Detection and Nonparametric Outlier Detection.",
      "properties": {
        "subtypes": [
          "parametric",
          "nonparametric"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Experimental Design": {
      "type": "field",
      "domain": "Research Methods",
      "definition": "The process of planning experiments to ensure that data collection answers the research question while controlling confounders.",
      "description": "Includes design choices such as RCTs, crossover designs, sample size planning, randomization, and blocking.",
      "properties": {
        "elements": [
          "randomization",
          "control groups",
          "replication",
          "blocking"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Poor Experimental Design": {
      "type": "issue",
      "domain": "Research Methods",
      "definition": "Design flaws that reduce validity, such as lack of controls, small sample sizes, biased sampling, or inadequate blinding.",
      "description": "Leads to biased or non-generalizable results and may render data unusable for inference.",
      "properties": {
        "consequences": [
          "bias",
          "low power",
          "confounding"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Sampling Rate Adjustment": {
      "type": "method",
      "domain": "Sampling",
      "definition": "Practices for harmonizing or adjusting sampling frequencies (e.g., resampling, interpolation, downsampling) when merging data from multiple sources.",
      "description": "Addresses issues documented under Sampling Rate Mismatch and Sampling Synchronization.",
      "properties": {
        "techniques": [
          "interpolation",
          "GCD-based downsampling",
          "LCM-based oversampling"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Data Augmentation": {
      "type": "method",
      "domain": "Data Preprocessing",
      "definition": "Techniques for synthetically increasing dataset size or diversity (e.g., rotations, noise, synthetic samples) to improve model robustness.",
      "description": "Widely used in vision, audio and time-series to reduce overfitting and address class imbalance.",
      "properties": {
        "examples": [
          "noise injection",
          "time-warping",
          "synthetic sample generation"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Research Data Management": {
      "type": "practice",
      "domain": "Data Management",
      "definition": "Policies and processes for storing, documenting, preserving and sharing research data (metadata, provenance, access control).",
      "description": "Enables reproducibility and FAIR compliance; overlaps with Data Documentation and Metadata.",
      "properties": {
        "components": [
          "storage",
          "metadata",
          "access",
          "preservation"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Documentation": {
      "type": "artifact",
      "domain": "Data Management",
      "definition": "Written records describing dataset contents, methods, versioning, provenance, and usage constraints.",
      "description": "Broader than 'Data Documentation'—applies to code, protocols, experiment logs and data pipelines.",
      "properties": {
        "includes": [
          "data dictionaries",
          "readme",
          "protocol descriptions"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Design Relational Database Schemas": {
      "type": "process",
      "domain": "Database Management",
      "definition": "The activity of translating conceptual domain models into normalized relational table structures.",
      "description": "Uses ER modeling and considerations such as keys, constraints and normalization to avoid redundancy.",
      "properties": {
        "outputs": [
          "table definitions",
          "keys",
          "constraints"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Tabular Data": {
      "type": "datatype",
      "domain": "Data Structures",
      "definition": "Data organized in rows and columns (records and fields), typical for spreadsheets and relational database tables.",
      "description": "The primary input format for many classical machine learning and statistical methods.",
      "properties": {
        "representations": [
          "CSV",
          "SQL tables",
          "dataframes"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Data Protection Laws": {
      "type": "category",
      "domain": "Governance",
      "definition": "Legal frameworks that govern the collection, storage and processing of personal data (e.g., GDPR).",
      "description": "Includes regional and sectoral regulations that shape research design and data sharing.",
      "properties": {
        "examples": [
          "GDPR",
          "national data protection acts"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "GO FAIR Initiative": {
      "type": "initiative",
      "domain": "Open Science",
      "definition": "An international initiative promoting the FAIR principles for data (Findable, Accessible, Interoperable, Reusable).",
      "description": "Supports infrastructure, community practices and policies to make research data reusable.",
      "properties": {
        "goals": [
          "promote FAIR",
          "build infrastructure"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Human Research Ethics": {
      "type": "domain",
      "domain": "Ethics",
      "definition": "Ethical principles and regulatory requirements governing research involving human participants.",
      "description": "Encompasses Respect for Persons, Beneficence, Justice, informed consent, confidentiality, and protections for vulnerable groups.",
      "properties": {
        "components": [
          "informed consent",
          "risk assessment",
          "ethical approval"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Ethical Approval": {
      "type": "process",
      "domain": "Ethics",
      "definition": "The institutional review and permission process required before conducting research involving human participants.",
      "description": "Evaluates risk, consent procedures, data protection and participant recruitment strategies.",
      "properties": {
        "outputs": [
          "ethics approval letter",
          "conditions"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Privacy Preservation": {
      "type": "concept",
      "domain": "Governance",
      "definition": "Technical and policy measures intended to protect personal data and prevent re-identification.",
      "description": "Includes de-identification, k-anonymity, pseudonymisation and access controls.",
      "properties": {
        "techniques": [
          "k-Anonymity",
          "Pseudonymisation",
          "access controls"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Time Series Analysis": {
      "type": "field",
      "domain": "Time Series",
      "definition": "Methods for modelling, forecasting and understanding temporally ordered data and their dynamics.",
      "description": "Includes ARIMA, state-space models, spectral analysis and change-point detection.",
      "properties": {
        "tasks": [
          "forecasting",
          "anomaly detection",
          "seasonal decomposition"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Streaming Data Analysis": {
      "type": "field",
      "domain": "Time Series",
      "definition": "Techniques for real-time ingestion, processing and analysis of continuous data streams.",
      "description": "Requires low-latency algorithms and often online/incremental learning approaches.",
      "properties": {
        "challenges": [
          "concept drift",
          "throughput",
          "latency"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Time-Series Missing Data Handling": {
      "type": "category",
      "domain": "Missing Data",
      "definition": "Specialized methods for imputing or otherwise handling missingness in temporally-correlated data (e.g., LOCF, interpolation, state-space approaches).",
      "description": "Must account for autocorrelation; naive imputation can distort temporal dependencies.",
      "properties": {
        "techniques": [
          "LOCF",
          "BOCF",
          "interpolation",
          "state-space imputation"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Temporal Alignment": {
      "type": "process",
      "domain": "Time Series",
      "definition": "Aligning time stamps and synchronizing signals from different sources to a common temporal reference.",
      "description": "Includes timestamp synchronization, resampling, and offset correction.",
      "properties": {
        "methods": [
          "clock alignment",
          "interpolation",
          "offset correction"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Data Completeness": {
      "type": "quality_metric",
      "domain": "Data Quality",
      "definition": "The degree to which required data are present in a dataset (low missingness and full records).",
      "description": "Impacts analytic choices and validity; related to dropout rate and missing-data mechanisms.",
      "properties": {
        "measures": [
          "proportion complete records",
          "feature-level completeness"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Scaling": {
      "type": "operation",
      "domain": "Data Transformation",
      "definition": "Generic term for rescaling numeric variables to comparable ranges, including min-max, standardization and arbitrary-range mapping.",
      "description": "Completed nodes include Min-Max Normalization and Z-score Standardization; this node links to those concrete methods.",
      "properties": {
        "examples": [
          "Min-Max Normalization",
          "Z-score Standardization",
          "Scaling to Arbitrary Range"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Sensor Calibration": {
      "type": "process",
      "domain": "Data Quality",
      "definition": "Procedures to ensure sensor readings are accurate and comparable by adjusting for device-specific biases or drifts.",
      "description": "Reduces equipment error and calibration differences across devices.",
      "properties": {
        "outcomes": [
          "reduced bias",
          "improved comparability"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Signal Quality": {
      "type": "quality_metric",
      "domain": "Data Quality",
      "definition": "A measure of how usable a signal is for analysis, considering noise, saturation, dropouts and artifacts.",
      "description": "High signal quality facilitates reliable feature extraction and detection tasks.",
      "properties": {
        "components": [
          "SNR",
          "dropout_rate",
          "saturation_rate"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "0_extra_08.json"
        ]
      }
    },
    "Data Mining": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The process of finding previously unknown and potentially interesting patterns and relationships in large databases through statistical and algorithmic methods.",
      "description": "Data mining automates the extraction of knowledge from data and involves descriptive, predictive, and explanatory modeling. It simplifies and automates the statistical process from data sources to model application, supporting manual and automated methods.",
      "properties": {
        "Goal": "Discover patterns, relationships, and insights from large datasets.",
        "Applications": [
          "Customer segmentation",
          "Fraud detection",
          "Recommendation systems"
        ],
        "Methods": [
          "Classification",
          "Clustering",
          "Regression",
          "Association analysis"
        ],
        "Challenges": [
          "Data quality",
          "Scalability",
          "Interpretability"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "1_data.json"
        ]
      }
    },
    "Knowledge Discovery in Databases": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The overall process of discovering useful knowledge from data, including data selection, cleaning, transformation, mining, and interpretation.",
      "description": "KDD integrates data mining as a core step and emphasizes interpretation, prior knowledge, and iterative improvement. It transforms raw data into actionable knowledge through systematic analysis.",
      "properties": {
        "Steps": [
          "Data selection",
          "Data cleaning",
          "Data transformation",
          "Data mining",
          "Pattern evaluation",
          "Knowledge representation"
        ],
        "Applications": [
          "Scientific discovery",
          "Business intelligence",
          "Engineering analytics"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "1_data.json"
        ]
      }
    },
    "Data Pre-processing": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The step of preparing raw data for mining by cleaning, transforming, integrating, and reducing it to improve quality and usability.",
      "description": "Data pre-processing deals with incomplete, noisy, or inconsistent data, improving accuracy and ensuring that models are built correctly. It converts raw data into suitable forms for mining and analysis.",
      "properties": {
        "Tasks": [
          "Data cleaning",
          "Data integration",
          "Data transformation",
          "Data reduction",
          "Data discretization"
        ],
        "Benefits": [
          "Improved data quality",
          "Reduced noise",
          "Higher model accuracy"
        ],
        "Problems addressed": [
          "Missing values",
          "Outliers",
          "Inconsistencies",
          "Bias"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "1_data.json"
        ]
      }
    },
    "Data Quality": {
      "type": "Concept",
      "domain": "Data Management",
      "definition": "A measure of the condition of data based on factors such as accuracy, completeness, reliability, and relevance.",
      "description": "High-quality data ensures valid and actionable analysis. Characteristics include accuracy, precision, completeness, timeliness, consistency, and reliability.",
      "properties": {
        "Characteristics": [
          "Accuracy and precision",
          "Completeness",
          "Availability and accessibility",
          "Timeliness and relevance",
          "Granularity",
          "Reliability and consistency"
        ],
        "Risks": [
          "Inaccurate models",
          "Faulty decision-making",
          "Unreliable analytics"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "1_data.json"
        ]
      }
    },
    "Data Cleaning": {
      "type": "Task",
      "domain": "Data Science",
      "definition": "The process of detecting and correcting inaccurate or corrupt data to improve data quality.",
      "description": "Data cleaning involves handling missing values, removing noise, identifying outliers, and correcting inconsistencies using statistical or algorithmic methods.",
      "properties": {
        "Methods": [
          "Imputation",
          "Binning",
          "Clustering-based outlier removal",
          "Regression smoothing"
        ],
        "Goals": [
          "Ensure data accuracy",
          "Reduce noise",
          "Improve model performance"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "1_data.json"
        ]
      }
    },
    "Big Data": {
      "type": "Concept",
      "domain": "Data Science",
      "definition": "Extremely large data sets that can be analyzed computationally to reveal patterns, trends, and associations, especially relating to human behavior and interactions.",
      "description": "Big Data is characterized by the 5Vs: Volume, Velocity, Variety, Veracity, and Value. It requires specialized tools and techniques for storage, processing, and analysis.",
      "properties": {
        "Characteristics": [
          "Volume",
          "Velocity",
          "Variety",
          "Veracity",
          "Value"
        ],
        "Challenges": [
          "Scalability",
          "Quality assurance",
          "Privacy"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "1_data.json"
        ]
      }
    },
    "Data Literacy": {
      "type": "Skill",
      "domain": "Education",
      "definition": "The ability to read, understand, create, and communicate data as information.",
      "description": "Data literacy enables individuals and organizations to use data effectively, supporting evidence-based decision making and fostering a data-driven culture.",
      "properties": {
        "Components": [
          "Reading and interpreting data",
          "Communicating insights",
          "Applying statistical reasoning",
          "Understanding data ethics"
        ],
        "Relevance": [
          "Data science",
          "Business analytics",
          "Policy making"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "1_data.json"
        ]
      }
    },
    "Data Transformation": {
      "type": "Task",
      "domain": "Data Science",
      "definition": "The process of converting data into a suitable format for analysis or mining.",
      "description": "Transformation includes normalization, aggregation, generalization, and feature extraction, making the data compatible with mining algorithms.",
      "properties": {
        "Techniques": [
          "Normalization",
          "Standardization",
          "Feature extraction",
          "Aggregation"
        ],
        "Goals": [
          "Ensure consistency",
          "Enhance interpretability",
          "Prepare for modeling"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "1_data.json"
        ]
      }
    },
    "Data Reduction": {
      "type": "Task",
      "domain": "Data Science",
      "definition": "The process of reducing the volume of data while maintaining its analytical value.",
      "description": "Data reduction techniques simplify data without losing critical information, enabling efficient analysis and storage.",
      "properties": {
        "Methods": [
          "Sampling",
          "Aggregation",
          "Principal Component Analysis",
          "Clustering"
        ],
        "Purpose": [
          "Reduce computational cost",
          "Improve scalability"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "1_data.json",
          "7_data.json"
        ]
      }
    },
    "Data Ethics": {
      "type": "Principle",
      "domain": "Data Science",
      "definition": "A set of moral guidelines and practices governing responsible collection, analysis, and use of data, ensuring respect for privacy, transparency, and fairness.",
      "description": "Data ethics involves acting responsibly with the data entrusted to you. It acknowledges that data often represent people, and therefore, misuse can harm individuals or groups. Ethical data mining requires accountability, honesty, and consideration of the potential consequences of analysis outcomes.",
      "properties": {
        "Motives": [
          "Ensure positive social impact",
          "Avoid manipulation",
          "Promote fairness"
        ],
        "Methods": [
          "Transparency",
          "Accountability",
          "Consent"
        ],
        "Consequences": [
          "Respect privacy",
          "Prevent harm",
          "Foster trust"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "2_data.json"
        ]
      }
    },
    "Ethical Data Mining": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The practice of conducting data mining in a way that is fair, transparent, and respects privacy and consent.",
      "description": "Ethical data mining balances the potential benefits of insights gained from data with the responsibility to protect subjects from harm. It considers ownership, consent, data sensitivity, and the possible implications of findings.",
      "properties": {
        "Key Principles": [
          "Honesty",
          "Transparency",
          "Responsibility",
          "Non-maleficence"
        ],
        "Challenges": [
          "Unintended harm",
          "Biased data",
          "Lack of consent"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "2_data.json"
        ]
      }
    },
    "Informed Consent": {
      "type": "Principle",
      "domain": "Research Ethics",
      "definition": "The process of obtaining voluntary, informed permission from individuals whose data will be collected or used.",
      "description": "Informed consent ensures individuals understand what data is collected, how it will be used, and have the freedom to withdraw at any time. It is fundamental in research involving personal or sensitive data.",
      "properties": {
        "Requirements": [
          "Clear explanation of purpose",
          "Freedom to deny or withdraw",
          "Comprehensible to the participant"
        ],
        "Constraints": [
          "Use only for stated purpose",
          "Respect withdrawal",
          "Maintain anonymity"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "2_data.json"
        ]
      }
    },
    "General Data Protection Regulation (GDPR)": {
      "type": "Law",
      "domain": "European Data Protection",
      "definition": "A European Union regulation that governs data protection and privacy for individuals within the EU and EEA.",
      "description": "GDPR defines the lawful basis for data processing, grants rights to data subjects, and imposes obligations on data controllers and processors. It aims to give individuals control over their personal data and harmonize regulations across the EU.",
      "properties": {
        "Principles": [
          "Lawfulness",
          "Fairness",
          "Transparency",
          "Data minimisation",
          "Accountability"
        ],
        "Rights": [
          "Access",
          "Erasure",
          "Portability",
          "Restriction"
        ],
        "Sanctions": [
          "Fines for breaches",
          "Data protection impact assessments"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "2_data.json"
        ]
      }
    },
    "Information Security": {
      "type": "Concept",
      "domain": "Data Management",
      "definition": "The practice of protecting information from unauthorized access, modification, or destruction to ensure confidentiality, integrity, and availability.",
      "description": "Information security ensures that data remains accurate, private, and accessible only to authorized users. The CIA triad—Confidentiality, Integrity, Availability—summarizes its key principles.",
      "properties": {
        "Core Principles": [
          "Confidentiality",
          "Integrity",
          "Availability"
        ],
        "Threats": [
          "Hardware failure",
          "Malware",
          "Human error",
          "Unauthorized access"
        ],
        "Practices": [
          "Encryption",
          "Backups",
          "Access control",
          "Authentication"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "2_data.json"
        ]
      }
    },
    "Information Privacy": {
      "type": "Concept",
      "domain": "Data Protection",
      "definition": "The right and ability of individuals to control the collection, use, and dissemination of information about themselves.",
      "description": "Information privacy goes beyond the absence of surveillance—it concerns individuals’ control over personal information traces in digital society. Data mining poses unique challenges to privacy preservation.",
      "properties": {
        "Threats": [
          "Data theft",
          "Unethical use",
          "Government surveillance",
          "Lack of awareness"
        ],
        "Protective Measures": [
          "Anonymisation",
          "Pseudonymisation",
          "Access control"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "2_data.json"
        ]
      }
    },
    "Anonymisation": {
      "type": "Technique",
      "domain": "Data Privacy",
      "definition": "The process of removing personally identifiable information from data to prevent the identification of individuals.",
      "description": "Anonymisation ensures that personal data cannot be traced back to individuals, even when combined with external data. It is critical in ethical data mining and required for many research use cases.",
      "properties": {
        "Methods": [
          "Suppression",
          "Generalisation",
          "K-anonymity",
          "Randomization"
        ],
        "Challenges": [
          "Re-identification risk",
          "Data utility loss"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "2_data.json"
        ]
      }
    },
    "Open Data": {
      "type": "Concept",
      "domain": "Data Management",
      "definition": "Data that anyone can freely access, use, modify, and share for any purpose, subject at most to minimal conditions such as attribution.",
      "description": "Open data promotes transparency, innovation, and collaboration. It must be findable, accessible, interoperable, and reusable according to the FAIR principles.",
      "properties": {
        "Principles": [
          "Findable",
          "Accessible",
          "Interoperable",
          "Reusable"
        ],
        "Licenses": [
          "CC0",
          "CC BY",
          "ODbL"
        ],
        "Challenges": [
          "Loss of control",
          "Licensing complexity",
          "Data misuse"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "2_data.json"
        ]
      }
    },
    "FAIR Principles": {
      "type": "Framework",
      "domain": "Open Science",
      "definition": "A set of guidelines to make data Findable, Accessible, Interoperable, and Reusable.",
      "description": "The FAIR principles ensure data can be easily located, accessed, integrated, and reused by others, supporting open science and reproducibility.",
      "properties": {
        "Components": [
          "Findable",
          "Accessible",
          "Interoperable",
          "Reusable"
        ],
        "Benefits": [
          "Transparency",
          "Data sharing",
          "Reproducibility"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "2_data.json"
        ]
      }
    },
    "Data Management": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The set of practices that ensure data is properly stored, maintained, and accessible for processing and analysis.",
      "description": "Data management encompasses persistent storage, security, metadata curation, and accessibility. Proper management ensures reliability and longevity of data for research and applications.",
      "properties": {
        "Aspects": [
          "Storage",
          "Curation",
          "Security",
          "Metadata"
        ],
        "Tools": [
          "Relational Databases",
          "NoSQL Systems"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "2_data.json"
        ]
      }
    },
    "Data Collection": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The process of systematically gathering and measuring information from a variety of sources to answer research questions, test hypotheses, and evaluate outcomes.",
      "description": "Effective data collection requires careful planning, proper methodology, and awareness of ethical and technical constraints. Poorly designed collection can result in bias, errors, or data loss. Documentation and reproducibility are crucial for trustworthiness.",
      "properties": {
        "Steps": [
          "Define problem, goals, and objectives",
          "Plan methodology and instruments",
          "Conduct pilot collection",
          "Perform final collection",
          "Report and analyze problems"
        ],
        "Considerations": [
          "Repeatability",
          "Accuracy",
          "Stability",
          "Reproducibility"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "3_data.json"
        ]
      }
    },
    "Data Planning": {
      "type": "Task",
      "domain": "Data Science",
      "definition": "The preparatory phase of data collection that involves defining the purpose, method, timing, and logistics of gathering data.",
      "description": "Proper planning ensures that the data collected answers the research question, avoids unnecessary costs, and minimizes errors. It includes defining data types, intervals, devices, and storage procedures.",
      "properties": {
        "Elements": [
          "Problem definition",
          "Method selection",
          "Sample size determination",
          "Measurement intervals",
          "Documentation strategy"
        ],
        "Best Practices": [
          "Iterate procedures",
          "Pilot small samples",
          "Keep methods simple"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "3_data.json"
        ]
      }
    },
    "Sampling": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "The process of selecting a subset of individuals or observations from a population to estimate characteristics of the whole population.",
      "description": "Proper sampling ensures statistical significance and reduces bias. It requires defining population size, margin of error, confidence interval, and expected dropout rate.",
      "properties": {
        "Parameters": [
          "Population size",
          "Confidence interval (Z-score)",
          "Margin of error",
          "Standard deviation (σ)"
        ],
        "Challenges": [
          "Dropout rate",
          "Selection bias",
          "Underrepresentation"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "3_data.json"
        ]
      }
    },
    "Stationary Data": {
      "type": "Data Type",
      "domain": "Signal Processing",
      "definition": "Data whose statistical properties such as mean and variance do not change over time.",
      "description": "Stationary elements remain stable during observation and are easier to model. Examples include controlled mechanical systems and constant environmental measurements.",
      "properties": {
        "Examples": [
          "Temperature-controlled experiments",
          "Static sensors"
        ],
        "Characteristics": [
          "Constant mean",
          "Constant variance",
          "Time-invariant"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "3_data.json"
        ]
      }
    },
    "Non-Stationary Data": {
      "type": "Data Type",
      "domain": "Signal Processing",
      "definition": "Data whose statistical properties such as mean, variance, or correlations change over time.",
      "description": "Non-stationary data comes from systems or subjects that evolve, such as humans, biological processes, or changing environments. Proper modeling requires segmentation and adaptive methods.",
      "properties": {
        "Sources": [
          "Human behavior",
          "Environmental change",
          "Mechanical wear",
          "Seasonal variation"
        ],
        "Techniques": [
          "Windowing",
          "Adaptive filtering",
          "Time normalization"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "3_data.json"
        ]
      }
    },
    "Human Data Collection": {
      "type": "Process",
      "domain": "Human-Centered Data Science",
      "definition": "The collection of data involving human participants, requiring special ethical, methodological, and technical considerations.",
      "description": "Humans are non-stationary, complex systems that change over time. Data collection involving humans requires respect for autonomy, informed consent, privacy protection, and avoidance of harm. It also involves practical challenges like device failure, signal blocking, or behavioral variability.",
      "properties": {
        "Challenges": [
          "Device malfunction",
          "Human variability",
          "Consent management",
          "Sensor connectivity",
          "Instruction adherence"
        ],
        "Ethical Requirements": [
          "Respect for persons",
          "Beneficence",
          "Justice",
          "Confidentiality"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "3_data.json"
        ]
      }
    },
    "Bias": {
      "type": "Concept",
      "domain": "Research Methodology",
      "definition": "A systematic deviation or error in data collection, analysis, or interpretation that leads to incorrect conclusions.",
      "description": "Bias may arise from researchers, respondents, or sampling design. Recognizing and mitigating bias ensures fairness, accuracy, and generalizability of findings.",
      "properties": {
        "Types": [
          "Researcher bias",
          "Respondent bias",
          "Human data bias"
        ],
        "Examples": [
          "Selective reporting",
          "Overrepresentation of certain groups",
          "Unit conversion errors (e.g., Mars Climate Orbiter case)"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "3_data.json"
        ]
      }
    },
    "Randomized Controlled Trial (RCT)": {
      "type": "Method",
      "domain": "Experimental Research",
      "definition": "A type of scientific experiment designed to minimize bias by randomly assigning participants to experimental and control groups.",
      "description": "RCTs are widely used in medical and behavioral studies to establish causal relationships. Variants include parallel, crossover, cluster, and factorial designs. Randomization ensures comparability between groups.",
      "properties": {
        "Design Types": [
          "Parallel",
          "Crossover",
          "Cluster",
          "Factorial"
        ],
        "Strengths": [
          "Bias reduction",
          "Causal inference",
          "Reproducibility"
        ],
        "Weaknesses": [
          "Cost",
          "Complexity",
          "Ethical constraints"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "3_data.json"
        ]
      }
    },
    "Data Documentation": {
      "type": "Task",
      "domain": "Data Management",
      "definition": "The process of recording all relevant details about data collection, methods, devices, and issues to ensure reproducibility and traceability.",
      "description": "Documentation is an essential part of the data lifecycle. It ensures that datasets are interpretable and that future researchers can understand, reuse, and validate results. It includes metadata, logs, and notes on problems.",
      "properties": {
        "Benefits": [
          "Reproducibility",
          "Transparency",
          "Error tracking"
        ],
        "Common Tools": [
          "CSV logs",
          "Version control",
          "Metadata repositories"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "3_data.json"
        ]
      }
    },
    "Data Merging": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The process of combining data from multiple sources, sensors, or studies into a single unified dataset for analysis.",
      "description": "Data merging integrates data collected from different sources to improve completeness and richness. It requires alignment of formats, timestamps, sampling rates, and units to ensure compatibility. Poorly merged data can lead to inconsistent or misleading conclusions.",
      "properties": {
        "Motivation": [
          "Combine complementary data from multiple sensors or datasets",
          "Enhance context and completeness of analysis",
          "Support multi-modal data understanding"
        ],
        "Challenges": [
          "Different formats (CSV, TXT, JSON)",
          "Different timestamps or units",
          "Sensor calibration differences",
          "Incompatible sampling frequencies"
        ],
        "Requirements": [
          "Synchronized timestamps",
          "Uniform sampling rate",
          "Compatible data structure"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "4_data.json"
        ]
      }
    },
    "Sampling Synchronization": {
      "type": "Task",
      "domain": "Signal Processing",
      "definition": "The process of aligning data collected at different sampling frequencies to a common rate to allow joint analysis.",
      "description": "When data are collected from sensors with different sampling rates, synchronization ensures time alignment by either downsampling or oversampling. This process balances information preservation and computational efficiency.",
      "properties": {
        "Methods": [
          "Downsampling",
          "Oversampling"
        ],
        "Problems": [
          "Data loss during downsampling",
          "Increased processing cost during oversampling"
        ],
        "Examples": [
          "Combining 50Hz and 100Hz signals using a common rate of 50Hz or 100Hz"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "4_data.json"
        ]
      }
    },
    "Downsampling": {
      "type": "Method",
      "domain": "Signal Processing",
      "definition": "Reducing the sampling rate of signals to a lower frequency to match other data sources or to decrease data volume.",
      "description": "Downsampling is typically done using the greatest common divisor (GCD) of sampling rates. It reduces data volume and computation cost but may lead to information loss.",
      "properties": {
        "Technique": [
          "Use GCD of sampling rates",
          "Select every nth sample"
        ],
        "Benefits": [
          "Simplified synchronization",
          "Reduced processing load"
        ],
        "Drawbacks": [
          "Information loss",
          "Potential aliasing"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "4_data.json"
        ]
      }
    },
    "Oversampling": {
      "type": "Method",
      "domain": "Signal Processing",
      "definition": "Increasing the sampling rate of a dataset to match a higher-frequency source, typically by interpolation or replication.",
      "description": "Oversampling is performed using the least common multiple (LCM) of sampling rates. It allows finer temporal alignment across datasets but increases data volume and processing requirements.",
      "properties": {
        "Technique": [
          "Interpolate missing samples",
          "Use LCM of sampling rates"
        ],
        "Benefits": [
          "Improved time alignment",
          "Preserved signal fidelity"
        ],
        "Drawbacks": [
          "Increased computation time",
          "Possible overfitting or redundancy"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "4_data.json"
        ]
      }
    },
    "Sampling Methods": {
      "type": "Concept",
      "domain": "Data Science",
      "definition": "Techniques for selecting a subset of data points from a larger dataset to make statistical analysis feasible.",
      "description": "Sampling allows researchers to handle large datasets efficiently or generate artificial data when limited samples are available. Sampling can be with or without replacement, or balanced across classes.",
      "properties": {
        "Main Types": [
          "SRSWR",
          "SRSWOR",
          "Balanced Sampling"
        ],
        "Use Cases": [
          "Too much data",
          "Imbalanced datasets",
          "Computational constraints"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "4_data.json"
        ]
      }
    },
    "Simple Random Sampling Without Replacement (SRSWOR)": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "A sampling technique where each element of the dataset has an equal chance of being selected, and once selected, it is not replaced.",
      "description": "SRSWOR ensures unique samples by preventing duplicate selections. It is suitable when data volume is high, and balanced representation is needed without redundancy.",
      "properties": {
        "Formula": "Probability of selection = 1/N",
        "Benefits": [
          "No duplicates",
          "Representative sample"
        ],
        "Drawbacks": [
          "Limited sample diversity if dataset is small"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "4_data.json"
        ]
      }
    },
    "Simple Random Sampling With Replacement (SRSWR)": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "A sampling technique where each element can be selected more than once, as it is replaced back into the dataset after being drawn.",
      "description": "SRSWR allows multiple instances of the same data point. It is often used in bootstrapping and data augmentation where variability is desired despite small datasets.",
      "properties": {
        "Formula": "Each element has equal probability in every draw.",
        "Benefits": [
          "Allows bootstrapping",
          "Useful for small datasets"
        ],
        "Drawbacks": [
          "Potential bias if duplicates dominate"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "4_data.json"
        ]
      }
    },
    "Balanced Sampling": {
      "type": "Method",
      "domain": "Machine Learning",
      "definition": "A sampling approach designed to achieve a predefined class distribution within a dataset, typically used to mitigate imbalance.",
      "description": "Balanced sampling ensures that each class is proportionally represented in the training set. It can be implemented using random sampling or synthetic techniques.",
      "properties": {
        "Applications": [
          "Imbalanced classification",
          "Fair evaluation",
          "Data balancing"
        ],
        "Techniques": [
          "Stratified sampling",
          "SMOTE",
          "Resampling"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "4_data.json"
        ]
      }
    },
    "Synthetic Minority Oversampling Technique (SMOTE)": {
      "type": "Algorithm",
      "domain": "Machine Learning",
      "definition": "A synthetic data generation method that creates new samples for minority classes by interpolating between existing samples and their nearest neighbors.",
      "description": "SMOTE addresses data imbalance by generating synthetic instances instead of duplicating existing ones. It improves classifier performance in minority classes without distorting feature space distributions.",
      "properties": {
        "Process": [
          "Find k nearest neighbors",
          "Compute vector between sample and neighbor",
          "Multiply by random factor",
          "Add to sample to form synthetic data"
        ],
        "Benefits": [
          "Improves class balance",
          "Avoids overfitting on minority classes"
        ],
        "Challenges": [
          "Possible noise introduction",
          "Boundary overlapping"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "4_data.json"
        ]
      }
    },
    "Performance Metrics": {
      "type": "Concept",
      "domain": "Model Evaluation",
      "definition": "Quantitative measures used to evaluate the predictive performance and quality of machine learning models.",
      "description": "Performance metrics assess how well a model predicts or classifies data. In imbalanced datasets, traditional accuracy can be misleading; thus, alternative metrics such as F1-score, recall, and balanced accuracy are preferred.",
      "properties": {
        "Common Metrics": [
          "Accuracy",
          "Precision",
          "Recall",
          "F1-score",
          "Cohen’s Kappa",
          "Balanced Accuracy"
        ],
        "Problem": "Accuracy paradox—high accuracy may not indicate true model quality when class imbalance exists."
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "4_data.json"
        ]
      }
    },
    "Missing Data": {
      "type": "Concept",
      "domain": "Data Science",
      "definition": "Data values that are absent or undefined in a dataset due to various reasons such as measurement errors, data integration issues, or survey non-response.",
      "description": "Missing data are common in real-world datasets and can lead to biased estimates, invalid conclusions, and limited generalizability if not properly handled. Understanding the mechanism of missingness is critical for choosing the right imputation or estimation method.",
      "properties": {
        "Causes": [
          "Equipment malfunction or sensor errors",
          "Manual data entry mistakes",
          "Out-of-range measurements",
          "Survey non-response",
          "Combining datasets with incompatible schemas"
        ],
        "Consequences": [
          "Biased results",
          "Reduced statistical power",
          "Invalid modeling outcomes"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "5_data.json"
        ]
      }
    },
    "Missing Completely at Random (MCAR)": {
      "type": "Mechanism",
      "domain": "Statistics",
      "definition": "A missingness mechanism where the probability that data are missing is independent of both observed and unobserved data.",
      "description": "In MCAR, missing values occur purely by chance and are unrelated to any variables in the dataset. Analyses on MCAR data remain unbiased, although statistical power decreases due to data loss.",
      "properties": {
        "Example": "Random equipment failure or random sampling exclusion.",
        "Implication": "Deleting missing data under MCAR does not bias results but reduces dataset size."
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "5_data.json"
        ]
      }
    },
    "Missing at Random (MAR)": {
      "type": "Mechanism",
      "domain": "Statistics",
      "definition": "A missingness mechanism where the probability that data are missing depends on observed data but not on the missing data itself.",
      "description": "In MAR, the reason for missingness can be explained by other known variables in the dataset, such as a particular sensor brand failing more often. Many modern imputation and maximum likelihood methods assume MAR.",
      "properties": {
        "Example": "A certain device model frequently fails to record measurements.",
        "Implication": "Bias can be reduced if the imputation model includes the variables related to the missingness mechanism."
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "5_data.json"
        ]
      }
    },
    "Missing Not at Random (MNAR)": {
      "type": "Mechanism",
      "domain": "Statistics",
      "definition": "A missingness mechanism where the probability of missing data depends on the missing value itself or on unobserved factors.",
      "description": "MNAR occurs when the reason for missingness is related to the actual missing values, such as patients with severe illness skipping health surveys. MNAR models are complex and often difficult to fit correctly.",
      "properties": {
        "Example": "A sensor fails more often at high temperature readings.",
        "Implication": "Analyses assuming MAR may be biased when data are MNAR."
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "5_data.json"
        ]
      }
    },
    "Deletion": {
      "type": "Method",
      "domain": "Data Pre-processing",
      "definition": "A method for handling missing data by removing incomplete records or variables from analysis.",
      "description": "Deletion is the simplest way to deal with missing data but often reduces dataset size and statistical power. It can lead to bias if missingness is not completely random.",
      "properties": {
        "Variants": [
          "Listwise Deletion",
          "Pairwise Deletion"
        ],
        "Pros": [
          "Easy to implement",
          "No imputation needed"
        ],
        "Cons": [
          "Loss of data",
          "Possible bias under MAR or MNAR"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "5_data.json"
        ]
      }
    },
    "Imputation": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The process of replacing missing data with substituted values based on statistical or machine learning methods.",
      "description": "Imputation creates complete datasets by estimating missing values from observed data. It helps maintain sample size and reduce bias, provided the missingness mechanism is appropriately modeled.",
      "properties": {
        "Types": [
          "Mean Imputation",
          "Regression Imputation",
          "Stochastic Regression Imputation",
          "Hot Deck Imputation",
          "Cold Deck Imputation",
          "Multiple Imputation",
          "Machine Learning-Based Imputation"
        ],
        "Advantages": [
          "Preserves data volume",
          "Can reduce bias"
        ],
        "Limitations": [
          "Can distort variability",
          "Dependent on assumptions"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "5_data.json"
        ]
      }
    },
    "Mean Imputation": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "A simple imputation method where missing numerical values are replaced by the mean of the observed values.",
      "description": "Mean imputation is widely used but can reduce variance, distort correlations, and bias relationships between variables. It is only valid under MCAR assumptions.",
      "properties": {
        "Benefits": [
          "Easy to implement",
          "Restores dataset completeness"
        ],
        "Drawbacks": [
          "Underestimates variability",
          "Biases covariance and regression coefficients"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "5_data.json"
        ]
      }
    },
    "Regression Imputation": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "An imputation technique that predicts missing values using regression models based on other observed variables.",
      "description": "Regression imputation can preserve relationships between variables but reduces variance in the dataset. It assumes that missingness can be modeled using observed predictors.",
      "properties": {
        "Assumptions": [
          "Data is MAR",
          "Predictor variables are complete"
        ],
        "Challenges": [
          "May produce perfect fits",
          "Biased estimates if model misspecified"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "5_data.json"
        ]
      }
    },
    "Stochastic Regression Imputation": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "An extension of regression imputation that adds random noise to restore natural variability to the imputed values.",
      "description": "This method adds random residuals to regression predictions, maintaining relationships between variables while accounting for uncertainty. It avoids perfectly predicted values and better reflects true data variability.",
      "properties": {
        "Benefits": [
          "Restores variance",
          "Preserves relationships between variables"
        ],
        "Limitations": [
          "Still underestimates uncertainty",
          "Possible impossible values"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "5_data.json"
        ]
      }
    },
    "Multiple Imputation": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "A comprehensive imputation framework that replaces missing data with multiple plausible values, generating several complete datasets that are analyzed and combined.",
      "description": "Multiple imputation captures the uncertainty inherent in missing data by creating multiple imputed datasets, analyzing them separately, and pooling results. It provides unbiased parameter estimates and valid standard errors under MAR assumptions.",
      "properties": {
        "Phases": [
          "Imputation phase",
          "Analysis phase",
          "Pooling phase"
        ],
        "Advantages": [
          "Preserves variance",
          "Handles uncertainty",
          "Suitable for classification"
        ],
        "Metrics": [
          "Within-imputation variance (U)",
          "Between-imputation variance (B)",
          "Total variance (T)"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "5_data.json"
        ]
      }
    },
    "Maximum Likelihood Estimation (MLE)": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "A statistical estimation technique that finds parameter values maximizing the likelihood of observed data, including cases with missing values.",
      "description": "MLE utilizes all available data to estimate model parameters without directly imputing missing values. It works by optimizing a log-likelihood function based on the observed data. Expectation-Maximization (EM) is a common iterative algorithm for this purpose.",
      "properties": {
        "Steps": [
          "Define likelihood function",
          "Compute log-likelihood",
          "Optimize via EM or Newton-Raphson"
        ],
        "Benefits": [
          "Uses all available data",
          "Produces unbiased estimates under MAR"
        ],
        "Limitations": [
          "Underestimates standard errors",
          "Requires numerical data"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "5_data.json"
        ]
      }
    },
    "Noise": {
      "type": "Concept",
      "domain": "Data Quality",
      "definition": "Random or systematic errors in data that obscure the true underlying signal or pattern.",
      "description": "Noise can originate from measurement errors, equipment malfunctions, environmental factors, or data entry mistakes. It can distort models, reduce performance, and create false patterns during analysis, especially in supervised learning.",
      "properties": {
        "Types": [
          "Attribute noise",
          "Label noise"
        ],
        "Sources": [
          "Sensor malfunction",
          "Human error",
          "Data transmission issues"
        ],
        "Impact": [
          "Increased error rate",
          "Overfitting risk",
          "Reduced generalization"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "6_data.json"
        ]
      }
    },
    "Robust Learning Algorithms": {
      "type": "Method",
      "domain": "Machine Learning",
      "definition": "Algorithms capable of learning accurate models even when training data contains noise, outliers, or errors.",
      "description": "Robust learners are designed to minimize the impact of corrupted or noisy instances. They produce models similar to those obtained from clean data, often using regularization, pruning, or robust loss functions.",
      "properties": {
        "Examples": [
          "C4.5 Decision Tree with pruning",
          "Support Vector Machines",
          "Random Forests"
        ],
        "Techniques": [
          "Pruning",
          "Regularization",
          "Robust loss functions"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "6_data.json"
        ]
      }
    },
    "Data Polishing": {
      "type": "Technique",
      "domain": "Data Cleaning",
      "definition": "The process of identifying and correcting noisy or corrupted data before building predictive models.",
      "description": "Data polishing focuses on improving data quality by removing or correcting noise manually or through algorithms. It is particularly useful for small datasets where noisy samples can be easily isolated.",
      "properties": {
        "Steps": [
          "Detect noise",
          "Correct corrupted instances",
          "Validate polished data"
        ],
        "Use Cases": [
          "Small datasets",
          "Low noise frequency"
        ],
        "Limitations": [
          "Inefficient for large-scale data",
          "Risk of removing valid samples"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "6_data.json"
        ]
      }
    },
    "Noise Filters": {
      "type": "Method",
      "domain": "Signal Processing",
      "definition": "Techniques for identifying and removing noise from data, including specific frequency interference or corrupted samples.",
      "description": "Noise filters aim to isolate and eliminate unwanted variations in the data, such as electrical interference or measurement errors. Over-filtering can remove meaningful information.",
      "properties": {
        "Examples": [
          "Low-pass filter",
          "Notch filter",
          "Spectral filtering"
        ],
        "Sources of Noise": [
          "50 Hz power line",
          "Infrared absorbance errors",
          "Equipment malfunction"
        ],
        "Risks": [
          "Loss of valuable signal components"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "6_data.json"
        ]
      }
    },
    "Data Pollution": {
      "type": "Concept",
      "domain": "Data Quality",
      "definition": "The presence of incorrect, inconsistent, or irrelevant data values that do not conform to intended formats or meanings.",
      "description": "Data pollution occurs when datasets contain invalid entries, such as incorrect category values, formatting errors, or corrupted text fields. These issues often result from system repurposing or manual entry errors.",
      "properties": {
        "Examples": [
          "Gender field containing 'male', 'female', and 'business'",
          "Free text with delimiters",
          "Misplaced decimal points"
        ],
        "Causes": [
          "Software misuse",
          "Copy-paste errors",
          "CSV formatting issues"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "6_data.json"
        ]
      }
    },
    "Signal Saturation": {
      "type": "Phenomenon",
      "domain": "Signal Processing",
      "definition": "A condition where sensor readings reach their upper or lower limit and cannot record values beyond that threshold.",
      "description": "Signal saturation leads to repeated minimum or maximum values, reducing variability and masking true signal dynamics. It often occurs when sensors exceed their operational range.",
      "properties": {
        "Symptoms": [
          "Repeated min/max values",
          "Clipped signals",
          "Loss of detail"
        ],
        "Causes": [
          "Sensor range limits",
          "Hardware constraints"
        ],
        "Handling": [
          "Mark corrupted sections",
          "Exclude extreme sequences"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "6_data.json"
        ]
      }
    },
    "Outliers": {
      "type": "Concept",
      "domain": "Statistics",
      "definition": "Data points that significantly deviate from the expected pattern or distribution of a dataset.",
      "description": "Outliers can result from noise, measurement errors, or genuine rare events. They may distort statistical summaries and machine learning models. Outlier analysis involves detecting, interpreting, and deciding whether to correct or retain them.",
      "properties": {
        "Types": [
          "Global",
          "Contextual",
          "Collective"
        ],
        "Causes": [
          "Calibration errors",
          "External disruptions",
          "Rare phenomena"
        ],
        "Impact": [
          "Model distortion",
          "Misleading correlations"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "6_data.json"
        ]
      }
    },
    "Anomalies": {
      "type": "Concept",
      "domain": "Data Mining",
      "definition": "Rare and valid instances that deviate from normal behavior but carry valuable information rather than being errors.",
      "description": "Anomalies differ from outliers in that they often represent meaningful or significant deviations, such as fraudulent activity, medical conditions, or rare events. Detecting anomalies helps in security, finance, and environmental monitoring.",
      "properties": {
        "Applications": [
          "Intrusion detection",
          "Credit card fraud detection",
          "Medical diagnostics",
          "Climate and environmental monitoring"
        ],
        "Difference from Outliers": "Outliers may be noise or errors; anomalies are often valid and informative."
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "6_data.json"
        ]
      }
    },
    "Outlier Detection Methods": {
      "type": "Framework",
      "domain": "Machine Learning",
      "definition": "Algorithms and approaches designed to identify data points that deviate significantly from expected normal patterns.",
      "description": "Outlier detection methods vary based on whether labels exist (supervised), partial labels (semi-supervised), or no labels (unsupervised). They aim to separate normal behavior from abnormal instances using distance, density, or statistical assumptions.",
      "properties": {
        "Categories": [
          "Supervised",
          "Semi-supervised",
          "Unsupervised"
        ],
        "Examples": [
          "k-Nearest Neighbors (distance-based)",
          "Local Outlier Factor (density-based)",
          "Gaussian Distribution (parametric)",
          "Kernel Density Estimation (nonparametric)",
          "K-Means or GMM (clustering-based)",
          "SVM or Random Forest (classification-based)"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "6_data.json"
        ]
      }
    },
    "Proximity-Based Methods": {
      "type": "Method",
      "domain": "Outlier Detection",
      "definition": "Methods that define outliers as data points distant from their nearest neighbors in feature space.",
      "description": "These methods rely on distance metrics such as Euclidean or Mahalanobis distance to detect isolated instances. Variants include distance-based and density-based approaches.",
      "properties": {
        "Examples": [
          "k-Nearest Neighbors",
          "Local Outlier Factor (LOF)"
        ],
        "Strengths": [
          "Effective for global and local outliers"
        ],
        "Weaknesses": [
          "Sensitive to distance metric choice",
          "Poor scalability on large data"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "6_data.json"
        ]
      }
    },
    "Normalization": {
      "type": "Process",
      "domain": "Data Pre-processing",
      "definition": "The process of adjusting values measured on different scales to a notionally common scale without distorting differences in the ranges of values.",
      "description": "Normalization helps learning algorithms converge faster and ensures that attributes with large numerical ranges do not dominate those with smaller ranges. It is a core data preprocessing step required by many distance-based or gradient-based learning models.",
      "properties": {
        "Reasons": [
          "Improves model convergence and training stability",
          "Prevents attributes with large ranges from dominating smaller ones",
          "Required for certain algorithms (e.g., neural networks, k-NN, PCA)"
        ],
        "Considerations": [
          "Store scaling parameters for later use (e.g., for inference data)",
          "Choose normalization technique based on variable characteristics"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "7_data.json"
        ]
      }
    },
    "Min-Max Normalization": {
      "type": "Method",
      "domain": "Data Pre-processing",
      "definition": "A normalization method that scales data to a fixed range, typically [0, 1], based on the minimum and maximum values of each variable.",
      "description": "This linear transformation maps the minimum value to 0 and the maximum to 1 (or another defined interval [a, b]). It preserves relationships between data points but is sensitive to outliers.",
      "properties": {
        "Formula": "v' = (v - v_min) / (v_max - v_min)",
        "Range": "[0, 1] or [a, b]",
        "Advantages": [
          "Simple to compute",
          "Preserves shape of distribution"
        ],
        "Disadvantages": [
          "Sensitive to outliers"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "7_data.json"
        ]
      }
    },
    "Z-score Standardization": {
      "type": "Method",
      "domain": "Data Pre-processing",
      "definition": "A normalization technique that rescales data to have a mean of 0 and a standard deviation of 1.",
      "description": "Z-score standardization centers data by removing the mean and scales it by dividing by the standard deviation. It is especially useful when min and max values are unknown or when data contains outliers.",
      "properties": {
        "Formula": "v' = (v - mean(v)) / sd(v)",
        "Advantages": [
          "Handles outliers better than min-max scaling",
          "Useful when ranges are unknown"
        ],
        "Use Cases": [
          "Regression",
          "PCA",
          "Outlier detection under Gaussian assumption"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "7_data.json"
        ]
      }
    },
    "Decimal Scaling": {
      "type": "Method",
      "domain": "Data Pre-processing",
      "definition": "A normalization method that scales data by moving the decimal point of values based on the maximum absolute value of the variable.",
      "description": "Decimal scaling divides each value by a power of 10 such that all transformed values fall within the range [-1, 1]. The power is determined by the largest absolute value in the dataset.",
      "properties": {
        "Formula": "v' = v / 10^j where j = smallest integer such that max(|v'|) < 1",
        "Advantages": [
          "Simple to compute",
          "Effective when values vary by powers of ten"
        ],
        "Disadvantages": [
          "Less flexible for distributions with small variance"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "7_data.json"
        ]
      }
    },
    "Discretization": {
      "type": "Process",
      "domain": "Data Transformation",
      "definition": "The process of converting continuous variables into categorical ones by dividing their range into intervals or groups.",
      "description": "Discretization is used when models or analyses require categorical data, or when researchers want to study group-level differences instead of individual variations. However, it often reduces information and statistical power.",
      "properties": {
        "Methods": [
          "Median split",
          "Equal width binning",
          "Equal frequency binning",
          "Domain-based grouping"
        ],
        "Advantages": [
          "Simplifies analysis",
          "Enables categorical modeling"
        ],
        "Disadvantages": [
          "Loss of information",
          "Reduced precision",
          "Incompatibility across studies due to arbitrary cutoffs"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "7_data.json"
        ]
      }
    },
    "Dummy Coding": {
      "type": "Technique",
      "domain": "Data Transformation",
      "definition": "A method for encoding categorical variables as binary numeric features for use in regression and machine learning models.",
      "description": "Dummy coding represents K categories with K-1 binary variables. It allows categorical data to be incorporated into models that require numerical inputs.",
      "properties": {
        "Example": {
          "Variable": [
            "Red",
            "Blue",
            "Green",
            "Yellow"
          ],
          "Encoded": [
            "Dummy_Red",
            "Dummy_Blue",
            "Dummy_Green"
          ]
        },
        "Advantages": [
          "Enables inclusion of categorical variables in regression models"
        ],
        "Limitations": [
          "Increases dimensionality",
          "Risk of multicollinearity if not handled properly"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "7_data.json"
        ]
      }
    },
    "Principal Component Analysis (PCA)": {
      "type": "Method",
      "domain": "Dimensionality Reduction",
      "definition": "A statistical technique that transforms correlated variables into a smaller number of uncorrelated variables called principal components.",
      "description": "PCA reduces the dimensionality of data by finding orthogonal directions that capture the most variance. It is commonly used for visualization, compression, and to handle multicollinearity.",
      "properties": {
        "Steps": [
          "Compute covariance matrix",
          "Find eigenvalues and eigenvectors",
          "Project data onto principal components"
        ],
        "Advantages": [
          "Removes multicollinearity",
          "Improves computation efficiency"
        ],
        "Limitations": [
          "Loss of interpretability",
          "Assumes linearity"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "7_data.json"
        ]
      }
    },
    "Discrete Wavelet Transform (DWT)": {
      "type": "Method",
      "domain": "Signal Processing",
      "definition": "A linear signal processing technique used to decompose data into different frequency components and analyze each component with a resolution matched to its scale.",
      "description": "DWT is effective for dimensionality reduction in high-dimensional data, especially when the number of variables exceeds the number of samples. It preserves both frequency and location information, unlike Fourier transform.",
      "properties": {
        "Applications": [
          "Image compression",
          "Feature extraction",
          "Time-series analysis"
        ],
        "Advantages": [
          "Handles non-stationary data",
          "Reduces dimensionality efficiently"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "7_data.json"
        ]
      }
    },
    "Box-Cox Transformation": {
      "type": "Method",
      "domain": "Statistical Transformation",
      "definition": "A family of power transformations that aim to stabilize variance and make data more normally distributed.",
      "description": "Box-Cox transformation improves normality assumptions for statistical models. It only applies to positive data but can handle skewed distributions effectively.",
      "properties": {
        "Formula": "T(Y) = (Y^λ - 1) / λ for λ ≠ 0; T(Y) = ln(Y) for λ = 0",
        "Limitations": [
          "Only works with positive values",
          "No guarantee of perfect normality"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "7_data.json"
        ]
      }
    },
    "Model Generalization": {
      "type": "Concept",
      "domain": "Machine Learning",
      "definition": "The ability of a model to perform well on new, unseen data rather than merely on the data used for training.",
      "description": "Model generalization measures how effectively a model captures the underlying data distribution and applies learned patterns to new observations. High generalization indicates robustness and low overfitting. Poor generalization often results from excessive model complexity or insufficient training data.",
      "properties": {
        "Importance": [
          "Ensures model performance on real-world data",
          "Prevents overfitting to training data",
          "Improves reliability and interpretability"
        ],
        "Indicators": [
          "Low gap between training and testing error",
          "Stable performance across datasets"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "8_data.json"
        ]
      }
    },
    "Descriptive Modeling": {
      "type": "Model Type",
      "domain": "Data Mining",
      "definition": "A modeling approach focused on summarizing or representing data structure in a compact form without making predictions.",
      "description": "Descriptive models aim to find inherent patterns or relationships in data, such as grouping or correlations, rather than making future predictions. Common techniques include clustering, association rule mining, and dimensionality reduction.",
      "properties": {
        "Goal": "Describe structure and relationships within data",
        "Examples": [
          "Cluster Analysis",
          "Association Rules",
          "Principal Component Analysis (PCA)"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "8_data.json"
        ]
      }
    },
    "Explanatory Modeling": {
      "type": "Model Type",
      "domain": "Statistics",
      "definition": "A modeling approach used to test causal hypotheses by analyzing relationships between variables.",
      "description": "Explanatory models are designed to confirm theoretical relationships, such as cause-and-effect, and are commonly applied in medical, social, and behavioral sciences. They focus on inference rather than prediction.",
      "properties": {
        "Examples": [
          "Linear Regression",
          "Logistic Regression",
          "Structural Equation Modeling"
        ],
        "Goals": [
          "Understand relationships",
          "Test scientific hypotheses"
        ],
        "Approach": "Retrospective and confirmatory"
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "8_data.json"
        ]
      }
    },
    "Predictive Modeling": {
      "type": "Model Type",
      "domain": "Machine Learning",
      "definition": "A modeling approach that uses historical data to predict future or unseen observations.",
      "description": "Predictive models estimate output variables (Y) based on input variables (X). They are data-driven, focusing on accuracy and generalization rather than inference. Examples include decision trees, neural networks, and support vector machines.",
      "properties": {
        "Goal": "Predict outcomes for new data",
        "Characteristics": [
          "Exploratory",
          "Prospective",
          "Accuracy-driven"
        ],
        "Examples": [
          "Random Forest",
          "SVM",
          "Neural Network Classifier"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "8_data.json"
        ]
      }
    },
    "Overfitting": {
      "type": "Phenomenon",
      "domain": "Machine Learning",
      "definition": "A modeling error that occurs when a model learns noise or random fluctuations in the training data instead of the underlying pattern.",
      "description": "Overfitting leads to low training error but high testing error. It occurs when models are overly complex relative to the amount of available data, reducing generalization performance.",
      "properties": {
        "Symptoms": [
          "Low training error, high test error",
          "Unstable predictions on new data"
        ],
        "Causes": [
          "Small dataset",
          "Excessive model complexity",
          "Insufficient regularization"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "8_data.json"
        ]
      }
    },
    "Underfitting": {
      "type": "Phenomenon",
      "domain": "Machine Learning",
      "definition": "A modeling condition where the model is too simple to capture the underlying structure of the data, resulting in poor performance on both training and test data.",
      "description": "Underfitting occurs when a model fails to learn the essential relationships between input and output variables. It usually happens when the model lacks sufficient complexity or training iterations.",
      "properties": {
        "Symptoms": [
          "High bias",
          "Low variance",
          "Poor performance on all datasets"
        ],
        "Causes": [
          "Too simple model",
          "Inadequate training",
          "High regularization"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "8_data.json"
        ]
      }
    },
    "Sample Size": {
      "type": "Parameter",
      "domain": "Statistical Modeling",
      "definition": "The number of observations or data points used to train and evaluate a model.",
      "description": "Sample size affects both the bias and variance of a model. Small datasets may cause overfitting, while large ones improve generalizability but increase computational costs. Predictive models generally require larger samples than explanatory models.",
      "properties": {
        "Effects": [
          "Small sample size → overfitting and poor generalization",
          "Large sample size → reduced variance and improved precision",
          "Beyond certain size → diminishing returns"
        ],
        "Guidelines": [
          "Larger data for prediction",
          "Smaller for inference"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "8_data.json"
        ]
      }
    },
    "Cross-Validation": {
      "type": "Method",
      "domain": "Model Evaluation",
      "definition": "A resampling technique used to assess the performance and generalization of a model by partitioning the dataset into subsets.",
      "description": "Cross-validation divides the data into multiple folds, training the model on some while testing on others. It provides a more reliable estimate of model performance than a single train-test split, especially with small datasets.",
      "properties": {
        "Variants": [
          "k-fold cross-validation",
          "Leave-one-out validation"
        ],
        "Benefits": [
          "Reduces variance of performance estimates",
          "Detects overfitting",
          "Efficient use of data"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "8_data.json"
        ]
      }
    },
    "Model Selection": {
      "type": "Process",
      "domain": "Machine Learning",
      "definition": "The process of choosing the best-performing model or configuration among multiple candidates based on validation metrics.",
      "description": "Model selection involves comparing model performance, complexity, and interpretability to find the optimal balance between bias and variance. Simpler models are often more generalizable and interpretable.",
      "properties": {
        "Criteria": [
          "Low validation error",
          "High interpretability",
          "Minimal overfitting"
        ],
        "Factors": [
          "Model type",
          "Hyperparameters",
          "Feature selection"
        ]
      },
      "metadata": {
        "subjects": [
          "towards_data_mining"
        ],
        "source_files": [
          "8_data.json"
        ]
      }
    }
  },
  "edges": [
    {
      "source": "Continuous Optimization",
      "type": "has_component",
      "target": "Objective Function ($f$)"
    },
    {
      "source": "Continuous Optimization",
      "type": "has_component",
      "target": "Constraints ($c_i$)"
    },
    {
      "source": "Continuous Optimization",
      "type": "contrasts_with",
      "target": "Discrete Optimization"
    },
    {
      "source": "Objective Function ($f$)",
      "type": "has_property",
      "target": "Convexity"
    },
    {
      "source": "Objective Function ($f$)",
      "type": "characterized_by",
      "target": "Gradient (\\nabla f)"
    },
    {
      "source": "Gradient (\\nabla f)",
      "type": "is_orthogonal_to",
      "target": "Tangent Plane"
    },
    {
      "source": "Gradient (\\nabla f)",
      "type": "is_central_to",
      "target": "Smooth Optimization"
    },
    {
      "source": "Level Set ($L_c(f)$)",
      "type": "is_characterized_by",
      "target": "Gradient (\\nabla f)"
    },
    {
      "source": "Level Set ($L_c(f)$)",
      "type": "has_synonym",
      "target": "Contours of $f$"
    },
    {
      "source": "Convexity",
      "type": "is_property_of",
      "target": "Objective Function ($f$)"
    },
    {
      "source": "Convexity",
      "type": "related_to",
      "target": "Hessian Matrix"
    },
    {
      "source": "Linear Program (LP)",
      "type": "is_type_of",
      "target": "Constrained Optimization"
    },
    {
      "source": "Newton's Method",
      "type": "has_metric",
      "target": "Quadratic Convergence"
    },
    {
      "source": "Newton's Method",
      "type": "requires",
      "target": "Taylor's Theorem"
    },
    {
      "source": "Speed of Convergence (Order p)",
      "type": "is_metric_for",
      "target": "Iterative Method"
    },
    {
      "source": "Speed of Convergence (Order p)",
      "type": "has_type",
      "target": "Quadratic Convergence"
    },
    {
      "source": "Unconstrained Optimization",
      "type": "is_simplified_by",
      "target": "Convex Optimization"
    },
    {
      "source": "Local Minimizer ($x^*$)",
      "type": "is_prerequisite_for",
      "target": "Global Minimizer"
    },
    {
      "source": "Local Minimizer ($x^*$)",
      "type": "is_specialized_by",
      "target": "Strict Local Minimizer"
    },
    {
      "source": "First-Order Necessary Condition (FONC)",
      "type": "defines",
      "target": "Stationary Point"
    },
    {
      "source": "First-Order Necessary Condition (FONC)",
      "type": "is_required_by",
      "target": "Second-Order Sufficient Conditions (SOSC)"
    },
    {
      "source": "Positive Semidefinite Matrix",
      "type": "is_required_for",
      "target": "Second-Order Necessary Condition"
    },
    {
      "source": "Positive Semidefinite Matrix",
      "type": "is_strict_form_of",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Second-Order Sufficient Conditions (SOSC)",
      "type": "requires",
      "target": "First-Order Necessary Condition (FONC)"
    },
    {
      "source": "Second-Order Sufficient Conditions (SOSC)",
      "type": "requires",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Saddle Point",
      "type": "is_type_of",
      "target": "Stationary Point"
    },
    {
      "source": "Line Search Methods",
      "type": "is_type_of",
      "target": "Iterative Method"
    },
    {
      "source": "Line Search Methods",
      "type": "uses",
      "target": "Descent Direction ($p_k$)"
    },
    {
      "source": "Descent Direction ($p_k$)",
      "type": "is_component_of",
      "target": "Line Search Methods"
    },
    {
      "source": "Quadratic Function (Optimization Tool)",
      "type": "is_approximation_basis_for",
      "target": "Newton's Method"
    },
    {
      "source": "Descent Direction Condition",
      "type": "is_required_for",
      "target": "Line Search Methods"
    },
    {
      "source": "Direction of Steepest Descent",
      "type": "is_special_case_of",
      "target": "Descent Direction ($p_k$)"
    },
    {
      "source": "Direction of Steepest Descent",
      "type": "is_central_to",
      "target": "Steepest Descent Method"
    },
    {
      "source": "Newton's Method (Optimization)",
      "type": "has_convergence_order",
      "target": "Quadratic Convergence"
    },
    {
      "source": "Newton's Method (Optimization)",
      "type": "is_based_on",
      "target": "Hessian Matrix"
    },
    {
      "source": "Steepest Descent Method",
      "type": "is_governed_by",
      "target": "Condition Number (\\kappa(Q))"
    },
    {
      "source": "Steepest Descent Method",
      "type": "has_convergence_order",
      "target": "Linear Convergence"
    },
    {
      "source": "Condition Number (\\kappa(Q))",
      "type": "determines_speed_of",
      "target": "Steepest Descent Method"
    },
    {
      "source": "Quasi-Newton Methods",
      "type": "implements",
      "target": "Secant Condition"
    },
    {
      "source": "Quasi-Newton Methods",
      "type": "aims_for",
      "target": "Superlinear Convergence"
    },
    {
      "source": "Secant Condition",
      "type": "is_basis_for",
      "target": "SR1 Update"
    },
    {
      "source": "SR1 Update",
      "type": "is_type_of",
      "target": "Rank-One Update"
    },
    {
      "source": "SR1 Update",
      "type": "is_precursor_to",
      "target": "BFGS method"
    },
    {
      "source": "Bisection Method (Step Length)",
      "type": "is_used_to_solve",
      "target": "1D Subproblem (Line Search)"
    },
    {
      "source": "Bisection Method (Step Length)",
      "type": "has_convergence_order",
      "target": "Linear Convergence (C=1/2)"
    },
    {
      "source": "Nonlinear Least Squares Problems",
      "type": "is_type_of",
      "target": "Unconstrained Optimization"
    },
    {
      "source": "Bisection Method",
      "type": "Convergence Rate",
      "target": "Linear, $C = \\frac{1}{2}$"
    },
    {
      "source": "Bisection Method",
      "type": "Used In",
      "target": "Strong Wolfe Line Search"
    },
    {
      "source": "Bisection Method",
      "type": "Reference",
      "target": "Nocedal & Wright, p. 56"
    },
    {
      "source": "Least Squares Problems",
      "type": "Objective",
      "target": "$f(x) = \\frac{1}{2} \\| r(x) \\|^2$"
    },
    {
      "source": "Least Squares Problems",
      "type": "Gradient",
      "target": "$\\nabla f(x) = J(x)^T r(x)$"
    },
    {
      "source": "Least Squares Problems",
      "type": "Assumes",
      "target": "$m \\ge n$ (overdetermined)"
    },
    {
      "source": "Linear Least Squares Problem (Linear Regression)",
      "type": "Normal Equations",
      "target": "$A^T A x = A^T b$"
    },
    {
      "source": "Linear Least Squares Problem (Linear Regression)",
      "type": "Hessian",
      "target": "$\\nabla^2 f(x) = A^T A$"
    },
    {
      "source": "Linear Least Squares Problem (Linear Regression)",
      "type": "Stable Solve Via",
      "target": "QR: $A = QR$, $R x = Q^T b$"
    },
    {
      "source": "Gauss–Newton Method",
      "type": "Approximates",
      "target": "Newton’s method"
    },
    {
      "source": "Gauss–Newton Method",
      "type": "Hessian Approximation",
      "target": "$B_k = J_k^T J_k$"
    },
    {
      "source": "Gauss–Newton Method",
      "type": "Descent Property",
      "target": "$p_k^T \\nabla f_k = -\\| J_k p_k \\|^2 \\le 0$"
    },
    {
      "source": "Residual Functions ($r_j(x)$)",
      "type": "Forms Vector",
      "target": "$r(x) \\in \\mathbb{R}^m$"
    },
    {
      "source": "Residual Functions ($r_j(x)$)",
      "type": "Jacobian",
      "target": "$J(x) = \\frac{\\partial r_j}{\\partial x_i}$"
    },
    {
      "source": "Levenberg–Marquardt Method",
      "type": "Hybrid Of",
      "target": "Gauss–Newton + Gradient Descent"
    },
    {
      "source": "Levenberg–Marquardt Method",
      "type": "Trust-Region Subproblem",
      "target": "$\\| J_k p + r_k \\|^2$, $\\|p\\| \\le \\Delta_k$"
    },
    {
      "source": "Levenberg–Marquardt Method",
      "type": "Damping Parameter",
      "target": "$\\mu_k \\ge 0$"
    },
    {
      "source": "Constrained Optimization Problem",
      "type": "Stationary Points Via",
      "target": "Karush-Kuhn-Tucker (KKT) Conditions"
    },
    {
      "source": "Constrained Optimization Problem",
      "type": "Feasible Set Defined By",
      "target": "Active Set $\\mathcal{A}(x)$"
    },
    {
      "source": "Karush-Kuhn-Tucker (KKT) Conditions",
      "type": "Derived From",
      "target": "Lagrangian Function $\\mathcal{L}(x, \\lambda)$"
    },
    {
      "source": "Karush-Kuhn-Tucker (KKT) Conditions",
      "type": "Requires",
      "target": "Constraint Qualification (e.g., LICQ)"
    },
    {
      "source": "Karush-Kuhn-Tucker (KKT) Conditions",
      "type": "Proven Using",
      "target": "Farkas' Lemma"
    },
    {
      "source": "Lagrangian Function ($\\mathcal{L}(x, \\lambda)$)",
      "type": "Stationarity",
      "target": "$\\nabla_x \\mathcal{L}(x^*, \\lambda^*) = 0$"
    },
    {
      "source": "Lagrangian Function ($\\mathcal{L}(x, \\lambda)$)",
      "type": "Defines",
      "target": "Dual Function $q(\\lambda)$"
    },
    {
      "source": "Tangent Cone ($T_\\Omega(x)$)",
      "type": "Approximated By",
      "target": "Linearized Feasible Cone $\\mathcal{F}(x)$"
    },
    {
      "source": "Tangent Cone ($T_\\Omega(x)$)",
      "type": "Equal Under",
      "target": "LICQ"
    },
    {
      "source": "Linear Independence Constraint Qualification (LICQ)",
      "type": "Implies",
      "target": "$T_\\Omega(x) = \\mathcal{F}(x)$"
    },
    {
      "source": "Linear Independence Constraint Qualification (LICQ)",
      "type": "Stronger Than",
      "target": "MFCQ, Slater"
    },
    {
      "source": "Active Set of Indices ($\\mathcal{A}(x)$)",
      "type": "Used In",
      "target": "LICQ, $\\mathcal{F}(x)$, KKT"
    },
    {
      "source": "Farkas' Lemma",
      "type": "Used To Prove",
      "target": "KKT Necessity"
    },
    {
      "source": "Farkas' Lemma",
      "type": "Generalizes",
      "target": "Gordan's Theorem"
    },
    {
      "source": "Trust Region Methods (TRMs)",
      "type": "Solves Via",
      "target": "Trust Region Subproblem (52)"
    },
    {
      "source": "Trust Region Methods (TRMs)",
      "type": "Alternative To",
      "target": "Line Search Methods"
    },
    {
      "source": "Trust Region Methods (TRMs)",
      "type": "Global Convergence In",
      "target": "Nocedal & Wright, Chapter 4"
    },
    {
      "source": "Trust Region Subproblem (52)",
      "type": "Objective",
      "target": "Model Function $m_k(p)$"
    },
    {
      "source": "Trust Region Subproblem (52)",
      "type": "Constraint",
      "target": "Trust Region $\\|p\\| \\le \\Delta_k$"
    },
    {
      "source": "Trust Region Subproblem (52)",
      "type": "Characterized By",
      "target": "KKT Conditions (54)–(56)"
    },
    {
      "source": "Model Function ($m_k(p)$)",
      "type": "Components",
      "target": "$f_k = f(x_k)$, $g_k = \\nabla f(x_k)$, $B_k \\approx \\nabla^2 f(x_k)$"
    },
    {
      "source": "Model Function ($m_k(p)$)",
      "type": "Taylor Error",
      "target": "$O(\\|p\\|^3)$ when $B_k = \\nabla^2 f(x_k)$"
    },
    {
      "source": "Comparison Ratio ($\\rho_k$)",
      "type": "Numerator",
      "target": "Actual reduction $f(x_k) - f(x_k + p_k)$"
    },
    {
      "source": "Comparison Ratio ($\\rho_k$)",
      "type": "Denominator",
      "target": "Predicted reduction $m_k(0) - m_k(p_k)$"
    },
    {
      "source": "Krylov Subspace Approximation (TR Subproblem)",
      "type": "Approximates",
      "target": "Trust Region Subproblem (52)"
    },
    {
      "source": "Krylov Subspace Approximation (TR Subproblem)",
      "type": "Subspace",
      "target": "$\\mathcal{K}_2 = \\operatorname{span}\\{g_k, B_k^{-1}g_k\\}$"
    },
    {
      "source": "Krylov Subspace Approximation (TR Subproblem)",
      "type": "Related To",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Lagrangian for TR Subproblem ($\\mathcal{L}(p,\\lambda)$)",
      "type": "Yields KKT System",
      "target": "(54)–(56): $(B_k + \\lambda I)p = -g_k$, $\\|p\\| \\le \\Delta_k$, etc."
    },
    {
      "source": "Lagrangian for TR Subproblem ($\\mathcal{L}(p,\\lambda)$)",
      "type": "Constraint",
      "target": "$\\Delta_k^2 - \\|p\\|^2 \\ge 0$"
    },
    {
      "source": "Krylov Subspace Approximation (TRM)",
      "type": "Approximates Solution Of",
      "target": "Trust Region Subproblem (52)"
    },
    {
      "source": "Krylov Subspace Approximation (TRM)",
      "type": "Equivalent To",
      "target": "2-step Krylov method for $$ (I + 2\\lambda B^{-1})y = -q $$"
    },
    {
      "source": "Krylov Subspace Approximation (TRM)",
      "type": "Uses Subspace",
      "target": "$$ \\operatorname{span}\\{q, B^{-1}q\\} $$ (57)"
    },
    {
      "source": "Second Order Necessary Conditions (Constrained)",
      "type": "Formalized In",
      "target": "Theorem 12.5 (p. 332)"
    },
    {
      "source": "Second Order Necessary Conditions (Constrained)",
      "type": "Restricted To",
      "target": "Critical Cone $$ \\mathcal{C}(x^*, \\lambda^*) $$"
    },
    {
      "source": "Second Order Sufficient Conditions (Constrained)",
      "type": "Formalized In",
      "target": "Theorem 12.6 (p. 333)"
    },
    {
      "source": "Second Order Sufficient Conditions (Constrained)",
      "type": "Uses",
      "target": "Critical Cone $$ \\mathcal{C}(x^*, \\lambda^*) $$"
    },
    {
      "source": "Critical Cone ($$ \\mathcal{C}(x^*, \\lambda^*) $$)",
      "type": "Subset Of",
      "target": "Linearized Feasible Cone $$ \\mathcal{F}(x^*) $$"
    },
    {
      "source": "Critical Cone ($$ \\mathcal{C}(x^*, \\lambda^*) $$)",
      "type": "Orthogonal To",
      "target": "$$ \\nabla f(x^*) $$"
    },
    {
      "source": "Linear Program (LP) in Standard Form (64)",
      "type": "Converted From",
      "target": "LP in Canonical Form (65)"
    },
    {
      "source": "Linear Program (LP) in Standard Form (64)",
      "type": "Solved By",
      "target": "Simplex Method"
    },
    {
      "source": "Linear Program (LP) in Canonical Form (65)",
      "type": "Converted To",
      "target": "LP in Standard Form via $$ [A \\ I] \\begin{bmatrix} x \\\\ z \\end{bmatrix} = b $$ (66)"
    },
    {
      "source": "Slack Variables ($$ z $$)",
      "type": "Transforms",
      "target": "LP Canonical Form (65) → Standard Form (66)"
    },
    {
      "source": "Simplex Method",
      "type": "Applied To",
      "target": "Linear Program in Standard Form (64)"
    },
    {
      "source": "Simplex Method",
      "type": "Avoids",
      "target": "Enumerating $2^n$ KKT active-set combinations"
    },
    {
      "source": "Linear Program (LP) in Standard Form (64)",
      "type": "Assumed Condition",
      "target": "$m<n$ and $A$ has full row rank"
    },
    {
      "source": "Linear Program (LP) in Canonical Form (65)",
      "type": "Converted To",
      "target": "Linear Program (LP) in Standard Form (64)"
    },
    {
      "source": "Linear Program (LP) in Canonical Form (65)",
      "type": "Feasible Region Visualization",
      "target": "Polyhedron (polygon for $n=2$)"
    },
    {
      "source": "Simplex Method",
      "type": "Termination Condition (Optimal)",
      "target": "Reduced cost vector $s\\ge0$"
    },
    {
      "source": "Simplex Method",
      "type": "Termination Condition (Unbounded)",
      "target": "If $d = B^{-1}u \\le 0$ for the column $u$ of entering variable"
    },
    {
      "source": "Simplex Method",
      "type": "Worst Case Complexity",
      "target": "$\\binom{m}{n}$ (but typically $\\le 3m$ pivots)"
    },
    {
      "source": "Basic Feasible Solution (Vertex)",
      "type": "Components",
      "target": "$m$ basic variables (positive) and $n-m$ non-basic variables (zero)"
    },
    {
      "source": "Basic Feasible Solution (Vertex)",
      "type": "Condition for Feasibility",
      "target": "$B^{-1}b \\ge 0$"
    },
    {
      "source": "Minimum Ratio Rule",
      "type": "Uses Vector $u$",
      "target": "Column of $N$ corresponding to entering variable $x_j$"
    },
    {
      "source": "Critical Cone ($\\mathcal{C}(x^{*},\\lambda^{*})$)",
      "type": "Restricts",
      "target": "Hessian of Lagrangian $\\nabla_{xx}^{2}\\mathcal{L}(x^{*},\\lambda^{*})$"
    },
    {
      "source": "Second Order Necessary Condition (Eigenvalue Problem)",
      "type": "Related KKT Solution",
      "target": "Unit eigenvector $x^{*}$ with eigenvalue $\\lambda^{*}$"
    },
    {
      "source": "Second Order Necessary Condition (Eigenvalue Problem)",
      "type": "Related Lagrangian Hessian",
      "target": "$Q - \\lambda^{*}I$"
    },
    {
      "source": "Duality Theory (Convex)",
      "type": "Requires Convexity Of",
      "target": "$f(x)$"
    },
    {
      "source": "Duality Theory (Convex)",
      "type": "Requires Concavity Of",
      "target": "$c_j(x)$"
    },
    {
      "source": "Duality Theory (Convex Optimization)",
      "type": "Requires Convexity Of",
      "target": "$f(x)$"
    },
    {
      "source": "Duality Theory (Convex Optimization)",
      "type": "Requires Concavity Of",
      "target": "$c(x)$"
    },
    {
      "source": "Dual Objective Function ($q(\\lambda)$)",
      "type": "Is Infimum Of",
      "target": "Lagrangian $\\mathcal{L}(x,\\lambda)$"
    },
    {
      "source": "Dual Objective Function ($q(\\lambda)$)",
      "type": "Used To Formulate",
      "target": "Dual Problem (76)"
    },
    {
      "source": "Dual Problem (76)",
      "type": "Provides Bound For",
      "target": "Primal Problem (73)"
    },
    {
      "source": "Weak Duality",
      "type": "Formalized In",
      "target": "Theorem 12.11 (p. 345)"
    },
    {
      "source": "Weak Duality",
      "type": "Implies",
      "target": "Strong Duality (if additional conditions hold and equality is achieved)"
    },
    {
      "source": "Strong Duality (Theorem 12.12)",
      "type": "Requires Fulfillment Of",
      "target": "KKT conditions (at $x^* , \\lambda^*$)"
    },
    {
      "source": "Strong Duality (Theorem 12.12)",
      "type": "Formalized In",
      "target": "Theorem 12.12 (p. 346)"
    },
    {
      "source": "Dual Linear Program (79)",
      "type": "Primal Is",
      "target": "LP Problem (78)"
    },
    {
      "source": "Interior Point Methods",
      "type": "Addresses Difficulty Of",
      "target": "Inequality constraints ($2^m$ possible active sets)"
    },
    {
      "source": "Simplex Method Worst-Case Complexity",
      "type": "Motivated Replacement By",
      "target": "Polynomial-time LP algorithms (e.g., interior-point methods)"
    },
    {
      "source": "Simplex Method Worst-Case Complexity",
      "type": "Contrasts With Typical Performance",
      "target": "$\\le 2m$ or $3m$ pivots"
    },
    {
      "source": "Unconstrained Optimization",
      "type": "is_a",
      "target": "Continuous Optimization"
    },
    {
      "source": "Constrained Optimization Problem",
      "type": "is_a",
      "target": "Continuous Optimization"
    },
    {
      "source": "Linear Program (LP)",
      "type": "is_a",
      "target": "Constrained Optimization Problem"
    },
    {
      "source": "Nonlinear Least Squares Problems",
      "type": "is_a",
      "target": "Unconstrained Optimization"
    },
    {
      "source": "Least Squares Problems",
      "type": "is_a",
      "target": "Nonlinear Least Squares Problems"
    },
    {
      "source": "Steepest Descent Method",
      "type": "is_a",
      "target": "Line Search Methods"
    },
    {
      "source": "Newton's Method (Optimization)",
      "type": "is_a",
      "target": "Line Search Methods"
    },
    {
      "source": "Quasi-Newton Methods",
      "type": "is_a",
      "target": "Line Search Methods"
    },
    {
      "source": "Gauss–Newton Method",
      "type": "is_a",
      "target": "Line Search Methods"
    },
    {
      "source": "Trust Region Methods (TRMs)",
      "type": "alternative_to",
      "target": "Line Search Methods"
    },
    {
      "source": "Levenberg–Marquardt Method",
      "type": "is_a",
      "target": "Trust Region Methods (TRMs)"
    },
    {
      "source": "Direction of Steepest Descent",
      "type": "is_a",
      "target": "Descent Direction ($p_k$)"
    },
    {
      "source": "Newton's Method (Optimization)",
      "type": "uses_direction",
      "target": "Descent Direction ($p_k$)"
    },
    {
      "source": "Quasi-Newton Methods",
      "type": "computes",
      "target": "Descent Direction ($p_k$)"
    },
    {
      "source": "First-Order Necessary Condition (FONC)",
      "type": "generalized_by",
      "target": "Karush-Kuhn-Tucker (KKT) Conditions"
    },
    {
      "source": "Second-Order Sufficient Conditions (SOSC)",
      "type": "generalized_by",
      "target": "Second Order Sufficient Conditions (Constrained)"
    },
    {
      "source": "Second Order Necessary Conditions (Constrained)",
      "type": "requires",
      "target": "Critical Cone ($\\mathcal{C}(x^{*},\\lambda^{*})$)"
    },
    {
      "source": "Second Order Sufficient Conditions (Constrained)",
      "type": "requires",
      "target": "Critical Cone ($\\mathcal{C}(x^{*},\\lambda^{*})$)"
    },
    {
      "source": "Tangent Cone ($T_\\Omega(x)$)",
      "type": "contains",
      "target": "Critical Cone ($\\mathcal{C}(x^{*},\\lambda^{*})$)"
    },
    {
      "source": "Active Set of Indices ($\\mathcal{A}(x)$)",
      "type": "defines",
      "target": "Critical Cone ($\\mathcal{C}(x^{*},\\lambda^{*})$)"
    },
    {
      "source": "Linear Independence Constraint Qualification (LICQ)",
      "type": "ensures",
      "target": "Tangent Cone ($T_\\Omega(x)$) = Linearized Cone"
    },
    {
      "source": "Lagrangian Function ($\\mathcal{L}(x, \\lambda)$)",
      "type": "defines",
      "target": "Dual Objective Function ($q(\\lambda)$)"
    },
    {
      "source": "Dual Objective Function ($q(\\lambda)$)",
      "type": "maximized_in",
      "target": "Dual Problem (76)"
    },
    {
      "source": "Weak Duality",
      "type": "implies",
      "target": "Dual Problem (76) ≤ Primal"
    },
    {
      "source": "Strong Duality (Theorem 12.12)",
      "type": "holds_under",
      "target": "Slater's Condition"
    },
    {
      "source": "Strong Duality (Theorem 12.12)",
      "type": "holds_when",
      "target": "KKT satisfied"
    },
    {
      "source": "Linear Program (LP) in Canonical Form (65)",
      "type": "converted_to",
      "target": "Linear Program (LP) in Standard Form (64)"
    },
    {
      "source": "Slack Variables ($z$)",
      "type": "used_in",
      "target": "Conversion to Standard Form"
    },
    {
      "source": "Simplex Method",
      "type": "operates_on",
      "target": "Basic Feasible Solution (Vertex)"
    },
    {
      "source": "Simplex Method",
      "type": "uses",
      "target": "Minimum Ratio Rule"
    },
    {
      "source": "Simplex Method",
      "type": "terminates_when",
      "target": "All reduced costs ≥ 0"
    },
    {
      "source": "Residual Functions ($r_j(x)$)",
      "type": "define",
      "target": "Least Squares Problems"
    },
    {
      "source": "Jacobian J(x)",
      "type": "computed_from",
      "target": "Residual Functions ($r_j(x)$)"
    },
    {
      "source": "Gauss–Newton Method",
      "type": "approximates_Hessian_with",
      "target": "J^T J"
    },
    {
      "source": "Levenberg–Marquardt Method",
      "type": "regularizes",
      "target": "J^T J + μI"
    },
    {
      "source": "Linear Least Squares Problem (Linear Regression)",
      "type": "solved_by",
      "target": "Normal Equations"
    },
    {
      "source": "Linear Least Squares Problem (Linear Regression)",
      "type": "solved_stably_by",
      "target": "QR Decomposition"
    },
    {
      "source": "SR1 Update",
      "type": "is_a",
      "target": "Quasi-Newton Methods"
    },
    {
      "source": "BFGS method",
      "type": "is_a",
      "target": "Quasi-Newton Methods"
    },
    {
      "source": "Secant Condition",
      "type": "satisfied_by",
      "target": "SR1 Update"
    },
    {
      "source": "Secant Condition",
      "type": "satisfied_by",
      "target": "BFGS method"
    },
    {
      "source": "Steepest Descent Method",
      "type": "converges_with_rate",
      "target": "Linear (C = (κ-1)/(κ+1))"
    },
    {
      "source": "Newton's Method (Optimization)",
      "type": "converges_with_rate",
      "target": "Quadratic"
    },
    {
      "source": "Quasi-Newton Methods",
      "type": "converges_with_rate",
      "target": "Superlinear"
    },
    {
      "source": "Bisection Method (Step Length)",
      "type": "converges_with_rate",
      "target": "Linear (C=0.5)"
    },
    {
      "source": "Condition Number (κ(Q))",
      "type": "controls_rate_of",
      "target": "Steepest Descent Method"
    },
    {
      "source": "Trust Region Subproblem (52)",
      "type": "uses_model",
      "target": "Model Function ($m_k(p)$)"
    },
    {
      "source": "Trust Region Subproblem (52)",
      "type": "constrained_by",
      "target": "Trust Region Ball"
    },
    {
      "source": "Comparison Ratio ($\\rho_k$)",
      "type": "controls",
      "target": "Trust Region Radius Update"
    },
    {
      "source": "Krylov Subspace Approximation (TR Subproblem)",
      "type": "approximates",
      "target": "Trust Region Subproblem (52)"
    },
    {
      "source": "Steihaug-Toint CG",
      "type": "is_implementation_of",
      "target": "Krylov Subspace Approximation (TR Subproblem)"
    },
    {
      "source": "Bisection Method (Step Length)",
      "type": "used_in",
      "target": "Wolfe Line Search"
    },
    {
      "source": "Bisection Method (Step Length)",
      "type": "solves",
      "target": "φ'(α) = 0"
    },
    {
      "source": "Exact Line Search",
      "type": "ideal_for",
      "target": "Quadratic Function (Optimization Tool)"
    },
    {
      "source": "Quadratic Function (Optimization Tool)",
      "type": "derived_from",
      "target": "Taylor Expansion (2nd order)"
    },
    {
      "source": "Model Function ($m_k(p)$)",
      "type": "is",
      "target": "Quadratic Function (Optimization Tool)"
    },
    {
      "source": "Secant Condition",
      "type": "approximates",
      "target": "Hessian action via finite differences"
    },
    {
      "source": "Nonlinear Least Squares Problems",
      "type": "solved_by",
      "target": "Gauss–Newton Method"
    },
    {
      "source": "Nonlinear Least Squares Problems",
      "type": "solved_robustly_by",
      "target": "Levenberg–Marquardt Method"
    },
    {
      "source": "Linear Least Squares Problem (Linear Regression)",
      "type": "solved_by",
      "target": "QR or SVD"
    },
    {
      "source": "Constrained Optimization Problem",
      "type": "solved_by",
      "target": "Interior Point Methods"
    },
    {
      "source": "Constrained Optimization Problem",
      "type": "solved_by",
      "target": "SQP (Sequential Quadratic Programming)"
    },
    {
      "source": "Farkas' Lemma",
      "type": "proves",
      "target": "KKT Necessity"
    },
    {
      "source": "KKT Conditions",
      "type": "reduce_to",
      "target": "Normal Equations in Linear LS"
    },
    {
      "source": "Hessian Matrix",
      "type": "used_in",
      "target": "Newton's Method (Optimization)"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "ensures",
      "target": "Unique Minimizer of Quadratic"
    },
    {
      "source": "Saddle Point",
      "type": "has",
      "target": "Indefinite Hessian"
    },
    {
      "source": "Gauss–Newton Method",
      "type": "extends",
      "target": "Newton's Method (Optimization)"
    },
    {
      "source": "Levenberg–Marquardt Method",
      "type": "extends",
      "target": "Gauss–Newton Method"
    },
    {
      "source": "Trust Region Methods (TRMs)",
      "type": "related_to",
      "target": "Levenberg–Marquardt Method"
    },
    {
      "source": "Quasi-Newton Methods",
      "type": "generalizes",
      "target": "BFGS method"
    },
    {
      "source": "SR1 Update",
      "type": "is_alternative_to",
      "target": "BFGS method"
    },
    {
      "source": "Trust Region Subproblem (52)",
      "type": "solved_by",
      "target": "Steihaug-Toint CG"
    },
    {
      "source": "Newton's Method (Optimization)",
      "type": "approximated_by",
      "target": "Quasi-Newton Methods"
    },
    {
      "source": "Unconstrained Optimization",
      "type": "foundation_for",
      "target": "Constrained Optimization Problem"
    },
    {
      "source": "Karush-Kuhn-Tucker (KKT) Conditions",
      "type": "imply",
      "target": "Strong Duality (Theorem 12.12)"
    },
    {
      "source": "Second Order Necessary Conditions (Constrained)",
      "type": "applied_in",
      "target": "Eigenvalue Optimization Problems"
    },
    {
      "source": "Trust Region Methods (TRMs)",
      "type": "connects_to",
      "target": "Global Convergence Theorems"
    },
    {
      "source": "Pinhole Camera Model",
      "type": "extends",
      "target": "Camera extrinsics"
    },
    {
      "source": "Pinhole Camera Model",
      "type": "used_in",
      "target": "Triangulation"
    },
    {
      "source": "Pinhole Camera Model",
      "type": "foundation_for",
      "target": "Epipolar constraint"
    },
    {
      "source": "Pinhole Camera Model",
      "type": "contrasts_with",
      "target": "Perspective-3-point (P3P) problem"
    },
    {
      "source": "Radial Distortion",
      "type": "related_to",
      "target": "Pinhole Camera Model"
    },
    {
      "source": "Radial Distortion",
      "type": "corrected_by",
      "target": "Camera calibration"
    },
    {
      "source": "Radial Distortion",
      "type": "influences",
      "target": "Triangulation accuracy"
    },
    {
      "source": "HSV Color Space",
      "type": "used_in",
      "target": "Image segmentation"
    },
    {
      "source": "HSV Color Space",
      "type": "contrasts_with",
      "target": "RGB color space"
    },
    {
      "source": "HSV Color Space",
      "type": "supports",
      "target": "Texture analysis"
    },
    {
      "source": "Depth of Field",
      "type": "related_to",
      "target": "Aperture problem"
    },
    {
      "source": "Depth of Field",
      "type": "affects",
      "target": "Structure-from-Motion"
    },
    {
      "source": "Depth of Field",
      "type": "used_in",
      "target": "Depth estimation"
    },
    {
      "source": "Epipolar Constraint",
      "type": "extends",
      "target": "Pinhole Camera Model"
    },
    {
      "source": "Epipolar Constraint",
      "type": "used_in",
      "target": "Triangulation"
    },
    {
      "source": "Epipolar Constraint",
      "type": "foundation_for",
      "target": "Stereo matching"
    },
    {
      "source": "Epipolar Constraint",
      "type": "requires",
      "target": "Projection matrices"
    },
    {
      "source": "Aperture Problem",
      "type": "related_to",
      "target": "Optical flow"
    },
    {
      "source": "Aperture Problem",
      "type": "addressed_by",
      "target": "Lucas–Kanade method"
    },
    {
      "source": "Aperture Problem",
      "type": "contrasts_with",
      "target": "Depth of Field"
    },
    {
      "source": "Metamers",
      "type": "related_to",
      "target": "HSV Color Space"
    },
    {
      "source": "Metamers",
      "type": "contrasts_with",
      "target": "Chromatic aberration"
    },
    {
      "source": "Metamers",
      "type": "used_in",
      "target": "Color constancy models"
    },
    {
      "source": "Chromatic Aberration",
      "type": "contrasts_with",
      "target": "Metamers"
    },
    {
      "source": "Chromatic Aberration",
      "type": "influences",
      "target": "Color calibration"
    },
    {
      "source": "Chromatic Aberration",
      "type": "affects",
      "target": "Image quality assessment"
    },
    {
      "source": "Camera Extrinsics",
      "type": "extends",
      "target": "Pinhole Camera Model"
    },
    {
      "source": "Camera Extrinsics",
      "type": "used_in",
      "target": "Triangulation"
    },
    {
      "source": "Camera Extrinsics",
      "type": "required_for",
      "target": "Structure-from-Motion"
    },
    {
      "source": "SIFT",
      "type": "extends",
      "target": "Feature descriptor"
    },
    {
      "source": "SIFT",
      "type": "used_in",
      "target": "Bag-of-Words"
    },
    {
      "source": "SIFT",
      "type": "related_to",
      "target": "Harris Corner Detector"
    },
    {
      "source": "SIFT",
      "type": "supports",
      "target": "Structure-from-Motion"
    },
    {
      "source": "Harris Corner Detector",
      "type": "foundation_for",
      "target": "SIFT"
    },
    {
      "source": "Harris Corner Detector",
      "type": "used_in",
      "target": "Optical flow"
    },
    {
      "source": "Harris Corner Detector",
      "type": "related_to",
      "target": "K-means clustering"
    },
    {
      "source": "Harris Corner Detector",
      "type": "evaluated_by",
      "target": "Precision-Recall"
    },
    {
      "source": "Hough Transform",
      "type": "used_in",
      "target": "Image segmentation"
    },
    {
      "source": "Hough Transform",
      "type": "contrasts_with",
      "target": "RANSAC"
    },
    {
      "source": "Hough Transform",
      "type": "supports",
      "target": "Shape recognition"
    },
    {
      "source": "Hough Transform",
      "type": "requires",
      "target": "Edge detection"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "related_to",
      "target": "Least squares estimation"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "used_in",
      "target": "Triangulation"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "contrasts_with",
      "target": "Hough Transform"
    },
    {
      "source": "Random Sample Consensus (RANSAC)",
      "type": "foundation_for",
      "target": "Affine transformation estimation"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "extends",
      "target": "Texture analysis"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "used_with",
      "target": "Euclidean distance"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "compared_to",
      "target": "Filter bank methods"
    },
    {
      "source": "Local Binary Patterns (LBP)",
      "type": "supports",
      "target": "Seashell classification"
    },
    {
      "source": "K-Means Clustering",
      "type": "used_in",
      "target": "Image segmentation"
    },
    {
      "source": "K-Means Clustering",
      "type": "foundation_for",
      "target": "Bag-of-Words"
    },
    {
      "source": "K-Means Clustering",
      "type": "related_to",
      "target": "Unsupervised learning"
    },
    {
      "source": "K-Means Clustering",
      "type": "evaluated_by",
      "target": "Confusion matrix"
    },
    {
      "source": "Otsu’s Thresholding Method",
      "type": "used_in",
      "target": "Background subtraction"
    },
    {
      "source": "Otsu’s Thresholding Method",
      "type": "contrasts_with",
      "target": "K-Means Clustering"
    },
    {
      "source": "Otsu’s Thresholding Method",
      "type": "supports",
      "target": "Image segmentation"
    },
    {
      "source": "Harris–Laplace Detector",
      "type": "extends",
      "target": "Harris Corner Detector"
    },
    {
      "source": "Harris–Laplace Detector",
      "type": "related_to",
      "target": "SIFT"
    },
    {
      "source": "Harris–Laplace Detector",
      "type": "supports",
      "target": "Structure-from-Motion"
    },
    {
      "source": "Photometric Stereo",
      "type": "used_in",
      "target": "Surface reconstruction"
    },
    {
      "source": "Photometric Stereo",
      "type": "related_to",
      "target": "Diffuse reflection"
    },
    {
      "source": "Photometric Stereo",
      "type": "supports",
      "target": "Structure-from-Motion"
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "type": "extends",
      "target": "Feature descriptor"
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "type": "related_to",
      "target": "Bag-of-Words"
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "type": "used_in",
      "target": "Image segmentation"
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "type": "contrasts_with",
      "target": "SIFT"
    },
    {
      "source": "Bag-of-Words (BoW) Representation",
      "type": "relies_on",
      "target": "Feature descriptor"
    },
    {
      "source": "Bag-of-Words (BoW) Representation",
      "type": "uses",
      "target": "K-Means Clustering"
    },
    {
      "source": "Bag-of-Words (BoW) Representation",
      "type": "related_to",
      "target": "Visual words"
    },
    {
      "source": "Bag-of-Words (BoW) Representation",
      "type": "contrasts_with",
      "target": "Spatial pyramid matching"
    },
    {
      "source": "Image segmentation",
      "type": "is_a_goal_of",
      "target": "Computer Vision"
    },
    {
      "source": "Image segmentation",
      "type": "uses",
      "target": "Thresholding"
    },
    {
      "source": "Image segmentation",
      "type": "uses",
      "target": "K-Means Clustering"
    },
    {
      "source": "Image segmentation",
      "type": "related_to",
      "target": "Object detection"
    },
    {
      "source": "Background Subtraction",
      "type": "extends",
      "target": "Image segmentation"
    },
    {
      "source": "Background Subtraction",
      "type": "uses",
      "target": "Otsu’s Thresholding Method"
    },
    {
      "source": "Background Subtraction",
      "type": "supports",
      "target": "Optical flow"
    },
    {
      "source": "Background Subtraction",
      "type": "related_to",
      "target": "Motion estimation"
    },
    {
      "source": "Image Segmentation",
      "type": "foundation_for",
      "target": "Background Subtraction"
    },
    {
      "source": "Image Segmentation",
      "type": "used_with",
      "target": "HSV Color Space"
    },
    {
      "source": "Image Segmentation",
      "type": "supported_by",
      "target": "Otsu’s Thresholding Method"
    },
    {
      "source": "Image Segmentation",
      "type": "used_in",
      "target": "Texture classification"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "extends",
      "target": "Optical flow"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "addresses",
      "target": "Aperture Problem"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "used_in",
      "target": "Stereo matching"
    },
    {
      "source": "Lucas–Kanade Optical Flow",
      "type": "supports",
      "target": "Structure-from-Motion"
    },
    {
      "source": "Structure-from-Motion (SfM)",
      "type": "extends",
      "target": "Triangulation"
    },
    {
      "source": "Structure-from-Motion (SfM)",
      "type": "requires",
      "target": "Camera Extrinsics"
    },
    {
      "source": "Structure-from-Motion (SfM)",
      "type": "related_to",
      "target": "Epipolar Constraint"
    },
    {
      "source": "Structure-from-Motion (SfM)",
      "type": "used_with",
      "target": "RANSAC"
    },
    {
      "source": "Texture Analysis",
      "type": "foundation_for",
      "target": "Local Binary Patterns (LBP)"
    },
    {
      "source": "Texture Analysis",
      "type": "related_to",
      "target": "Texture Classification"
    },
    {
      "source": "Texture Analysis",
      "type": "used_with",
      "target": "Color features"
    },
    {
      "source": "Texture Analysis",
      "type": "evaluated_by",
      "target": "Mahalanobis distance"
    },
    {
      "source": "Filter Bank Methods",
      "type": "contrasts_with",
      "target": "Local Binary Patterns (LBP)"
    },
    {
      "source": "Filter Bank Methods",
      "type": "supports",
      "target": "Texture Classification"
    },
    {
      "source": "Filter Bank Methods",
      "type": "evaluated_by",
      "target": "Euclidean distance"
    },
    {
      "source": "Filter Bank Methods",
      "type": "used_with",
      "target": "K-Means Clustering"
    },
    {
      "source": "Edge Detection",
      "type": "foundation_for",
      "target": "Hough Transform"
    },
    {
      "source": "Edge Detection",
      "type": "used_with",
      "target": "RANSAC"
    },
    {
      "source": "Edge Detection",
      "type": "supports",
      "target": "Segmentation"
    },
    {
      "source": "Edge Detection",
      "type": "related_to",
      "target": "Optical Flow"
    },
    {
      "source": "Graph-Based Segmentation",
      "type": "extends",
      "target": "Image segmentation"
    },
    {
      "source": "Graph-Based Segmentation",
      "type": "related_to",
      "target": "K-Means Clustering"
    },
    {
      "source": "Graph-Based Segmentation",
      "type": "supports",
      "target": "Object detection"
    },
    {
      "source": "Graph-Based Segmentation",
      "type": "evaluated_by",
      "target": "Confusion matrix"
    },
    {
      "source": "Mahalanobis Distance",
      "type": "generalizes",
      "target": "Euclidean Distance"
    },
    {
      "source": "Mahalanobis Distance",
      "type": "used_in",
      "target": "k-Nearest Neighbor Classification"
    },
    {
      "source": "Mahalanobis Distance",
      "type": "used_in",
      "target": "Gaussian Mixture Models"
    },
    {
      "source": "Mahalanobis Distance",
      "type": "related_to",
      "target": "Covariance Matrix"
    },
    {
      "source": "Mahalanobis Distance",
      "type": "supports",
      "target": "Feature Normalization"
    },
    {
      "source": "K-Nearest Neighbor Classification",
      "type": "uses",
      "target": "Mahalanobis Distance"
    },
    {
      "source": "K-Nearest Neighbor Classification",
      "type": "uses",
      "target": "Euclidean Distance"
    },
    {
      "source": "K-Nearest Neighbor Classification",
      "type": "uses",
      "target": "Chi-Square Distance"
    },
    {
      "source": "K-Nearest Neighbor Classification",
      "type": "part_of",
      "target": "Pattern Recognition"
    },
    {
      "source": "K-Nearest Neighbor Classification",
      "type": "alternative_to",
      "target": "Support Vector Machine"
    },
    {
      "source": "K-Nearest Neighbor Classification",
      "type": "requires",
      "target": "Feature Extraction"
    },
    {
      "source": "Gabor Filters",
      "type": "inspired_by",
      "target": "Human Visual System"
    },
    {
      "source": "Gabor Filters",
      "type": "used_in",
      "target": "Texture Analysis"
    },
    {
      "source": "Gabor Filters",
      "type": "alternative_to",
      "target": "Gray-Level Co-occurrence Matrix"
    },
    {
      "source": "Gabor Filters",
      "type": "part_of",
      "target": "Filter Banks"
    },
    {
      "source": "Gabor Filters",
      "type": "supports",
      "target": "Local Binary Patterns"
    },
    {
      "source": "Gabor Filters",
      "type": "related_to",
      "target": "Wavelet Transform"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "used_in",
      "target": "Texture Classification"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "used_with",
      "target": "K-Nearest Neighbor Classification"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "used_with",
      "target": "Support Vector Machine"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "alternative_to",
      "target": "Gabor Filters"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "precedes",
      "target": "Haralick Features"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "requires",
      "target": "Image Quantization"
    },
    {
      "source": "Binary Mask",
      "type": "used_in",
      "target": "Connected Component Labeling"
    },
    {
      "source": "Binary Mask",
      "type": "used_in",
      "target": "Morphological Operations"
    },
    {
      "source": "Binary Mask",
      "type": "produced_by",
      "target": "Thresholding"
    },
    {
      "source": "Binary Mask",
      "type": "enables",
      "target": "Region of Interest (ROI) Analysis"
    },
    {
      "source": "Binary Mask",
      "type": "supports",
      "target": "Image Segmentation"
    },
    {
      "source": "Erosion",
      "type": "part_of",
      "target": "Morphological Operations"
    },
    {
      "source": "Erosion",
      "type": "used_with",
      "target": "Structuring Element"
    },
    {
      "source": "Erosion",
      "type": "precedes",
      "target": "Dilation"
    },
    {
      "source": "Erosion",
      "type": "composes",
      "target": "Opening"
    },
    {
      "source": "Erosion",
      "type": "used_in",
      "target": "Binary Image Preprocessing"
    },
    {
      "source": "Erosion",
      "type": "inverse_of",
      "target": "Dilation"
    },
    {
      "source": "Dilation",
      "type": "part_of",
      "target": "Morphological Operations"
    },
    {
      "source": "Dilation",
      "type": "used_with",
      "target": "Structuring Element"
    },
    {
      "source": "Dilation",
      "type": "follows",
      "target": "Erosion"
    },
    {
      "source": "Dilation",
      "type": "composes",
      "target": "Closing"
    },
    {
      "source": "Dilation",
      "type": "used_in",
      "target": "Binary Image Enhancement"
    },
    {
      "source": "Dilation",
      "type": "inverse_of",
      "target": "Erosion"
    },
    {
      "source": "Opening",
      "type": "composed_of",
      "target": "Erosion"
    },
    {
      "source": "Opening",
      "type": "composed_of",
      "target": "Dilation"
    },
    {
      "source": "Opening",
      "type": "part_of",
      "target": "Morphological Operations"
    },
    {
      "source": "Opening",
      "type": "used_in",
      "target": "Image Segmentation Pipeline"
    },
    {
      "source": "Opening",
      "type": "improves",
      "target": "Connected Component Labeling"
    },
    {
      "source": "Opening",
      "type": "alternative_to",
      "target": "Median Filtering"
    },
    {
      "source": "Closing",
      "type": "composed_of",
      "target": "Dilation"
    },
    {
      "source": "Closing",
      "type": "composed_of",
      "target": "Erosion"
    },
    {
      "source": "Closing",
      "type": "part_of",
      "target": "Morphological Operations"
    },
    {
      "source": "Closing",
      "type": "used_in",
      "target": "Image Segmentation Pipeline"
    },
    {
      "source": "Closing",
      "type": "complements",
      "target": "Opening"
    },
    {
      "source": "Closing",
      "type": "improves",
      "target": "Region Integrity"
    },
    {
      "source": "Connected Component Labeling",
      "type": "requires",
      "target": "Binary Mask"
    },
    {
      "source": "Connected Component Labeling",
      "type": "improved_by",
      "target": "Opening"
    },
    {
      "source": "Connected Component Labeling",
      "type": "improved_by",
      "target": "Closing"
    },
    {
      "source": "Connected Component Labeling",
      "type": "enables",
      "target": "Per-Object Feature Extraction"
    },
    {
      "source": "Connected Component Labeling",
      "type": "part_of",
      "target": "Image Segmentation"
    },
    {
      "source": "Connected Component Labeling",
      "type": "used_in",
      "target": "Object Detection Pipeline"
    },
    {
      "source": "Computer Vision",
      "type": "related_to",
      "target": "Artificial Intelligence"
    },
    {
      "source": "Computer Vision",
      "type": "related_to",
      "target": "Machine Learning"
    },
    {
      "source": "Computer Vision",
      "type": "related_to",
      "target": "Image Processing"
    },
    {
      "source": "Computer Vision",
      "type": "used_in",
      "target": "Image Recognition"
    },
    {
      "source": "Computer Vision",
      "type": "used_in",
      "target": "3D Perception"
    },
    {
      "source": "Machine Vision",
      "type": "is_a",
      "target": "Computer Vision"
    },
    {
      "source": "Machine Vision",
      "type": "contrasted_with",
      "target": "Computer Vision"
    },
    {
      "source": "Image Processing",
      "type": "supports",
      "target": "Computer Vision"
    },
    {
      "source": "Image Processing",
      "type": "differs_from",
      "target": "Image Understanding"
    },
    {
      "source": "Image Understanding",
      "type": "part_of",
      "target": "Computer Vision"
    },
    {
      "source": "Image Understanding",
      "type": "requires",
      "target": "Image Processing"
    },
    {
      "source": "Image Recognition",
      "type": "part_of",
      "target": "Computer Vision"
    },
    {
      "source": "Image Recognition",
      "type": "uses",
      "target": "Machine Learning"
    },
    {
      "source": "3D Perception",
      "type": "part_of",
      "target": "Computer Vision"
    },
    {
      "source": "3D Perception",
      "type": "related_to",
      "target": "Human Visual System"
    },
    {
      "source": "3D Perception",
      "type": "uses",
      "target": "Perceptual Cues"
    },
    {
      "source": "Multi-view Stereo",
      "type": "used_for",
      "target": "3D Perception"
    },
    {
      "source": "Multi-view Stereo",
      "type": "part_of",
      "target": "Structure from Motion"
    },
    {
      "source": "Computational Photography",
      "type": "extends",
      "target": "Image Processing"
    },
    {
      "source": "Computational Photography",
      "type": "part_of",
      "target": "Computer Vision"
    },
    {
      "source": "Pinhole Model",
      "type": "basis_for",
      "target": "Perspective Projection"
    },
    {
      "source": "Pinhole Model",
      "type": "approximated_by",
      "target": "Camera"
    },
    {
      "source": "Pinhole Model",
      "type": "used_in",
      "target": "Camera Calibration"
    },
    {
      "source": "Pinhole Model",
      "type": "related_to",
      "target": "Camera Obscura"
    },
    {
      "source": "Perspective Projection",
      "type": "extends",
      "target": "Pinhole Model"
    },
    {
      "source": "Perspective Projection",
      "type": "causes",
      "target": "Perspective Distortion"
    },
    {
      "source": "Perspective Projection",
      "type": "used_in",
      "target": "3D Perception"
    },
    {
      "source": "Perspective Projection",
      "type": "inverse_of",
      "target": "Triangulation"
    },
    {
      "source": "Camera Obscura",
      "type": "inspired",
      "target": "Pinhole Model"
    },
    {
      "source": "Camera Obscura",
      "type": "predecessor_of",
      "target": "Photographic Camera"
    },
    {
      "source": "Camera Obscura",
      "type": "demonstrates",
      "target": "Perspective Projection"
    },
    {
      "source": "Perspective Distortion",
      "type": "result_of",
      "target": "Perspective Projection"
    },
    {
      "source": "Perspective Distortion",
      "type": "differs_from",
      "target": "Lens Distortion"
    },
    {
      "source": "Perspective Distortion",
      "type": "mitigated_by",
      "target": "Viewpoint Normalization"
    },
    {
      "source": "Focal Length",
      "type": "parameter_in",
      "target": "Pinhole Model"
    },
    {
      "source": "Focal Length",
      "type": "affects",
      "target": "Field of View"
    },
    {
      "source": "Focal Length",
      "type": "used_in",
      "target": "Perspective Projection"
    },
    {
      "source": "Image Plane",
      "type": "part_of",
      "target": "Pinhole Model"
    },
    {
      "source": "Image Plane",
      "type": "corresponds_to",
      "target": "Sensor Plane"
    },
    {
      "source": "Image Plane",
      "type": "receives",
      "target": "Projected Rays"
    },
    {
      "source": "Simplified Imaging Model",
      "type": "includes",
      "target": "Geometric Model"
    },
    {
      "source": "Simplified Imaging Model",
      "type": "includes",
      "target": "Optical Model"
    },
    {
      "source": "Simplified Imaging Model",
      "type": "includes",
      "target": "Sensor Model"
    },
    {
      "source": "Simplified Imaging Model",
      "type": "foundation_for",
      "target": "Camera Calibration"
    },
    {
      "source": "Specular Reflection",
      "type": "contrasts_with",
      "target": "Diffuse Reflection"
    },
    {
      "source": "Specular Reflection",
      "type": "part_of",
      "target": "Bidirectional Reflectance Distribution Function"
    },
    {
      "source": "Specular Reflection",
      "type": "used_in",
      "target": "Phong Reflection Model"
    },
    {
      "source": "Specular Reflection",
      "type": "related_to",
      "target": "Fresnel Equations"
    },
    {
      "source": "Diffuse Reflection",
      "type": "contrasts_with",
      "target": "Specular Reflection"
    },
    {
      "source": "Diffuse Reflection",
      "type": "part_of",
      "target": "Bidirectional Reflectance Distribution Function"
    },
    {
      "source": "Diffuse Reflection",
      "type": "modeled_by",
      "target": "Lambertian Reflectance"
    },
    {
      "source": "Diffuse Reflection",
      "type": "used_in",
      "target": "Shape from Shading"
    },
    {
      "source": "Light",
      "type": "measured_in",
      "target": "Wavelength"
    },
    {
      "source": "Light",
      "type": "interacts_via",
      "target": "Reflection"
    },
    {
      "source": "Light",
      "type": "interacts_via",
      "target": "Transmission"
    },
    {
      "source": "Light",
      "type": "interacts_via",
      "target": "Absorption"
    },
    {
      "source": "Light",
      "type": "perceived_as",
      "target": "Color"
    },
    {
      "source": "Visible Spectrum",
      "type": "part_of",
      "target": "Electromagnetic Spectrum"
    },
    {
      "source": "Visible Spectrum",
      "type": "perceived_by",
      "target": "Human Visual System"
    },
    {
      "source": "Visible Spectrum",
      "type": "captured_by",
      "target": "RGB Sensor"
    },
    {
      "source": "Visible Spectrum",
      "type": "basis_for",
      "target": "Color"
    },
    {
      "source": "Illuminating Source",
      "type": "contrasts_with",
      "target": "Reflecting Source"
    },
    {
      "source": "Illuminating Source",
      "type": "follows",
      "target": "Additive Rule"
    },
    {
      "source": "Illuminating Source",
      "type": "emits",
      "target": "Light"
    },
    {
      "source": "Illuminating Source",
      "type": "affects",
      "target": "Scene Illumination"
    },
    {
      "source": "Reflecting Source",
      "type": "contrasts_with",
      "target": "Illuminating Source"
    },
    {
      "source": "Reflecting Source",
      "type": "follows",
      "target": "Subtractive Rule"
    },
    {
      "source": "Reflecting Source",
      "type": "reflects",
      "target": "Light"
    },
    {
      "source": "Reflecting Source",
      "type": "depends_on",
      "target": "Surface Material"
    },
    {
      "source": "Additive Rule",
      "type": "applies_to",
      "target": "Illuminating Source"
    },
    {
      "source": "Additive Rule",
      "type": "basis_for",
      "target": "RGB Color Model"
    },
    {
      "source": "Additive Rule",
      "type": "contrasts_with",
      "target": "Subtractive Rule"
    },
    {
      "source": "Subtractive Rule",
      "type": "applies_to",
      "target": "Reflecting Source"
    },
    {
      "source": "Subtractive Rule",
      "type": "basis_for",
      "target": "CMYK Color Model"
    },
    {
      "source": "Subtractive Rule",
      "type": "contrasts_with",
      "target": "Additive Rule"
    },
    {
      "source": "Reflection",
      "type": "includes",
      "target": "Specular Reflection"
    },
    {
      "source": "Reflection",
      "type": "includes",
      "target": "Diffuse Reflection"
    },
    {
      "source": "Reflection",
      "type": "contrasts_with",
      "target": "Transmission"
    },
    {
      "source": "Reflection",
      "type": "contrasts_with",
      "target": "Absorption"
    },
    {
      "source": "Reflection",
      "type": "governed_by",
      "target": "Surface Properties"
    },
    {
      "source": "Transmission",
      "type": "contrasts_with",
      "target": "Reflection"
    },
    {
      "source": "Transmission",
      "type": "contrasts_with",
      "target": "Absorption"
    },
    {
      "source": "Transmission",
      "type": "causes",
      "target": "Refraction"
    },
    {
      "source": "Transmission",
      "type": "modeled_in",
      "target": "Optical Model"
    },
    {
      "source": "Absorption",
      "type": "contrasts_with",
      "target": "Reflection"
    },
    {
      "source": "Absorption",
      "type": "contrasts_with",
      "target": "Transmission"
    },
    {
      "source": "Absorption",
      "type": "determines",
      "target": "Perceived Color"
    },
    {
      "source": "Absorption",
      "type": "modeled_by",
      "target": "Spectral Absorption Curve"
    },
    {
      "source": "Binary Image",
      "type": "produced_by",
      "target": "Thresholding"
    },
    {
      "source": "Binary Image",
      "type": "input_to",
      "target": "Morphological Operations"
    },
    {
      "source": "Binary Image",
      "type": "used_in",
      "target": "Connected Component Labeling"
    },
    {
      "source": "Binary Image",
      "type": "basis_for",
      "target": "Shape Analysis"
    },
    {
      "source": "Thresholding",
      "type": "produces",
      "target": "Binary Image"
    },
    {
      "source": "Thresholding",
      "type": "part_of",
      "target": "Segmentation"
    },
    {
      "source": "Thresholding",
      "type": "precedes",
      "target": "Morphological Processing"
    },
    {
      "source": "Thresholding",
      "type": "enhanced_by",
      "target": "Preprocessing"
    },
    {
      "source": "Preprocessing",
      "type": "precedes",
      "target": "Segmentation"
    },
    {
      "source": "Preprocessing",
      "type": "improves",
      "target": "Thresholding"
    },
    {
      "source": "Preprocessing",
      "type": "part_of",
      "target": "Binary Image Analysis Pipeline"
    },
    {
      "source": "Preprocessing",
      "type": "uses",
      "target": "Image Filtering"
    },
    {
      "source": "Segmentation",
      "type": "follows",
      "target": "Preprocessing"
    },
    {
      "source": "Segmentation",
      "type": "precedes",
      "target": "Post-processing"
    },
    {
      "source": "Segmentation",
      "type": "produces",
      "target": "Binary Image"
    },
    {
      "source": "Segmentation",
      "type": "enables",
      "target": "Shape Representation"
    },
    {
      "source": "Post-processing",
      "type": "follows",
      "target": "Segmentation"
    },
    {
      "source": "Post-processing",
      "type": "precedes",
      "target": "Labeling"
    },
    {
      "source": "Post-processing",
      "type": "uses",
      "target": "Morphological Operations"
    },
    {
      "source": "Post-processing",
      "type": "improves",
      "target": "Binary Image"
    },
    {
      "source": "Shape Representation",
      "type": "follows",
      "target": "Labeling"
    },
    {
      "source": "Shape Representation",
      "type": "input_to",
      "target": "Recognition"
    },
    {
      "source": "Shape Representation",
      "type": "uses",
      "target": "Connected Components"
    },
    {
      "source": "Shape Representation",
      "type": "extracts",
      "target": "Geometric Features"
    },
    {
      "source": "Labeling",
      "type": "follows",
      "target": "Post-processing"
    },
    {
      "source": "Labeling",
      "type": "precedes",
      "target": "Shape Representation"
    },
    {
      "source": "Labeling",
      "type": "uses",
      "target": "Connected Components"
    },
    {
      "source": "Labeling",
      "type": "enables",
      "target": "Object Measurement"
    },
    {
      "source": "Recognition",
      "type": "follows",
      "target": "Shape Representation"
    },
    {
      "source": "Recognition",
      "type": "uses",
      "target": "Geometric Features"
    },
    {
      "source": "Recognition",
      "type": "part_of",
      "target": "Binary Image Analysis Pipeline"
    },
    {
      "source": "Recognition",
      "type": "outputs",
      "target": "Semantic Labels"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Preprocessing"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Segmentation"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Post-processing"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Labeling"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Shape Representation"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "includes",
      "target": "Recognition"
    },
    {
      "source": "Binary Image Analysis Pipeline",
      "type": "foundation_for",
      "target": "Classical Computer Vision"
    },
    {
      "source": "Texture",
      "type": "complements",
      "target": "Color"
    },
    {
      "source": "Texture",
      "type": "used_in",
      "target": "Texture Segmentation"
    },
    {
      "source": "Texture",
      "type": "analyzed_by",
      "target": "Texture Descriptors"
    },
    {
      "source": "Texture",
      "type": "differs_from",
      "target": "Shape"
    },
    {
      "source": "Texture",
      "type": "related_to",
      "target": "Surface Material"
    },
    {
      "source": "Texture Segmentation",
      "type": "extends",
      "target": "Segmentation"
    },
    {
      "source": "Texture Segmentation",
      "type": "uses",
      "target": "Texture Descriptors"
    },
    {
      "source": "Texture Segmentation",
      "type": "precedes",
      "target": "Object Recognition"
    },
    {
      "source": "Texture Segmentation",
      "type": "part_of",
      "target": "Mid-level Vision"
    },
    {
      "source": "Texture Descriptors",
      "type": "input_to",
      "target": "Texture Segmentation"
    },
    {
      "source": "Texture Descriptors",
      "type": "output_of",
      "target": "Feature Extraction"
    },
    {
      "source": "Texture Descriptors",
      "type": "enables",
      "target": "Texture Classification"
    },
    {
      "source": "Texture Descriptors",
      "type": "includes",
      "target": "Local Binary Patterns"
    },
    {
      "source": "Local Binary Patterns",
      "type": "is_a",
      "target": "Texture Descriptors"
    },
    {
      "source": "Local Binary Patterns",
      "type": "robust_to",
      "target": "Illumination Changes"
    },
    {
      "source": "Local Binary Patterns",
      "type": "used_in",
      "target": "Texture Classification"
    },
    {
      "source": "Local Binary Patterns",
      "type": "extended_by",
      "target": "Multi-scale LBP"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "includes",
      "target": "Gray-Level Co-occurrence Matrix"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "contrasts_with",
      "target": "Structural Texture Analysis"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "foundation_for",
      "target": "Texture Classification"
    },
    {
      "source": "Statistical Texture Analysis",
      "type": "used_in",
      "target": "Texture Segmentation"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "part_of",
      "target": "Statistical Texture Analysis"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "input_to",
      "target": "Haralick Features"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "analyzes",
      "target": "Spatial Dependency"
    },
    {
      "source": "Gray-Level Co-occurrence Matrix",
      "type": "used_in",
      "target": "Texture Descriptors"
    },
    {
      "source": "Filter Bank Methods",
      "type": "is_a",
      "target": "Texture Descriptors"
    },
    {
      "source": "Filter Bank Methods",
      "type": "inspired_by",
      "target": "Human Visual Cortex"
    },
    {
      "source": "Filter Bank Methods",
      "type": "used_in",
      "target": "Texture Segmentation"
    },
    {
      "source": "Filter Bank Methods",
      "type": "includes",
      "target": "Gabor Filters"
    },
    {
      "source": "Structural Texture Analysis",
      "type": "contrasts_with",
      "target": "Statistical Texture Analysis"
    },
    {
      "source": "Structural Texture Analysis",
      "type": "uses",
      "target": "Textons"
    },
    {
      "source": "Structural Texture Analysis",
      "type": "applies_to",
      "target": "Regular Textures"
    },
    {
      "source": "Structural Texture Analysis",
      "type": "foundation_for",
      "target": "Texture Synthesis"
    },
    {
      "source": "Local Features",
      "type": "used_in",
      "target": "Image Matching"
    },
    {
      "source": "Local Features",
      "type": "enables",
      "target": "3D Reconstruction"
    },
    {
      "source": "Local Features",
      "type": "includes",
      "target": "Keypoint Detection"
    },
    {
      "source": "Local Features",
      "type": "includes",
      "target": "Feature Description"
    },
    {
      "source": "Local Features",
      "type": "precedes",
      "target": "Feature Matching"
    },
    {
      "source": "Harris Corner Detector",
      "type": "is_a",
      "target": "Keypoint Detection"
    },
    {
      "source": "Harris Corner Detector",
      "type": "uses",
      "target": "Image Gradient"
    },
    {
      "source": "Harris Corner Detector",
      "type": "precedes",
      "target": "Feature Description"
    },
    {
      "source": "Harris Corner Detector",
      "type": "robust_to",
      "target": "Translation"
    },
    {
      "source": "Harris Corner Detector",
      "type": "not_robust_to",
      "target": "Scale Changes"
    },
    {
      "source": "Keypoint Detection",
      "type": "part_of",
      "target": "Local Features"
    },
    {
      "source": "Keypoint Detection",
      "type": "precedes",
      "target": "Feature Description"
    },
    {
      "source": "Keypoint Detection",
      "type": "input_to",
      "target": "Descriptor Extraction"
    },
    {
      "source": "Keypoint Detection",
      "type": "includes",
      "target": "Harris Corner Detector"
    },
    {
      "source": "Feature Description",
      "type": "part_of",
      "target": "Local Features"
    },
    {
      "source": "Feature Description",
      "type": "follows",
      "target": "Keypoint Detection"
    },
    {
      "source": "Feature Description",
      "type": "input_to",
      "target": "Feature Matching"
    },
    {
      "source": "Feature Description",
      "type": "uses",
      "target": "Orientation Normalization"
    },
    {
      "source": "Feature Matching",
      "type": "part_of",
      "target": "Local Features"
    },
    {
      "source": "Feature Matching",
      "type": "follows",
      "target": "Feature Description"
    },
    {
      "source": "Feature Matching",
      "type": "uses",
      "target": "Descriptor Distance"
    },
    {
      "source": "Feature Matching",
      "type": "filtered_by",
      "target": "RANSAC"
    },
    {
      "source": "Image Gradient",
      "type": "used_in",
      "target": "Harris Corner Detector"
    },
    {
      "source": "Image Gradient",
      "type": "basis_for",
      "target": "Structure Tensor"
    },
    {
      "source": "Image Gradient",
      "type": "input_to",
      "target": "Edge Detection"
    },
    {
      "source": "Image Gradient",
      "type": "related_to",
      "target": "Edge"
    },
    {
      "source": "Structure Tensor",
      "type": "used_in",
      "target": "Harris Corner Detector"
    },
    {
      "source": "Structure Tensor",
      "type": "computed_from",
      "target": "Image Gradient"
    },
    {
      "source": "Structure Tensor",
      "type": "enables",
      "target": "Corner Classification"
    },
    {
      "source": "Structure Tensor",
      "type": "related_to",
      "target": "Autocorrelation Function"
    },
    {
      "source": "Pattern Recognition",
      "type": "enables",
      "target": "Object Recognition"
    },
    {
      "source": "Pattern Recognition",
      "type": "part_of",
      "target": "Artificial Intelligence"
    },
    {
      "source": "Pattern Recognition",
      "type": "uses",
      "target": "Feature Extraction"
    },
    {
      "source": "Pattern Recognition",
      "type": "includes",
      "target": "Statistical Pattern Recognition"
    },
    {
      "source": "Pattern Recognition",
      "type": "includes",
      "target": "Structural Pattern Recognition"
    },
    {
      "source": "Statistical Pattern Recognition",
      "type": "part_of",
      "target": "Pattern Recognition"
    },
    {
      "source": "Statistical Pattern Recognition",
      "type": "uses",
      "target": "Feature Vector"
    },
    {
      "source": "Statistical Pattern Recognition",
      "type": "foundation_for",
      "target": "Machine Learning Classifiers"
    },
    {
      "source": "Statistical Pattern Recognition",
      "type": "contrasts_with",
      "target": "Structural Pattern Recognition"
    },
    {
      "source": "Structural Pattern Recognition",
      "type": "part_of",
      "target": "Pattern Recognition"
    },
    {
      "source": "Structural Pattern Recognition",
      "type": "uses",
      "target": "Symbolic Representation"
    },
    {
      "source": "Structural Pattern Recognition",
      "type": "contrasts_with",
      "target": "Statistical Pattern Recognition"
    },
    {
      "source": "Structural Pattern Recognition",
      "type": "applies_to",
      "target": "Hierarchical Patterns"
    },
    {
      "source": "Feature Vector",
      "type": "input_to",
      "target": "Classifier"
    },
    {
      "source": "Feature Vector",
      "type": "output_of",
      "target": "Feature Extraction"
    },
    {
      "source": "Feature Vector",
      "type": "used_in",
      "target": "Statistical Pattern Recognition"
    },
    {
      "source": "Feature Vector",
      "type": "transformed_by",
      "target": "PCA"
    },
    {
      "source": "Classifier",
      "type": "core_component_of",
      "target": "Statistical Pattern Recognition"
    },
    {
      "source": "Classifier",
      "type": "uses",
      "target": "Feature Vector"
    },
    {
      "source": "Classifier",
      "type": "trained_on",
      "target": "Training Data"
    },
    {
      "source": "Classifier",
      "type": "outputs",
      "target": "Class Label"
    },
    {
      "source": "Template Matching",
      "type": "is_a",
      "target": "Pattern Recognition"
    },
    {
      "source": "Template Matching",
      "type": "requires",
      "target": "Template"
    },
    {
      "source": "Template Matching",
      "type": "sensitive_to",
      "target": "Geometric Variations"
    },
    {
      "source": "Template Matching",
      "type": "extended_by",
      "target": "Deformable Templates"
    },
    {
      "source": "Bayes Classifier",
      "type": "is_a",
      "target": "Classifier"
    },
    {
      "source": "Bayes Classifier",
      "type": "foundation_for",
      "target": "Probabilistic Classification"
    },
    {
      "source": "Bayes Classifier",
      "type": "requires",
      "target": "Prior Probability"
    },
    {
      "source": "Bayes Classifier",
      "type": "uses",
      "target": "Likelihood Model"
    },
    {
      "source": "Object Recognition",
      "type": "is_a",
      "target": "Pattern Recognition"
    },
    {
      "source": "Object Recognition",
      "type": "requires",
      "target": "Feature Extraction"
    },
    {
      "source": "Object Recognition",
      "type": "enabled_by",
      "target": "Local Features"
    },
    {
      "source": "Object Recognition",
      "type": "modernized_by",
      "target": "Deep Learning"
    },
    {
      "source": "Motion",
      "type": "analyzed_by",
      "target": "Optical Flow"
    },
    {
      "source": "Motion",
      "type": "used_in",
      "target": "Motion Segmentation"
    },
    {
      "source": "Motion",
      "type": "enables",
      "target": "3D Reconstruction"
    },
    {
      "source": "Motion",
      "type": "related_to",
      "target": "Temporal Analysis"
    },
    {
      "source": "Motion",
      "type": "caused_by",
      "target": "Relative Movement"
    },
    {
      "source": "Optical Flow",
      "type": "is_a",
      "target": "Motion Estimation"
    },
    {
      "source": "Optical Flow",
      "type": "assumes",
      "target": "Brightness Constancy"
    },
    {
      "source": "Optical Flow",
      "type": "used_in",
      "target": "Motion Segmentation"
    },
    {
      "source": "Optical Flow",
      "type": "extended_by",
      "target": "Deep Optical Flow"
    },
    {
      "source": "Optical Flow",
      "type": "contrasts_with",
      "target": "Feature Tracking"
    },
    {
      "source": "Brightness Constancy Assumption",
      "type": "core_of",
      "target": "Differential Optical Flow"
    },
    {
      "source": "Brightness Constancy Assumption",
      "type": "violated_by",
      "target": "Specular Highlights"
    },
    {
      "source": "Brightness Constancy Assumption",
      "type": "violated_by",
      "target": "Occlusion"
    },
    {
      "source": "Brightness Constancy Assumption",
      "type": "complemented_by",
      "target": "Smoothness Constraint"
    },
    {
      "source": "Aperture Problem",
      "type": "limitation_of",
      "target": "Local Motion Estimation"
    },
    {
      "source": "Aperture Problem",
      "type": "solved_by",
      "target": "Global Optimization"
    },
    {
      "source": "Aperture Problem",
      "type": "solved_by",
      "target": "Corner Features"
    },
    {
      "source": "Aperture Problem",
      "type": "related_to",
      "target": "Optical Flow Constraint"
    },
    {
      "source": "Lucas-Kanade Method",
      "type": "is_a",
      "target": "Sparse Optical Flow"
    },
    {
      "source": "Lucas-Kanade Method",
      "type": "assumes",
      "target": "Local Smoothness"
    },
    {
      "source": "Lucas-Kanade Method",
      "type": "uses",
      "target": "Image Gradient"
    },
    {
      "source": "Lucas-Kanade Method",
      "type": "robust_to",
      "target": "Small Motions"
    },
    {
      "source": "Lucas-Kanade Method",
      "type": "extended_by",
      "target": "Pyramidal Lucas-Kanade"
    },
    {
      "source": "Horn-Schunck Method",
      "type": "is_a",
      "target": "Dense Optical Flow"
    },
    {
      "source": "Horn-Schunck Method",
      "type": "uses",
      "target": "Global Smoothness"
    },
    {
      "source": "Horn-Schunck Method",
      "type": "complements",
      "target": "Lucas-Kanade Method"
    },
    {
      "source": "Horn-Schunck Method",
      "type": "foundation_for",
      "target": "Variational Flow Methods"
    },
    {
      "source": "Motion Segmentation",
      "type": "uses",
      "target": "Optical Flow"
    },
    {
      "source": "Motion Segmentation",
      "type": "enables",
      "target": "Object Tracking"
    },
    {
      "source": "Motion Segmentation",
      "type": "part_of",
      "target": "Video Analysis"
    },
    {
      "source": "Motion Segmentation",
      "type": "related_to",
      "target": "Layered Representation"
    },
    {
      "source": "Feature Tracking",
      "type": "alternative_to",
      "target": "Dense Optical Flow"
    },
    {
      "source": "Feature Tracking",
      "type": "uses",
      "target": "Local Features"
    },
    {
      "source": "Feature Tracking",
      "type": "input_to",
      "target": "Structure from Motion"
    },
    {
      "source": "Feature Tracking",
      "type": "implemented_as",
      "target": "KLT Tracker"
    },
    {
      "source": "Image Matching",
      "type": "enables",
      "target": "3D Reconstruction"
    },
    {
      "source": "Image Matching",
      "type": "includes",
      "target": "Feature Matching"
    },
    {
      "source": "Image Matching",
      "type": "includes",
      "target": "Dense Matching"
    },
    {
      "source": "Image Matching",
      "type": "uses",
      "target": "Local Features"
    },
    {
      "source": "Image Matching",
      "type": "related_to",
      "target": "Geometric Transformation"
    },
    {
      "source": "Epipolar Geometry",
      "type": "foundation_for",
      "target": "Stereo Matching"
    },
    {
      "source": "Epipolar Geometry",
      "type": "encapsulated_by",
      "target": "Fundamental Matrix"
    },
    {
      "source": "Epipolar Geometry",
      "type": "encapsulated_by",
      "target": "Essential Matrix"
    },
    {
      "source": "Epipolar Geometry",
      "type": "used_in",
      "target": "Wide Baseline Matching"
    },
    {
      "source": "Epipolar Geometry",
      "type": "related_to",
      "target": "Baseline"
    },
    {
      "source": "Fundamental Matrix",
      "type": "represents",
      "target": "Epipolar Geometry"
    },
    {
      "source": "Fundamental Matrix",
      "type": "estimated_from",
      "target": "Point Correspondences"
    },
    {
      "source": "Fundamental Matrix",
      "type": "generalizes",
      "target": "Essential Matrix"
    },
    {
      "source": "Fundamental Matrix",
      "type": "input_to",
      "target": "Image Rectification"
    },
    {
      "source": "Fundamental Matrix",
      "type": "decomposed_into",
      "target": "Camera Matrices"
    },
    {
      "source": "Essential Matrix",
      "type": "special_case_of",
      "target": "Fundamental Matrix"
    },
    {
      "source": "Essential Matrix",
      "type": "requires",
      "target": "Camera Calibration"
    },
    {
      "source": "Essential Matrix",
      "type": "decomposed_into",
      "target": "Rotation Matrix"
    },
    {
      "source": "Essential Matrix",
      "type": "decomposed_into",
      "target": "Translation Direction"
    },
    {
      "source": "Essential Matrix",
      "type": "used_in",
      "target": "Relative Pose Estimation"
    },
    {
      "source": "RANSAC",
      "type": "used_in",
      "target": "Fundamental Matrix Estimation"
    },
    {
      "source": "RANSAC",
      "type": "used_in",
      "target": "Homography Estimation"
    },
    {
      "source": "RANSAC",
      "type": "filters",
      "target": "Outliers"
    },
    {
      "source": "RANSAC",
      "type": "complemented_by",
      "target": "LO-RANSAC"
    },
    {
      "source": "RANSAC",
      "type": "foundation_for",
      "target": "Robust Fitting"
    },
    {
      "source": "Stereo Matching",
      "type": "uses",
      "target": "Epipolar Geometry"
    },
    {
      "source": "Stereo Matching",
      "type": "requires",
      "target": "Image Rectification"
    },
    {
      "source": "Stereo Matching",
      "type": "outputs",
      "target": "Disparity Map"
    },
    {
      "source": "Stereo Matching",
      "type": "enables",
      "target": "Depth Estimation"
    },
    {
      "source": "Stereo Matching",
      "type": "part_of",
      "target": "Passive 3D Vision"
    },
    {
      "source": "Image Rectification",
      "type": "precedes",
      "target": "Stereo Matching"
    },
    {
      "source": "Image Rectification",
      "type": "uses",
      "target": "Fundamental Matrix"
    },
    {
      "source": "Image Rectification",
      "type": "produces",
      "target": "Rectified Images"
    },
    {
      "source": "Image Rectification",
      "type": "simplifies",
      "target": "Correspondence Search"
    },
    {
      "source": "Homography",
      "type": "special_case_of",
      "target": "Projective Transformation"
    },
    {
      "source": "Homography",
      "type": "estimated_from",
      "target": "Planar Correspondences"
    },
    {
      "source": "Homography",
      "type": "used_in",
      "target": "Image Stitching"
    },
    {
      "source": "Homography",
      "type": "contrasts_with",
      "target": "Fundamental Matrix"
    },
    {
      "source": "Homography",
      "type": "induced_by",
      "target": "Planar Scene"
    },
    {
      "source": "3D Perception",
      "type": "enables",
      "target": "Scene Understanding"
    },
    {
      "source": "3D Perception",
      "type": "uses",
      "target": "Image Matching"
    },
    {
      "source": "3D Perception",
      "type": "includes",
      "target": "Depth Estimation"
    },
    {
      "source": "3D Perception",
      "type": "includes",
      "target": "3D Reconstruction"
    },
    {
      "source": "3D Perception",
      "type": "complements",
      "target": "2D Vision"
    },
    {
      "source": "Depth from Stereo",
      "type": "is_a",
      "target": "Passive 3D Perception"
    },
    {
      "source": "Depth from Stereo",
      "type": "requires",
      "target": "Camera Calibration"
    },
    {
      "source": "Depth from Stereo",
      "type": "uses",
      "target": "Stereo Matching"
    },
    {
      "source": "Depth from Stereo",
      "type": "outputs",
      "target": "Depth Map"
    },
    {
      "source": "Depth from Stereo",
      "type": "limited_by",
      "target": "Baseline Length"
    },
    {
      "source": "Structure from Motion",
      "type": "is_a",
      "target": "Multi-view 3D Reconstruction"
    },
    {
      "source": "Structure from Motion",
      "type": "uses",
      "target": "Feature Matching"
    },
    {
      "source": "Structure from Motion",
      "type": "uses",
      "target": "RANSAC"
    },
    {
      "source": "Structure from Motion",
      "type": "input_to",
      "target": "Multi-view Stereo"
    },
    {
      "source": "Structure from Motion",
      "type": "enables",
      "target": "Dense Reconstruction"
    },
    {
      "source": "Monocular Depth Cues",
      "type": "enables",
      "target": "Single Image 3D"
    },
    {
      "source": "Monocular Depth Cues",
      "type": "includes",
      "target": "Occlusion"
    },
    {
      "source": "Monocular Depth Cues",
      "type": "includes",
      "target": "Perspective"
    },
    {
      "source": "Monocular Depth Cues",
      "type": "includes",
      "target": "Shading"
    },
    {
      "source": "Monocular Depth Cues",
      "type": "contrasts_with",
      "target": "Binocular Cues"
    },
    {
      "source": "Shape from Shading",
      "type": "is_a",
      "target": "Monocular Depth Cues"
    },
    {
      "source": "Shape from Shading",
      "type": "assumes",
      "target": "Lambertian Reflectance"
    },
    {
      "source": "Shape from Shading",
      "type": "requires",
      "target": "Known Light Source"
    },
    {
      "source": "Shape from Shading",
      "type": "extended_by",
      "target": "Photometric Stereo"
    },
    {
      "source": "Shape from Shading",
      "type": "outputs",
      "target": "Surface Normals"
    },
    {
      "source": "Photometric Stereo",
      "type": "extends",
      "target": "Shape from Shading"
    },
    {
      "source": "Photometric Stereo",
      "type": "requires",
      "target": "Multiple Known Lights"
    },
    {
      "source": "Photometric Stereo",
      "type": "handles",
      "target": "Unknown Albedo"
    },
    {
      "source": "Photometric Stereo",
      "type": "input_to",
      "target": "Surface Integration"
    },
    {
      "source": "Photometric Stereo",
      "type": "robust_to",
      "target": "Cast Shadows (with care)"
    },
    {
      "source": "Multi-view Stereo",
      "type": "follows",
      "target": "Structure from Motion"
    },
    {
      "source": "Multi-view Stereo",
      "type": "generalizes",
      "target": "Stereo Matching"
    },
    {
      "source": "Multi-view Stereo",
      "type": "uses",
      "target": "Photo-consistency"
    },
    {
      "source": "Multi-view Stereo",
      "type": "outputs",
      "target": "Dense Point Cloud"
    },
    {
      "source": "Multi-view Stereo",
      "type": "enables",
      "target": "Mesh Generation"
    },
    {
      "source": "Triangulation",
      "type": "core_of",
      "target": "Stereo Vision"
    },
    {
      "source": "Triangulation",
      "type": "core_of",
      "target": "Structure from Motion"
    },
    {
      "source": "Triangulation",
      "type": "requires",
      "target": "Camera Calibration"
    },
    {
      "source": "Triangulation",
      "type": "requires",
      "target": "Point Correspondences"
    },
    {
      "source": "Triangulation",
      "type": "outputs",
      "target": "3D Point"
    },
    {
      "source": "Computer Vision",
      "type": "is closely related to",
      "target": "Artificial Intelligence"
    },
    {
      "source": "Computer Vision",
      "type": "is more application oriented than",
      "target": "Machine Vision"
    },
    {
      "source": "Image Recognition",
      "type": "is a component of",
      "target": "Image Understanding"
    },
    {
      "source": "Deep Neural Networks (DNNs)",
      "type": "is a type of",
      "target": "Machine Learning"
    },
    {
      "source": "Deep Neural Networks (DNNs)",
      "type": "requires reliance on",
      "target": "Large Datasets (e.g., ImageNet)"
    },
    {
      "source": "Thresholding",
      "type": "is a fundamental step in",
      "target": "Binary Image Analysis Pipeline"
    },
    {
      "source": "Structure from Motion (SfM)",
      "type": "is constrained by",
      "target": "Scale Ambiguity"
    },
    {
      "source": "Structure from Motion (SfM)",
      "type": "feeds into",
      "target": "Surface Reconstruction"
    },
    {
      "source": "Sparsity Structure",
      "type": "defines",
      "target": "Standard Matrix Subspace"
    },
    {
      "source": "Sparsity Structure",
      "type": "used_in",
      "target": "Incomplete LU Factorization"
    },
    {
      "source": "Sparsity Structure",
      "type": "related_to",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Standard Matrix Subspace",
      "type": "based_on",
      "target": "Sparsity Structure"
    },
    {
      "source": "Standard Matrix Subspace",
      "type": "used_in",
      "target": "LU Factorization within Subspace"
    },
    {
      "source": "Standard Matrix Subspace",
      "type": "related_to",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Orthogonal Projector (Matrix Subspace)",
      "type": "projects_onto",
      "target": "Matrix Subspace"
    },
    {
      "source": "Orthogonal Projector (Matrix Subspace)",
      "type": "used_in",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Orthogonal Projector (Matrix Subspace)",
      "type": "related_to",
      "target": "Projection Operator"
    },
    {
      "source": "Invertible Matrix Subspace",
      "type": "opposes",
      "target": "Singular Matrix Subspace"
    },
    {
      "source": "Invertible Matrix Subspace",
      "type": "related_to",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "Invertible Matrix Subspace",
      "type": "enables",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Singular Matrix Subspace",
      "type": "opposes",
      "target": "Invertible Matrix Subspace"
    },
    {
      "source": "Singular Matrix Subspace",
      "type": "subset_of",
      "target": "Matrix Subspace"
    },
    {
      "source": "Singular Matrix Subspace",
      "type": "related_to",
      "target": "Low Rank Approximation"
    },
    {
      "source": "Polynomially Closed Matrix Subspace",
      "type": "supports",
      "target": "Invertible Matrix Subspace"
    },
    {
      "source": "Polynomially Closed Matrix Subspace",
      "type": "related_to",
      "target": "Matrix Polynomials"
    },
    {
      "source": "Polynomially Closed Matrix Subspace",
      "type": "enables",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Closure of V₁V₂",
      "type": "extends",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Closure of V₁V₂",
      "type": "used_in",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Closure of V₁V₂",
      "type": "related_to",
      "target": "Matrix Approximation Problem"
    },
    {
      "source": "Matrix Computations",
      "type": "part_of",
      "target": "Numerical Analysis"
    },
    {
      "source": "Matrix Computations",
      "type": "used_in",
      "target": "Scientific Computing"
    },
    {
      "source": "Matrix Computations",
      "type": "depends_on",
      "target": "Linear Algebra"
    },
    {
      "source": "Matrix Computations",
      "type": "related_to",
      "target": "Partial Differential Equations"
    },
    {
      "source": "Matrix Computations",
      "type": "related_to",
      "target": "Matrix Factorization"
    },
    {
      "source": "Inner Product",
      "type": "generalization_of",
      "target": "Dot Product"
    },
    {
      "source": "Inner Product",
      "type": "used_in",
      "target": "Gram-Schmidt Process"
    },
    {
      "source": "Inner Product",
      "type": "foundation_for",
      "target": "Norm"
    },
    {
      "source": "Inner Product",
      "type": "related_to",
      "target": "Orthogonality"
    },
    {
      "source": "Computational Complexity (Matrix Multiplication)",
      "type": "applies_to",
      "target": "Matrix Multiplication"
    },
    {
      "source": "Computational Complexity (Matrix Multiplication)",
      "type": "contrasts_with",
      "target": "Subcubic Algorithms"
    },
    {
      "source": "Computational Complexity (Matrix Multiplication)",
      "type": "related_to",
      "target": "Strassen Algorithm"
    },
    {
      "source": "Matrix Computations",
      "type": "includes",
      "target": "LU Factorization"
    },
    {
      "source": "Matrix Computations",
      "type": "includes",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Matrix Computations",
      "type": "foundation_for",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Matrix Computations",
      "type": "related_to",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "LU Factorization",
      "type": "part_of",
      "target": "Matrix Factorization"
    },
    {
      "source": "LU Factorization",
      "type": "used_in",
      "target": "Linear System Solving"
    },
    {
      "source": "LU Factorization",
      "type": "approximated_by",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "LU Factorization",
      "type": "related_to",
      "target": "Gram-Schmidt Orthogonalization"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "part_of",
      "target": "Matrix Factorization"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "alternative_to",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "related_to",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "used_for",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "solved_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "related_to",
      "target": "LU Factorization"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "approximated_by",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "used_in",
      "target": "Partial Differential Equations"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "alternative_to",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "used_for",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "related_to",
      "target": "Gram-Schmidt Orthogonalization"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "applied_to",
      "target": "Triangular Matrices"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "used_in",
      "target": "QR Factorization"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "related_to",
      "target": "LU Factorization"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "applied_in",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "foundation_for",
      "target": "Orthogonal Matrices"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "achieved_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "alternative_via",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "used_in",
      "target": "Matrix Factorization"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "measures",
      "target": "Frobenius Norm"
    },
    {
      "source": "Hermitian Matrix",
      "type": "special_case_of",
      "target": "Symmetric Matrix (real case)"
    },
    {
      "source": "Hermitian Matrix",
      "type": "approximated_by",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Hermitian Matrix",
      "type": "related_to",
      "target": "Unitary Matrix"
    },
    {
      "source": "Hermitian Matrix",
      "type": "decomposed_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "uses",
      "target": "Unitary Matrix"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "uses",
      "target": "Singular Value"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "enables",
      "target": "Low Rank Approximation"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "related_to",
      "target": "Principal Component Analysis"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "solves",
      "target": "Matrix Approximation Problem"
    },
    {
      "source": "Singular Value",
      "type": "part_of",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Singular Value",
      "type": "computed_from",
      "target": "Eigenvalue"
    },
    {
      "source": "Singular Value",
      "type": "determines",
      "target": "Matrix Rank"
    },
    {
      "source": "Singular Value",
      "type": "equals",
      "target": "Operator Norm"
    },
    {
      "source": "Unitary Matrix",
      "type": "used_in",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Unitary Matrix",
      "type": "preserves",
      "target": "Euclidean Norm"
    },
    {
      "source": "Unitary Matrix",
      "type": "part_of",
      "target": "QR Decomposition"
    },
    {
      "source": "Unitary Matrix",
      "type": "generalizes",
      "target": "Orthogonal Matrix"
    },
    {
      "source": "Low Rank Approximation",
      "type": "solved_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Low Rank Approximation",
      "type": "uses",
      "target": "Frobenius Norm"
    },
    {
      "source": "Low Rank Approximation",
      "type": "enables",
      "target": "Principal Component Analysis"
    },
    {
      "source": "Low Rank Approximation",
      "type": "minimizes",
      "target": "Reconstruction Error"
    },
    {
      "source": "Frobenius Norm",
      "type": "used_in",
      "target": "Low Rank Approximation"
    },
    {
      "source": "Frobenius Norm",
      "type": "equivalent_to",
      "target": "Euclidean Norm"
    },
    {
      "source": "Frobenius Norm",
      "type": "sum_of",
      "target": "Singular Value Squared"
    },
    {
      "source": "Frobenius Norm",
      "type": "invariant_under",
      "target": "Unitary Transformation"
    },
    {
      "source": "Operator Norm",
      "type": "equals",
      "target": "Largest Singular Value"
    },
    {
      "source": "Operator Norm",
      "type": "part_of",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Operator Norm",
      "type": "measures",
      "target": "Linear Transformation Stretch"
    },
    {
      "source": "Matrix Subspace",
      "type": "generalization_of",
      "target": "Vector Subspace"
    },
    {
      "source": "Matrix Subspace",
      "type": "used_in",
      "target": "Krylov Subspace"
    },
    {
      "source": "Matrix Subspace",
      "type": "foundation_for",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "Matrix Subspace",
      "type": "related_to",
      "target": "Invariant Subspace"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "subtype_of",
      "target": "Matrix Subspace"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "enables",
      "target": "LU Factorization"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "defines",
      "target": "Inv(V)"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "related_to",
      "target": "GL(n,ℂ)"
    },
    {
      "source": "Inv(V)",
      "type": "derived_from",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "Inv(V)",
      "type": "is_a",
      "target": "Matrix Subspace"
    },
    {
      "source": "Inv(V)",
      "type": "preserves_under",
      "target": "Similarity Transformation"
    },
    {
      "source": "Inv(V)",
      "type": "used_in",
      "target": "LU Factorization"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "requires",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "uses",
      "target": "Inv(V)"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "extends",
      "target": "LU Factorization"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "related_to",
      "target": "Schur Complement"
    },
    {
      "source": "Krylov Subspace",
      "type": "subtype_of",
      "target": "Matrix Subspace"
    },
    {
      "source": "Krylov Subspace",
      "type": " invariant_under",
      "target": "Power Iteration"
    },
    {
      "source": "Krylov Subspace",
      "type": "foundation_for",
      "target": "GMRES"
    },
    {
      "source": "Krylov Subspace",
      "type": "bounded_by",
      "target": "Minimal Polynomial"
    },
    {
      "source": "Matrix Polynomials",
      "type": "generalizes",
      "target": "Scalar Polynomial"
    },
    {
      "source": "Matrix Polynomials",
      "type": "satisfies",
      "target": "Cayley-Hamilton Theorem"
    },
    {
      "source": "Matrix Polynomials",
      "type": "defines",
      "target": "Minimal Polynomial"
    },
    {
      "source": "Matrix Polynomials",
      "type": "preserves",
      "target": "Matrix Subspace"
    },
    {
      "source": "Projection Operator",
      "type": "special_case_of",
      "target": "Idempotent Operator"
    },
    {
      "source": "Projection Operator",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Projection Operator",
      "type": "orthogonal_to",
      "target": "I−P"
    },
    {
      "source": "Projection Operator",
      "type": "related_to",
      "target": "Oblique Projection"
    },
    {
      "source": "Krylov Subspace",
      "type": "generated_from",
      "target": "Matrix Powers"
    },
    {
      "source": "Krylov Subspace",
      "type": "used_in",
      "target": "Iterative Methods"
    },
    {
      "source": "Krylov Subspace",
      "type": "related_to",
      "target": "Invariant Subspace"
    },
    {
      "source": "Krylov Subspace",
      "type": "dimension",
      "target": "j"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "approximates",
      "target": "Square Matrix A"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "related_to",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "uses",
      "target": "Subspaces V1 V2"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "enables",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Projection Operator",
      "type": "satisfies",
      "target": "Idempotency P^2 = P"
    },
    {
      "source": "Projection Operator",
      "type": "defines",
      "target": "Range R(P)"
    },
    {
      "source": "Projection Operator",
      "type": "complements_with",
      "target": "I - P"
    },
    {
      "source": "Projection Operator",
      "type": "used_in",
      "target": "Subspace Decomposition"
    },
    {
      "source": "Range of Projection",
      "type": "defined_by",
      "target": "Projection Operator"
    },
    {
      "source": "Range of Projection",
      "type": "orthogonal_to",
      "target": "R(I - P)"
    },
    {
      "source": "Range of Projection",
      "type": "part_of",
      "target": "Direct Sum Decomposition"
    },
    {
      "source": "Range of Projection",
      "type": "related_to",
      "target": "Kernel of I - P"
    },
    {
      "source": "Orthogonal Complement",
      "type": "property_of",
      "target": "Orthogonal Projection"
    },
    {
      "source": "Orthogonal Complement",
      "type": "ensures",
      "target": "Direct Sum"
    },
    {
      "source": "Orthogonal Complement",
      "type": "related_to",
      "target": "Inner Product Space"
    },
    {
      "source": "Orthogonal Complement",
      "type": "used_in",
      "target": "Subspace Factoring"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "uses",
      "target": "Krylov Subspace"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "incorporates",
      "target": "Projection Operator"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "alternative_to",
      "target": "Direct Factorization"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "applied_to",
      "target": "Square Matrix A"
    },
    {
      "source": "Invariant Subspace",
      "type": "related_to",
      "target": "Krylov Subspace"
    },
    {
      "source": "Invariant Subspace",
      "type": "projected_onto",
      "target": "Projection Operator"
    },
    {
      "source": "Invariant Subspace",
      "type": "used_in",
      "target": "Matrix Factoring"
    },
    {
      "source": "Invariant Subspace",
      "type": "contains",
      "target": "Eigenvectors"
    },
    {
      "source": "Square Matrix A",
      "type": "factored_by",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Square Matrix A",
      "type": "generates",
      "target": "Krylov Subspace"
    },
    {
      "source": "Square Matrix A",
      "type": "acts_on",
      "target": "Invariant Subspace"
    },
    {
      "source": "Square Matrix A",
      "type": "projected_by",
      "target": "Projection Operator"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "improves",
      "target": "Gaussian Elimination"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "uses",
      "target": "Permutation Matrix"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "produces",
      "target": "Lower Triangular Matrix"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "produces",
      "target": "Upper Triangular Matrix"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "controls",
      "target": "Growth Factor"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "enables",
      "target": "Backward Stable Solver"
    },
    {
      "source": "Partial Pivoting",
      "type": "part_of",
      "target": "LU Factorization with Partial Pivoting"
    },
    {
      "source": "Partial Pivoting",
      "type": "reduces",
      "target": "Round-off Error"
    },
    {
      "source": "Partial Pivoting",
      "type": "bounds",
      "target": "Growth Factor"
    },
    {
      "source": "Partial Pivoting",
      "type": "contrasts",
      "target": "Complete Pivoting"
    },
    {
      "source": "Permutation Matrix",
      "type": "used_in",
      "target": "LU Factorization with Partial Pivoting"
    },
    {
      "source": "Permutation Matrix",
      "type": "preserves",
      "target": "Euclidean Norm"
    },
    {
      "source": "Permutation Matrix",
      "type": "inverts_to",
      "target": "Itself"
    },
    {
      "source": "Permutation Matrix",
      "type": "represents",
      "target": "Row Permutation"
    },
    {
      "source": "Growth Factor",
      "type": "bounded_by",
      "target": "Partial Pivoting"
    },
    {
      "source": "Growth Factor",
      "type": "affects",
      "target": "Backward Stability"
    },
    {
      "source": "Growth Factor",
      "type": "measured_in",
      "target": "Upper Triangular Factor"
    },
    {
      "source": "Condition Number",
      "type": "estimated_via",
      "target": "LU Factorization"
    },
    {
      "source": "Condition Number",
      "type": "equals",
      "target": "Ratio of Extreme Singular Values"
    },
    {
      "source": "Condition Number",
      "type": "impacts",
      "target": "Solution Accuracy"
    },
    {
      "source": "Gaussian Elimination",
      "type": "foundation_of",
      "target": "LU Factorization"
    },
    {
      "source": "Gaussian Elimination",
      "type": "enhanced_by",
      "target": "Partial Pivoting"
    },
    {
      "source": "Gaussian Elimination",
      "type": "performs",
      "target": "Row Reduction"
    },
    {
      "source": "Cholesky Factorization",
      "type": "specializes",
      "target": "LU Factorization"
    },
    {
      "source": "Cholesky Factorization",
      "type": "requires",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Cholesky Factorization",
      "type": "outputs",
      "target": "Upper Triangular Factor"
    },
    {
      "source": "Cholesky Factorization",
      "type": "related_to",
      "target": "LDL* Decomposition"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "generalization_of",
      "target": "Positive Semidefinite"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "enables",
      "target": "Cholesky Factorization"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "preserved_under",
      "target": "Congruence Transformation"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "related_to",
      "target": "Elliptic PDE"
    },
    {
      "source": "Sylvester Equation",
      "type": "generalizes",
      "target": "Lyapunov Equation"
    },
    {
      "source": "Sylvester Equation",
      "type": "solved_via",
      "target": "Schur Triangulation"
    },
    {
      "source": "Sylvester Equation",
      "type": "used_in",
      "target": "Control Synthesis"
    },
    {
      "source": "Sylvester Equation",
      "type": "related_to",
      "target": "Kronecker Product"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "computed_by",
      "target": "Fast Fourier Transform"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "diagonalizes",
      "target": "Circulant Matrix"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "unitary_up_to_scaling",
      "target": "Fₙ*/√n"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "related_to",
      "target": "Vandermonde Matrix"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "implements",
      "target": "Discrete Fourier Transform"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "complexity_reduction",
      "target": "O(n²) to O(n log n)"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "uses",
      "target": "Twiddle Factors"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "related_to",
      "target": "Butterfly Diagram"
    },
    {
      "source": "Schur Triangulation",
      "type": "generalizes",
      "target": "Hessenberg Form"
    },
    {
      "source": "Schur Triangulation",
      "type": "enables",
      "target": "Sylvester Equation Solution"
    },
    {
      "source": "Schur Triangulation",
      "type": "computed_by",
      "target": "QR Algorithm"
    },
    {
      "source": "Schur Triangulation",
      "type": "related_to",
      "target": "Unitary Similarity"
    },
    {
      "source": "Iterative Methods",
      "type": "alternative_to",
      "target": "Direct Methods"
    },
    {
      "source": "Iterative Methods",
      "type": "uses",
      "target": "Krylov Subspace"
    },
    {
      "source": "Iterative Methods",
      "type": "improved_by",
      "target": "Preconditioning"
    },
    {
      "source": "Iterative Methods",
      "type": "measures_convergence",
      "target": "Residual Norm"
    },
    {
      "source": "Linear System",
      "type": "solved_by",
      "target": "Iterative Methods"
    },
    {
      "source": "Linear System",
      "type": "preconditioned_as",
      "target": "M^{-1} A x = M^{-1} b"
    },
    {
      "source": "Linear System",
      "type": "residual",
      "target": "r = b - A x"
    },
    {
      "source": "Linear System",
      "type": "convergence_bound",
      "target": "Polynomial Minimization"
    },
    {
      "source": "Krylov Subspace",
      "type": "generated_by",
      "target": "Matrix Powers"
    },
    {
      "source": "Krylov Subspace",
      "type": "orthogonalized_by",
      "target": "Arnoldi Process"
    },
    {
      "source": "Krylov Subspace",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Krylov Subspace",
      "type": "invariant_under",
      "target": "A"
    },
    {
      "source": "Arnoldi Process",
      "type": "orthogonalises",
      "target": "Krylov Subspace"
    },
    {
      "source": "Arnoldi Process",
      "type": "produces",
      "target": "Hessenberg Matrix"
    },
    {
      "source": "Arnoldi Process",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Arnoldi Process",
      "type": "similar_to",
      "target": "Lanczos Algorithm"
    },
    {
      "source": "GMRES",
      "type": "based_on",
      "target": "Arnoldi Process"
    },
    {
      "source": "GMRES",
      "type": "minimises",
      "target": "Residual Norm"
    },
    {
      "source": "GMRES",
      "type": "for",
      "target": "Nonsymmetric Matrices"
    },
    {
      "source": "GMRES",
      "type": "variant_of",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "equivalent_to",
      "target": "Lanczos Algorithm (for SPD)"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "minimises",
      "target": "A-Norm Error"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "for",
      "target": "Symmetric Positive Definite Matrices"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "improved_by",
      "target": "Preconditioning"
    },
    {
      "source": "Preconditioning",
      "type": "improves",
      "target": "Iterative Methods"
    },
    {
      "source": "Preconditioning",
      "type": "approximates",
      "target": "A⁻¹"
    },
    {
      "source": "Preconditioning",
      "type": "used_in",
      "target": "PCG"
    },
    {
      "source": "Preconditioning",
      "type": "related_to",
      "target": "Condition Number"
    },
    {
      "source": "Residual Norm",
      "type": "minimised_by",
      "target": "GMRES"
    },
    {
      "source": "Residual Norm",
      "type": "orthogonal_to",
      "target": "Search Directions (in CG)"
    },
    {
      "source": "Residual Norm",
      "type": "computed_in",
      "target": "Iterative Methods"
    },
    {
      "source": "Residual Norm",
      "type": "bounds_convergence",
      "target": "Polynomial Approximation"
    },
    {
      "source": "A-Norm Error",
      "type": "minimised_by",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "A-Norm Error",
      "type": "related_to",
      "target": "Quadratic Form"
    },
    {
      "source": "A-Norm Error",
      "type": "for",
      "target": "SPD Matrices"
    },
    {
      "source": "A-Norm Error",
      "type": "bounds_via",
      "target": "Chebyshev Polynomials"
    },
    {
      "source": "Polynomial Approximation",
      "type": "underlies",
      "target": "Krylov Methods Convergence"
    },
    {
      "source": "Polynomial Approximation",
      "type": "uses",
      "target": "Eigenvalue Spectrum"
    },
    {
      "source": "Polynomial Approximation",
      "type": "improved_by",
      "target": "Preconditioning (clustering eigenvalues)"
    },
    {
      "source": "Polynomial Approximation",
      "type": "related_to",
      "target": "Condition Number"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "produced_by",
      "target": "Arnoldi Process"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "used_in",
      "target": "GMRES Least Squares"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "similar_to",
      "target": "Tridiagonal Matrix (symmetric case)"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "decomposed_by",
      "target": "QR Factorisation"
    },
    {
      "source": "Preconditioning",
      "type": "improves",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Preconditioning",
      "type": "improves",
      "target": "GMRES"
    },
    {
      "source": "Preconditioning",
      "type": "requires",
      "target": "Approximate Inverse"
    },
    {
      "source": "Preconditioning",
      "type": "reduces",
      "target": "Spectral Radius"
    },
    {
      "source": "Preconditioning",
      "type": "clusters",
      "target": "Eigenvalues"
    },
    {
      "source": "Preconditioning",
      "type": "used_with",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "is_a",
      "target": "Diagonal Preconditioner"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "part_of",
      "target": "Splitting Methods"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "used_in",
      "target": "Jacobi Iteration"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "simpler_than",
      "target": "Gauss-Seidel Preconditioner"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "approximates",
      "target": "LU Factorization"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "used_as",
      "target": "Preconditioner"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "extends",
      "target": "Gaussian Elimination"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "controls",
      "target": "Fill-in"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "approximates",
      "target": "Matrix Inverse"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "avoids",
      "target": "Triangular Solves"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "competes_with",
      "target": "Incomplete LU"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "enables",
      "target": "High Parallelism"
    },
    {
      "source": "Left Preconditioning",
      "type": "variant_of",
      "target": "Preconditioning"
    },
    {
      "source": "Left Preconditioning",
      "type": "preserves",
      "target": "Solution Vector"
    },
    {
      "source": "Left Preconditioning",
      "type": "changes",
      "target": "Residual Norm"
    },
    {
      "source": "Left Preconditioning",
      "type": "contrasts",
      "target": "Right Preconditioning"
    },
    {
      "source": "Right Preconditioning",
      "type": "variant_of",
      "target": "Preconditioning"
    },
    {
      "source": "Right Preconditioning",
      "type": "preserves",
      "target": "Residual Norm"
    },
    {
      "source": "Right Preconditioning",
      "type": "requires",
      "target": "Extra Solve"
    },
    {
      "source": "Right Preconditioning",
      "type": "used_in",
      "target": "Domain Decomposition"
    },
    {
      "source": "Splitting Methods",
      "type": "underlies",
      "target": "Jacobi Method"
    },
    {
      "source": "Splitting Methods",
      "type": "underlies",
      "target": "Gauss-Seidel Method"
    },
    {
      "source": "Splitting Methods",
      "type": "generalizes",
      "target": "Richardson Iteration"
    },
    {
      "source": "Splitting Methods",
      "type": "foundation_of",
      "target": "Preconditioning"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "generalizes_to",
      "target": "Generalized Eigenvalue Problem"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "solved_by",
      "target": "QR Algorithm"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "foundation_for",
      "target": "Matrix Functions"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "related_to",
      "target": "Jordan Canonical Form"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "bounds",
      "target": "Spectrum"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "refined_by",
      "target": "Brauer Cassini Ovals"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "used_in",
      "target": "Initial Eigenvalue Estimation"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "related_to",
      "target": "Spectral Radius"
    },
    {
      "source": "Power Iteration",
      "type": "special_case_of",
      "target": "Subspace Iteration"
    },
    {
      "source": "Power Iteration",
      "type": "enhanced_by",
      "target": "Rayleigh Quotient Iteration"
    },
    {
      "source": "Power Iteration",
      "type": "requires",
      "target": "Spectral Gap"
    },
    {
      "source": "Power Iteration",
      "type": "related_to",
      "target": "Inverse Iteration"
    },
    {
      "source": "Householder Reflection",
      "type": "building_block_of",
      "target": "QR Algorithm"
    },
    {
      "source": "Householder Reflection",
      "type": "unitary",
      "target": "Reflection"
    },
    {
      "source": "Householder Reflection",
      "type": "used_in",
      "target": "Hessenberg Reduction"
    },
    {
      "source": "Householder Reflection",
      "type": "related_to",
      "target": "Givens Rotation"
    },
    {
      "source": "QR Algorithm",
      "type": "computes",
      "target": "Schur Form"
    },
    {
      "source": "QR Algorithm",
      "type": "uses",
      "target": "Householder Reflection"
    },
    {
      "source": "QR Algorithm",
      "type": "accelerated_by",
      "target": "Francis Shift"
    },
    {
      "source": "QR Algorithm",
      "type": "related_to",
      "target": "Hessenberg Form"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "solved_by",
      "target": "QZ Algorithm"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "reduces_to",
      "target": "Standard Eigenvalue Problem"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "generalizes",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "related_to",
      "target": "Matrix Pencil"
    },
    {
      "source": "Matrix Function",
      "type": "computed_via",
      "target": "Schur Decomposition"
    },
    {
      "source": "Matrix Function",
      "type": "preserves",
      "target": "Spectral Mapping Theorem"
    },
    {
      "source": "Matrix Function",
      "type": "used_in",
      "target": "Exponential Integrators"
    },
    {
      "source": "Matrix Function",
      "type": "related_to",
      "target": "Cauchy Integral Formula"
    },
    {
      "source": "Field of Values",
      "type": "contains",
      "target": "Spectrum"
    },
    {
      "source": "Field of Values",
      "type": "equals_for",
      "target": "Normal Matrix"
    },
    {
      "source": "Field of Values",
      "type": "bounds",
      "target": "Numerical Radius"
    },
    {
      "source": "Field of Values",
      "type": "related_to",
      "target": "Pseudospectrum"
    },
    {
      "source": "Matrix Function",
      "type": "computed_via",
      "target": "Newton's Iteration"
    },
    {
      "source": "Matrix Function",
      "type": "applied_to",
      "target": "Square Matrix A"
    },
    {
      "source": "Matrix Function",
      "type": "used_in",
      "target": "Eigenvalue Problems"
    },
    {
      "source": "Matrix Function",
      "type": "related_to",
      "target": "Matrix Square Root"
    },
    {
      "source": "Matrix Square Root",
      "type": "instance_of",
      "target": "Matrix Function"
    },
    {
      "source": "Matrix Square Root",
      "type": "computed_by",
      "target": "Newton's Iteration for Square Root"
    },
    {
      "source": "Matrix Square Root",
      "type": "requires",
      "target": "Positive Definite A (for principal)"
    },
    {
      "source": "Matrix Square Root",
      "type": "related_to",
      "target": "Eigenvalue Decomposition"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "computes",
      "target": "Matrix Square Root"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "variant_of",
      "target": "Newton's Method"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "requires",
      "target": "Matrix Inversion"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "converges_to",
      "target": "sqrt(A)"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "solved_by",
      "target": "Iterative Methods"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "shifted_as",
      "target": "A - μ I"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "spectrum",
      "target": "Λ(A)"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "related_to",
      "target": "Singular Value Problem"
    },
    {
      "source": "Spectral Shift",
      "type": "transforms",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Spectral Shift",
      "type": "enables",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Spectral Shift",
      "type": "used_in",
      "target": "Inverse Iteration"
    },
    {
      "source": "Spectral Shift",
      "type": "estimates",
      "target": "Approximate μ"
    },
    {
      "source": "Positive Definite Shift",
      "type": "variant_of",
      "target": "Spectral Shift"
    },
    {
      "source": "Positive Definite Shift",
      "type": "ensures",
      "target": "Positive Definiteness"
    },
    {
      "source": "Positive Definite Shift",
      "type": "used_for",
      "target": "Conjugate Gradient"
    },
    {
      "source": "Positive Definite Shift",
      "type": "based_on",
      "target": "Eigenvalue Estimate ˆμ"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "of",
      "target": "Square Matrix A"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "shifted_by",
      "target": "Spectral Shift"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "estimated_via",
      "target": "Iterative Methods"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "related_to",
      "target": "Jordan Form"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "uses",
      "target": "Krylov Subspace"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "variant_of",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "has_variant",
      "target": "Preconditioned Conjugate Gradient"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "requires",
      "target": "Symmetric Positive Definite Matrices"
    },
    {
      "source": "Preconditioned Conjugate Gradient",
      "type": "extends",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Preconditioned Conjugate Gradient",
      "type": "uses",
      "target": "Preconditioning"
    },
    {
      "source": "Preconditioned Conjugate Gradient",
      "type": "uses",
      "target": "Krylov Subspace"
    },
    {
      "source": "Preconditioned Conjugate Gradient",
      "type": "related_to",
      "target": "Incomplete LU Factorization"
    },
    {
      "source": "Krylov Subspace",
      "type": "foundation_for",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Krylov Subspace",
      "type": "used_in",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Krylov Subspace",
      "type": "related_to",
      "target": "Polynomial Approximation"
    },
    {
      "source": "Krylov Subspace Methods",
      "type": "generalization_of",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Krylov Subspace Methods",
      "type": "generalization_of",
      "target": "GMRES"
    },
    {
      "source": "Krylov Subspace Methods",
      "type": "built_on",
      "target": "Krylov Subspace"
    },
    {
      "source": "Matrix Factorization",
      "type": "includes",
      "target": "LU Factorization"
    },
    {
      "source": "Matrix Factorization",
      "type": "includes",
      "target": "QR Factorization"
    },
    {
      "source": "Matrix Factorization",
      "type": "includes",
      "target": "Cholesky Factorization"
    },
    {
      "source": "Matrix Factorization",
      "type": "includes",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Matrix Factorization",
      "type": "includes",
      "target": "Schur Decomposition"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "approximation_of",
      "target": "LU Factorization"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "used_in",
      "target": "Preconditioned Conjugate Gradient"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "related_to",
      "target": "Sparsity Structure"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "used_in",
      "target": "Spectral Radius"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "used_in",
      "target": "Gershgorin Circle Theorem"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "computed_by",
      "target": "Schur Decomposition"
    },
    {
      "source": "Projection Operator",
      "type": "has_property",
      "target": "Idempotency"
    },
    {
      "source": "Projection Operator",
      "type": "includes",
      "target": "Orthogonal Projection"
    },
    {
      "source": "Projection Operator",
      "type": "includes",
      "target": "Oblique Projection"
    },
    {
      "source": "Projection Operator",
      "type": "used_in",
      "target": "Matrix Subspace"
    },
    {
      "source": "Projection Operator",
      "type": "used_in",
      "target": "Factorization Algorithms"
    },
    {
      "source": "A - μ I",
      "type": "related_to",
      "target": "Spectral Shift"
    },
    {
      "source": "A - μ I",
      "type": "used_in",
      "target": "Inverse Iteration"
    },
    {
      "source": "Backward Stability",
      "type": "related_to",
      "target": "Growth Factor"
    },
    {
      "source": "Backward Stability",
      "type": "related_to",
      "target": "Gaussian Elimination"
    },
    {
      "source": "Brauer Cassini Ovals",
      "type": "related_to",
      "target": "Gershgorin Circle Theorem"
    },
    {
      "source": "Brauer Cassini Ovals",
      "type": "generalization_of",
      "target": "Gershgorin Circle Theorem"
    },
    {
      "source": "Cauchy Integral Formula",
      "type": "foundation_for",
      "target": "Matrix Function"
    },
    {
      "source": "Cauchy Integral Formula",
      "type": "related_to",
      "target": "Spectrum Λ(A)"
    },
    {
      "source": "Chebyshev Polynomials",
      "type": "related_to",
      "target": "Polynomial Approximation"
    },
    {
      "source": "Chebyshev Polynomials",
      "type": "used_in",
      "target": "Iterative Methods"
    },
    {
      "source": "Circulant Matrix",
      "type": "related_to",
      "target": "Discrete Fourier Transform (DFT)"
    },
    {
      "source": "Circulant Matrix",
      "type": "related_to",
      "target": "Fast Fourier Transform (FFT)"
    },
    {
      "source": "Complete Pivoting",
      "type": "variant_of",
      "target": "LU Factorization with Partial Pivoting"
    },
    {
      "source": "Complete Pivoting",
      "type": "related_to",
      "target": "Growth Factor"
    },
    {
      "source": "Congruence Transformation",
      "type": "related_to",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Congruence Transformation",
      "type": "related_to",
      "target": "Lyapunov Equation"
    },
    {
      "source": "Control Synthesis",
      "type": "related_to",
      "target": "Lyapunov Equation"
    },
    {
      "source": "Control Synthesis",
      "type": "related_to",
      "target": "Matrix Pencil"
    },
    {
      "source": "Direct Sum",
      "type": "related_to",
      "target": "Orthogonal Complement"
    },
    {
      "source": "Direct Sum",
      "type": "related_to",
      "target": "Matrix Subspace"
    },
    {
      "source": "Domain Decomposition",
      "type": "related_to",
      "target": "Elliptic PDE"
    },
    {
      "source": "Domain Decomposition",
      "type": "used_in",
      "target": "Preconditioning"
    },
    {
      "source": "Elliptic PDE",
      "type": "related_to",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Elliptic PDE",
      "type": "related_to",
      "target": "Symmetric Positive Definite Matrices"
    },
    {
      "source": "Eigenvalue Decomposition",
      "type": "related_to",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Eigenvalue Decomposition",
      "type": "subtype_of",
      "target": "Spectral Decomposition"
    },
    {
      "source": "Exponential Integrators",
      "type": "related_to",
      "target": "Matrix Function"
    },
    {
      "source": "Exponential Integrators",
      "type": "related_to",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "generalizes",
      "target": "Discrete Fourier Transform (DFT)"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "related_to",
      "target": "Circulant Matrix"
    },
    {
      "source": "Fill-in",
      "type": "related_to",
      "target": "Sparse Approximate Inverse"
    },
    {
      "source": "Fill-in",
      "type": "related_to",
      "target": "Sparsity Structure"
    },
    {
      "source": "Francis Shift",
      "type": "used_in",
      "target": "QR Algorithm"
    },
    {
      "source": "Francis Shift",
      "type": "related_to",
      "target": "Hessenberg Matrix"
    },
    {
      "source": "Givens Rotation",
      "type": "variant_of",
      "target": "QR Factorization"
    },
    {
      "source": "Givens Rotation",
      "type": "related_to",
      "target": "Householder Reflection"
    },
    {
      "source": "GL(n, ℂ)",
      "type": "related_to",
      "target": "Invertible Matrix Subspace"
    },
    {
      "source": "GL(n, ℂ)",
      "type": "related_to",
      "target": "Matrix Function"
    },
    {
      "source": "Hessenberg Form",
      "type": "related_to",
      "target": "Hessenberg Matrix"
    },
    {
      "source": "Hessenberg Form",
      "type": "used_in",
      "target": "QR Algorithm"
    },
    {
      "source": "Hessenberg Reduction",
      "type": "produces",
      "target": "Hessenberg Form"
    },
    {
      "source": "Hessenberg Reduction",
      "type": "used_in",
      "target": "QR Algorithm"
    },
    {
      "source": "High Parallelism",
      "type": "related_to",
      "target": "Fast Fourier Transform (FFT)"
    },
    {
      "source": "High Parallelism",
      "type": "related_to",
      "target": "Matrix Multiplication"
    },
    {
      "source": "Idempotency",
      "type": "related_to",
      "target": "Projection Operator"
    },
    {
      "source": "Idempotency",
      "type": "related_to",
      "target": "Oblique Projection"
    },
    {
      "source": "Imaginary Unit j",
      "type": "related_to",
      "target": "Fast Fourier Transform (FFT)"
    },
    {
      "source": "Imaginary Unit j",
      "type": "related_to",
      "target": "Discrete Fourier Transform (DFT)"
    },
    {
      "source": "Inverse Iteration",
      "type": "uses",
      "target": "A - μ I"
    },
    {
      "source": "Inverse Iteration",
      "type": "variant_of",
      "target": "Rayleigh Quotient Iteration"
    },
    {
      "source": "Jordan Canonical Form",
      "type": "related_to",
      "target": "Eigenvalue Decomposition"
    },
    {
      "source": "Jordan Canonical Form",
      "type": "related_to",
      "target": "Minimal Polynomial"
    },
    {
      "source": "Kronecker Product",
      "type": "related_to",
      "target": "Sylvester Equation"
    },
    {
      "source": "Kronecker Product",
      "type": "related_to",
      "target": "Matrix Multiplication"
    },
    {
      "source": "Lower Triangular Matrix",
      "type": "related_to",
      "target": "LU Factorization"
    },
    {
      "source": "Lower Triangular Matrix",
      "type": "related_to",
      "target": "Upper Triangular Matrix"
    },
    {
      "source": "Matrix Pencil",
      "type": "related_to",
      "target": "Generalized Eigenvalue Problem"
    },
    {
      "source": "Matrix Pencil",
      "type": "used_in",
      "target": "QZ Algorithm"
    },
    {
      "source": "Matrix Rank",
      "type": "related_to",
      "target": "Singular Value"
    },
    {
      "source": "Matrix Rank",
      "type": "related_to",
      "target": "Low Rank Approximation"
    },
    {
      "source": "Minimal Polynomial",
      "type": "related_to",
      "target": "Jordan Canonical Form"
    },
    {
      "source": "Minimal Polynomial",
      "type": "related_to",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Normal Matrix",
      "type": "generalization_of",
      "target": "Hermitian Matrix"
    },
    {
      "source": "Normal Matrix",
      "type": "generalization_of",
      "target": "Unitary Matrix"
    },
    {
      "source": "Numerical Radius",
      "type": "related_to",
      "target": "Field of Values"
    },
    {
      "source": "Numerical Radius",
      "type": "related_to",
      "target": "Spectral Radius"
    },
    {
      "source": "Pseudospectrum",
      "type": "related_to",
      "target": "Spectrum Λ(A)"
    },
    {
      "source": "Pseudospectrum",
      "type": "related_to",
      "target": "Field of Values"
    },
    {
      "source": "Positive Semidefinite Matrix",
      "type": "generalization_of",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Positive Semidefinite Matrix",
      "type": "related_to",
      "target": "Quadratic Form"
    },
    {
      "source": "Quadratic Form",
      "type": "related_to",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Quadratic Form",
      "type": "related_to",
      "target": "Positive Semidefinite Matrix"
    },
    {
      "source": "QZ Algorithm",
      "type": "related_to",
      "target": "Matrix Pencil"
    },
    {
      "source": "QZ Algorithm",
      "type": "generalization_of",
      "target": "QR Algorithm"
    },
    {
      "source": "Reflection",
      "type": "generalization_of",
      "target": "Householder Reflection"
    },
    {
      "source": "Reflection",
      "type": "related_to",
      "target": "Orthogonal Matrix"
    },
    {
      "source": "Row Reduction",
      "type": "foundation_for",
      "target": "Gaussian Elimination"
    },
    {
      "source": "Row Reduction",
      "type": "related_to",
      "target": "Matrix Rank"
    },
    {
      "source": "Schur Complement",
      "type": "related_to",
      "target": "Block Matrix"
    },
    {
      "source": "Schur Complement",
      "type": "related_to",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Search Directions (in CG)",
      "type": "subcomponent_of",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Search Directions (in CG)",
      "type": "related_to",
      "target": "Krylov Subspace"
    },
    {
      "source": "Similarity Transformation",
      "type": "used_in",
      "target": "Jordan Canonical Form"
    },
    {
      "source": "Similarity Transformation",
      "type": "used_in",
      "target": "Eigenvalue Decomposition"
    },
    {
      "source": "Spectral Gap",
      "type": "related_to",
      "target": "Spectrum Λ(A)"
    },
    {
      "source": "Spectral Gap",
      "type": "related_to",
      "target": "Power Iteration"
    },
    {
      "source": "Spectral Mapping Theorem",
      "type": "foundation_for",
      "target": "Matrix Function"
    },
    {
      "source": "Spectral Mapping Theorem",
      "type": "related_to",
      "target": "Spectrum Λ(A)"
    },
    {
      "source": "Spectral Radius",
      "type": "related_to",
      "target": "Power Iteration"
    },
    {
      "source": "Spectral Radius",
      "type": "related_to",
      "target": "Numerical Radius"
    },
    {
      "source": "Strassen Algorithm",
      "type": "subcategory_of",
      "target": "Subcubic Algorithms"
    },
    {
      "source": "Strassen Algorithm",
      "type": "related_to",
      "target": "Computational Complexity (Matrix Multiplication)"
    },
    {
      "source": "Subcubic Algorithms",
      "type": "generalization_of",
      "target": "Strassen Algorithm"
    },
    {
      "source": "Subcubic Algorithms",
      "type": "related_to",
      "target": "Matrix Multiplication"
    },
    {
      "source": "Subspace Decomposition",
      "type": "related_to",
      "target": "Direct Sum"
    },
    {
      "source": "Subspace Decomposition",
      "type": "related_to",
      "target": "Matrix Subspace"
    },
    {
      "source": "Subspace Iteration",
      "type": "related_to",
      "target": "Power Iteration"
    },
    {
      "source": "Subspace Iteration",
      "type": "related_to",
      "target": "Krylov Subspace"
    },
    {
      "source": "Sylvester Equation Solution",
      "type": "related_to",
      "target": "Sylvester Equation"
    },
    {
      "source": "Sylvester Equation Solution",
      "type": "used_in",
      "target": "Lyapunov Equation"
    },
    {
      "source": "Symmetric Matrix",
      "type": "generalization_of",
      "target": "Symmetric Positive Definite Matrices"
    },
    {
      "source": "Symmetric Matrix",
      "type": "related_to",
      "target": "Hermitian Matrix"
    },
    {
      "source": "Symmetric Positive Definite Matrices",
      "type": "subtype_of",
      "target": "Symmetric Matrix"
    },
    {
      "source": "Symmetric Positive Definite Matrices",
      "type": "related_to",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Triangular Matrices",
      "type": "related_to",
      "target": "Lower Triangular Matrix"
    },
    {
      "source": "Triangular Matrices",
      "type": "related_to",
      "target": "Upper Triangular Matrix"
    },
    {
      "source": "Triangular Solves",
      "type": "related_to",
      "target": "LU Factorization"
    },
    {
      "source": "Triangular Solves",
      "type": "related_to",
      "target": "Triangular Matrices"
    },
    {
      "source": "Tridiagonal Matrix",
      "type": "used_in",
      "target": "Lanczos Algorithm"
    },
    {
      "source": "Tridiagonal Matrix",
      "type": "related_to",
      "target": "SPD Matrices"
    },
    {
      "source": "Unitary Similarity",
      "type": "related_to",
      "target": "Unitary Matrix"
    },
    {
      "source": "Unitary Similarity",
      "type": "related_to",
      "target": "Schur Triangulation"
    },
    {
      "source": "Unitary Transformation",
      "type": "generalization_of",
      "target": "Givens Rotation"
    },
    {
      "source": "Unitary Transformation",
      "type": "generalization_of",
      "target": "Householder Reflection"
    },
    {
      "source": "Upper Triangular Factor",
      "type": "subcomponent_of",
      "target": "LU Factorization"
    },
    {
      "source": "Upper Triangular Factor",
      "type": "related_to",
      "target": "Triangular Matrices"
    },
    {
      "source": "Upper Triangular Matrix",
      "type": "related_to",
      "target": "Triangular Matrices"
    },
    {
      "source": "Upper Triangular Matrix",
      "type": "related_to",
      "target": "QR Algorithm"
    },
    {
      "source": "Vandermonde Matrix",
      "type": "related_to",
      "target": "Polynomial Approximation"
    },
    {
      "source": "Vandermonde Matrix",
      "type": "related_to",
      "target": "Matrix Rank"
    },
    {
      "source": "Vector Subspace",
      "type": "foundation_for",
      "target": "Matrix Subspace"
    },
    {
      "source": "Vector Subspace",
      "type": "related_to",
      "target": "Orthogonal Complement"
    },
    {
      "source": "Residual r = b - Ax",
      "type": "related_to",
      "target": "Residual Norm"
    },
    {
      "source": "Residual r = b - Ax",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Matrix Square Root (Alternative Notation)",
      "type": "synonym_of",
      "target": "Matrix Square Root"
    },
    {
      "source": "Numerosity Reduction",
      "type": "part_of",
      "target": "Data Reduction"
    },
    {
      "source": "Numerosity Reduction",
      "type": "related_to",
      "target": "Sampling"
    },
    {
      "source": "Numerosity Reduction",
      "type": "related_to",
      "target": "Clustering"
    },
    {
      "source": "Numerosity Reduction",
      "type": "related_to",
      "target": "Histograms"
    },
    {
      "source": "Numerosity Reduction",
      "type": "contrasts_with",
      "target": "Dimensionality Reduction"
    },
    {
      "source": "Accuracy Paradox",
      "type": "contrasts_with",
      "target": "Balanced Accuracy"
    },
    {
      "source": "Accuracy Paradox",
      "type": "related_to",
      "target": "Imbalanced Data"
    },
    {
      "source": "Accuracy Paradox",
      "type": "related_to",
      "target": "Model Evaluation Metrics"
    },
    {
      "source": "Validation Set",
      "type": "part_of",
      "target": "Data Partitioning"
    },
    {
      "source": "Validation Set",
      "type": "related_to",
      "target": "Model Selection"
    },
    {
      "source": "Validation Set",
      "type": "used_in",
      "target": "Avoiding Overfitting"
    },
    {
      "source": "Artificial Data Generation",
      "type": "related_to",
      "target": "Artificial Data"
    },
    {
      "source": "Artificial Data Generation",
      "type": "related_to",
      "target": "SMOTE"
    },
    {
      "source": "Artificial Data Generation",
      "type": "related_to",
      "target": "Imbalanced Data"
    },
    {
      "source": "Artificial Data Generation",
      "type": "used_in",
      "target": "Data Augmentation"
    },
    {
      "source": "Artificial Data",
      "type": "instance_of",
      "target": "Artificial Data Generation"
    },
    {
      "source": "Artificial Data",
      "type": "related_to",
      "target": "Noise Injection"
    },
    {
      "source": "Artificial Data",
      "type": "related_to",
      "target": "SMOTE"
    },
    {
      "source": "Personal Data",
      "type": "protected_by",
      "target": "General Data Protection Regulation (GDPR)"
    },
    {
      "source": "Pseudonymisation",
      "type": "related_to",
      "target": "Personal Data"
    },
    {
      "source": "Pseudonymisation",
      "type": "contrasts_with",
      "target": "Anonymisation"
    },
    {
      "source": "k-Anonymity",
      "type": "applied_to",
      "target": "Tabular Data"
    },
    {
      "source": "k-Anonymity",
      "type": "supports",
      "target": "Anonymisation"
    },
    {
      "source": "Advanced Data Modification",
      "type": "derived_from",
      "target": "Anonymisation"
    },
    {
      "source": "Relational Database",
      "type": "supports",
      "target": "SQL"
    },
    {
      "source": "NoSQL Databases",
      "type": "contrasts_with",
      "target": "Relational Database"
    },
    {
      "source": "MongoDB",
      "type": "instance_of",
      "target": "NoSQL Databases"
    },
    {
      "source": "Entity-Relationship Modeling",
      "type": "used_to",
      "target": "Design Relational Database schemas"
    },
    {
      "source": "Relational Algebra",
      "type": "foundation_of",
      "target": "SQL"
    },
    {
      "source": "SQL",
      "type": "used_in",
      "target": "Relational Database"
    },
    {
      "source": "CIA Triad",
      "type": "applies_to",
      "target": "Information Security"
    },
    {
      "source": "Open Data Licences",
      "type": "applies_to",
      "target": "Open Data"
    },
    {
      "source": "Metadata",
      "type": "supports",
      "target": "FAIR Principles"
    },
    {
      "source": "ActivityMeasurement",
      "type": "part_of",
      "target": "Activity Monitoring Schema"
    },
    {
      "source": "Data Collection Procedure",
      "type": "supports",
      "target": "Data Collection"
    },
    {
      "source": "Data Collection Procedure",
      "type": "related_to",
      "target": "Repeatability"
    },
    {
      "source": "Data Collection Failure Modes",
      "type": "affects",
      "target": "Data Quality"
    },
    {
      "source": "Signal Filtering Errors",
      "type": "instance_of",
      "target": "Data Collection Failure Modes"
    },
    {
      "source": "Sensor Placement Error",
      "type": "related_to",
      "target": "Human Data Collection"
    },
    {
      "source": "Battery Failure",
      "type": "affects",
      "target": "Data Completeness"
    },
    {
      "source": "Repeatability",
      "type": "supports",
      "target": "Reproducibility"
    },
    {
      "source": "Reproducibility",
      "type": "requires",
      "target": "Documentation"
    },
    {
      "source": "Stability",
      "type": "related_to",
      "target": "Non-Stationary Elements"
    },
    {
      "source": "Margin of Error",
      "type": "part_of",
      "target": "Confidence Interval"
    },
    {
      "source": "Confidence Interval",
      "type": "requires",
      "target": "Z-Score"
    },
    {
      "source": "Z-Score (Statistical)",
      "type": "used_in",
      "target": "Confidence Interval"
    },
    {
      "source": "Dropout Rate",
      "type": "affects",
      "target": "Sample Size"
    },
    {
      "source": "Stationary Elements",
      "type": "contrasts_with",
      "target": "Non-Stationary Elements"
    },
    {
      "source": "Non-Stationary Elements",
      "type": "causes",
      "target": "Temporal Drift"
    },
    {
      "source": "Temporal Drift",
      "type": "derived_from",
      "target": "Non-Stationary Elements"
    },
    {
      "source": "Seasonal Variability",
      "type": "instance_of",
      "target": "Non-Stationary Elements"
    },
    {
      "source": "Time Series Data Collection",
      "type": "used_in",
      "target": "Model Generalization"
    },
    {
      "source": "Respect for Persons",
      "type": "part_of",
      "target": "Human Research Ethics"
    },
    {
      "source": "Beneficence",
      "type": "part_of",
      "target": "Human Research Ethics"
    },
    {
      "source": "Justice",
      "type": "part_of",
      "target": "Human Research Ethics"
    },
    {
      "source": "Data Format Differences",
      "type": "affects",
      "target": "Data Merging"
    },
    {
      "source": "Data Format Differences",
      "type": "related_to",
      "target": "Data Quality"
    },
    {
      "source": "Cultural Formatting Differences",
      "type": "related_to",
      "target": "Data Format Differences"
    },
    {
      "source": "Unit Mismatch",
      "type": "affects",
      "target": "Data Merging"
    },
    {
      "source": "Timestamp Mismatch",
      "type": "requires",
      "target": "Timestamp Synchronization"
    },
    {
      "source": "Timestamp Synchronization",
      "type": "supports",
      "target": "Data Merging"
    },
    {
      "source": "Label Inconsistency",
      "type": "affects",
      "target": "Supervised Learning"
    },
    {
      "source": "Sensor Placement Variability",
      "type": "related_to",
      "target": "Calibration Differences"
    },
    {
      "source": "Environmental Condition Variability",
      "type": "affects",
      "target": "Data Merging"
    },
    {
      "source": "Calibration Differences",
      "type": "affects",
      "target": "Sensor Placement Variability"
    },
    {
      "source": "Calibration Differences",
      "type": "related_to",
      "target": "Data Merging"
    },
    {
      "source": "Incompatible Data Collection Protocols",
      "type": "prevents",
      "target": "Data Merging"
    },
    {
      "source": "Sampling Rate Mismatch",
      "type": "requires",
      "target": "GCD-Based Downsampling"
    },
    {
      "source": "Sampling Rate Mismatch",
      "type": "requires",
      "target": "LCM-Based Oversampling"
    },
    {
      "source": "Battery–Sampling Trade-Off",
      "type": "related_to",
      "target": "Sampling Rate Mismatch"
    },
    {
      "source": "GCD-Based Downsampling",
      "type": "addresses",
      "target": "Sampling Rate Mismatch"
    },
    {
      "source": "LCM-Based Oversampling",
      "type": "addresses",
      "target": "Sampling Rate Mismatch"
    },
    {
      "source": "Confusion Matrix",
      "type": "foundation_of",
      "target": "Precision"
    },
    {
      "source": "Confusion Matrix",
      "type": "foundation_of",
      "target": "Recall"
    },
    {
      "source": "Confusion Matrix",
      "type": "foundation_of",
      "target": "F1 Score"
    },
    {
      "source": "Balanced Accuracy",
      "type": "improves",
      "target": "Accuracy"
    },
    {
      "source": "Balanced Accuracy",
      "type": "related_to",
      "target": "Imbalanced Data"
    },
    {
      "source": "Precision",
      "type": "related_to",
      "target": "Recall"
    },
    {
      "source": "Precision",
      "type": "component_of",
      "target": "F1 Score"
    },
    {
      "source": "Recall",
      "type": "related_to",
      "target": "Precision"
    },
    {
      "source": "Recall",
      "type": "component_of",
      "target": "F1 Score"
    },
    {
      "source": "Specificity",
      "type": "related_to",
      "target": "Recall"
    },
    {
      "source": "F1 Score",
      "type": "depends_on",
      "target": "Precision"
    },
    {
      "source": "F1 Score",
      "type": "depends_on",
      "target": "Recall"
    },
    {
      "source": "Cohen’s Kappa",
      "type": "alternative_to",
      "target": "Accuracy"
    },
    {
      "source": "Available-Case Analysis",
      "type": "contrast_with",
      "target": "Complete-Case Analysis"
    },
    {
      "source": "Available-Case Analysis",
      "type": "related_to",
      "target": "Deletion"
    },
    {
      "source": "Complete-Case Analysis",
      "type": "contrast_with",
      "target": "Available-Case Analysis"
    },
    {
      "source": "Complete-Case Analysis",
      "type": "related_to",
      "target": "Deletion"
    },
    {
      "source": "Hot Deck Imputation",
      "type": "contrast_with",
      "target": "Cold Deck Imputation"
    },
    {
      "source": "Hot Deck Imputation",
      "type": "instance_of",
      "target": "Imputation"
    },
    {
      "source": "Cold Deck Imputation",
      "type": "contrast_with",
      "target": "Hot Deck Imputation"
    },
    {
      "source": "Cold Deck Imputation",
      "type": "instance_of",
      "target": "Imputation"
    },
    {
      "source": "Last Observation Carried Forward",
      "type": "related_to",
      "target": "Baseline Observation Carried Forward"
    },
    {
      "source": "Last Observation Carried Forward",
      "type": "part_of",
      "target": "Time-Series Missing Data Handling"
    },
    {
      "source": "Baseline Observation Carried Forward",
      "type": "related_to",
      "target": "Last Observation Carried Forward"
    },
    {
      "source": "Full Information Maximum Likelihood",
      "type": "uses",
      "target": "Expectation-Maximization Algorithm"
    },
    {
      "source": "Full Information Maximum Likelihood",
      "type": "instance_of",
      "target": "Maximum Likelihood Estimation"
    },
    {
      "source": "Expectation-Maximization Algorithm",
      "type": "computes",
      "target": "Full Information Maximum Likelihood"
    },
    {
      "source": "Expectation-Maximization Algorithm",
      "type": "related_to",
      "target": "Maximum Likelihood Estimation"
    },
    {
      "source": "Imputation Pile-Up",
      "type": "caused_by",
      "target": "Mean Imputation"
    },
    {
      "source": "Loss of Variability in Imputed Data",
      "type": "related_to",
      "target": "Imputation Pile-Up"
    },
    {
      "source": "Impossible Imputation Values",
      "type": "related_to",
      "target": "Regression Imputation"
    },
    {
      "source": "Within-Imputation Variance",
      "type": "part_of",
      "target": "Total Variance in Multiple Imputation"
    },
    {
      "source": "Between-Imputation Variance",
      "type": "part_of",
      "target": "Total Variance in Multiple Imputation"
    },
    {
      "source": "Total Variance in Multiple Imputation",
      "type": "includes",
      "target": "Within-Imputation Variance"
    },
    {
      "source": "Total Variance in Multiple Imputation",
      "type": "includes",
      "target": "Between-Imputation Variance"
    },
    {
      "source": "Attribute Noise",
      "type": "related_to",
      "target": "Noise"
    },
    {
      "source": "Attribute Noise",
      "type": "affects",
      "target": "Supervised Learning"
    },
    {
      "source": "Label Noise",
      "type": "part_of",
      "target": "Noisy Data"
    },
    {
      "source": "Label Noise",
      "type": "affects",
      "target": "Supervised Learning"
    },
    {
      "source": "Noise Frequency Filtering",
      "type": "instance_of",
      "target": "Noise Filters"
    },
    {
      "source": "Equipment Error",
      "type": "produces",
      "target": "Attribute Noise"
    },
    {
      "source": "Garbage Values",
      "type": "part_of",
      "target": "Data Pollution"
    },
    {
      "source": "Delimiter Contamination",
      "type": "related_to",
      "target": "Data Pollution"
    },
    {
      "source": "Free-Text Field Contamination",
      "type": "related_to",
      "target": "Data Pollution"
    },
    {
      "source": "Thousand-Separator Contamination",
      "type": "related_to",
      "target": "Data Format Differences"
    },
    {
      "source": "Domain Drift",
      "type": "part_of",
      "target": "Data Pollution"
    },
    {
      "source": "Global Outliers",
      "type": "instance_of",
      "target": "Outliers"
    },
    {
      "source": "Contextual Outliers",
      "type": "contrast_with",
      "target": "Global Outliers"
    },
    {
      "source": "Collective Outliers",
      "type": "related_to",
      "target": "Contextual Outliers"
    },
    {
      "source": "Outlier Score",
      "type": "used_in",
      "target": "Unsupervised Outlier Detection"
    },
    {
      "source": "Binary Outlier Classification",
      "type": "derived_from",
      "target": "Outlier Score"
    },
    {
      "source": "Subjective Outlier Thresholding",
      "type": "related_to",
      "target": "Binary Outlier Classification"
    },
    {
      "source": "Supervised Outlier Detection",
      "type": "contrast_with",
      "target": "Unsupervised Outlier Detection"
    },
    {
      "source": "Semi-Supervised Outlier Detection",
      "type": "contrast_with",
      "target": "Supervised Outlier Detection"
    },
    {
      "source": "Unsupervised Outlier Detection",
      "type": "contrast_with",
      "target": "Supervised Outlier Detection"
    },
    {
      "source": "Parametric Outlier Detection",
      "type": "instance_of",
      "target": "Statistical Outlier Detection"
    },
    {
      "source": "Nonparametric Outlier Detection",
      "type": "instance_of",
      "target": "Statistical Outlier Detection"
    },
    {
      "source": "Gaussian Outlier Modeling",
      "type": "part_of",
      "target": "Parametric Outlier Detection"
    },
    {
      "source": "Kernel Density Estimation for Outlier Detection",
      "type": "part_of",
      "target": "Nonparametric Outlier Detection"
    },
    {
      "source": "Distance-Based Outlier Detection",
      "type": "related_to",
      "target": "Proximity-Based Methods"
    },
    {
      "source": "Density-Based Outlier Detection",
      "type": "related_to",
      "target": "Proximity-Based Methods"
    },
    {
      "source": "Clustering-Based Outlier Detection",
      "type": "related_to",
      "target": "Clustering Algorithms"
    },
    {
      "source": "Sparse-Cluster Outliers",
      "type": "instance_of",
      "target": "Clustering-Based Outlier Detection"
    },
    {
      "source": "Gaussian Mixture Model Outlier Detection",
      "type": "related_to",
      "target": "Gaussian Outlier Modeling"
    },
    {
      "source": "One-Class Classification",
      "type": "used_in",
      "target": "Supervised Outlier Detection"
    },
    {
      "source": "Outlier Detection via SVM",
      "type": "instance_of",
      "target": "One-Class Classification"
    },
    {
      "source": "Outlier Detection via Random Forest",
      "type": "related_to",
      "target": "Supervised Outlier Detection"
    },
    {
      "source": "Predictive Outlier Detection",
      "type": "used_in",
      "target": "Streaming Data Analysis"
    },
    {
      "source": "Streaming Outlier Detection",
      "type": "related_to",
      "target": "Predictive Outlier Detection"
    },
    {
      "source": "Unusual-Shape Outlier Detection",
      "type": "related_to",
      "target": "Streaming Outlier Detection"
    },
    {
      "source": "Multidimensional Streaming Outlier Detection",
      "type": "related_to",
      "target": "Streaming Outlier Detection"
    },
    {
      "source": "Change-Point Outlier Detection",
      "type": "instance_of",
      "target": "Time-Series Outlier Detection"
    },
    {
      "source": "Rare-Class Detection in Time Series",
      "type": "related_to",
      "target": "Multidimensional Streaming Outlier Detection"
    },
    {
      "source": "Scaling to Arbitrary Range",
      "type": "related_to",
      "target": "Min-Max Normalization"
    },
    {
      "source": "Scaling to Arbitrary Range",
      "type": "part_of",
      "target": "Data Transformation"
    },
    {
      "source": "Storing Normalization Parameters",
      "type": "supports",
      "target": "Model Generalization"
    },
    {
      "source": "Storing Normalization Parameters",
      "type": "required_by",
      "target": "Min-Max Normalization"
    },
    {
      "source": "Extreme Z-Score Threshold Rule",
      "type": "related_to",
      "target": "Z-score Standardization"
    },
    {
      "source": "Extreme Z-Score Threshold Rule",
      "type": "instance_of",
      "target": "Outlier Detection Methods"
    },
    {
      "source": "Measurement Error Visualization",
      "type": "supports",
      "target": "Outlier Detection Methods"
    },
    {
      "source": "Measurement Error Visualization",
      "type": "related_to",
      "target": "Data Quality"
    },
    {
      "source": "Median Split",
      "type": "instance_of",
      "target": "Discretization"
    },
    {
      "source": "Distinctive Grouping",
      "type": "part_of",
      "target": "Discretization"
    },
    {
      "source": "Equal-Length Binning",
      "type": "contrasts_with",
      "target": "Equal-Size Binning"
    },
    {
      "source": "Equal-Size Binning",
      "type": "contrasts_with",
      "target": "Equal-Length Binning"
    },
    {
      "source": "Natural Cut Points",
      "type": "related_to",
      "target": "Discretization"
    },
    {
      "source": "Concept Hierarchies",
      "type": "used_in",
      "target": "Data Generalization"
    },
    {
      "source": "Concept Hierarchies",
      "type": "part_of",
      "target": "Discretization"
    },
    {
      "source": "Dichotomization",
      "type": "related_to",
      "target": "Median Split"
    },
    {
      "source": "Problems of Discretization",
      "type": "related_to",
      "target": "Discretization"
    },
    {
      "source": "Information Loss Through Dichotomization",
      "type": "instance_of",
      "target": "Problems of Discretization"
    },
    {
      "source": "Misclassification Risk at Cut Points",
      "type": "related_to",
      "target": "Discretization"
    },
    {
      "source": "Factor Variables",
      "type": "used_in",
      "target": "Dummy Coding"
    },
    {
      "source": "Frequency Tables",
      "type": "related_to",
      "target": "Categorical Data"
    },
    {
      "source": "Correspondence Tests",
      "type": "related_to",
      "target": "Frequency Tables"
    },
    {
      "source": "Categorical vs Continuous Visualization",
      "type": "supports",
      "target": "Factor Variables"
    },
    {
      "source": "Data Aggregation",
      "type": "part_of",
      "target": "Data Reduction"
    },
    {
      "source": "Data Generalization",
      "type": "uses",
      "target": "Concept Hierarchies"
    },
    {
      "source": "Variable Construction",
      "type": "related_to",
      "target": "Data Reduction"
    },
    {
      "source": "Nonparametric Data Reduction Methods",
      "type": "related_to",
      "target": "Data Reduction"
    },
    {
      "source": "Transformation to Normality",
      "type": "related_to",
      "target": "Box-Cox Transformation"
    },
    {
      "source": "Normality Tests",
      "type": "supports",
      "target": "Transformation to Normality"
    },
    {
      "source": "Solutions for Non-Normality",
      "type": "uses",
      "target": "Normality Tests"
    },
    {
      "source": "Solutions for Non-Normality",
      "type": "related_to",
      "target": "Transformation to Normality"
    },
    {
      "source": "Generalizability",
      "type": "related_to",
      "target": "Model Generalization"
    },
    {
      "source": "Generalizability",
      "type": "part_of",
      "target": "Model Evaluation"
    },
    {
      "source": "Generalization Error",
      "type": "related_to",
      "target": "Generalizability"
    },
    {
      "source": "Generalization Error",
      "type": "contrast_with",
      "target": "Training Error"
    },
    {
      "source": "Generalization Gap",
      "type": "related_to",
      "target": "Overfitting"
    },
    {
      "source": "Dataset Generalization",
      "type": "related_to",
      "target": "Generalizability"
    },
    {
      "source": "Retrospective Models",
      "type": "instance_of",
      "target": "Descriptive Modeling"
    },
    {
      "source": "Prospective Models",
      "type": "instance_of",
      "target": "Predictive Modeling"
    },
    {
      "source": "Theory-Driven Models",
      "type": "contrast_with",
      "target": "Data-Driven Models"
    },
    {
      "source": "Data-Driven Models",
      "type": "contrast_with",
      "target": "Theory-Driven Models"
    },
    {
      "source": "Explanatory Power",
      "type": "often_increases",
      "target": "Model Simplicity"
    },
    {
      "source": "Predictive Power",
      "type": "related_to",
      "target": "Predictive Modeling"
    },
    {
      "source": "Minimum Sample Size for Statistical Inference",
      "type": "related_to",
      "target": "Sample Size"
    },
    {
      "source": "Sample Size for Predictive Modeling",
      "type": "related_to",
      "target": "Sample Size"
    },
    {
      "source": "Precision of Parameter Estimation",
      "type": "related_to",
      "target": "Sample Size"
    },
    {
      "source": "Population Representativeness",
      "type": "supports",
      "target": "Generalizability"
    },
    {
      "source": "Access to Data",
      "type": "affects",
      "target": "Sample Size"
    },
    {
      "source": "Training Set",
      "type": "part_of",
      "target": "Data Partitioning"
    },
    {
      "source": "Test Set",
      "type": "part_of",
      "target": "Data Partitioning"
    },
    {
      "source": "Hold-Out Method",
      "type": "instance_of",
      "target": "Data Partitioning"
    },
    {
      "source": "Eighty-Twenty Split",
      "type": "instance_of",
      "target": "Hold-Out Method"
    },
    {
      "source": "Two-Thirds One-Third Split",
      "type": "instance_of",
      "target": "Hold-Out Method"
    },
    {
      "source": "Training-Validation-Test Split",
      "type": "related_to",
      "target": "Cross-Validation"
    },
    {
      "source": "Participant-Wise Cross-Validation",
      "type": "related_to",
      "target": "Cross-Validation"
    },
    {
      "source": "Leave-One-Out Validation",
      "type": "part_of",
      "target": "Cross-Validation"
    },
    {
      "source": "Temporal Dependency in Data",
      "type": "affects",
      "target": "Data Partitioning"
    },
    {
      "source": "No Random Sampling Under Temporal Dependence",
      "type": "related_to",
      "target": "Temporal Dependency in Data"
    },
    {
      "source": "Day-Based Partitioning",
      "type": "addresses",
      "target": "Temporal Leakage"
    },
    {
      "source": "Temporal Leakage",
      "type": "caused_by",
      "target": "Temporal Dependency in Data"
    },
    {
      "source": "Sliding Windows",
      "type": "related_to",
      "target": "Window-Based Feature Extraction"
    },
    {
      "source": "Overlapping Windows",
      "type": "causes",
      "target": "Temporal Dependency in Data"
    },
    {
      "source": "Window-Based Feature Extraction",
      "type": "uses",
      "target": "Sliding Windows"
    },
    {
      "source": "Individual Models",
      "type": "contrast_with",
      "target": "Population Models"
    },
    {
      "source": "Population Models",
      "type": "contrast_with",
      "target": "Individual Models"
    },
    {
      "source": "Activity Recognition",
      "type": "uses",
      "target": "Window-Based Feature Extraction"
    },
    {
      "source": "Sensor-Based Activity Detection",
      "type": "related_to",
      "target": "Activity Recognition"
    },
    {
      "source": "Multi-Modal Feature Inputs",
      "type": "used_in",
      "target": "Activity Recognition"
    },
    {
      "source": "Model Complexity–Generalization Tradeoff",
      "type": "related_to",
      "target": "Model Selection"
    },
    {
      "source": "Simpler Models Generalize Better",
      "type": "supports",
      "target": "Model Complexity–Generalization Tradeoff"
    },
    {
      "source": "Overcomplex Models",
      "type": "related_to",
      "target": "Overfitting"
    },
    {
      "source": "Hyperparameter Selection",
      "type": "related_to",
      "target": "Model Selection"
    },
    {
      "source": "Hyperparameter Selection",
      "type": "related_to",
      "target": "Cross-Validation"
    },
    {
      "source": "Feature Selection",
      "type": "part_of",
      "target": "Model Selection"
    },
    {
      "source": "Machine Learning",
      "type": "related_to",
      "target": "Predictive Modeling"
    },
    {
      "source": "Machine Learning",
      "type": "parent_of",
      "target": "Machine Learning Models"
    },
    {
      "source": "Machine Learning Models",
      "type": "instance_of",
      "target": "Machine Learning"
    },
    {
      "source": "Machine Learning Models",
      "type": "related_to",
      "target": "Model Selection"
    },
    {
      "source": "Supervised Learning",
      "type": "related_to",
      "target": "Predictive Modeling"
    },
    {
      "source": "Supervised Learning",
      "type": "related_to",
      "target": "Label Noise"
    },
    {
      "source": "Avoiding Overfitting",
      "type": "mitigates",
      "target": "Overfitting"
    },
    {
      "source": "Avoiding Overfitting",
      "type": "related_to",
      "target": "Hyperparameter Selection"
    },
    {
      "source": "Bias-Variance Tradeoff",
      "type": "related_to",
      "target": "Model Complexity–Generalization Tradeoff"
    },
    {
      "source": "Bias-Variance Tradeoff",
      "type": "affects",
      "target": "Generalization Error"
    },
    {
      "source": "Regularization",
      "type": "mitigates",
      "target": "Overfitting"
    },
    {
      "source": "Regularization",
      "type": "related_to",
      "target": "Hyperparameter Selection"
    },
    {
      "source": "Model Bias",
      "type": "related_to",
      "target": "Bias"
    },
    {
      "source": "Model Bias",
      "type": "affects",
      "target": "Model Evaluation"
    },
    {
      "source": "Accuracy",
      "type": "alternative_to",
      "target": "Balanced Accuracy"
    },
    {
      "source": "Accuracy",
      "type": "related_to",
      "target": "Performance Metrics"
    },
    {
      "source": "Training Error",
      "type": "contrast_with",
      "target": "Generalization Error"
    },
    {
      "source": "Training Error",
      "type": "component_of",
      "target": "Generalization Gap"
    },
    {
      "source": "Statistical Methods",
      "type": "includes",
      "target": "Maximum Likelihood Estimation (MLE)"
    },
    {
      "source": "Statistical Methods",
      "type": "related_to",
      "target": "Statistical Power"
    },
    {
      "source": "Specialized Statistical Modeling",
      "type": "related_to",
      "target": "Parameterized Models"
    },
    {
      "source": "Specialized Statistical Modeling",
      "type": "related_to",
      "target": "Time Series Analysis"
    },
    {
      "source": "Regression Modeling",
      "type": "related_to",
      "target": "Regression Imputation"
    },
    {
      "source": "Regression Modeling",
      "type": "related_to",
      "target": "Statistical Methods"
    },
    {
      "source": "Causal Hypothesis",
      "type": "related_to",
      "target": "Explanatory Modeling"
    },
    {
      "source": "Causal Hypothesis",
      "type": "related_to",
      "target": "Randomized Controlled Trial (RCT)"
    },
    {
      "source": "Statistical Power",
      "type": "related_to",
      "target": "Minimum Sample Size for Statistical Inference"
    },
    {
      "source": "Statistical Significance",
      "type": "related_to",
      "target": "Statistical Methods"
    },
    {
      "source": "Statistical Significance",
      "type": "related_to",
      "target": "Statistical Power"
    },
    {
      "source": "Nonlinear PCA",
      "type": "related_to",
      "target": "Principal Component Analysis (PCA)"
    },
    {
      "source": "Statistical Outlier Detection",
      "type": "parent_of",
      "target": "Parametric Outlier Detection"
    },
    {
      "source": "Statistical Outlier Detection",
      "type": "parent_of",
      "target": "Nonparametric Outlier Detection"
    },
    {
      "source": "Experimental Design",
      "type": "related_to",
      "target": "Randomized Controlled Trial (RCT)"
    },
    {
      "source": "Experimental Design",
      "type": "related_to",
      "target": "Sample Size"
    },
    {
      "source": "Poor Experimental Design",
      "type": "related_to",
      "target": "Human Data Collection"
    },
    {
      "source": "Poor Experimental Design",
      "type": "contrasts_with",
      "target": "Experimental Design"
    },
    {
      "source": "Sampling Rate Adjustment",
      "type": "related_to",
      "target": "Sampling Rate Mismatch"
    },
    {
      "source": "Sampling Rate Adjustment",
      "type": "related_to",
      "target": "Timestamp Synchronization"
    },
    {
      "source": "Data Augmentation",
      "type": "related_to",
      "target": "Synthetic Minority Oversampling Technique (SMOTE)"
    },
    {
      "source": "Data Augmentation",
      "type": "mitigates",
      "target": "Overfitting"
    },
    {
      "source": "Research Data Management",
      "type": "related_to",
      "target": "Metadata"
    },
    {
      "source": "Research Data Management",
      "type": "related_to",
      "target": "FAIR Principles"
    },
    {
      "source": "Documentation",
      "type": "related_to",
      "target": "Data Documentation"
    },
    {
      "source": "Documentation",
      "type": "supports",
      "target": "Reproducibility"
    },
    {
      "source": "Design Relational Database Schemas",
      "type": "related_to",
      "target": "Entity-Relationship Modeling"
    },
    {
      "source": "Design Relational Database Schemas",
      "type": "supports",
      "target": "Relational Database"
    },
    {
      "source": "Tabular Data",
      "type": "related_to",
      "target": "Relational Database"
    },
    {
      "source": "Tabular Data",
      "type": "used_in",
      "target": "Predictive Modeling"
    },
    {
      "source": "Data Protection Laws",
      "type": "includes",
      "target": "General Data Protection Regulation (GDPR)"
    },
    {
      "source": "Data Protection Laws",
      "type": "related_to",
      "target": "Pseudonymisation"
    },
    {
      "source": "GO FAIR Initiative",
      "type": "related_to",
      "target": "FAIR Principles"
    },
    {
      "source": "GO FAIR Initiative",
      "type": "related_to",
      "target": "Open Data"
    },
    {
      "source": "Human Research Ethics",
      "type": "related_to",
      "target": "Respect for Persons"
    },
    {
      "source": "Human Research Ethics",
      "type": "supports",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Ethical Approval",
      "type": "required_for",
      "target": "Human Data Collection"
    },
    {
      "source": "Ethical Approval",
      "type": "related_to",
      "target": "Informed Consent"
    },
    {
      "source": "Privacy Preservation",
      "type": "related_to",
      "target": "k-Anonymity"
    },
    {
      "source": "Privacy Preservation",
      "type": "related_to",
      "target": "Pseudonymisation"
    },
    {
      "source": "Time Series Analysis",
      "type": "related_to",
      "target": "Time Series Data Collection"
    },
    {
      "source": "Time Series Analysis",
      "type": "related_to",
      "target": "Temporal Drift"
    },
    {
      "source": "Streaming Data Analysis",
      "type": "related_to",
      "target": "Streaming Outlier Detection"
    },
    {
      "source": "Streaming Data Analysis",
      "type": "related_to",
      "target": "Multidimensional Streaming Outlier Detection"
    },
    {
      "source": "Time-Series Missing Data Handling",
      "type": "related_to",
      "target": "Last Observation Carried Forward"
    },
    {
      "source": "Time-Series Missing Data Handling",
      "type": "related_to",
      "target": "Baseline Observation Carried Forward"
    },
    {
      "source": "Temporal Alignment",
      "type": "related_to",
      "target": "Timestamp Synchronization"
    },
    {
      "source": "Temporal Alignment",
      "type": "related_to",
      "target": "Timestamp Mismatch"
    },
    {
      "source": "Data Completeness",
      "type": "related_to",
      "target": "Missing Data"
    },
    {
      "source": "Data Completeness",
      "type": "affects",
      "target": "Reproducibility"
    },
    {
      "source": "Scaling",
      "type": "related_to",
      "target": "Min-Max Normalization"
    },
    {
      "source": "Scaling",
      "type": "related_to",
      "target": "Z-score Standardization"
    },
    {
      "source": "Sensor Calibration",
      "type": "related_to",
      "target": "Calibration Differences"
    },
    {
      "source": "Sensor Calibration",
      "type": "mitigates",
      "target": "Equipment Error"
    },
    {
      "source": "Signal Quality",
      "type": "related_to",
      "target": "Noise"
    },
    {
      "source": "Signal Quality",
      "type": "related_to",
      "target": "Signal Saturation"
    },
    {
      "source": "Data Mining",
      "type": "part_of",
      "target": "Knowledge Discovery in Databases"
    },
    {
      "source": "Data Mining",
      "type": "requires",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Mining",
      "type": "related_to",
      "target": "Machine Learning"
    },
    {
      "source": "Knowledge Discovery in Databases",
      "type": "includes",
      "target": "Data Mining"
    },
    {
      "source": "Knowledge Discovery in Databases",
      "type": "depends_on",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Pre-processing",
      "type": "enables",
      "target": "Data Mining"
    },
    {
      "source": "Data Pre-processing",
      "type": "requires",
      "target": "Raw Data"
    },
    {
      "source": "Data Quality",
      "type": "affects",
      "target": "Data Mining"
    },
    {
      "source": "Data Quality",
      "type": "improved_by",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Cleaning",
      "type": "part_of",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Cleaning",
      "type": "enables",
      "target": "Data Mining"
    },
    {
      "source": "Big Data",
      "type": "context_for",
      "target": "Data Mining"
    },
    {
      "source": "Big Data",
      "type": "related_to",
      "target": "Machine Learning"
    },
    {
      "source": "Data Literacy",
      "type": "required_for",
      "target": "Data Mining"
    },
    {
      "source": "Data Literacy",
      "type": "enhances",
      "target": "Decision Making"
    },
    {
      "source": "Data Transformation",
      "type": "part_of",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Transformation",
      "type": "used_by",
      "target": "Machine Learning Models"
    },
    {
      "source": "Data Reduction",
      "type": "part_of",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Reduction",
      "type": "supports",
      "target": "Data Mining"
    },
    {
      "source": "Data Ethics",
      "type": "related_to",
      "target": "Information Privacy"
    },
    {
      "source": "Data Ethics",
      "type": "governed_by",
      "target": "Data Protection Laws"
    },
    {
      "source": "Data Ethics",
      "type": "component_of",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Ethical Data Mining",
      "type": "requires",
      "target": "Informed Consent"
    },
    {
      "source": "Ethical Data Mining",
      "type": "related_to",
      "target": "Privacy Preservation"
    },
    {
      "source": "Ethical Data Mining",
      "type": "regulated_by",
      "target": "GDPR"
    },
    {
      "source": "Informed Consent",
      "type": "required_by",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Informed Consent",
      "type": "related_to",
      "target": "Privacy Preservation"
    },
    {
      "source": "Informed Consent",
      "type": "regulated_by",
      "target": "GDPR"
    },
    {
      "source": "General Data Protection Regulation (GDPR)",
      "type": "governs",
      "target": "Personal Data"
    },
    {
      "source": "General Data Protection Regulation (GDPR)",
      "type": "related_to",
      "target": "Data Ethics"
    },
    {
      "source": "General Data Protection Regulation (GDPR)",
      "type": "enforces",
      "target": "Informed Consent"
    },
    {
      "source": "Information Security",
      "type": "related_to",
      "target": "Information Privacy"
    },
    {
      "source": "Information Security",
      "type": "supports",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Information Security",
      "type": "includes",
      "target": "Confidentiality"
    },
    {
      "source": "Information Privacy",
      "type": "protected_by",
      "target": "Information Security"
    },
    {
      "source": "Information Privacy",
      "type": "threatened_by",
      "target": "Data Mining"
    },
    {
      "source": "Information Privacy",
      "type": "regulated_by",
      "target": "GDPR"
    },
    {
      "source": "Anonymisation",
      "type": "used_in",
      "target": "Privacy Preservation"
    },
    {
      "source": "Anonymisation",
      "type": "required_by",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Anonymisation",
      "type": "related_to",
      "target": "Pseudonymisation"
    },
    {
      "source": "Open Data",
      "type": "supports",
      "target": "Scientific Research"
    },
    {
      "source": "Open Data",
      "type": "regulated_by",
      "target": "Open Knowledge Foundation"
    },
    {
      "source": "Open Data",
      "type": "guided_by",
      "target": "FAIR Principles"
    },
    {
      "source": "FAIR Principles",
      "type": "applies_to",
      "target": "Open Data"
    },
    {
      "source": "FAIR Principles",
      "type": "promoted_by",
      "target": "GO FAIR Initiative"
    },
    {
      "source": "FAIR Principles",
      "type": "used_in",
      "target": "Research Data Management"
    },
    {
      "source": "Data Management",
      "type": "includes",
      "target": "Database Management"
    },
    {
      "source": "Data Management",
      "type": "related_to",
      "target": "Open Data"
    },
    {
      "source": "Data Management",
      "type": "supports",
      "target": "Data Mining"
    },
    {
      "source": "Data Collection",
      "type": "part_of",
      "target": "Knowledge Discovery in Databases"
    },
    {
      "source": "Data Collection",
      "type": "requires",
      "target": "Data Planning"
    },
    {
      "source": "Data Collection",
      "type": "related_to",
      "target": "Data Quality"
    },
    {
      "source": "Data Planning",
      "type": "enables",
      "target": "Data Collection"
    },
    {
      "source": "Data Planning",
      "type": "supports",
      "target": "Experimental Design"
    },
    {
      "source": "Data Planning",
      "type": "improves",
      "target": "Data Quality"
    },
    {
      "source": "Sampling",
      "type": "used_in",
      "target": "Data Collection"
    },
    {
      "source": "Sampling",
      "type": "related_to",
      "target": "Statistical Significance"
    },
    {
      "source": "Stationary Data",
      "type": "contrasts_with",
      "target": "Non-Stationary Data"
    },
    {
      "source": "Stationary Data",
      "type": "used_in",
      "target": "Time Series Analysis"
    },
    {
      "source": "Non-Stationary Data",
      "type": "contrasts_with",
      "target": "Stationary Data"
    },
    {
      "source": "Non-Stationary Data",
      "type": "used_in",
      "target": "Human-Centered Data Collection"
    },
    {
      "source": "Human Data Collection",
      "type": "requires",
      "target": "Informed Consent"
    },
    {
      "source": "Human Data Collection",
      "type": "related_to",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Human Data Collection",
      "type": "example_of",
      "target": "Non-Stationary Data"
    },
    {
      "source": "Bias",
      "type": "mitigated_by",
      "target": "Randomized Controlled Trial"
    },
    {
      "source": "Bias",
      "type": "caused_by",
      "target": "Poor Experimental Design"
    },
    {
      "source": "Bias",
      "type": "related_to",
      "target": "Data Quality"
    },
    {
      "source": "Randomized Controlled Trial (RCT)",
      "type": "reduces",
      "target": "Bias"
    },
    {
      "source": "Randomized Controlled Trial (RCT)",
      "type": "used_in",
      "target": "Human Data Collection"
    },
    {
      "source": "Randomized Controlled Trial (RCT)",
      "type": "requires",
      "target": "Ethical Approval"
    },
    {
      "source": "Data Documentation",
      "type": "required_by",
      "target": "Data Collection"
    },
    {
      "source": "Data Documentation",
      "type": "supports",
      "target": "Open Science"
    },
    {
      "source": "Data Documentation",
      "type": "related_to",
      "target": "FAIR Principles"
    },
    {
      "source": "Data Merging",
      "type": "requires",
      "target": "Sampling Synchronization"
    },
    {
      "source": "Data Merging",
      "type": "affected_by",
      "target": "Sensor Calibration"
    },
    {
      "source": "Data Merging",
      "type": "supports",
      "target": "Data Integration"
    },
    {
      "source": "Sampling Synchronization",
      "type": "used_in",
      "target": "Data Merging"
    },
    {
      "source": "Sampling Synchronization",
      "type": "requires",
      "target": "Sampling Rate Adjustment"
    },
    {
      "source": "Sampling Synchronization",
      "type": "related_to",
      "target": "Temporal Alignment"
    },
    {
      "source": "Downsampling",
      "type": "complementary_to",
      "target": "Oversampling"
    },
    {
      "source": "Downsampling",
      "type": "used_in",
      "target": "Sampling Synchronization"
    },
    {
      "source": "Oversampling",
      "type": "complementary_to",
      "target": "Downsampling"
    },
    {
      "source": "Oversampling",
      "type": "used_in",
      "target": "Sampling Synchronization"
    },
    {
      "source": "Sampling Methods",
      "type": "includes",
      "target": "SRSWR"
    },
    {
      "source": "Sampling Methods",
      "type": "includes",
      "target": "SRSWOR"
    },
    {
      "source": "Sampling Methods",
      "type": "includes",
      "target": "Balanced Sampling"
    },
    {
      "source": "Simple Random Sampling Without Replacement (SRSWOR)",
      "type": "subtype_of",
      "target": "Sampling Methods"
    },
    {
      "source": "Simple Random Sampling Without Replacement (SRSWOR)",
      "type": "alternative_to",
      "target": "SRSWR"
    },
    {
      "source": "Simple Random Sampling With Replacement (SRSWR)",
      "type": "subtype_of",
      "target": "Sampling Methods"
    },
    {
      "source": "Simple Random Sampling With Replacement (SRSWR)",
      "type": "alternative_to",
      "target": "SRSWOR"
    },
    {
      "source": "Simple Random Sampling With Replacement (SRSWR)",
      "type": "used_in",
      "target": "Artificial Data Generation"
    },
    {
      "source": "Balanced Sampling",
      "type": "used_in",
      "target": "Imbalanced Data Handling"
    },
    {
      "source": "Balanced Sampling",
      "type": "requires",
      "target": "Sampling Methods"
    },
    {
      "source": "Synthetic Minority Oversampling Technique (SMOTE)",
      "type": "used_in",
      "target": "Imbalanced Data Handling"
    },
    {
      "source": "Synthetic Minority Oversampling Technique (SMOTE)",
      "type": "related_to",
      "target": "Balanced Sampling"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "Accuracy"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "Precision"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "Recall"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "F1-score"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "Cohen’s Kappa"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "Balanced Accuracy"
    },
    {
      "source": "Missing Data",
      "type": "has_subtype",
      "target": "Missing Completely at Random"
    },
    {
      "source": "Missing Data",
      "type": "has_subtype",
      "target": "Missing at Random"
    },
    {
      "source": "Missing Data",
      "type": "has_subtype",
      "target": "Missing Not at Random"
    },
    {
      "source": "Missing Data",
      "type": "handled_by",
      "target": "Imputation"
    },
    {
      "source": "Missing Data",
      "type": "handled_by",
      "target": "Deletion"
    },
    {
      "source": "Missing Data",
      "type": "handled_by",
      "target": "Maximum Likelihood Estimation"
    },
    {
      "source": "Missing Completely at Random (MCAR)",
      "type": "subtype_of",
      "target": "Missing Data"
    },
    {
      "source": "Missing Completely at Random (MCAR)",
      "type": "related_to",
      "target": "Complete-Case Analysis"
    },
    {
      "source": "Missing at Random (MAR)",
      "type": "subtype_of",
      "target": "Missing Data"
    },
    {
      "source": "Missing at Random (MAR)",
      "type": "used_in",
      "target": "Multiple Imputation"
    },
    {
      "source": "Missing at Random (MAR)",
      "type": "used_in",
      "target": "Maximum Likelihood Estimation"
    },
    {
      "source": "Missing Not at Random (MNAR)",
      "type": "subtype_of",
      "target": "Missing Data"
    },
    {
      "source": "Missing Not at Random (MNAR)",
      "type": "requires",
      "target": "Specialized Statistical Modeling"
    },
    {
      "source": "Deletion",
      "type": "used_in",
      "target": "Data Cleaning"
    },
    {
      "source": "Deletion",
      "type": "alternative_to",
      "target": "Imputation"
    },
    {
      "source": "Deletion",
      "type": "influences",
      "target": "Bias"
    },
    {
      "source": "Imputation",
      "type": "applied_to",
      "target": "Missing Data"
    },
    {
      "source": "Imputation",
      "type": "implemented_by",
      "target": "Regression Imputation"
    },
    {
      "source": "Imputation",
      "type": "implemented_by",
      "target": "Multiple Imputation"
    },
    {
      "source": "Imputation",
      "type": "implemented_by",
      "target": "Mean Imputation"
    },
    {
      "source": "Mean Imputation",
      "type": "subtype_of",
      "target": "Imputation"
    },
    {
      "source": "Mean Imputation",
      "type": "related_to",
      "target": "Regression Imputation"
    },
    {
      "source": "Regression Imputation",
      "type": "subtype_of",
      "target": "Imputation"
    },
    {
      "source": "Regression Imputation",
      "type": "improved_by",
      "target": "Stochastic Regression Imputation"
    },
    {
      "source": "Stochastic Regression Imputation",
      "type": "extends",
      "target": "Regression Imputation"
    },
    {
      "source": "Stochastic Regression Imputation",
      "type": "related_to",
      "target": "Multiple Imputation"
    },
    {
      "source": "Multiple Imputation",
      "type": "subtype_of",
      "target": "Imputation"
    },
    {
      "source": "Multiple Imputation",
      "type": "requires",
      "target": "MAR Assumption"
    },
    {
      "source": "Multiple Imputation",
      "type": "related_to",
      "target": "Maximum Likelihood Estimation"
    },
    {
      "source": "Maximum Likelihood Estimation (MLE)",
      "type": "implemented_by",
      "target": "Expectation-Maximization Algorithm"
    },
    {
      "source": "Maximum Likelihood Estimation (MLE)",
      "type": "alternative_to",
      "target": "Multiple Imputation"
    },
    {
      "source": "Maximum Likelihood Estimation (MLE)",
      "type": "applied_to",
      "target": "Missing Data"
    },
    {
      "source": "Noise",
      "type": "related_to",
      "target": "Outliers"
    },
    {
      "source": "Noise",
      "type": "handled_by",
      "target": "Noise Filters"
    },
    {
      "source": "Noise",
      "type": "handled_by",
      "target": "Data Polishing"
    },
    {
      "source": "Noise",
      "type": "mitigated_by",
      "target": "Robust Learning Algorithms"
    },
    {
      "source": "Robust Learning Algorithms",
      "type": "used_in",
      "target": "Supervised Learning"
    },
    {
      "source": "Robust Learning Algorithms",
      "type": "mitigates",
      "target": "Noise"
    },
    {
      "source": "Robust Learning Algorithms",
      "type": "alternative_to",
      "target": "Data Polishing"
    },
    {
      "source": "Data Polishing",
      "type": "alternative_to",
      "target": "Robust Learning Algorithms"
    },
    {
      "source": "Data Polishing",
      "type": "related_to",
      "target": "Noise Filters"
    },
    {
      "source": "Data Polishing",
      "type": "used_in",
      "target": "Data Cleaning"
    },
    {
      "source": "Noise Filters",
      "type": "used_in",
      "target": "Data Preprocessing"
    },
    {
      "source": "Noise Filters",
      "type": "mitigates",
      "target": "Noise"
    },
    {
      "source": "Noise Filters",
      "type": "related_to",
      "target": "Data Polishing"
    },
    {
      "source": "Data Pollution",
      "type": "related_to",
      "target": "Noise"
    },
    {
      "source": "Data Pollution",
      "type": "handled_by",
      "target": "Data Cleaning"
    },
    {
      "source": "Data Pollution",
      "type": "influences",
      "target": "Data Quality"
    },
    {
      "source": "Signal Saturation",
      "type": "related_to",
      "target": "Noise"
    },
    {
      "source": "Signal Saturation",
      "type": "handled_by",
      "target": "Data Cleaning"
    },
    {
      "source": "Signal Saturation",
      "type": "affects",
      "target": "Signal Quality"
    },
    {
      "source": "Outliers",
      "type": "contrasts_with",
      "target": "Anomalies"
    },
    {
      "source": "Outliers",
      "type": "handled_by",
      "target": "Outlier Detection Methods"
    },
    {
      "source": "Outliers",
      "type": "related_to",
      "target": "Noise"
    },
    {
      "source": "Anomalies",
      "type": "contrasts_with",
      "target": "Outliers"
    },
    {
      "source": "Anomalies",
      "type": "detected_by",
      "target": "Anomaly Detection Methods"
    },
    {
      "source": "Anomalies",
      "type": "related_to",
      "target": "Novelty Detection"
    },
    {
      "source": "Outlier Detection Methods",
      "type": "used_in",
      "target": "Anomaly Detection"
    },
    {
      "source": "Outlier Detection Methods",
      "type": "applies_to",
      "target": "Outliers"
    },
    {
      "source": "Outlier Detection Methods",
      "type": "includes",
      "target": "Proximity-Based Methods"
    },
    {
      "source": "Outlier Detection Methods",
      "type": "includes",
      "target": "Statistical Methods"
    },
    {
      "source": "Outlier Detection Methods",
      "type": "includes",
      "target": "Clustering Methods"
    },
    {
      "source": "Proximity-Based Methods",
      "type": "subtype_of",
      "target": "Outlier Detection Methods"
    },
    {
      "source": "Proximity-Based Methods",
      "type": "related_to",
      "target": "Distance Measures"
    },
    {
      "source": "Proximity-Based Methods",
      "type": "used_in",
      "target": "Unsupervised Outlier Detection"
    },
    {
      "source": "Normalization",
      "type": "has_subtype",
      "target": "Min-Max Normalization"
    },
    {
      "source": "Normalization",
      "type": "has_subtype",
      "target": "Z-score Standardization"
    },
    {
      "source": "Normalization",
      "type": "has_subtype",
      "target": "Decimal Scaling"
    },
    {
      "source": "Normalization",
      "type": "related_to",
      "target": "Data Transformation"
    },
    {
      "source": "Normalization",
      "type": "used_in",
      "target": "Machine Learning"
    },
    {
      "source": "Min-Max Normalization",
      "type": "subtype_of",
      "target": "Normalization"
    },
    {
      "source": "Min-Max Normalization",
      "type": "related_to",
      "target": "Scaling"
    },
    {
      "source": "Z-score Standardization",
      "type": "subtype_of",
      "target": "Normalization"
    },
    {
      "source": "Z-score Standardization",
      "type": "used_in",
      "target": "Outlier Detection"
    },
    {
      "source": "Z-score Standardization",
      "type": "related_to",
      "target": "Standard Deviation"
    },
    {
      "source": "Decimal Scaling",
      "type": "subtype_of",
      "target": "Normalization"
    },
    {
      "source": "Decimal Scaling",
      "type": "related_to",
      "target": "Scaling"
    },
    {
      "source": "Discretization",
      "type": "used_in",
      "target": "Data Classification"
    },
    {
      "source": "Discretization",
      "type": "alternative_to",
      "target": "Continuous Variable Modeling"
    },
    {
      "source": "Discretization",
      "type": "related_to",
      "target": "Concept Hierarchy"
    },
    {
      "source": "Dummy Coding",
      "type": "used_in",
      "target": "Regression Modeling"
    },
    {
      "source": "Dummy Coding",
      "type": "related_to",
      "target": "Categorical Variables"
    },
    {
      "source": "Data Reduction",
      "type": "related_to",
      "target": "Feature Engineering"
    },
    {
      "source": "Data Reduction",
      "type": "includes",
      "target": "Principal Component Analysis"
    },
    {
      "source": "Data Reduction",
      "type": "includes",
      "target": "Discrete Wavelet Transform"
    },
    {
      "source": "Principal Component Analysis (PCA)",
      "type": "subtype_of",
      "target": "Data Reduction"
    },
    {
      "source": "Principal Component Analysis (PCA)",
      "type": "alternative_to",
      "target": "Nonlinear PCA"
    },
    {
      "source": "Principal Component Analysis (PCA)",
      "type": "related_to",
      "target": "Feature Extraction"
    },
    {
      "source": "Discrete Wavelet Transform (DWT)",
      "type": "subtype_of",
      "target": "Data Reduction"
    },
    {
      "source": "Discrete Wavelet Transform (DWT)",
      "type": "related_to",
      "target": "Principal Component Analysis"
    },
    {
      "source": "Box-Cox Transformation",
      "type": "used_in",
      "target": "Transformation to Normality"
    },
    {
      "source": "Box-Cox Transformation",
      "type": "related_to",
      "target": "Data Normalization"
    },
    {
      "source": "Model Generalization",
      "type": "affected_by",
      "target": "Sample Size"
    },
    {
      "source": "Model Generalization",
      "type": "affected_by",
      "target": "Model Complexity"
    },
    {
      "source": "Model Generalization",
      "type": "ensured_by",
      "target": "Cross-Validation"
    },
    {
      "source": "Model Generalization",
      "type": "related_to",
      "target": "Overfitting"
    },
    {
      "source": "Model Generalization",
      "type": "related_to",
      "target": "Underfitting"
    },
    {
      "source": "Descriptive Modeling",
      "type": "contrasts_with",
      "target": "Predictive Modeling"
    },
    {
      "source": "Descriptive Modeling",
      "type": "contrasts_with",
      "target": "Explanatory Modeling"
    },
    {
      "source": "Descriptive Modeling",
      "type": "used_in",
      "target": "Exploratory Data Analysis"
    },
    {
      "source": "Explanatory Modeling",
      "type": "contrasts_with",
      "target": "Predictive Modeling"
    },
    {
      "source": "Explanatory Modeling",
      "type": "related_to",
      "target": "Statistical Inference"
    },
    {
      "source": "Explanatory Modeling",
      "type": "requires",
      "target": "Causal Hypothesis"
    },
    {
      "source": "Predictive Modeling",
      "type": "contrasts_with",
      "target": "Explanatory Modeling"
    },
    {
      "source": "Predictive Modeling",
      "type": "related_to",
      "target": "Model Generalization"
    },
    {
      "source": "Predictive Modeling",
      "type": "evaluated_by",
      "target": "Cross-Validation"
    },
    {
      "source": "Predictive Modeling",
      "type": "related_to",
      "target": "Overfitting"
    },
    {
      "source": "Overfitting",
      "type": "opposite_of",
      "target": "Underfitting"
    },
    {
      "source": "Overfitting",
      "type": "reduced_by",
      "target": "Cross-Validation"
    },
    {
      "source": "Overfitting",
      "type": "reduced_by",
      "target": "Regularization"
    },
    {
      "source": "Overfitting",
      "type": "related_to",
      "target": "Model Generalization"
    },
    {
      "source": "Underfitting",
      "type": "opposite_of",
      "target": "Overfitting"
    },
    {
      "source": "Underfitting",
      "type": "related_to",
      "target": "Model Bias"
    },
    {
      "source": "Underfitting",
      "type": "affects",
      "target": "Model Generalization"
    },
    {
      "source": "Sample Size",
      "type": "affects",
      "target": "Model Generalization"
    },
    {
      "source": "Sample Size",
      "type": "affects",
      "target": "Statistical Power"
    },
    {
      "source": "Sample Size",
      "type": "used_in",
      "target": "Training and Testing"
    },
    {
      "source": "Cross-Validation",
      "type": "used_in",
      "target": "Model Evaluation"
    },
    {
      "source": "Cross-Validation",
      "type": "reduces",
      "target": "Overfitting"
    },
    {
      "source": "Cross-Validation",
      "type": "requires",
      "target": "Data Partitioning"
    },
    {
      "source": "Model Selection",
      "type": "used_in",
      "target": "Model Generalization"
    },
    {
      "source": "Model Selection",
      "type": "related_to",
      "target": "Bias-Variance Tradeoff"
    },
    {
      "source": "Model Selection",
      "type": "requires",
      "target": "Validation Set"
    }
  ],
  "metadata": {
    "built_at": "2026-01-22T07:06:53.598987",
    "subjects": {
      "introduction_to_optimization": {
        "subject_id": "introduction_to_optimization",
        "display_name": "Introduction To Optimization",
        "files": [
          "01_data.json",
          "02_data.json",
          "03_data.json",
          "04_data.json",
          "05_data.json",
          "06_data.json",
          "07_data.json",
          "08_data.json",
          "09_data.json",
          "optimization.json",
          "optimization_2.json"
        ]
      },
      "machine_vision": {
        "subject_id": "machine_vision",
        "display_name": "Machine Vision",
        "files": [
          "00_01_core_concept.json",
          "00_02_01_feature_extraction_detection_methods.json",
          "00_02_02_learning_segmentation_hybrid_algorithms.json",
          "00_extra_01.json",
          "00_extra_02.json",
          "01_data.json",
          "02_data.json",
          "03_data.json",
          "04_data.json",
          "05_data.json",
          "06_data.json",
          "07_data.json",
          "08_data.json",
          "09_data.json",
          "10_data.json"
        ]
      },
      "machine_vision_v1": {
        "subject_id": "machine_vision_v1",
        "display_name": "Machine Vision V1",
        "files": [
          "00_01_core_concept.json",
          "00_02_01_feature_extraction_detection_methods.json",
          "00_02_02_learning_segmentation_hybrid_algorithms.json",
          "initial.json"
        ]
      },
      "numerical_matrix": {
        "subject_id": "numerical_matrix",
        "display_name": "Numerical Matrix",
        "files": [
          "00_extra_01.json",
          "01_introduction.json",
          "02_product_of_matrix_subspaces_in_factoring_matrices.json",
          "03_the_singular_value_decomposition.json",
          "04_matrix_subspaces_for_factoring_matrices.json",
          "05_factoring_algorithmically.json",
          "06_computing_the_LU_factorization_with_partial_pivoting.json",
          "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.json",
          "08_iterative_methods_for_linear_systems.json",
          "09_preconditioning.json",
          "10_eigenvalue_problems_and_functions_of_matrices.json",
          "11_Iterative methods_for_eigenvalue_problems.json",
          "extra_00_01.json",
          "extra_00_02.json",
          "extra_00_03.json",
          "extra_00_04.json",
          "extra_00_05.json",
          "extra_00_06.json",
          "extra_00_07.json",
          "extra_01.json"
        ]
      },
      "towards_data_mining": {
        "subject_id": "towards_data_mining",
        "display_name": "Towards Data Mining",
        "files": [
          "0_extra_00.json",
          "0_extra_01.json",
          "0_extra_02.json",
          "0_extra_03.json",
          "0_extra_04.json",
          "0_extra_05.json",
          "0_extra_06.json",
          "0_extra_07.json",
          "0_extra_08.json",
          "1_data.json",
          "2_data.json",
          "3_data.json",
          "4_data.json",
          "5_data.json",
          "6_data.json",
          "7_data.json",
          "8_data.json"
        ]
      }
    }
  }
}