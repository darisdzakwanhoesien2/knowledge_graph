[
  {
    "front": "ğŸ§© Singular Value Decomposition\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** The Singular Value Decomposition (SVD) of a matrix A âˆˆ â„‚^{mÃ—n} is a factorization A = U Î£ V^*, where U and V are unitary matrices with orthonormal columns, and Î£ is a diagonal matrix with non-negative singular values Ïƒâ‚ â‰¥ Ïƒâ‚‚ â‰¥ â‹¯ â‰¥ Ïƒ_n â‰¥ 0 on the diagonal.\n\n**Description:** SVD provides the best low-rank approximation of a matrix in both spectral and Frobenius norms, enabling optimal matrix compression, dimensionality reduction, and solving least squares problems. It decomposes any matrix into orthogonal bases that capture the principal directions of variation.\n\n**Goal:** Provide orthogonal factorization and low-rank approximations.\n**Applications:** Data compression, Principal component analysis, Pseudo-inverse\n**Methods:** Golub-Reinsch algorithm, Divide-and-conquer, Iterative methods\n**Examples:** A â‰ˆ U_k Î£_k V_k^* for rank-k approximation"
  },
  {
    "front": "ğŸ§© Singular Value\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** The singular values of a matrix A are the non-negative square roots of the eigenvalues of A^*A (or AA^*), ordered as Ïƒâ‚ â‰¥ Ïƒâ‚‚ â‰¥ â‹¯ â‰¥ Ïƒ_r > 0, where r is the rank of A.\n\n**Description:** Singular values measure the 'importance' or 'strength' of each principal direction in the matrix. The largest singular value Ïƒâ‚ equals the operator norm ||A||, and they uniquely determine the best low-rank approximations.\n\n**Goal:** Quantify the magnitude of principal components in matrix decomposition.\n**Applications:** Rank determination, Condition number computation (Ïƒâ‚/Ïƒ_r), Numerical stability analysis\n**Methods:** Eigenvalue decomposition of A^*A, Square root of eigenvalues\n**Examples:** Ïƒâ‚ = ||A|| = max_{||x||=1} ||Ax|| represents maximum stretch"
  },
  {
    "front": "ğŸ§© Unitary Matrix\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** A unitary matrix Q âˆˆ â„‚^{nÃ—n} satisfies Q^*Q = I, meaning its columns (and rows) form an orthonormal basis that preserves the Euclidean norm: ||Qx|| = ||x|| for all x âˆˆ â„‚^n.\n\n**Description:** Unitary matrices represent rotations and reflections in complex space. They appear in QR decomposition, SVD, and eigenvalue problems, preserving distances and angles during transformations.\n\n**Goal:** Preserve norms and inner products in linear transformations.\n**Applications:** QR decomposition, Singular Value Decomposition, Numerical linear algebra algorithms, Quantum computing gates\n**Methods:** Gram-Schmidt process, Householder reflections\n**Examples:** Q = [qâ‚ â‹¯ qâ‚™] where {qâ±¼} are orthonormal vectors"
  },
  {
    "front": "ğŸ§© Low Rank Approximation\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** The low-rank approximation problem finds a rank-k matrix F_k that minimizes ||A - F_k||_F among all rank-k matrices, solved uniquely by SVD: F_k = âˆ‘_{j=1}^k Ïƒâ±¼ uâ±¼ vâ±¼^*.\n\n**Description:** SVD provides the optimal rank-k approximation in Frobenius norm, equivalent to keeping the k largest singular values. This compresses data while minimizing reconstruction error.\n\n**Goal:** Approximate high-dimensional matrix with lower-rank version minimizing ||A - F_k||_F.\n**Applications:** Data compression, Dimensionality reduction, Image denoising, Recommendation systems\n**Methods:** Truncated SVD, Eckart-Young theorem\n**Examples:** F_k = U_k Î£_k V_k^* where error = âˆ‘_{j=k+1}^r Ïƒâ±¼Â²"
  },
  {
    "front": "ğŸ§© Frobenius Norm\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** The Frobenius norm of A âˆˆ â„‚^{mÃ—n} is ||A||_F = âˆš(âˆ‘_{j=1}^m âˆ‘_{k=1}^n |a_{jk}|Â²) = âˆštrace(A^*A), measuring the Euclidean norm of the matrix treated as a vector in â„‚^{mn}.\n\n**Description:** Widely used in matrix approximation problems due to computational convenience and equivalence to SVD error. It's unitarily invariant: ||Qâ‚AQâ‚‚||_F = ||A||_F for unitary Qâ‚, Qâ‚‚.\n\n**Goal:** Measure matrix 'size' as vector in high-dimensional space.\n**Applications:** Low-rank approximation error, Matrix compression, Least squares problems\n**Methods:** âˆš(sum of squared entries), âˆštrace(A^*A)\n**Examples:** ||A||_FÂ² = âˆ‘ Ïƒâ±¼Â² where Ïƒâ±¼ are singular values"
  },
  {
    "front": "ğŸ§© Operator Norm\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** The operator norm ||A|| = max_{||x||=1} ||Ax|| measures the maximum stretch factor of the linear transformation A: â„‚^n â†’ â„‚^m.\n\n**Description:** For SVD, ||A|| = Ïƒâ‚, the largest singular value. It quantifies how much A can amplify unit vectors and determines numerical stability.\n\n**Goal:** Measure maximum amplification by linear transformation.\n**Applications:** Condition number (||A|| â‹… ||Aâ»Â¹||), Stability analysis, Spectral radius bounds\n**Methods:** Largest singular value Ïƒâ‚, max_{||x||=1} ||Ax||"
  },
  {
    "front": "ğŸ§© Matrix Function\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** A matrix function f(A) extends scalar functions to matrices A, defined via Jordan form, power series, or other representations, enabling operations like exponentials or square roots on matrices.\n\n**Description:** Matrix functions are computed iteratively for large matrices, used in differential equations, control theory, and eigenvalue problems.\n\n**Goal:** Extend scalar functions to matrices while preserving algebraic structure.\n**Applications:** Matrix exponential in ODEs, Matrix logarithm in geometry, Sign function in control\n**Methods:** Schur-Parlett, Scaling-and-squaring, PadÃ© approximation, Contour integral\n**Examples:** exp(A), A^{1/2}, sign(A)"
  },
  {
    "front": "ğŸ§© Matrix Square Root\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** The matrix square root X satisfies X^2 = A for a matrix A, with the principal square root being positive definite if A is.\n\n**Description:** It is computed iteratively, e.g., via Newton's method, for applications in statistics, control, and geometry.\n\n**Goal:** Find X such that X^2 = A.\n**Applications:** Covariance matrices, Riemannian metrics, Polar decomposition\n**Methods:** Newton iteration, Denman-Beavers, Schur method\n**Examples:** X = sqrt(A) for SPD A"
  },
  {
    "front": "ğŸ§© Newton's Iteration for Square Root\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** Newton's iteration for the matrix square root updates X_{k+1} = (X_k + A X_k^{-1}) / 2, starting from X_0 = I or other, converging quadratically to sqrt(A).\n\n**Description:** It is stable for positive definite A, with safeguards for convergence, used in large-scale computations via iterative solvers.\n\n**Goal:** Compute sqrt(A) iteratively.\n**Applications:** Matrix sign function, Algebraic Riccati equations\n**Methods:** Matrix inversion at each step, Quadratic convergence\n**Examples:** X_{k+1} = (X_k + A X_k^{-1}) / 2"
  },
  {
    "front": "ğŸ§© Eigenvalue Problem\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** The eigenvalue problem seeks scalars Î» and vectors x â‰  0 such that A x = Î» x for matrix A.\n\n**Description:** Iterative methods like power iteration or Lanczos are used for large sparse matrices, shifting spectrum for better conditioning.\n\n**Goal:** Compute spectrum and invariant subspaces for analysis and transformation of linear operators.\n**Applications:** Vibration analysis, Stability of dynamical systems, Google PageRank, Quantum mechanics\n**Methods:** Power iteration, QR algorithm, Arnoldi iteration, Jacobi-Davidson\n**Examples:** Ax = Î»x, det(Aâˆ’Î»I)=0 (characteristic equation)"
  },
  {
    "front": "ğŸ§© Spectral Shift\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** Spectral shift transforms the eigenvalue problem to (A - Î¼ I) x = (Î» - Î¼) x, shifting eigenvalues by Î¼ to target specific parts of the spectrum or improve conditioning.\n\n**Description:** Choosing Î¼ near a target eigenvalue accelerates convergence in iterative methods; for real Î¼, it can make the matrix positive definite.\n\n**Goal:** Adjust spectrum for better numerical properties.\n**Applications:** Interior eigenvalues, Deflation, Preconditioning\n**Methods:** Î¼ close to target Î», Î¼ = (Î»_min + Î»_max)/2\n**Examples:** Î›(A - Î¼ I) = Î›(A) - Î¼"
  },
  {
    "front": "ğŸ§© Positive Definite Shift\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** Positive definite shift chooses Î¼ such that A - Î¼ I is positive definite, e.g., Î¼ < Î»_min(A) for symmetric A, enabling use of CG or other SPD methods.\n\n**Description:** It stabilizes iterations and bounds condition number; for estimated Ë†Î¼ â‰ˆ Î»_min, adjust with a > 0 to ensure positivity.\n\n**Goal:** Make shifted matrix SPD for efficient solving.\n**Applications:** Shift-and-invert, Preconditioned eigensolvers\n**Methods:** Î¼ = Ë†Î¼ - a, a > 0, Rayleigh quotient estimate\n**Examples:** A - Î¼ I with Î¼ = Ë†Î¼ - a"
  },
  {
    "front": "ğŸ§© Spectrum Î›(A)\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** The spectrum Î›(A) is the set of all eigenvalues of matrix A.\n\n**Description:** Shifting modifies the spectrum as Î›(A - Î¼ I) = Î›(A) - Î¼, used to isolate eigenvalues or improve numerical properties.\n\n**Goal:** Characterize matrix via its eigenvalues.\n**Applications:** Spectral radius, Conditioning, Stability\n**Methods:** Eigen decomposition, Characteristic polynomial\n**Examples:** Î›(A) subset C for A in C^{n x n}"
  },
  {
    "front": "ğŸ§© Matrix Computations\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** The study and development of algorithms for performing operations on matrices, including addition, multiplication, inversion, decomposition, and solving linear systems.\n\n**Description:** Matrix computations are central to numerical analysis, scientific computing, and computational science. They underpin applications in physics simulations, data analysis, machine learning, and computer graphics. The computational complexity of classical algorithms for matrix multiplication is O(nÂ³), while storage is typically O(nÂ²).\n\n**Goal:** Solve matrix-related problems efficiently in numerical contexts.\n**Applications:** Scientific computing, Data analysis, PDE solving, Linear algebra problems\n**Methods:** Factorizations (LU, SVD), Eigenvalue computations, Iterative solvers, Subspace approximations\n**Examples:** Solving Ax = b, Approximating large matrices"
  },
  {
    "front": "ğŸ§© Inner Product\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** A binary operation on two vectors in â„‚â¿ that produces a scalar, defined as âŸ¨x, yâŸ© = yáµ€xÌ„ = âˆ‘â±¼ xâ±¼È³â±¼, where x, y âˆˆ â„‚â¿.\n\n**Description:** The inner product (also known as dot product in real vector spaces) measures similarity between vectors and forms the foundation of orthogonality, norms, and projections. In complex spaces, it involves conjugation to ensure positive definiteness of the induced norm.\n\n**Goal:** Quantify angle and similarity between vectors; enable orthogonal decomposition\n**Applications:** Gram-Schmidt orthogonalization, Least squares, Signal processing, Quantum mechanics\n**Methods:** N/A\n**Examples:** âŸ¨x, yâŸ© = âˆ‘ xâ±¼È³â±¼ over j=1 to n"
  },
  {
    "front": "ğŸ§© Computational Complexity (Matrix Multiplication)\nğŸ“˜ Domain: Algorithm Analysis",
    "back": "**Definition:** The asymptotic resource requirement for matrix multiplication and related operations, classically O(nÂ³) time and O(nÂ²) space for nÃ—n matrices.\n\n**Description:** Standard matrix multiplication of two nÃ—n matrices requires O(nÂ³) arithmetic operations using the naive algorithm. While theoretical improvements exist (e.g., Strassenâ€™s O(nÂ².â¸â°â·)), practical methods remain close to O(nÂ³). Storage scales quadratically as O(nÂ²).\n\n**Goal:** Assess efficiency and scalability of matrix algorithms\n**Applications:** Performance prediction, Algorithm selection, Hardware design\n**Methods:** Big-O notation, Arithmetic circuit complexity\n**Examples:** O(nÂ³) time, O(nÂ²) space"
  },
  {
    "front": "ğŸ§© LU Factorization\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** LU factorization decomposes a matrix A into a lower triangular matrix L and an upper triangular matrix U such that A = LU, or with pivoting PA = LU, enabling efficient solution of linear systems.\n\n**Description:** It is a fundamental algorithm with O(n^3) complexity, used for solving Ax = b by forward and backward substitution, and approximated for large matrices using subspace products.\n\n**Goal:** Decompose matrices for efficient linear system solving.\n**Applications:** Numerical linear algebra, PDE discretization, Data processing\n**Methods:** Gaussian elimination, Pivoting for stability, Partial pivoting\n**Examples:** A = LU for square matrices, PA = LU with permutation P"
  },
  {
    "front": "ğŸ§© Product of Matrix Subspaces\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** The product of matrix subspaces V1 V2 is defined as {V1 V2 : V1 âˆˆ V1, V2 âˆˆ V2}, providing a framework for approximating matrices with fewer parameters than full rank.\n\n**Description:** It allows factoring matrices into low-complexity forms like sum of outer products, useful for large n with small k, achieving 2nk parameters and potential for lower computational costs.\n\n**Goal:** Approximate matrices efficiently using subspace products.\n**Applications:** Large matrix factorization, PDE discretization, Data storage\n**Methods:** Subspace multiplication, Rank-k approximation, Norm minimization\n**Examples:** A â‰ˆ sum_{j=1}^k u_j v_j^*, I + V1 V2 inversion"
  },
  {
    "front": "ğŸ§© Gram-Schmidt Orthogonalization\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** Gram-Schmidt orthogonalization transforms a set of linearly independent vectors into an orthonormal set using projections and normalization.\n\n**Description:** It is used in QR factorization and subspace computations, with classical and modified variants for numerical stability.\n\n**Goal:** Produce orthonormal bases from vector sets.\n**Applications:** QR decomposition, Subspace orthogonalization, Least squares\n**Methods:** Classical Gram-Schmidt, Modified Gram-Schmidt, Householder reflections\n**Examples:** Orthogonalizing columns of A"
  },
  {
    "front": "ğŸ§© Low-Rank Approximation\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** Low-rank approximation finds a matrix Fk of rank k that minimizes ||A - Fk|| for a given norm, often using SVD or subspace products for efficiency.\n\n**Description:** It reduces storage and computation for large matrices, with subspace products offering 2nk parameters for rank-k approximations.\n\n**Goal:** Approximate high-dimensional matrices with lower rank.\n**Applications:** Data compression, Noise reduction, Machine learning\n**Methods:** SVD truncation, Subspace product, Randomized algorithms\n**Examples:** Fk = sum u_j v_j^*, min_{rank(F)=k} ||A - F||"
  },
  {
    "front": "ğŸ§© Hermitian Matrix\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** A Hermitian matrix M satisfies M^* = M, where * denotes conjugate transpose, with real eigenvalues and orthogonal eigenvectors.\n\n**Description:** It requires n^2 real parameters for storage, or fewer in subspace approximations, used in quantum mechanics and signal processing.\n\n**Goal:** Model self-adjoint operators in complex spaces.\n**Applications:** Quantum computing, Covariance matrices, Spectral analysis\n**Methods:** Eigen decomposition, Cholesky factorization (positive definite)\n**Examples:** M with real diagonal and conjugate symmetric off-diagonals"
  },
  {
    "front": "ğŸ§© Matrix Subspace\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** A subset V âŠ† â„‚^{nÃ—n} of matrices closed under addition and scalar multiplication, forming a vector space over â„‚.\n\n**Description:** Matrix subspaces are used to model families of matrices sharing structural properties (e.g., invertibility, symmetry, triangularity). They enable low-rank approximations, invariant subspaces in eigenvalue problems, and structured matrix factorizations such as LU or SVD within constrained sets.\n\n**Goal:** Group matrices with common algebraic or analytic traits to simplify computations and preserve structure in factorizations.\n**Applications:** Structured matrix factorization, Low-rank approximation, Krylov methods, Invariant subspace computation\n**Methods:** Span construction, Closure under operations, Equivalence via similarity\n**Examples:** span{I, A}, span{A, B}, upper triangular matrices"
  },
  {
    "front": "ğŸ§© Nonsingular Matrix Subspace\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** A matrix subspace V âŠ† â„‚^{nÃ—n} that contains at least one invertible matrix (det V â‰  0 for some V âˆˆ V).\n\n**Description:** Nonsingular subspaces guarantee the existence of invertible elements, enabling the definition of the set of inverses Inv(V) = {Vâ»Â¹ : V âˆˆ V, det V â‰  0}. Such subspaces are crucial for ensuring well-defined inverse-based factorizations (e.g., LU within V).\n\n**Goal:** Ensure invertibility within a structured family of matrices to support factorization algorithms.\n**Applications:** LU factorization, Generalized inverse, Structured preconditioning\n**Methods:** Perturbation arguments, Open-dense property in finite dimensions\n**Examples:** V = span{I, A} for nonsingular A, Upper triangular matrices with nonzero diagonals"
  },
  {
    "front": "ğŸ§© Inv(V)\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** The set of inverses of all invertible matrices in a matrix subspace V: Inv(V) = {W : W = Vâ»Â¹, V âˆˆ V, det V â‰  0}.\n\n**Description:** For nonsingular matrix subspaces, Inv(V) is itself a matrix subspace under mild conditions. This enables dual-space factorizations and ensures closure under inversion within structured sets, vital for algorithmic stability in LU-type methods.\n\n**Goal:** Construct a subspace of inverses to maintain structure across factorization steps.\n**Applications:** LU within subspace, Preconditioner design, Group-inverse problems\n**Methods:** Similarity transformation, Closure proof via nonsingularity\n**Examples:** Inv(upper triangular) = lower triangular, Inv(Hermitian) = Hermitian"
  },
  {
    "front": "ğŸ§© LU Factorization within Subspace\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** Decomposition of a matrix A âˆˆ V into A = LU where L is lower triangular and U is upper triangular, both belonging to predefined matrix subspaces derived from V.\n\n**Description:** By identifying nonsingular matrix subspaces V and W = Inv(V), LU factorization can be confined within V Ã— W, ensuring all intermediate matrices remain structured. This supports specialized algorithms for banded, symmetric positive-definite, or approximate low-rank problems.\n\n**Goal:** Perform Gaussian elimination while preserving membership in designated matrix subspaces.\n**Applications:** Banded LU, SPD factorization, Low-rank updates, Krylov-based solvers\n**Methods:** Schur complement, Pivot-free elimination, Subspace projection\n**Examples:** A âˆˆ upper triangular â†’ L = I, U = A, A âˆˆ V, L âˆˆ V, U âˆˆ Inv(V)"
  },
  {
    "front": "ğŸ§© Krylov Subspace\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** The matrix subspace K_j(A; I) = span{I, A, AÂ², ..., A^{j-1}} generated by powers of a matrix A starting from the identity.\n\n**Description:** Krylov subspaces lie at the core of iterative methods for large-scale eigenvalue and linear system problems (GMRES, Lanczos, Arnoldi). Their dimension grows linearly until saturation at the degree of the minimal polynomial of A.\n\n**Goal:** Generate subspaces for iterative numerical methods.\n**Applications:** Eigenvalue computation, Linear solvers, Matrix exponentiation\n**Methods:** Arnoldi iteration, Lanczos algorithm, GMRES\n**Examples:** K_1(A; I) = span{I}, K_2(A; I) = span{I, A}"
  },
  {
    "front": "ğŸ§© Matrix Polynomials\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** Functions p(z) = âˆ‘_{k=0}^m c_k z^k evaluated at matrices: p(A) = âˆ‘_{k=0}^m c_k A^k for A âˆˆ â„‚^{nÃ—n}.\n\n**Description:** Matrix polynomials map matrix subspaces to themselves if V is closed under powers of its members. They define minimal and characteristic polynomials, enable Cayley-Hamilton applications, and support function-based iterative methods.\n\n**Goal:** Extend scalar polynomial theory to matrices for spectral analysis and function approximation.\n**Applications:** Cayley-Hamilton theorem, Matrix exponential,  preconditioning, Spectral projectors\n**Methods:** Horner scheme, Paterson-Stockmeyer, Jordan form evaluation\n**Examples:** p(z) = zÂ²âˆ’2z+1 â†’ p(A) = AÂ²âˆ’2A+I = 0, q(z) = det(Aâˆ’zI) â†’ q(A)=0"
  },
  {
    "front": "ğŸ§© Projection Operator\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** A linear operator P on â„‚^n such that PÂ² = P, projecting vectors onto a subspace R(P) with kernel R(Iâˆ’P).\n\n**Description:** Projections decompose spaces into direct sums and are building blocks for oblique and orthogonal projections in iterative solvers. Orthogonal projections minimize least-squares error and appear in GMRES and conjugate gradients.\n\n**Goal:** Project vectors onto subspaces.\n**Applications:** Least squares, Subspace methods, Decomposition\n**Methods:** Orthogonal projection, Oblique projection\n**Examples:** P with P^2 = P, I - P as complement projection"
  },
  {
    "front": "ğŸ§© Sparsity Structure\nğŸ“˜ Domain: Matrix Computations",
    "back": "**Definition:** The sparsity structure of a matrix or matrix subspace specifies the pattern of entries that may be nonzero, independent of the actual numerical values.\n\n**Description:** Sparsity structures capture the combinatorial pattern of allowed nonzeros in matrices, enabling algorithms that exploit memory efficiency and reduce computational cost. They are essential in large-scale problems such as PDE discretizations, graph Laplacians, and structured matrix factorizations.\n\n**Goal:** Identify and exploit zero-patterns for efficient computation.\n**Applications:** Sparse LU factorization, Finite difference discretizations, Graph-based matrix algorithms\n**Methods:** Pattern analysis, Fill-in minimization, Graph reordering\n**Examples:** Tridiagonal pattern, Band matrix structure"
  },
  {
    "front": "ğŸ§© Standard Matrix Subspace\nğŸ“˜ Domain: Matrix Computations",
    "back": "**Definition:** A standard matrix subspace is a subspace that admits a basis consisting of standard matrices, each with exactly one entry equal to 1 and all other entries equal to 0.\n\n**Description:** Standard matrix subspaces represent matrix sets defined purely by sparsity patterns, without structural constraints such as symmetry. Orthogonal projection onto these subspaces is straightforwardâ€”simply zero out entries outside the allowed pattern.\n\n**Goal:** Model matrix sets defined solely by sparsity constraints.\n**Applications:** Sparse matrix approximations, Projection methods, Structured LU factorization\n**Methods:** Entrywise projection, Basis construction from standard matrices\n**Examples:** Diagonal matrices, Strictly lower triangular matrices"
  },
  {
    "front": "ğŸ§© Orthogonal Projector (Matrix Subspace)\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** An orthogonal projector onto a matrix subspace V is a linear operator P such that PÂ² = P and the range of P is orthogonal to the nullspace of P.\n\n**Description:** Orthogonal projectors provide optimal approximations in least-squares and matrix approximation problems. Projection onto matrix subspaces is fundamental in algorithmic factoring, dimension reduction, and solving underdetermined systems.\n\n**Goal:** Project matrices onto a subspace with minimal error under the Frobenius inner product.\n**Applications:** Approximate factoring, Subspace splitting, Sylvester-type equations\n**Methods:** Construction via orthonormal basis, Symmetrization operators\n**Examples:** P(A) = (A + A*)/2 for Hermitian matrices"
  },
  {
    "front": "ğŸ§© Invertible Matrix Subspace\nğŸ“˜ Domain: Matrix Theory",
    "back": "**Definition:** A matrix subspace V is invertible if the set of inverses of its nonsingular elements forms another matrix subspace Vâ»Â¹.\n\n**Description:** Invertible matrix subspaces allow algorithmic factorization because their inverses preserve linear structure. Classical examples include triangular matrices and Hermitian matrices, which maintain structure under inversion.\n\n**Goal:** Support structured factorization where both A and its factors belong to linear matrix families.\n**Applications:** LU factorization, Symmetric factorizations, Matrix subspace factoring\n**Methods:** Closure under inversion, Polynomial relations\n**Examples:** Lower triangular matrices, Hermitian matrices"
  },
  {
    "front": "ğŸ§© Singular Matrix Subspace\nğŸ“˜ Domain: Matrix Theory",
    "back": "**Definition:** A matrix subspace is singular if every matrix within it is singular, i.e., no element has a nonzero determinant.\n\n**Description:** Singular matrix subspaces arise in low-rank approximations, special matrix pencils, and structured operator families. They often encode degenerate behavior and limit the applicability of classical factorizations.\n\n**Goal:** Characterize spaces where invertibility is impossible.\n**Applications:** Rank-k matrix sets, SVD truncation analysis, Generalized eigenvalue pencils\n**Methods:** Nullspace analysis, Subspace closure\n**Examples:** Rank-k matrices for k < n"
  },
  {
    "front": "ğŸ§© Polynomially Closed Matrix Subspace\nğŸ“˜ Domain: Matrix Theory",
    "back": "**Definition:** A matrix subspace V is polynomially closed if p(A) âˆˆ V for every A âˆˆ V and every polynomial p with scalar coefficients.\n\n**Description:** Polynomial closure ensures that structural properties of matrices are preserved under algebraic operations such as exponentiation, inversion, or functional calculus. This property is essential when applying iterative or polynomial-based algorithms within the subspace.\n\n**Goal:** Guarantee closure under matrix polynomial transformations.\n**Applications:** Iterative methods, Matrix functions, Structured inversion\n**Methods:** Polynomial evaluation, Minimal polynomial arguments\n**Examples:** Hermitian matrices, Complex symmetric matrices"
  },
  {
    "front": "ğŸ§© Closure of Vâ‚Vâ‚‚\nğŸ“˜ Domain: Matrix Subspaces",
    "back": "**Definition:** The closure of the product set Vâ‚Vâ‚‚ consists of all matrices that can be approximated arbitrarily well by products of matrices from subspaces Vâ‚ and Vâ‚‚.\n\n**Description:** The closure of Vâ‚Vâ‚‚ determines whether approximate factorizations exist even when exact ones do not. This concept plays a central role in understanding numerical stability, perturbation behavior, and feasibility of approximate matrix factorizations.\n\n**Goal:** Characterize attainable approximate factorizations.\n**Applications:** Approximate LU, Low-rank approximations, Structured matrix decompositions\n**Methods:** Topology of matrix spaces, Perturbation analysis\n**Examples:** Every matrix is arbitrarily close to one with an LU factorization"
  },
  {
    "front": "ğŸ§© Preconditioning\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** Preconditioning is the transformation of a linear system Ax = b into an equivalent system M^{-1}Ax = M^{-1}b (left preconditioning) or AM^{-1}y = b with x = M^{-1}y (right), where M â‰ˆ A is a nonsingular matrix that is inexpensive to invert or solve with, designed to improve the convergence rate of iterative methods.\n\n**Description:** The goal is to cluster the eigenvalues of the preconditioned matrix away from zero, making Krylov subspace methods like CG, GMRES, or MINRES converge in significantly fewer iterations. Effective preconditioners balance accuracy (M close to A) with computational cost (O(n) or O(n log n) per application).\n\n**Goal:** Accelerate iterative solver convergence.\n**Applications:** Large sparse systems, PDE solvers\n**Methods:** Incomplete LU, Jacobi, Multigrid, Domain decomposition\n**Examples:** Left preconditioning: solve M y = c then A x = M y"
  },
  {
    "front": "ğŸ§© Jacobi Preconditioner\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** The Jacobi preconditioner is M = diag(A) + Ï‰I, where diag(A) contains the diagonal entries of A and Ï‰ âˆˆ â„‚ is an optional damping parameter (often Ï‰ = 0). It is the simplest splitting M = D, N = A - D.\n\n**Description:** Extremely cheap to apply (O(n) scaling) and parallelizable. Effective when A is diagonally dominant. Corresponds to the classical Jacobi iterative method. Often used as a baseline or building block in more sophisticated preconditioners.\n\n**Goal:** Damp high-frequency error components using only diagonal information.\n**Applications:** Smooth initial guess for multigrid, Baseline for preconditioner comparison, Diagonally dominant systems\n**Methods:** Extract diagonal, Optional damping Ï‰, Inverse is element-wise division\n**Examples:** M_i = diag(A) + Ï‰I with Ï‰ = 0 for standard Jacobi"
  },
  {
    "front": "ğŸ§© Incomplete LU Factorization\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** Incomplete LU (ILU) computes factors LÌ‚ and UÌ‚ such that LÌ‚UÌ‚ â‰ˆ A with the same sparsity pattern as A (or a prescribed superset), dropping fill-in during Gaussian elimination to control memory and cost.\n\n**Description:** Widely used for general sparse matrices. Variants include ILU(0) (no fill), ILU(k) (level-k fill), and ILUT (threshold dropping). Provides a trade-off between robustness and efficiency. Often combined with reordering (e.g., RCM, nested dissection).\n\n**Goal:** Approximate LU factorization while preserving sparsity for use as preconditioner.\n**Applications:** Finite element systems, CFD problems, Circuit simulation\n**Methods:** Modified Gaussian elimination with drop tolerance, Dual-threshold ILUT, Level-of-fill ILU(k)\n**Examples:** ILU(0): drop all fill-in outside original nonzero pattern"
  },
  {
    "front": "ğŸ§© Sparse Approximate Inverse\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** A sparse approximate inverse (SAI) preconditioner computes a sparse matrix M such that ||I - MA||_F or ||I - AW||_F is minimized over all matrices W with a prescribed sparsity pattern, effectively approximating A^{-1} directly.\n\n**Description:** Application cost is O(nnz(M)) matrix-vector products. Excellent parallel performance due to explicit form. Often constructed via Frobenius norm minimization on independent columns or using QR factorizations of local submatrices.\n\n**Goal:** Construct explicit sparse approximation to A^{-1} for fast matrix-vector products\n**Applications:** Highly parallel architectures (GPU, many-core), Unstructured grids, High-performance computing\n**Methods:** Frobenius norm minimization per column, SPAID (sparse approximate inverse by distance), FSAI (factorized sparse approximate inverse)\n**Examples:** min_W ||AW - I||_F with W constrained to sparsity pattern of A^k"
  },
  {
    "front": "ğŸ§© Left Preconditioning\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** Left preconditioning transforms Ax = b into M^{-1}Ax = M^{-1}b, preserving the solution x but altering the residual norm to ||M^{-1}(b - Ax)||.\n\n**Description:** Common in practice because it directly improves the convergence diagnostics used by Krylov methods (residual-based stopping criteria). Does not change the right-hand side in a way that affects eigenvalue clustering as strongly as right preconditioning.\n\n**Goal:** Improve convergence while keeping solution unchanged and monitoring preconditioned residuals.\n**Applications:** Standard choice in most libraries (PETSc, Trilinos), GMRES with ILU\n**Methods:** Apply M^{-1} to system matrix and RHS"
  },
  {
    "front": "ğŸ§© Right Preconditioning\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** Right preconditioning transforms Ax = b into AM^{-1}y = b with x = M^{-1}y, preserving the residual norm ||b - Ax|| but solving for an intermediate variable y.\n\n**Description:** Often preferred when the preconditioner naturally approximates the inverse action from the right. Common in domain decomposition and multigrid methods. Requires extra step to recover x.\n\n**Goal:** Cluster eigenvalues of AM^{-1} while monitoring true residuals.\n**Applications:** Additive Schwarz, Algebraic multigrid (AMG), When M approximates A from the right\n**Methods:** Solve AM^{-1}y = b, then x = M^{-1}y"
  },
  {
    "front": "ğŸ§© Splitting Methods\nğŸ“˜ Domain: Iterative Methods",
    "back": "**Definition:** A matrix splitting decomposes A = M - N where M is nonsingular and easy to invert. The iteration x^{k+1} = M^{-1}Nx^k + M^{-1}b converges if Ï(M^{-1}N) < 1.\n\n**Description:** Foundation of classical iterative methods (Jacobi, Gauss-Seidel, SOR) and modern preconditioning. The spectral radius of the iteration matrix B = M^{-1}N determines convergence rate.\n\n**Goal:** Construct fixed-point iterations via A = M - N with Ï(M^{-1}N) < 1.\n**Applications:** Stationary iterative methods, Preconditioner design, Convergence theory\n**Methods:** M = D (Jacobi), M = D+L (Gauss-Seidel), M = (D+Ï‰L)/Ï‰ (SOR)"
  },
  {
    "front": "ğŸ§© Cholesky Factorization\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** A decomposition of a Hermitian positive definite matrix A into A = R*R, where R is an upper triangular matrix with positive diagonal entries.\n\n**Description:** Cholesky factorization exploits symmetry and positive definiteness to reduce computational cost from O(nÂ³) for general LU to approximately nÂ³/3 flops while requiring only nÂ²/2 storage. It is widely used in optimization, Monte Carlo simulations, and solving normal equations.\n\n**Goal:** Efficiently factorize Hermitian positive definite matrices with half the storage and one-third the operations of LU.\n**Applications:** Quadratic programming, Kalman filtering, Covariance decomposition, Monte Carlo methods\n**Methods:** Block elimination, Outer product form, Inner product form\n**Examples:** A = R*R with R upper triangular and diag(R) > 0"
  },
  {
    "front": "ğŸ§© Positive Definite Matrix\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** A Hermitian matrix A âˆˆ â„‚^{nÃ—n} such that (Ax, x) > 0 for all nonzero x âˆˆ â„‚^n.\n\n**Description:** Positive definite matrices arise in energy minimization, covariance modeling, and elliptic PDEs. They guarantee unique Cholesky factors, stable inverses, and real positive eigenvalues. The property is preserved under congruence: M*A*M is positive definite if M is invertible.\n\n**Goal:** Model strictly convex quadratic forms and ensure numerical stability in factorizations.\n**Applications:** Optimization, Statistics, Physics simulations, Control theory\n**Methods:** Sylvester's criterion, Cholesky test, Eigenvalue analysis\n**Examples:** Covariance matrices, Hessians of convex functions, A = R*R"
  },
  {
    "front": "ğŸ§© Sylvester Equation\nğŸ“˜ Domain: Control Theory",
    "back": "**Definition:** A matrix equation of the form AX âˆ’ XB = C, where A, B, C are given matrices and X is unknown.\n\n**Description:** The Sylvester equation models linear system interconnections and appears in control design, model reduction, and eigenvalue assignment. When Ïƒ(A) âˆ© Ïƒ(B) = âˆ…, it has a unique solution solvable in O(nÂ³) via Schur triangulation or vectorization (kronecker form).\n\n**Goal:** Solve for coupling matrix X in interconnected linear systems or compute Lyapunov functions.\n**Applications:** Stability analysis, Model order reduction, Riccati equations, Pole placement\n**Methods:** Schur method, Hessenberg-Schur algorithm, Bartels-Stewart, Vectorization\n**Examples:** AX âˆ’ XA* = âˆ’BB* (Lyapunov), AX âˆ’ XB = C with diagonal A, B"
  },
  {
    "front": "ğŸ§© Discrete Fourier Transform (DFT)\nğŸ“˜ Domain: Signal Processing",
    "back": "**Definition:** A linear transformation mapping a sequence xâ‚€, ..., x_{n-1} to coefficients câ±¼ = âˆ‘â‚– xâ‚– Ï‰^{jk}, where Ï‰ = e^{-2Ï€i/n}, represented by the Vandermonde matrix Fâ‚™.\n\n**Description:** The DFT diagonalizes circulant matrices and enables fast convolution, filtering, and spectral analysis. The Fast Fourier Transform (FFT) computes it in O(n log n) using divide-and-conquer on power-of-two sizes.\n\n**Goal:** Decompose signals into frequency components and accelerate convolution/correlation\n**Applications:** Audio processing, Image compression, PDE solvers, Polynomial multiplication\n**Methods:** Cooley-Tukey FFT, Radix-2, Split-radix, Bluestein\n**Examples:** Fâ‚„ = [[1,1,1,1], [1,-1,1,-1], ...], FFT of length 2^l"
  },
  {
    "front": "ğŸ§© Fast Fourier Transform (FFT)\nğŸ“˜ Domain: Numerical Algorithms",
    "back": "**Definition:** An efficient algorithm for computing the DFT in O(n log n) operations by recursively splitting into even/odd indices when n is a power of two.\n\n**Description:** The Cooley-Tukey FFT reduces DFT complexity from O(nÂ²) to O(n log n) using butterfly operations and twiddle factors. It is foundational in digital signal processing and enables real-time spectral analysis.\n\n**Goal:** Compute DFT with minimal arithmetic operations using recursive decomposition\n**Applications:** Spectral methods, FFT-based convolution, MRI reconstruction, Audio synthesis\n**Methods:** Decimation-in-time, Decimation-in-frequency, Bit-reversal, In-place computation\n**Examples:** Radix-2 butterfly: yâ±¼ = x_{2j} + Ï‰^j x_{2j+1}"
  },
  {
    "front": "ğŸ§© Schur Triangulation\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** A similarity transformation A = Q T Q* where T is upper triangular and Q is unitary, revealing eigenvalues on the diagonal of T.\n\n**Description:** Schur form is the foundation for robust eigenvalue computation and solving Sylvester equations. The QR-based Francis algorithm computes it in O(nÂ³) with high backward stability. Real Schur form handles complex conjugate pairs.\n\n**Goal:** Reduce matrix to triangular form under unitary similarity to expose eigenvalues and enable block algorithms.\n**Applications:** Eigenvalue problems, Sylvester solvers, Matrix functions, Control theory\n**Methods:** Francis QR algorithm, Hessenberg reduction, Double shift\n**Examples:** A = Q T Q* with T upper triangular, diag(T) = eigenvalues"
  },
  {
    "front": "ğŸ§© Iterative Methods\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** Iterative methods are algorithms that generate a sequence of approximations x_j to the solution x of a linear system Ax = b, starting from an initial guess and refining it until convergence.\n\n**Description:** They are preferred for large sparse systems where direct methods like Gaussian elimination are O(n^3) and computationally expensive, with per-iteration costs often O(n^2) or less, such as O(n) for sparse or O(n log n) with structured matrices.\n\n**Goal:** Solve large linear systems efficiently without full factorization.\n**Applications:** PDE discretizations, Optimization, Eigenproblems\n**Methods:** Krylov subspace methods, Conjugate gradient, GMRES, Preconditioned iterations\n**Examples:** x_{j+1} = x_j + correction, Convergence when ||r_j|| small"
  },
  {
    "front": "ğŸ§© Linear System\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** A linear system is an equation of the form Ax = b where A is an n x n matrix, x is the unknown vector, and b is the right-hand side vector.\n\n**Description:** For large n (e.g., 10^4 to 10^8), iterative methods are used due to high O(n^3) cost of direct solvers, especially when A is sparse or structured.\n\n**Goal:** Find x such that Ax = b.\n**Applications:** Scientific simulations, Machine learning, Engineering\n**Methods:** Direct (LU, QR), Iterative (CG, GMRES)\n**Examples:** A in C^{n x n}, b in C^n"
  },
  {
    "front": "ğŸ§© Arnoldi Process\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** The Arnoldi process builds an orthonormal basis Qâ±¼ for the Krylov subspace Kâ±¼(A; b) and a Hessenberg matrix Hâ±¼ such that A Qâ±¼ = Qâ±¼â‚Šâ‚ HÌ‚â±¼.\n\n**Description:** It uses Gramâ€“Schmidt-like orthogonalisation to compute basis vectors qâ‚– recursively, enabling reduced-order projections for solving systems or eigenvalues.\n\n**Goal:** Orthogonalise Krylov basis for stable computations.\n**Applications:** GMRES, Eigenvalue solvers, Matrix functions\n**Methods:** Recursive computation: hâ‚–,â‚–â‚‹â‚ qâ‚– = A qâ‚–â‚‹â‚ âˆ’ Î£ hâ‚—,â‚–â‚‹â‚ qâ‚—\n**Examples:** Qâ±¼â‚Šâ‚ HÌ‚â±¼ = A Qâ±¼"
  },
  {
    "front": "ğŸ§© GMRES\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** Generalised Minimal Residual (GMRES) is an iterative method that finds xâ±¼ in xâ‚€ + Kâ±¼(A; râ‚€) minimising â€–b âˆ’ A xâ±¼â€–â‚‚ for nonsymmetric systems.\n\n**Description:** It uses Arnoldi to build the basis and solves a least-squares problem with the Hessenberg matrix at each step, restarting when j is large.\n\n**Goal:** Minimise residual norm over Krylov subspace.\n**Applications:** Nonsymmetric linear systems, PDE solvers\n**Methods:** Arnoldi orthogonalisation, Least squares on HÌ‚â±¼ y âˆ’ Î± eâ‚\n**Examples:** xâ±¼ = Qâ±¼ yâ±¼ where yâ±¼ minimises â€–HÌ‚â±¼ y âˆ’ Î± eâ‚â€–"
  },
  {
    "front": "ğŸ§© Conjugate Gradient Method\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** The Conjugate Gradient (CG) method solves symmetric positive definite systems Ax = b by minimising the A-norm error â€–x âˆ’ xâ±¼â€–â‚ over the Krylov subspace.\n\n**Description:** It generates A-conjugate search directions implicitly via a three-term recurrence, equivalent to Lanczos for symmetric matrices, with optimal polynomial approximation properties.\n\n**Goal:** Minimise quadratic form (x, A x)/2 âˆ’ (b, x).\n**Applications:** SPD linear systems, Optimisation (Newton-CG)\n**Methods:** Conjugate directions, Residual orthogonalisation\n**Examples:** xâ±¼â‚Šâ‚ = xâ±¼ + Î±â±¼ pâ±¼ with pâ±¼â‚Šâ‚ = râ±¼â‚Šâ‚ + Î²â±¼ pâ±¼"
  },
  {
    "front": "ğŸ§© Residual Norm\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** The residual norm â€–râ±¼â€– = â€–b âˆ’ A xâ±¼â€– measures how well the approximate solution xâ±¼ satisfies the system Ax = b.\n\n**Description:** In iterative methods, residuals decrease monotonically in GMRES, and are used to test convergence.\n\n**Goal:** Quantify approximation error in equation satisfaction.\n**Applications:** Convergence testing, Stopping criteria\n**Methods:** Euclidean norm, Relative residual\n**Examples:** â€–râ±¼â‚Šâ‚â€– â‰¤ â€–râ±¼â€– in GMRES"
  },
  {
    "front": "ğŸ§© A-Norm Error\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** The A-norm error â€–x âˆ’ xâ±¼â€–â‚ = ( (x âˆ’ xâ±¼), A (x âˆ’ xâ±¼) )^{1/2} measures the error in the energy norm for SPD A.\n\n**Description:** CG minimises this norm over the Krylov subspace, relating to the quadratic form minimised in the system.\n\n**Goal:** Quantify solution error in energy sense.\n**Applications:** CG convergence analysis, Variational problems\n**Methods:** Defined via inner product (x, A y)\n**Examples:** Min â€–x âˆ’ xâ±¼â€–â‚ in CG"
  },
  {
    "front": "ğŸ§© Polynomial Approximation\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** Polynomial approximation in iterative methods views xâ±¼ = pâ±¼â‚‹â‚(A) b as a polynomial in A applied to b, minimising residuals or errors via min-max problems over polynomials.\n\n**Description:** Convergence bounds use Chebyshev or other polynomials to estimate rates based on eigenvalue distribution.\n\n**Goal:** Approximate Aâ»Â¹ b via polynomials in A.\n**Applications:** Convergence analysis, Accelerated methods\n**Methods:** Min-max over deg â‰¤ j-1, Chebyshev acceleration\n**Examples:** min_{deg p â‰¤ j-1, p(0)=1} max_Î» |p(Î»)|"
  },
  {
    "front": "ğŸ§© Hessenberg Matrix\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** A Hessenberg matrix H is upper triangular except for the subdiagonal, arising in Arnoldi as the projection of A onto the Krylov basis.\n\n**Description:** It simplifies least-squares solves in GMRES and eigenvalue computations.\n\n**Goal:** Reduce matrix for efficient projections.\n**Applications:** GMRES minimisation, QR algorithm\n**Methods:** From Arnoldi: HÌ‚â±¼ with subdiagonal\n**Examples:** Hâ±¼ tridiagonal in Lanczos"
  },
  {
    "front": "ğŸ§© Orthogonal Complement\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** The orthogonal complement of a subspace V is the set of vectors perpendicular to all elements in V, denoted V^âŠ¥.\n\n**Description:** For projections, if P is orthogonal, then R(P) âŠ¥ R(I - P), ensuring the decomposition is orthogonal.\n\n**Goal:** Decompose space into perpendicular subspaces.\n**Applications:** Gram-Schmidt, Least squares, Spectral methods\n**Methods:** Dot product zero condition, Null space of transpose\n**Examples:** R(I - P) as complement of R(P)"
  },
  {
    "front": "ğŸ§© Gershgorin Circle Theorem\nğŸ“˜ Domain: Matrix Analysis",
    "back": "**Definition:** Every eigenvalue of A âˆˆ â„‚^{nÃ—n} lies in at least one disk Dâ±¼ = {z : |z âˆ’ aâ±¼â±¼| â‰¤ Râ±¼}, where Râ±¼ = Î£_{lâ‰ j} |aâ±¼â‚—|}.\n\n**Description:** Provides eigenvalue localization without computation. Disks are centered at diagonal entries with radii equal to off-diagonal row sums. Useful for bounding spectral radius and detecting diagonal dominance.\n\n**Goal:** Locate eigenvalues in complex plane using only matrix entries.\n**Applications:** Convergence analysis, Error bounding, Preconditioning design\n**Methods:** Row-sum computation, Union of disks, Refinement via similarity\n**Examples:** Strictly diagonally dominant â†’ eigenvalues in disjoint disks"
  },
  {
    "front": "ğŸ§© Power Iteration\nğŸ“˜ Domain: Eigenvalue Algorithms",
    "back": "**Definition:** An iterative method that computes the dominant eigenvalue and eigenvector by repeatedly applying A to a vector and normalizing: q^{(k)} = A q^{(k-1)} / ||A q^{(k-1)}||, Î»^{(k)} = (A q^{(k)}, q^{(k)}).\n\n**Description:** Converges linearly to the eigenvector corresponding to the largest-magnitude eigenvalue if |Î»â‚| > |Î»â‚‚| â‰¥ ... â‰¥ |Î»â‚™|. Convergence rate is |Î»â‚‚/Î»â‚|. Inverse iteration targets smallest or shifted eigenvalues.\n\n**Goal:** Find dominant eigenpair with minimal storage and simple operations.\n**Applications:** PageRank, Principal Component Analysis, Vibration modes\n**Methods:** Rayleigh quotient, Normalization, Deflation, Shift-and-invert\n**Examples:** q^{(k)} â‰ˆ xâ‚ + O((Î»â‚‚/Î»â‚)^k)"
  },
  {
    "front": "ğŸ§© Householder Reflection\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** A unitary matrix H = I âˆ’ 2vv*/(v*v) that reflects a vector x across the hyperplane perpendicular to v, mapping x to Ïƒeâ‚.\n\n**Description:** Used in QR factorization and Hessenberg reduction to introduce zeros below the subdiagonal. Numerically stable and requires O(n) operations per reflection. Essential for implicit QR algorithm.\n\n**Goal:** Zero out selected entries via unitary similarity while preserving eigenvalues.\n**Applications:** QR decomposition, Hessenberg form, Tridiagonalization\n**Methods:** Sign choice for stability, WY representation, Blocked Householder\n**Examples:** H x = âˆ’sign(xâ‚) ||x|| eâ‚"
  },
  {
    "front": "ğŸ§© QR Algorithm\nğŸ“˜ Domain: Eigenvalue Computation",
    "back": "**Definition:** An iterative method that computes the Schur form via repeated QR decompositions: A_{k+1} = R_k Q_k, with A_0 = A. With shifts, converges cubically to upper triangular form.\n\n**Description:** The de facto standard for dense eigenvalue problems. Implicit version uses Householder/Bulge chasing to reduce cost from O(nÂ³) per iteration to O(n). Francis shift accelerates convergence.\n\n**Goal:** Compute all eigenvalues (and optionally eigenvectors) via unitary similarity to triangular form.\n**Applications:** MATLAB eig, LAPACK, Control theory, PDE solvers\n**Methods:** Hessenberg reduction, Francis double shift, Deflation, Balancing\n**Examples:** A_{k+1} = Q_k* A_k Q_k"
  },
  {
    "front": "ğŸ§© Generalized Eigenvalue Problem\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** Find Î» âˆˆ â„‚ and nonzero x âˆˆ â„‚â¿ such that Ax = Î»Bx, with A, B âˆˆ â„‚^{nÃ—n}.\n\n**Description:** Arises in structural dynamics, control, and Markov chains. Reduced to standard form if B invertible (M = Bâ»Â¹A). QZ algorithm generalizes QR using unitary transformations to triangularize both matrices.\n\n**Goal:** Solve coupled systems or weighted eigenvalue problems.\n**Applications:** Vibration with constraints, Markov chain stationary distribution, Optimal control\n**Methods:** QZ algorithm, Cholesky + standard EVP, Shift-and-invert\n**Examples:** Ax = Î»Bx with B positive definite"
  },
  {
    "front": "ğŸ§© Field of Values\nğŸ“˜ Domain: Matrix Analysis",
    "back": "**Definition:** The set F(A) = {x*Ax : x âˆˆ â„‚â¿, ||x||=1}, also known as the numerical range.\n\n**Description:** Convex, compact set containing all eigenvalues. Bounds spectral radius and condition number. For normal matrices, F(A) is the convex hull of eigenvalues.\n\n**Goal:** Characterize operator behavior beyond eigenvalues.\n**Applications:** Stability analysis, Convergence of iterations, Pseudospectrum approximation\n**Methods:** Rayleigh quotient, Hausdorff-Toeplitz theorem, Discretization\n**Examples:** F(A) = conv(Ïƒ(A)) if A normal"
  },
  {
    "front": "ğŸ§© LU Factorization with Partial Pivoting\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** LU factorization with partial pivoting decomposes a matrix A âˆˆ â„‚^{nÃ—n} into PA = LU, where P is a permutation matrix, L is unit lower triangular with |l_{ij}| â‰¤ 1 for i > j, and U is upper triangular. Partial pivoting selects the largest absolute entry in the current column as the pivot to minimize numerical instability.\n\n**Description:** This algorithm enhances the stability of Gaussian elimination by row permutations to avoid small pivots. It is the standard method for solving linear systems Ax = b in practice, balancing computational cost (O(nÂ³)) with robustness against round-off errors in floating-point arithmetic.\n\n**Goal:** Stable triangular factorization of a matrix for solving linear systems and computing inverses.\n**Applications:** Solving Ax = b, Matrix inversion, Determinant computation, Condition number estimation\n**Methods:** Gaussian elimination with row pivoting, In-place storage using single array, Compact WY representation for L\n**Examples:** 4Ã—4 matrix example with pivots 8, 17/4, -6/7, 2 showing growth control"
  },
  {
    "front": "ğŸ§© Partial Pivoting\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** A pivoting strategy in Gaussian elimination that, at step k, permutes rows so that the entry of largest magnitude in column k (from row k to n) becomes the pivot, ensuring |pivot| = max_{iâ‰¥k} |a_{ik}|.\n\n**Description:** Partial pivoting prevents division by small pivots, reducing amplification of round-off errors. It guarantees that all subdiagonal entries in L satisfy |l_{ij}| â‰¤ 1, bounding the growth factor Ï â‰¤ 2^{n-1} in theory, though typically much smaller in practice.\n\n**Goal:** Minimize numerical error propagation during elimination by choosing largest available pivot.\n**Applications:** LU factorization, Linear system solving, Matrix decomposition in finite precision\n**Methods:** Row interchange before elimination step, Column scanning for max absolute value\n**Examples:** P1 swaps rows to bring largest entry to diagonal position"
  },
  {
    "front": "ğŸ§© Permutation Matrix\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** A permutation matrix P is a square matrix with exactly one 1 in each row and column and 0s elsewhere. Multiplying PA permutes the rows of A; AP permutes columns.\n\n**Description:** In LU with partial pivoting, P represents the cumulative row interchanges. Since P^{-1} = P^T = P^*, it preserves norms: ||Px|| = ||x||. The final factorization is PA = LU.\n\n**Goal:** Represent row or column reordering in matrix factorizations.\n**Applications:** Pivoting in LU, Reordering for sparsity, Graph relabeling\n**Methods:** Identity with swapped rows, Product of elementary permutation matrices\n**Examples:** P = [0 1; 1 0] swaps rows 1 and 2"
  },
  {
    "front": "ğŸ§© Growth Factor\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** The growth factor rho in LU factorization with partial pivoting is defined as rho = max_{i,j} |u_{ij}| / max_{i,j} |a_{ij}|, measuring the largest entry in U relative to the original matrix A.\n\n**Description:** Controls backward stability: computed factors satisfy L_hat U_hat = P A + delta A with ||delta A|| / ||A|| = O(rho epsilon_machine). Partial pivoting keeps rho moderate in practice, though worst-case rho = 2^{n-1} is possible.\n\n**Goal:** Quantify element growth during Gaussian elimination to assess numerical stability.\n**Applications:** Backward error analysis, Condition estimation, Pivoting strategy evaluation\n**Methods:** Ratio of max |u_{ij}| to max |a_{ij}|, Monitored during factorization\n**Examples:** Wilkinson's matrix gives rho approx 2^{n-1}"
  },
  {
    "front": "ğŸ§© Condition Number\nğŸ“˜ Domain: Numerical Linear Algebra",
    "back": "**Definition:** The condition number Îº(W) of a nonsingular matrix W is Îº(W) = ||W|| â‹… ||W^{-1}||, with respect to any consistent matrix norm. For the 1-norm or âˆ-norm, Îºâ‚(W) = Ïƒâ‚/Ïƒâ‚™ where Ïƒ are singular values.\n\n**Description:** Measures sensitivity of linear system solution to perturbations. Large Îº implies ill-conditioned system: small changes in input cause large output changes. In LU context, related to pivot size and growth.\n\n**Goal:** Quantify sensitivity of Ax = b to perturbations in A or b.\n**Applications:** Error bounds in linear solvers, Preconditioning design, Numerical stability analysis\n**Methods:** SVD-based: Îºâ‚‚ = Ïƒ_max / Ïƒ_min, 1-norm estimation via Hager's method\n**Examples:** Îº(A) â‰ˆ 10^k â‡’ lose k digits of accuracy"
  },
  {
    "front": "ğŸ§© Gaussian Elimination\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** Gaussian elimination transforms a linear system Ax = b into upper triangular form Ux = c via row operations: adding multiples of one row to another. With pivoting, it forms the basis of LU factorization.\n\n**Description:** Core algorithm for solving linear systems. Without pivoting, unstable for small pivots. With partial pivoting, becomes robust standard method. Can be expressed as sequence of rank-1 updates or multiplier storage in L.\n\n**Goal:** Reduce system to triangular form for back substitution.\n**Applications:** Linear system solving, Matrix factorization, Determinant via product of diagonals\n**Methods:** Forward elimination, Back substitution, Pivoting variants\n**Examples:** 4Ã—4 system reduced step-by-step with P, L, U shown"
  },
  {
    "front": "ğŸ§© Matrix Product V1 V2\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** The matrix product V1 V2 represents the set of all matrices formed by multiplying elements from subspaces V1 and V2, i.e., {V1 V2 : V1 âˆˆ V1, V2 âˆˆ V2}.\n\n**Description:** This construct is used to approximate or factor large matrices A by finding subspaces such that A lies in V1 V2, enabling low-parameter representations.\n\n**Goal:** Represent matrices as products of subspace elements for factorization.\n**Applications:** Matrix approximation, Low-rank factoring, Compression\n**Methods:** Subspace identification, Optimization over subspaces\n**Examples:** A â‰ˆ V1 V2 for V1, V2 low-dimensional"
  },
  {
    "front": "ğŸ§© Range of Projection\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** The range R(P) of a projection P is the set {y : y = Px for some x}, representing the subspace onto which P projects.\n\n**Description:** For orthogonal projections, R(P) is perpendicular to R(I - P), forming an orthogonal decomposition of the space.\n\n**Goal:** Define the projected subspace.\n**Applications:** Subspace identification, Dimensionality reduction\n**Methods:** Column span of P, Fixed points of P\n**Examples:** R(P) = span{columns of P}"
  },
  {
    "front": "ğŸ§© Algorithmic Factoring\nğŸ“˜ Domain: Numerical Analysis",
    "back": "**Definition:** Algorithmic factoring refers to computational methods for decomposing matrices into structured forms like products of subspaces or triangular factors.\n\n**Description:** It leverages Krylov subspaces and projections to factor matrices efficiently, especially for large-scale problems.\n\n**Goal:** Factor matrices using algorithmic techniques.\n**Applications:** Large linear systems, Eigenproblems, Approximations\n**Methods:** Subspace iteration, Projection-based factoring\n**Examples:** A = V1 W^{-1} using subspaces"
  },
  {
    "front": "ğŸ§© Invariant Subspace\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** An invariant subspace W for matrix A satisfies A W âŠ† W, meaning A maps W into itself.\n\n**Description:** Invariant subspaces are key in spectral theory and factoring, often identified via Krylov methods or projections.\n\n**Goal:** Find subspaces stable under matrix action.\n**Applications:** Eigen decomposition, Schur form, Model reduction\n**Methods:** Subspace iteration, Arnoldi process\n**Examples:** Eigenvector spans 1D invariant subspace"
  },
  {
    "front": "ğŸ§© Square Matrix A\nğŸ“˜ Domain: Linear Algebra",
    "back": "**Definition:** A square matrix A âˆˆ â„‚^{nÃ—n} is a matrix with equal rows and columns, central to linear transformations and eigenvalue problems.\n\n**Description:** In factoring contexts, A is decomposed using subspaces, projections, or iterations for computational efficiency.\n\n**Goal:** Represent linear operators on finite-dimensional spaces.\n**Applications:** Systems of equations, Transformations, Spectral analysis\n**Methods:** Factorization, Iteration, Diagonalization\n**Examples:** A with complex entries"
  }
]