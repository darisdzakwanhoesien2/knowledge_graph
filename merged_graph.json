{
  "nodes": {
    "Attribute Noise": {
      "type": "error_type",
      "domain": "Data Quality",
      "definition": "Noise arising from erroneous, missing, or corrupted feature values in data.",
      "description": "Attribute noise distorts the representation of instances and can weaken classifiers by shifting feature distributions.",
      "properties": {
        "examples": [
          "typos",
          "invalid numeric values",
          "sensor failures"
        ],
        "effects": [
          "class boundary distortion",
          "small erroneous clusters"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Label Noise": {
      "type": "error_type",
      "domain": "Data Quality",
      "definition": "Noise caused by incorrect, inconsistent, or subjective class labels.",
      "description": "Label noise is particularly harmful for supervised learning, as it directly corrupts ground truth.",
      "properties": {
        "sources": [
          "human mistakes",
          "ambiguous labeling",
          "subjective judgments"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Noise Frequency Filtering": {
      "type": "method",
      "domain": "Signal Processing",
      "definition": "The removal of specific high- or low-frequency noise components from continuous signals.",
      "description": "Includes removing characteristic electrical interference, such as 50 Hz noise from power lines.",
      "properties": {
        "examples": [
          "notch filter",
          "band-pass filter"
        ],
        "risks": [
          "loss of useful information"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Equipment Error": {
      "type": "error_type",
      "domain": "Data Quality",
      "definition": "Noise or erroneous values caused by malfunctioning or poorly calibrated measurement devices.",
      "description": "Common in sensor-based data collection, producing sudden spikes or unrealistic readings.",
      "properties": {
        "causes": [
          "device failure",
          "poor calibration"
        ],
        "effects": [
          "outliers",
          "attribute noise"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Garbage Values": {
      "type": "error_type",
      "domain": "Data Pollution",
      "definition": "Invalid or meaningless values inserted into data fields due to errors, incompatibilities, or manual mistakes.",
      "description": "Garbage values may originate from free text fields, broken encodings, or copy-paste errors.",
      "properties": {
        "examples": [
          "nonsense strings",
          "unexpected tokens"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Delimiter Contamination": {
      "type": "error_type",
      "domain": "Data Pollution",
      "definition": "A data corruption issue where field values include delimiter characters, causing misalignment in structured formats.",
      "description": "Occurs in CSV or TSV files when text fields contain commas, tabs, or semicolons.",
      "properties": {
        "effects": [
          "row fragmentation",
          "column shifts"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Free-Text Field Contamination": {
      "type": "error_type",
      "domain": "Data Pollution",
      "definition": "Contamination caused by unstructured free-text fields containing unexpected patterns that break parsing.",
      "description": "Includes unescaped punctuation, symbols, or embedded delimiters.",
      "properties": {
        "sources": [
          "manual entry",
          "copy-paste"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Thousand-Separator Contamination": {
      "type": "error_type",
      "domain": "Data Pollution",
      "definition": "The presence of thousands separators in numeric fields causing parsing issues or incorrect numerical conversion.",
      "description": "Common when merging data across locales with different numeric conventions.",
      "properties": {
        "examples": [
          "1,000.50",
          "1.000,50"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Domain Drift": {
      "type": "phenomenon",
      "domain": "Data Quality",
      "definition": "A mismatch between the original data schema design and current data usage, leading to invalid category values.",
      "description": "Occurs when systems are repurposed, such as a 'Gender' field receiving business-related labels.",
      "properties": {
        "effects": [
          "category misalignment",
          "data pollution"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Global Outliers": {
      "type": "outlier_type",
      "domain": "Outlier Detection",
      "definition": "Data points that deviate significantly from the overall distribution of the dataset.",
      "description": "Most classical outlier detection methods target global outliers.",
      "properties": {
        "examples": [
          "extreme sensor readings"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Contextual Outliers": {
      "type": "outlier_type",
      "domain": "Outlier Detection",
      "definition": "Values that are only outliers within a specific contextual setting or subset of conditions.",
      "description": "A temperature of +25°C is an outlier in Oulu in January but not in July.",
      "properties": {
        "requires": [
          "contextual attributes"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Collective Outliers": {
      "type": "outlier_type",
      "domain": "Outlier Detection",
      "definition": "Groups of observations that together deviate significantly from the dataset’s normal behavior.",
      "description": "Individually normal points form an abnormal cluster, such as suspicious block transactions.",
      "properties": {
        "examples": [
          "market manipulation patterns"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Outlier Score": {
      "type": "metric",
      "domain": "Outlier Detection",
      "definition": "A continuous measure estimating how strongly an instance differs from expected normal behavior.",
      "description": "Used by many unsupervised and distance-based outlier detection models.",
      "properties": {
        "scale": "continuous"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Binary Outlier Classification": {
      "type": "metric",
      "domain": "Outlier Detection",
      "definition": "A binary decision mechanism indicating whether an instance is labeled as outlier or normal.",
      "description": "Often derived from thresholding outlier scores.",
      "properties": {
        "outputs": [
          "outlier",
          "non-outlier"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Subjective Outlier Thresholding": {
      "type": "method",
      "domain": "Outlier Detection",
      "definition": "Human-defined thresholding used to determine whether deviation magnitude is sufficient to consider a point an outlier.",
      "description": "Cutoff values depend on domain knowledge and application-specific tolerances.",
      "properties": {
        "difficulty": "non-universal thresholds"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Supervised Outlier Detection": {
      "type": "method",
      "domain": "Outlier Detection",
      "definition": "Outlier detection using labeled datasets, typically training classifiers to distinguish normal and abnormal instances.",
      "description": "Challenges include extreme class imbalance and insufficient outlier examples.",
      "properties": {
        "models": [
          "binary classifier",
          "one-class models"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Semi-Supervised Outlier Detection": {
      "type": "method",
      "domain": "Outlier Detection",
      "definition": "A hybrid outlier detection method using both labeled and unlabeled data.",
      "description": "Most useful for modeling normal behavior rather than rare anomalies.",
      "properties": {
        "strength": "leverages unlabeled data"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Unsupervised Outlier Detection": {
      "type": "method",
      "domain": "Outlier Detection",
      "definition": "Outlier detection assuming that normal instances form clusters, while outliers lie far from these clusters.",
      "description": "Useful when labels are unavailable; many clustering and density-based techniques apply.",
      "properties": {
        "assumptions": [
          "normal data clustered"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Parametric Outlier Detection": {
      "type": "method",
      "domain": "Statistical Outlier Detection",
      "definition": "Outlier detection assuming that normal data follow a known parametric distribution.",
      "description": "Includes Gaussian modeling, where low-probability instances are flagged as outliers.",
      "properties": {
        "examples": [
          "Gaussian distribution"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Nonparametric Outlier Detection": {
      "type": "method",
      "domain": "Statistical Outlier Detection",
      "definition": "Outlier detection without assuming any prior data distribution, relying instead on data-driven density estimation.",
      "description": "Often uses kernel density estimation to identify low-density regions.",
      "properties": {
        "examples": [
          "kernel density estimation"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Gaussian Outlier Modeling": {
      "type": "method",
      "domain": "Statistical Outlier Detection",
      "definition": "A parametric approach modeling normal data using a Gaussian distribution and identifying low-likelihood instances as outliers.",
      "description": "Effective when data approximately follow normal distributions.",
      "properties": {
        "requirements": [
          "mean and covariance estimation"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Kernel Density Estimation for Outlier Detection": {
      "type": "method",
      "domain": "Statistical Outlier Detection",
      "definition": "Outlier detection using a nonparametric density estimate of the normal data distribution.",
      "description": "Instances in low-density areas under the estimated kernel distribution are considered outliers.",
      "properties": {
        "techniques": [
          "Gaussian kernels",
          "Epanechnikov kernels"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Distance-Based Outlier Detection": {
      "type": "method",
      "domain": "Proximity-Based Methods",
      "definition": "Outlier detection based on distances to neighbors using global parameters such as radius and neighbor threshold.",
      "description": "Points with too few neighbors within a specified distance are flagged as outliers.",
      "properties": {
        "parameters": [
          "radius r",
          "threshold t"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Density-Based Outlier Detection": {
      "type": "method",
      "domain": "Proximity-Based Methods",
      "definition": "Outlier detection comparing local density of a point to the densities of its neighbors.",
      "description": "Low-density points relative to nearby points are considered outliers; LOF is a common example.",
      "properties": {
        "examples": [
          "Local Outlier Factor (LOF)"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Clustering-Based Outlier Detection": {
      "type": "method",
      "domain": "Outlier Detection",
      "definition": "Outlier detection assuming normal data form large dense clusters and outliers form small, sparse clusters.",
      "description": "Methods include K-Means and Gaussian Mixture Models.",
      "properties": {
        "weakness": [
          "sensitive to noise"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Sparse-Cluster Outliers": {
      "type": "outlier_type",
      "domain": "Clustering-Based Outlier Detection",
      "definition": "Outliers that belong to clusters with very low density or membership count.",
      "description": "Common in clustering-based detection where small clusters are treated as abnormal.",
      "properties": {
        "identification": [
          "cluster size threshold"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Gaussian Mixture Model Outlier Detection": {
      "type": "method",
      "domain": "Clustering-Based Outlier Detection",
      "definition": "Outlier detection using mixture models where low-probability points under the model are flagged as outliers.",
      "description": "Each Gaussian component models a cluster; outliers lie outside high-likelihood areas.",
      "properties": {
        "technique": "Expectation-Maximization for model fitting"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "One-Class Classification": {
      "type": "method",
      "domain": "Classification-Based Outlier Detection",
      "definition": "A classification method modeling only the normal class, treating deviations as outliers.",
      "description": "Widely used for anomaly detection when outliers are scarce or unknown.",
      "properties": {
        "algorithms": [
          "One-Class SVM",
          "Autoencoders"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Outlier Detection via SVM": {
      "type": "method",
      "domain": "Classification-Based Outlier Detection",
      "definition": "Use of Support Vector Machines to identify outliers based on deviation from the learned normal boundary.",
      "description": "Includes both binary SVM approaches and one-class SVMs for anomaly detection.",
      "properties": {
        "advantages": [
          "robust boundaries",
          "works in high dimensions"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Outlier Detection via Random Forest": {
      "type": "method",
      "domain": "Classification-Based Outlier Detection",
      "definition": "Outlier detection using ensemble tree models to identify points poorly represented by learned structure.",
      "description": "Random forests can identify unusual patterns based on path length or leaf rarity.",
      "properties": {
        "mechanism": [
          "rare-leaf detection"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Predictive Outlier Detection": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Outlier detection in streaming data using predictive models such as autoregressive or multivariate regressors.",
      "description": "Points are anomalous when observed values deviate significantly from predictions.",
      "properties": {
        "models": [
          "AR models",
          "regression models"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Streaming Outlier Detection": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Methods for detecting outliers in continuous data streams where time-order and latency constraints apply.",
      "description": "Includes monitoring individual points, shape irregularities, and aggregated change points.",
      "properties": {
        "challenges": [
          "real-time constraints",
          "concept drift"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Unusual-Shape Outlier Detection": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Identification of anomalous temporal patterns based on shape irregularities rather than pointwise deviations.",
      "description": "Useful in detecting unusual motion patterns or abnormal physiological waveforms.",
      "properties": {
        "techniques": [
          "transform-based",
          "distance-based"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Multidimensional Streaming Outlier Detection": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Outlier detection extended to multivariate streaming systems, considering correlations across multiple dimensions.",
      "description": "Detects single-point anomalies or aggregated shifts across sensors or modalities.",
      "properties": {
        "outputs": [
          "point anomalies",
          "change points",
          "rare classes"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Change-Point Outlier Detection": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Detection of abrupt shifts in the mean, variance, or structural properties of a time series.",
      "description": "Change points indicate transitions to abnormal system states.",
      "properties": {
        "effects": [
          "distributional shift",
          "anomaly onset"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Rare-Class Detection in Time Series": {
      "type": "method",
      "domain": "Time-Series Outlier Detection",
      "definition": "Detection of rare or unusual event types in time-series streams.",
      "description": "Targets low-frequency events that have significant domain relevance, such as faults or intrusions.",
      "properties": {
        "challenges": [
          "class imbalance",
          "concept drift"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Data Ethics": {
      "type": "Principle",
      "domain": "Data Science",
      "definition": "A set of moral guidelines and practices governing responsible collection, analysis, and use of data, ensuring respect for privacy, transparency, and fairness.",
      "description": "Data ethics involves acting responsibly with the data entrusted to you. It acknowledges that data often represent people, and therefore, misuse can harm individuals or groups. Ethical data mining requires accountability, honesty, and consideration of the potential consequences of analysis outcomes.",
      "properties": {
        "Motives": [
          "Ensure positive social impact",
          "Avoid manipulation",
          "Promote fairness"
        ],
        "Methods": [
          "Transparency",
          "Accountability",
          "Consent"
        ],
        "Consequences": [
          "Respect privacy",
          "Prevent harm",
          "Foster trust"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_2_slides.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Ethical Data Mining": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The practice of conducting data mining in a way that is fair, transparent, and respects privacy and consent.",
      "description": "Ethical data mining balances the potential benefits of insights gained from data with the responsibility to protect subjects from harm. It considers ownership, consent, data sensitivity, and the possible implications of findings.",
      "properties": {
        "Key Principles": [
          "Honesty",
          "Transparency",
          "Responsibility",
          "Non-maleficence"
        ],
        "Challenges": [
          "Unintended harm",
          "Biased data",
          "Lack of consent"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_2_slides.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Informed Consent": {
      "type": "Principle",
      "domain": "Research Ethics",
      "definition": "The process of obtaining voluntary, informed permission from individuals whose data will be collected or used.",
      "description": "Informed consent ensures individuals understand what data is collected, how it will be used, and have the freedom to withdraw at any time. It is fundamental in research involving personal or sensitive data.",
      "properties": {
        "Requirements": [
          "Clear explanation of purpose",
          "Freedom to deny or withdraw",
          "Comprehensible to the participant"
        ],
        "Constraints": [
          "Use only for stated purpose",
          "Respect withdrawal",
          "Maintain anonymity"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_2_slides.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "General Data Protection Regulation (GDPR)": {
      "type": "Law",
      "domain": "European Data Protection",
      "definition": "A European Union regulation that governs data protection and privacy for individuals within the EU and EEA.",
      "description": "GDPR defines the lawful basis for data processing, grants rights to data subjects, and imposes obligations on data controllers and processors. It aims to give individuals control over their personal data and harmonize regulations across the EU.",
      "properties": {
        "Principles": [
          "Lawfulness",
          "Fairness",
          "Transparency",
          "Data minimisation",
          "Accountability"
        ],
        "Rights": [
          "Access",
          "Erasure",
          "Portability",
          "Restriction"
        ],
        "Sanctions": [
          "Fines for breaches",
          "Data protection impact assessments"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_2_slides.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Information Security": {
      "type": "Concept",
      "domain": "Data Management",
      "definition": "The practice of protecting information from unauthorized access, modification, or destruction to ensure confidentiality, integrity, and availability.",
      "description": "Information security ensures that data remains accurate, private, and accessible only to authorized users. The CIA triad—Confidentiality, Integrity, Availability—summarizes its key principles.",
      "properties": {
        "Core Principles": [
          "Confidentiality",
          "Integrity",
          "Availability"
        ],
        "Threats": [
          "Hardware failure",
          "Malware",
          "Human error",
          "Unauthorized access"
        ],
        "Practices": [
          "Encryption",
          "Backups",
          "Access control",
          "Authentication"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_2_slides.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Information Privacy": {
      "type": "Concept",
      "domain": "Data Protection",
      "definition": "The right and ability of individuals to control the collection, use, and dissemination of information about themselves.",
      "description": "Information privacy goes beyond the absence of surveillance—it concerns individuals’ control over personal information traces in digital society. Data mining poses unique challenges to privacy preservation.",
      "properties": {
        "Threats": [
          "Data theft",
          "Unethical use",
          "Government surveillance",
          "Lack of awareness"
        ],
        "Protective Measures": [
          "Anonymisation",
          "Pseudonymisation",
          "Access control"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_2_slides.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Anonymisation": {
      "type": "Technique",
      "domain": "Data Privacy",
      "definition": "The process of removing personally identifiable information from data to prevent the identification of individuals.",
      "description": "Anonymisation ensures that personal data cannot be traced back to individuals, even when combined with external data. It is critical in ethical data mining and required for many research use cases.",
      "properties": {
        "Methods": [
          "Suppression",
          "Generalisation",
          "K-anonymity",
          "Randomization"
        ],
        "Challenges": [
          "Re-identification risk",
          "Data utility loss"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_2_slides.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Open Data": {
      "type": "Concept",
      "domain": "Data Management",
      "definition": "Data that anyone can freely access, use, modify, and share for any purpose, subject at most to minimal conditions such as attribution.",
      "description": "Open data promotes transparency, innovation, and collaboration. It must be findable, accessible, interoperable, and reusable according to the FAIR principles.",
      "properties": {
        "Principles": [
          "Findable",
          "Accessible",
          "Interoperable",
          "Reusable"
        ],
        "Licenses": [
          "CC0",
          "CC BY",
          "ODbL"
        ],
        "Challenges": [
          "Loss of control",
          "Licensing complexity",
          "Data misuse"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_2_slides.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "FAIR Principles": {
      "type": "Framework",
      "domain": "Open Science",
      "definition": "A set of guidelines to make data Findable, Accessible, Interoperable, and Reusable.",
      "description": "The FAIR principles ensure data can be easily located, accessed, integrated, and reused by others, supporting open science and reproducibility.",
      "properties": {
        "Components": [
          "Findable",
          "Accessible",
          "Interoperable",
          "Reusable"
        ],
        "Benefits": [
          "Transparency",
          "Data sharing",
          "Reproducibility"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_2_slides.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Management": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The set of practices that ensure data is properly stored, maintained, and accessible for processing and analysis.",
      "description": "Data management encompasses persistent storage, security, metadata curation, and accessibility. Proper management ensures reliability and longevity of data for research and applications.",
      "properties": {
        "Aspects": [
          "Storage",
          "Curation",
          "Security",
          "Metadata"
        ],
        "Tools": [
          "Relational Databases",
          "NoSQL Systems"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_2_slides.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Machine Learning": {
      "type": "field",
      "domain": "Machine Learning",
      "definition": "A field of study and practice focused on algorithms that improve performance at tasks through experience (data).",
      "description": "Encompasses supervised, unsupervised, and reinforcement approaches and emphasizes predictive performance and automation.",
      "properties": {
        "subfields": [
          "Supervised Learning",
          "Unsupervised Learning",
          "Deep Learning"
        ],
        "typical_outputs": [
          "classifiers",
          "regressors",
          "clusterings"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Machine Learning Models": {
      "type": "category",
      "domain": "Machine Learning",
      "definition": "Parametric or non-parametric algorithm instantiations (models) learned from data to perform tasks such as classification or regression.",
      "description": "Examples include decision trees, SVMs, neural networks, ensemble models and one-class models used for anomaly detection.",
      "properties": {
        "examples": [
          "SVM",
          "Random Forest",
          "Neural Network",
          "One-Class Classification"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Supervised Learning": {
      "type": "method_category",
      "domain": "Machine Learning",
      "definition": "Learning where models are trained on input–output pairs (X, Y) to predict outputs from new inputs.",
      "description": "Requires labelled data and is sensitive to label noise and class imbalance.",
      "properties": {
        "common_algorithms": [
          "Logistic Regression",
          "SVM",
          "Random Forest",
          "Neural Networks"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Avoiding Overfitting": {
      "type": "practice",
      "domain": "Modeling Practice",
      "definition": "Techniques and design choices intended to reduce overfitting and improve model generalization.",
      "description": "Includes cross-validation, regularization, simpler models, early stopping, and appropriate data partitioning.",
      "properties": {
        "techniques": [
          "cross-validation",
          "regularization",
          "feature selection"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Bias-Variance Tradeoff": {
      "type": "concept",
      "domain": "Modeling Theory",
      "definition": "A conceptual decomposition of prediction error into bias (systematic error) and variance (estimation variability).",
      "description": "Guides model complexity choices: higher complexity reduces bias but increases variance and vice versa.",
      "properties": {
        "components": [
          "bias",
          "variance",
          "irreducible error"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Regularization": {
      "type": "method",
      "domain": "Modeling Practice",
      "definition": "Techniques that penalize model complexity during training to prevent overfitting (e.g., L1/L2 penalties, dropout).",
      "description": "Reduces variance at the cost of potentially increasing bias, improving generalization when tuned properly.",
      "properties": {
        "examples": [
          "L1 (lasso)",
          "L2 (ridge)",
          "dropout",
          "early stopping"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Model Bias": {
      "type": "issue",
      "domain": "Modeling",
      "definition": "Systematic errors in model outputs caused by incorrect model assumptions, biased training data, or simplistic model structures.",
      "description": "Closely related to societal bias: biased training data or labels can propagate harmful outcomes.",
      "properties": {
        "sources": [
          "sampling bias",
          "label bias",
          "feature omission"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Accuracy": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "Proportion of correct predictions over all predictions for classification tasks.",
      "description": "Simple metric but can be misleading with imbalanced classes (see Balanced Accuracy, F1 Score).",
      "properties": {
        "formula": "accuracy = (TP + TN) / (TP + TN + FP + FN)"
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Training Error": {
      "type": "metric",
      "domain": "Model Evaluation",
      "definition": "Error measured on the training dataset after model fitting.",
      "description": "Used together with test error to compute the generalization gap; overly low training error may signal overfitting.",
      "properties": {
        "used_for": [
          "monitoring fit",
          "early stopping"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Statistical Methods": {
      "type": "category",
      "domain": "Statistics",
      "definition": "Formal techniques for describing data and drawing inferences about populations from samples.",
      "description": "Encompasses hypothesis testing, estimation, regression, likelihood methods and nonparametric alternatives.",
      "properties": {
        "families": [
          "parametric",
          "nonparametric",
          "Bayesian",
          "frequentist"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Specialized Statistical Modeling": {
      "type": "category",
      "domain": "Statistics",
      "definition": "Domain-specific statistical models (e.g., survival models, mixed-effects models, time-series models) used when standard models are inadequate.",
      "description": "Often required for hierarchical data, longitudinal measurements, or nonstandard error structures.",
      "properties": {
        "examples": [
          "mixed-effects",
          "survival analysis",
          "state-space models"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Regression Modeling": {
      "type": "method",
      "domain": "Statistics",
      "definition": "Models that estimate relationships between dependent variables and one or more explanatory variables (linear and nonlinear forms).",
      "description": "Includes linear regression, generalized linear models, and specialized regression variants for complex data.",
      "properties": {
        "variants": [
          "linear",
          "logistic",
          "Poisson",
          "survival"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Causal Hypothesis": {
      "type": "concept",
      "domain": "Inference",
      "definition": "A proposed causal relationship between variables that is typically tested using explanatory models and experimental design.",
      "description": "Causal claims require careful design, confounding control and sometimes randomized interventions.",
      "properties": {
        "requires": [
          "counterfactual reasoning",
          "control of confounders"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Statistical Power": {
      "type": "metric",
      "domain": "Statistics",
      "definition": "The probability that a test will correctly reject a false null hypothesis (1 - type II error rate).",
      "description": "Determines required sample sizes for hypothesis-driven (explanatory) studies.",
      "properties": {
        "depends_on": [
          "effect_size",
          "sample_size",
          "alpha"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Statistical Significance": {
      "type": "concept",
      "domain": "Statistics",
      "definition": "A conclusion that an observed effect is unlikely to be due to chance under a specified null model, typically operationalized by p-values.",
      "description": "Statistical significance does not imply practical importance.",
      "properties": {
        "common_thresholds": [
          "p < 0.05",
          "p < 0.01"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Nonlinear PCA": {
      "type": "method",
      "domain": "Dimensionality Reduction",
      "definition": "Extensions of PCA that capture nonlinear relationships (e.g., kernel PCA, autoencoder-based approaches).",
      "description": "Useful when principal directions of variation are curved or manifold-structured.",
      "properties": {
        "examples": [
          "kernel PCA",
          "autoencoder PCA",
          "Isomap variants"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Statistical Outlier Detection": {
      "type": "category",
      "domain": "Outlier Detection",
      "definition": "Methods that identify outliers using statistical modelling — either parametric (distributional) or nonparametric (density estimation).",
      "description": "Serves as a parent for Parametric Outlier Detection and Nonparametric Outlier Detection.",
      "properties": {
        "subtypes": [
          "parametric",
          "nonparametric"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Experimental Design": {
      "type": "field",
      "domain": "Research Methods",
      "definition": "The process of planning experiments to ensure that data collection answers the research question while controlling confounders.",
      "description": "Includes design choices such as RCTs, crossover designs, sample size planning, randomization, and blocking.",
      "properties": {
        "elements": [
          "randomization",
          "control groups",
          "replication",
          "blocking"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Poor Experimental Design": {
      "type": "issue",
      "domain": "Research Methods",
      "definition": "Design flaws that reduce validity, such as lack of controls, small sample sizes, biased sampling, or inadequate blinding.",
      "description": "Leads to biased or non-generalizable results and may render data unusable for inference.",
      "properties": {
        "consequences": [
          "bias",
          "low power",
          "confounding"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Sampling Rate Adjustment": {
      "type": "method",
      "domain": "Sampling",
      "definition": "Practices for harmonizing or adjusting sampling frequencies (e.g., resampling, interpolation, downsampling) when merging data from multiple sources.",
      "description": "Addresses issues documented under Sampling Rate Mismatch and Sampling Synchronization.",
      "properties": {
        "techniques": [
          "interpolation",
          "GCD-based downsampling",
          "LCM-based oversampling"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Data Augmentation": {
      "type": "method",
      "domain": "Data Preprocessing",
      "definition": "Techniques for synthetically increasing dataset size or diversity (e.g., rotations, noise, synthetic samples) to improve model robustness.",
      "description": "Widely used in vision, audio and time-series to reduce overfitting and address class imbalance.",
      "properties": {
        "examples": [
          "noise injection",
          "time-warping",
          "synthetic sample generation"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Research Data Management": {
      "type": "practice",
      "domain": "Data Management",
      "definition": "Policies and processes for storing, documenting, preserving and sharing research data (metadata, provenance, access control).",
      "description": "Enables reproducibility and FAIR compliance; overlaps with Data Documentation and Metadata.",
      "properties": {
        "components": [
          "storage",
          "metadata",
          "access",
          "preservation"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Documentation": {
      "type": "artifact",
      "domain": "Data Management",
      "definition": "Written records describing dataset contents, methods, versioning, provenance, and usage constraints.",
      "description": "Broader than 'Data Documentation'—applies to code, protocols, experiment logs and data pipelines.",
      "properties": {
        "includes": [
          "data dictionaries",
          "readme",
          "protocol descriptions"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Design Relational Database Schemas": {
      "type": "process",
      "domain": "Database Management",
      "definition": "The activity of translating conceptual domain models into normalized relational table structures.",
      "description": "Uses ER modeling and considerations such as keys, constraints and normalization to avoid redundancy.",
      "properties": {
        "outputs": [
          "table definitions",
          "keys",
          "constraints"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Tabular Data": {
      "type": "datatype",
      "domain": "Data Structures",
      "definition": "Data organized in rows and columns (records and fields), typical for spreadsheets and relational database tables.",
      "description": "The primary input format for many classical machine learning and statistical methods.",
      "properties": {
        "representations": [
          "CSV",
          "SQL tables",
          "dataframes"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Data Protection Laws": {
      "type": "category",
      "domain": "Governance",
      "definition": "Legal frameworks that govern the collection, storage and processing of personal data (e.g., GDPR).",
      "description": "Includes regional and sectoral regulations that shape research design and data sharing.",
      "properties": {
        "examples": [
          "GDPR",
          "national data protection acts"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "GO FAIR Initiative": {
      "type": "initiative",
      "domain": "Open Science",
      "definition": "An international initiative promoting the FAIR principles for data (Findable, Accessible, Interoperable, Reusable).",
      "description": "Supports infrastructure, community practices and policies to make research data reusable.",
      "properties": {
        "goals": [
          "promote FAIR",
          "build infrastructure"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Human Research Ethics": {
      "type": "domain",
      "domain": "Ethics",
      "definition": "Ethical principles and regulatory requirements governing research involving human participants.",
      "description": "Encompasses Respect for Persons, Beneficence, Justice, informed consent, confidentiality, and protections for vulnerable groups.",
      "properties": {
        "components": [
          "informed consent",
          "risk assessment",
          "ethical approval"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Ethical Approval": {
      "type": "process",
      "domain": "Ethics",
      "definition": "The institutional review and permission process required before conducting research involving human participants.",
      "description": "Evaluates risk, consent procedures, data protection and participant recruitment strategies.",
      "properties": {
        "outputs": [
          "ethics approval letter",
          "conditions"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Privacy Preservation": {
      "type": "concept",
      "domain": "Governance",
      "definition": "Technical and policy measures intended to protect personal data and prevent re-identification.",
      "description": "Includes de-identification, k-anonymity, pseudonymisation and access controls.",
      "properties": {
        "techniques": [
          "k-Anonymity",
          "Pseudonymisation",
          "access controls"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Time Series Analysis": {
      "type": "field",
      "domain": "Time Series",
      "definition": "Methods for modelling, forecasting and understanding temporally ordered data and their dynamics.",
      "description": "Includes ARIMA, state-space models, spectral analysis and change-point detection.",
      "properties": {
        "tasks": [
          "forecasting",
          "anomaly detection",
          "seasonal decomposition"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Streaming Data Analysis": {
      "type": "field",
      "domain": "Time Series",
      "definition": "Techniques for real-time ingestion, processing and analysis of continuous data streams.",
      "description": "Requires low-latency algorithms and often online/incremental learning approaches.",
      "properties": {
        "challenges": [
          "concept drift",
          "throughput",
          "latency"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Time-Series Missing Data Handling": {
      "type": "category",
      "domain": "Missing Data",
      "definition": "Specialized methods for imputing or otherwise handling missingness in temporally-correlated data (e.g., LOCF, interpolation, state-space approaches).",
      "description": "Must account for autocorrelation; naive imputation can distort temporal dependencies.",
      "properties": {
        "techniques": [
          "LOCF",
          "BOCF",
          "interpolation",
          "state-space imputation"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Temporal Alignment": {
      "type": "process",
      "domain": "Time Series",
      "definition": "Aligning time stamps and synchronizing signals from different sources to a common temporal reference.",
      "description": "Includes timestamp synchronization, resampling, and offset correction.",
      "properties": {
        "methods": [
          "clock alignment",
          "interpolation",
          "offset correction"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Data Completeness": {
      "type": "quality_metric",
      "domain": "Data Quality",
      "definition": "The degree to which required data are present in a dataset (low missingness and full records).",
      "description": "Impacts analytic choices and validity; related to dropout rate and missing-data mechanisms.",
      "properties": {
        "measures": [
          "proportion complete records",
          "feature-level completeness"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Scaling": {
      "type": "operation",
      "domain": "Data Transformation",
      "definition": "Generic term for rescaling numeric variables to comparable ranges, including min-max, standardization and arbitrary-range mapping.",
      "description": "Completed nodes include Min-Max Normalization and Z-score Standardization; this node links to those concrete methods.",
      "properties": {
        "examples": [
          "Min-Max Normalization",
          "Z-score Standardization",
          "Scaling to Arbitrary Range"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Sensor Calibration": {
      "type": "process",
      "domain": "Data Quality",
      "definition": "Procedures to ensure sensor readings are accurate and comparable by adjusting for device-specific biases or drifts.",
      "description": "Reduces equipment error and calibration differences across devices.",
      "properties": {
        "outcomes": [
          "reduced bias",
          "improved comparability"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Signal Quality": {
      "type": "quality_metric",
      "domain": "Data Quality",
      "definition": "A measure of how usable a signal is for analysis, considering noise, saturation, dropouts and artifacts.",
      "description": "High signal quality facilitates reliable feature extraction and detection tasks.",
      "properties": {
        "components": [
          "SNR",
          "dropout_rate",
          "saturation_rate"
        ]
      },
      "metadata": {
        "source_pdf": false
      }
    },
    "Available-Case Analysis": {
      "type": "method",
      "domain": "Missing Data Handling",
      "definition": "A deletion method where all available observations for each variable or pair of variables are used without requiring complete records.",
      "description": "Also known as pairwise deletion. It uses all observed values for each statistical computation, enabling retention of more data than listwise deletion.",
      "properties": {
        "advantages": [
          "retains more data",
          "useful for covariance estimation"
        ],
        "disadvantages": [
          "inconsistent sample sizes",
          "may bias associations under MAR or MNAR"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Complete-Case Analysis": {
      "type": "method",
      "domain": "Missing Data Handling",
      "definition": "A deletion method where entire rows containing any missing values are removed from analysis.",
      "description": "Also known as listwise deletion. Many tools do this automatically, which may reduce sample size and introduce bias.",
      "properties": {
        "advantages": [
          "simple to implement"
        ],
        "disadvantages": [
          "loss of data",
          "biased estimates unless MCAR"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Hot Deck Imputation": {
      "type": "method",
      "domain": "Missing Data Handling",
      "definition": "An imputation technique where missing values are replaced with values drawn from similar cases within the same dataset.",
      "description": "Values are selected from cases that are most similar to the incomplete record; appropriate when missingness is rare.",
      "properties": {
        "source": "same dataset",
        "requirements": [
          "sufficient similar donors"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Cold Deck Imputation": {
      "type": "method",
      "domain": "Missing Data Handling",
      "definition": "An imputation technique where missing values are replaced with values drawn from an external but similar dataset.",
      "description": "Used when in-dataset donor values are insufficient or not representative.",
      "properties": {
        "source": "external dataset",
        "risk": [
          "dataset mismatch bias"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Last Observation Carried Forward": {
      "type": "method",
      "domain": "Time-Series Missing Data",
      "definition": "A method that fills missing time-series values using the last observed measurement in the sequence.",
      "description": "Often used in clinical trials; can introduce bias even under MCAR.",
      "properties": {
        "assumption": "stability of observation over time",
        "problems": [
          "bias",
          "distorted autocorrelation"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Baseline Observation Carried Forward": {
      "type": "method",
      "domain": "Time-Series Missing Data",
      "definition": "A method where all missing values are replaced with the baseline measurement.",
      "description": "Simplifies analysis but can severely distort temporal trends.",
      "properties": {
        "strength": "preserves baseline values",
        "weakness": [
          "bias",
          "underestimates change"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Full Information Maximum Likelihood": {
      "type": "method",
      "domain": "Missing Data Modeling",
      "definition": "A maximum likelihood estimation approach that uses all available data to estimate parameters without explicitly imputing missing values.",
      "description": "Relies on likelihood contributions from incomplete cases; improves overall parameter accuracy.",
      "properties": {
        "advantages": [
          "uses all available data",
          "works under MAR"
        ],
        "limitations": [
          "underestimates standard errors",
          "requires correct model specification"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Expectation-Maximization Algorithm": {
      "type": "algorithm",
      "domain": "Statistical Estimation",
      "definition": "An iterative algorithm for maximum likelihood estimation in the presence of missing or latent variables.",
      "description": "Alternates between estimating expected log-likelihood (E-step) and maximizing it (M-step) until convergence.",
      "properties": {
        "steps": [
          "E-step",
          "M-step"
        ],
        "requirements": [
          "numerical data",
          "convergence criteria"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Imputation Pile-Up": {
      "type": "phenomenon",
      "domain": "Imputation Issues",
      "definition": "The tendency of imputed values to cluster around central tendencies such as the mean.",
      "description": "Common in mean imputation, reducing variability and distorting distributions.",
      "properties": {
        "effects": [
          "loss of variance",
          "biased correlations"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Loss of Variability in Imputed Data": {
      "type": "phenomenon",
      "domain": "Imputation Issues",
      "definition": "Reduction in variance of the dataset caused by imputation methods that fail to incorporate uncertainty.",
      "description": "Occurs in mean and regression imputation, affecting model estimates.",
      "properties": {
        "consequences": [
          "biased standard errors",
          "overconfident estimates"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Impossible Imputation Values": {
      "type": "error_type",
      "domain": "Imputation Issues",
      "definition": "Imputed values that are logically or physically impossible within the domain of the variable.",
      "description": "Examples include negative ozone values or impossible categorical combinations.",
      "properties": {
        "causes": [
          "regression imputation",
          "improper model constraints"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Within-Imputation Variance": {
      "type": "statistical_measure",
      "domain": "Multiple Imputation",
      "definition": "The component of variance resulting from sampling uncertainty within each imputed dataset.",
      "description": "Represents traditional sampling variance from standard complete-data analyses.",
      "properties": {
        "symbol": "Ū"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Between-Imputation Variance": {
      "type": "statistical_measure",
      "domain": "Multiple Imputation",
      "definition": "The variance arising from differences among the multiple imputed datasets.",
      "description": "Reflects uncertainty about the correct value to impute.",
      "properties": {
        "symbol": "B"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Total Variance in Multiple Imputation": {
      "type": "statistical_measure",
      "domain": "Multiple Imputation",
      "definition": "The combined variance estimate incorporating both within- and between-imputation variance.",
      "description": "Ensures correct standard errors and confidence intervals under MI.",
      "properties": {
        "formula": "T = Ū + (1 + 1/M) * B"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Data Merging": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The process of combining data from multiple sources, sensors, or studies into a single unified dataset for analysis.",
      "description": "Data merging integrates data collected from different sources to improve completeness and richness. It requires alignment of formats, timestamps, sampling rates, and units to ensure compatibility. Poorly merged data can lead to inconsistent or misleading conclusions.",
      "properties": {
        "Motivation": [
          "Combine complementary data from multiple sensors or datasets",
          "Enhance context and completeness of analysis",
          "Support multi-modal data understanding"
        ],
        "Challenges": [
          "Different formats (CSV, TXT, JSON)",
          "Different timestamps or units",
          "Sensor calibration differences",
          "Incompatible sampling frequencies"
        ],
        "Requirements": [
          "Synchronized timestamps",
          "Uniform sampling rate",
          "Compatible data structure"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Sampling Synchronization": {
      "type": "Task",
      "domain": "Signal Processing",
      "definition": "The process of aligning data collected at different sampling frequencies to a common rate to allow joint analysis.",
      "description": "When data are collected from sensors with different sampling rates, synchronization ensures time alignment by either downsampling or oversampling. This process balances information preservation and computational efficiency.",
      "properties": {
        "Methods": [
          "Downsampling",
          "Oversampling"
        ],
        "Problems": [
          "Data loss during downsampling",
          "Increased processing cost during oversampling"
        ],
        "Examples": [
          "Combining 50Hz and 100Hz signals using a common rate of 50Hz or 100Hz"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Downsampling": {
      "type": "Method",
      "domain": "Signal Processing",
      "definition": "Reducing the sampling rate of signals to a lower frequency to match other data sources or to decrease data volume.",
      "description": "Downsampling is typically done using the greatest common divisor (GCD) of sampling rates. It reduces data volume and computation cost but may lead to information loss.",
      "properties": {
        "Technique": [
          "Use GCD of sampling rates",
          "Select every nth sample"
        ],
        "Benefits": [
          "Simplified synchronization",
          "Reduced processing load"
        ],
        "Drawbacks": [
          "Information loss",
          "Potential aliasing"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Oversampling": {
      "type": "Method",
      "domain": "Signal Processing",
      "definition": "Increasing the sampling rate of a dataset to match a higher-frequency source, typically by interpolation or replication.",
      "description": "Oversampling is performed using the least common multiple (LCM) of sampling rates. It allows finer temporal alignment across datasets but increases data volume and processing requirements.",
      "properties": {
        "Technique": [
          "Interpolate missing samples",
          "Use LCM of sampling rates"
        ],
        "Benefits": [
          "Improved time alignment",
          "Preserved signal fidelity"
        ],
        "Drawbacks": [
          "Increased computation time",
          "Possible overfitting or redundancy"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Sampling Methods": {
      "type": "Concept",
      "domain": "Data Science",
      "definition": "Techniques for selecting a subset of data points from a larger dataset to make statistical analysis feasible.",
      "description": "Sampling allows researchers to handle large datasets efficiently or generate artificial data when limited samples are available. Sampling can be with or without replacement, or balanced across classes.",
      "properties": {
        "Main Types": [
          "SRSWR",
          "SRSWOR",
          "Balanced Sampling"
        ],
        "Use Cases": [
          "Too much data",
          "Imbalanced datasets",
          "Computational constraints"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Simple Random Sampling Without Replacement (SRSWOR)": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "A sampling technique where each element of the dataset has an equal chance of being selected, and once selected, it is not replaced.",
      "description": "SRSWOR ensures unique samples by preventing duplicate selections. It is suitable when data volume is high, and balanced representation is needed without redundancy.",
      "properties": {
        "Formula": "Probability of selection = 1/N",
        "Benefits": [
          "No duplicates",
          "Representative sample"
        ],
        "Drawbacks": [
          "Limited sample diversity if dataset is small"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Simple Random Sampling With Replacement (SRSWR)": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "A sampling technique where each element can be selected more than once, as it is replaced back into the dataset after being drawn.",
      "description": "SRSWR allows multiple instances of the same data point. It is often used in bootstrapping and data augmentation where variability is desired despite small datasets.",
      "properties": {
        "Formula": "Each element has equal probability in every draw.",
        "Benefits": [
          "Allows bootstrapping",
          "Useful for small datasets"
        ],
        "Drawbacks": [
          "Potential bias if duplicates dominate"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Balanced Sampling": {
      "type": "Method",
      "domain": "Machine Learning",
      "definition": "A sampling approach designed to achieve a predefined class distribution within a dataset, typically used to mitigate imbalance.",
      "description": "Balanced sampling ensures that each class is proportionally represented in the training set. It can be implemented using random sampling or synthetic techniques.",
      "properties": {
        "Applications": [
          "Imbalanced classification",
          "Fair evaluation",
          "Data balancing"
        ],
        "Techniques": [
          "Stratified sampling",
          "SMOTE",
          "Resampling"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Synthetic Minority Oversampling Technique (SMOTE)": {
      "type": "Algorithm",
      "domain": "Machine Learning",
      "definition": "A synthetic data generation method that creates new samples for minority classes by interpolating between existing samples and their nearest neighbors.",
      "description": "SMOTE addresses data imbalance by generating synthetic instances instead of duplicating existing ones. It improves classifier performance in minority classes without distorting feature space distributions.",
      "properties": {
        "Process": [
          "Find k nearest neighbors",
          "Compute vector between sample and neighbor",
          "Multiply by random factor",
          "Add to sample to form synthetic data"
        ],
        "Benefits": [
          "Improves class balance",
          "Avoids overfitting on minority classes"
        ],
        "Challenges": [
          "Possible noise introduction",
          "Boundary overlapping"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Performance Metrics": {
      "type": "Concept",
      "domain": "Model Evaluation",
      "definition": "Quantitative measures used to evaluate the predictive performance and quality of machine learning models.",
      "description": "Performance metrics assess how well a model predicts or classifies data. In imbalanced datasets, traditional accuracy can be misleading; thus, alternative metrics such as F1-score, recall, and balanced accuracy are preferred.",
      "properties": {
        "Common Metrics": [
          "Accuracy",
          "Precision",
          "Recall",
          "F1-score",
          "Cohen’s Kappa",
          "Balanced Accuracy"
        ],
        "Problem": "Accuracy paradox—high accuracy may not indicate true model quality when class imbalance exists."
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Collection": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The process of systematically gathering and measuring information from a variety of sources to answer research questions, test hypotheses, and evaluate outcomes.",
      "description": "Effective data collection requires careful planning, proper methodology, and awareness of ethical and technical constraints. Poorly designed collection can result in bias, errors, or data loss. Documentation and reproducibility are crucial for trustworthiness.",
      "properties": {
        "Steps": [
          "Define problem, goals, and objectives",
          "Plan methodology and instruments",
          "Conduct pilot collection",
          "Perform final collection",
          "Report and analyze problems"
        ],
        "Considerations": [
          "Repeatability",
          "Accuracy",
          "Stability",
          "Reproducibility"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_3_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Planning": {
      "type": "Task",
      "domain": "Data Science",
      "definition": "The preparatory phase of data collection that involves defining the purpose, method, timing, and logistics of gathering data.",
      "description": "Proper planning ensures that the data collected answers the research question, avoids unnecessary costs, and minimizes errors. It includes defining data types, intervals, devices, and storage procedures.",
      "properties": {
        "Elements": [
          "Problem definition",
          "Method selection",
          "Sample size determination",
          "Measurement intervals",
          "Documentation strategy"
        ],
        "Best Practices": [
          "Iterate procedures",
          "Pilot small samples",
          "Keep methods simple"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_3_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Sampling": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "The process of selecting a subset of individuals or observations from a population to estimate characteristics of the whole population.",
      "description": "Proper sampling ensures statistical significance and reduces bias. It requires defining population size, margin of error, confidence interval, and expected dropout rate.",
      "properties": {
        "Parameters": [
          "Population size",
          "Confidence interval (Z-score)",
          "Margin of error",
          "Standard deviation (σ)"
        ],
        "Challenges": [
          "Dropout rate",
          "Selection bias",
          "Underrepresentation"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_3_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Stationary Data": {
      "type": "Data Type",
      "domain": "Signal Processing",
      "definition": "Data whose statistical properties such as mean and variance do not change over time.",
      "description": "Stationary elements remain stable during observation and are easier to model. Examples include controlled mechanical systems and constant environmental measurements.",
      "properties": {
        "Examples": [
          "Temperature-controlled experiments",
          "Static sensors"
        ],
        "Characteristics": [
          "Constant mean",
          "Constant variance",
          "Time-invariant"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_3_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Non-Stationary Data": {
      "type": "Data Type",
      "domain": "Signal Processing",
      "definition": "Data whose statistical properties such as mean, variance, or correlations change over time.",
      "description": "Non-stationary data comes from systems or subjects that evolve, such as humans, biological processes, or changing environments. Proper modeling requires segmentation and adaptive methods.",
      "properties": {
        "Sources": [
          "Human behavior",
          "Environmental change",
          "Mechanical wear",
          "Seasonal variation"
        ],
        "Techniques": [
          "Windowing",
          "Adaptive filtering",
          "Time normalization"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_3_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Human Data Collection": {
      "type": "Process",
      "domain": "Human-Centered Data Science",
      "definition": "The collection of data involving human participants, requiring special ethical, methodological, and technical considerations.",
      "description": "Humans are non-stationary, complex systems that change over time. Data collection involving humans requires respect for autonomy, informed consent, privacy protection, and avoidance of harm. It also involves practical challenges like device failure, signal blocking, or behavioral variability.",
      "properties": {
        "Challenges": [
          "Device malfunction",
          "Human variability",
          "Consent management",
          "Sensor connectivity",
          "Instruction adherence"
        ],
        "Ethical Requirements": [
          "Respect for persons",
          "Beneficence",
          "Justice",
          "Confidentiality"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_3_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Bias": {
      "type": "Concept",
      "domain": "Research Methodology",
      "definition": "A systematic deviation or error in data collection, analysis, or interpretation that leads to incorrect conclusions.",
      "description": "Bias may arise from researchers, respondents, or sampling design. Recognizing and mitigating bias ensures fairness, accuracy, and generalizability of findings.",
      "properties": {
        "Types": [
          "Researcher bias",
          "Respondent bias",
          "Human data bias"
        ],
        "Examples": [
          "Selective reporting",
          "Overrepresentation of certain groups",
          "Unit conversion errors (e.g., Mars Climate Orbiter case)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_3_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Randomized Controlled Trial (RCT)": {
      "type": "Method",
      "domain": "Experimental Research",
      "definition": "A type of scientific experiment designed to minimize bias by randomly assigning participants to experimental and control groups.",
      "description": "RCTs are widely used in medical and behavioral studies to establish causal relationships. Variants include parallel, crossover, cluster, and factorial designs. Randomization ensures comparability between groups.",
      "properties": {
        "Design Types": [
          "Parallel",
          "Crossover",
          "Cluster",
          "Factorial"
        ],
        "Strengths": [
          "Bias reduction",
          "Causal inference",
          "Reproducibility"
        ],
        "Weaknesses": [
          "Cost",
          "Complexity",
          "Ethical constraints"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_3_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Documentation": {
      "type": "Task",
      "domain": "Data Management",
      "definition": "The process of recording all relevant details about data collection, methods, devices, and issues to ensure reproducibility and traceability.",
      "description": "Documentation is an essential part of the data lifecycle. It ensures that datasets are interpretable and that future researchers can understand, reuse, and validate results. It includes metadata, logs, and notes on problems.",
      "properties": {
        "Benefits": [
          "Reproducibility",
          "Transparency",
          "Error tracking"
        ],
        "Common Tools": [
          "CSV logs",
          "Version control",
          "Metadata repositories"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_3_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Format Differences": {
      "type": "concept",
      "domain": "Data Integration",
      "definition": "Differences in structural or syntactic representation of data originating from multiple sources.",
      "description": "Common variations include CSV vs TXT, comma vs tab separators, and presence or absence of header rows.",
      "properties": {
        "examples": [
          "csv",
          "txt",
          "tab-separated",
          "comma-separated"
        ],
        "challenges": [
          "inconsistent parsing",
          "schema mismatch"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Cultural Formatting Differences": {
      "type": "concept",
      "domain": "Data Integration",
      "definition": "Variations in numeric and textual conventions across regions that affect data interpretation.",
      "description": "Typical issues include decimal comma versus decimal point and different date formats.",
      "properties": {
        "examples": [
          "1,23 vs 1.23",
          "DD/MM/YYYY vs MM/DD/YYYY"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Unit Mismatch": {
      "type": "error_type",
      "domain": "Data Integration",
      "definition": "A discrepancy where multiple data sources represent the same measurement using different units.",
      "description": "Examples include centimeters vs inches or acceleration measured in g versus m/s^2.",
      "properties": {
        "consequences": [
          "incorrect model inputs",
          "distorted feature magnitudes"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Timestamp Mismatch": {
      "type": "error_type",
      "domain": "Temporal Data",
      "definition": "Differences in the timestamp formats or reference scales used by multiple sensors or data sources.",
      "description": "Examples include Unix timestamps versus human-readable dates.",
      "properties": {
        "examples": [
          "Unix epoch",
          "ISO date strings"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Timestamp Synchronization": {
      "type": "method",
      "domain": "Temporal Data",
      "definition": "The process of aligning timestamps from multiple sources to a common reference scale.",
      "description": "Necessary when merging data from different devices or modalities.",
      "properties": {
        "methods": [
          "clock alignment",
          "interpolation",
          "offset correction"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Label Inconsistency": {
      "type": "error_type",
      "domain": "Data Integration",
      "definition": "A mismatch in semantic meaning or granularity of labels across datasets.",
      "description": "Example: one dataset defines 'walking' as flat-ground walking while another includes stairs.",
      "properties": {
        "effects": [
          "ambiguity",
          "inconsistent supervision"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Sensor Placement Variability": {
      "type": "concept",
      "domain": "Sensor Data",
      "definition": "Differences in sensor orientation or placement on the body or environment.",
      "description": "Placement differences significantly alter the recorded signals even for identical activities.",
      "properties": {
        "examples": [
          "upper arm vs wrist",
          "left vs right side"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Environmental Condition Variability": {
      "type": "concept",
      "domain": "Data Collection",
      "definition": "Changes in external conditions that affect the comparability of collected data.",
      "description": "Examples include season, terrain, indoor versus outdoor conditions, and environmental noise.",
      "properties": {
        "variables": [
          "temperature",
          "surface gradient",
          "lighting"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Calibration Differences": {
      "type": "error_type",
      "domain": "Sensor Data",
      "definition": "Variability in sensor output caused by imperfect or inconsistent calibration between devices.",
      "description": "Occurs even when sensors are of the same model and manufacturer.",
      "properties": {
        "causes": [
          "manufacturing tolerance",
          "wear",
          "environmental effects"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Incompatible Data Collection Protocols": {
      "type": "concept",
      "domain": "Data Collection",
      "definition": "A situation where datasets cannot be merged due to incompatible procedures, sampling rates, or measurement rules.",
      "description": "Arises when two studies follow fundamentally different data-collection methodologies.",
      "properties": {
        "reasons": [
          "different sampling rates",
          "non-matching units",
          "inconsistent label definitions"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Sampling Rate Mismatch": {
      "type": "error_type",
      "domain": "Sampling",
      "definition": "A discrepancy in sampling frequencies across sensors or data sources.",
      "description": "Makes direct merging difficult without resampling.",
      "properties": {
        "examples": [
          "50 Hz vs 100 Hz"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Battery–Sampling Trade-Off": {
      "type": "concept",
      "domain": "Sensor Data",
      "definition": "A trade-off between sampling rate and device battery consumption.",
      "description": "Higher sampling frequencies drain battery faster, limiting experiment duration.",
      "properties": {
        "factors": [
          "sampling rate",
          "battery capacity"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "GCD-Based Downsampling": {
      "type": "method",
      "domain": "Sampling",
      "definition": "Downsampling multiple signals to a shared sampling frequency derived from their greatest common divisor.",
      "description": "Can reduce data quantity but aligns signals to a common time base.",
      "properties": {
        "effect": "information loss"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "LCM-Based Oversampling": {
      "type": "method",
      "domain": "Sampling",
      "definition": "Oversampling multiple signals to a shared sampling frequency derived from their least common multiple.",
      "description": "Increases data size but avoids discarding information.",
      "properties": {
        "effect": "increased computational cost"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Confusion Matrix": {
      "type": "data_structure",
      "domain": "Performance Metrics",
      "definition": "A contingency table summarizing prediction results in classification tasks.",
      "description": "Captures true positives, false positives, true negatives, and false negatives.",
      "properties": {
        "entries": [
          "TP",
          "FP",
          "TN",
          "FN"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Balanced Accuracy": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "The average of recall obtained on each class.",
      "description": "Designed for imbalanced datasets where standard accuracy is misleading.",
      "properties": {
        "formula": "(TP/(TP+FN) + TN/(TN+FP)) / 2"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Precision": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "The proportion of predicted positives that are true positives.",
      "description": "Indicates how many positive predictions are correct.",
      "properties": {
        "formula": "TP / (TP + FP)"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Recall": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "The proportion of actual positives that are correctly identified.",
      "description": "Also called sensitivity; important in medical and anomaly detection tasks.",
      "properties": {
        "formula": "TP / (TP + FN)"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Specificity": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "The proportion of actual negatives that are correctly identified.",
      "description": "Complementary to recall, especially important for false-positive control.",
      "properties": {
        "formula": "TN / (TN + FP)"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "F1 Score": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "The harmonic mean of precision and recall.",
      "description": "Balances false positives and false negatives and is robust to imbalance.",
      "properties": {
        "formula": "2 * (precision * recall) / (precision + recall)"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Cohen’s Kappa": {
      "type": "metric",
      "domain": "Performance Metrics",
      "definition": "A statistic measuring inter-rater or model–true-label agreement adjusted for chance.",
      "description": "Useful for assessing classifier reliability beyond simple accuracy.",
      "properties": {
        "formula": "kappa = (p0 - pe) / (1 - pe)"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Model Generalization": {
      "type": "Concept",
      "domain": "Machine Learning",
      "definition": "The ability of a model to perform well on new, unseen data rather than merely on the data used for training.",
      "description": "Model generalization measures how effectively a model captures the underlying data distribution and applies learned patterns to new observations. High generalization indicates robustness and low overfitting. Poor generalization often results from excessive model complexity or insufficient training data.",
      "properties": {
        "Importance": [
          "Ensures model performance on real-world data",
          "Prevents overfitting to training data",
          "Improves reliability and interpretability"
        ],
        "Indicators": [
          "Low gap between training and testing error",
          "Stable performance across datasets"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_8_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Descriptive Modeling": {
      "type": "Model Type",
      "domain": "Data Mining",
      "definition": "A modeling approach focused on summarizing or representing data structure in a compact form without making predictions.",
      "description": "Descriptive models aim to find inherent patterns or relationships in data, such as grouping or correlations, rather than making future predictions. Common techniques include clustering, association rule mining, and dimensionality reduction.",
      "properties": {
        "Goal": "Describe structure and relationships within data",
        "Examples": [
          "Cluster Analysis",
          "Association Rules",
          "Principal Component Analysis (PCA)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_8_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Explanatory Modeling": {
      "type": "Model Type",
      "domain": "Statistics",
      "definition": "A modeling approach used to test causal hypotheses by analyzing relationships between variables.",
      "description": "Explanatory models are designed to confirm theoretical relationships, such as cause-and-effect, and are commonly applied in medical, social, and behavioral sciences. They focus on inference rather than prediction.",
      "properties": {
        "Examples": [
          "Linear Regression",
          "Logistic Regression",
          "Structural Equation Modeling"
        ],
        "Goals": [
          "Understand relationships",
          "Test scientific hypotheses"
        ],
        "Approach": "Retrospective and confirmatory"
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_8_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Predictive Modeling": {
      "type": "Model Type",
      "domain": "Machine Learning",
      "definition": "A modeling approach that uses historical data to predict future or unseen observations.",
      "description": "Predictive models estimate output variables (Y) based on input variables (X). They are data-driven, focusing on accuracy and generalization rather than inference. Examples include decision trees, neural networks, and support vector machines.",
      "properties": {
        "Goal": "Predict outcomes for new data",
        "Characteristics": [
          "Exploratory",
          "Prospective",
          "Accuracy-driven"
        ],
        "Examples": [
          "Random Forest",
          "SVM",
          "Neural Network Classifier"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_8_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Overfitting": {
      "type": "Phenomenon",
      "domain": "Machine Learning",
      "definition": "A modeling error that occurs when a model learns noise or random fluctuations in the training data instead of the underlying pattern.",
      "description": "Overfitting leads to low training error but high testing error. It occurs when models are overly complex relative to the amount of available data, reducing generalization performance.",
      "properties": {
        "Symptoms": [
          "Low training error, high test error",
          "Unstable predictions on new data"
        ],
        "Causes": [
          "Small dataset",
          "Excessive model complexity",
          "Insufficient regularization"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_8_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Underfitting": {
      "type": "Phenomenon",
      "domain": "Machine Learning",
      "definition": "A modeling condition where the model is too simple to capture the underlying structure of the data, resulting in poor performance on both training and test data.",
      "description": "Underfitting occurs when a model fails to learn the essential relationships between input and output variables. It usually happens when the model lacks sufficient complexity or training iterations.",
      "properties": {
        "Symptoms": [
          "High bias",
          "Low variance",
          "Poor performance on all datasets"
        ],
        "Causes": [
          "Too simple model",
          "Inadequate training",
          "High regularization"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_8_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Sample Size": {
      "type": "Parameter",
      "domain": "Statistical Modeling",
      "definition": "The number of observations or data points used to train and evaluate a model.",
      "description": "Sample size affects both the bias and variance of a model. Small datasets may cause overfitting, while large ones improve generalizability but increase computational costs. Predictive models generally require larger samples than explanatory models.",
      "properties": {
        "Effects": [
          "Small sample size → overfitting and poor generalization",
          "Large sample size → reduced variance and improved precision",
          "Beyond certain size → diminishing returns"
        ],
        "Guidelines": [
          "Larger data for prediction",
          "Smaller for inference"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_8_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Cross-Validation": {
      "type": "Method",
      "domain": "Model Evaluation",
      "definition": "A resampling technique used to assess the performance and generalization of a model by partitioning the dataset into subsets.",
      "description": "Cross-validation divides the data into multiple folds, training the model on some while testing on others. It provides a more reliable estimate of model performance than a single train-test split, especially with small datasets.",
      "properties": {
        "Variants": [
          "k-fold cross-validation",
          "Leave-one-out validation"
        ],
        "Benefits": [
          "Reduces variance of performance estimates",
          "Detects overfitting",
          "Efficient use of data"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_8_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Model Selection": {
      "type": "Process",
      "domain": "Machine Learning",
      "definition": "The process of choosing the best-performing model or configuration among multiple candidates based on validation metrics.",
      "description": "Model selection involves comparing model performance, complexity, and interpretability to find the optimal balance between bias and variance. Simpler models are often more generalizable and interpretable.",
      "properties": {
        "Criteria": [
          "Low validation error",
          "High interpretability",
          "Minimal overfitting"
        ],
        "Factors": [
          "Model type",
          "Hyperparameters",
          "Feature selection"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_8_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Collection Procedure": {
      "type": "process",
      "domain": "Data Collection",
      "definition": "A structured sequence of steps used to design, execute, and refine the process of gathering data for research or analysis.",
      "description": "Includes planning, initial pilot collection, iteration, and final data collection, followed by problem reporting.",
      "properties": {
        "steps": [
          "planning",
          "pilot collection",
          "iteration",
          "final collection",
          "problem reporting"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Data Collection Failure Modes": {
      "type": "concept",
      "domain": "Data Collection",
      "definition": "Common ways in which data collection systems or processes fail, producing incomplete, biased, or invalid data.",
      "description": "Includes sensor errors, unit conversion errors, device malfunction, filtering errors, and human mistakes.",
      "properties": {
        "examples": [
          "sensor placement error",
          "incorrect filtering",
          "signal saturation",
          "unit mismatch"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Signal Filtering Errors": {
      "type": "error_type",
      "domain": "Data Collection",
      "definition": "Errors caused by incorrect or overly aggressive filtering that removes legitimate data patterns.",
      "description": "A famous example is the ozone-hole data being removed due to filters discarding large drops as errors.",
      "properties": {
        "common_causes": [
          "thresholding errors",
          "misconfigured filters",
          "incorrect assumptions"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Sensor Placement Error": {
      "type": "error_type",
      "domain": "Sensor Data",
      "definition": "Incorrect attachment or orientation of sensors leading to inaccurate or unusable data.",
      "description": "Examples include reversing coils in welding studies or misplacing wearable devices.",
      "properties": {
        "effects": [
          "signal distortion",
          "activity mislabeling"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Battery Failure": {
      "type": "error_type",
      "domain": "Data Collection",
      "definition": "Data loss caused by device power depletion during recording sessions.",
      "description": "A common cause of missing data in field sensors, wearable devices, and mobile data collection.",
      "properties": {
        "impact": [
          "incomplete sessions",
          "loss of temporal continuity"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Repeatability": {
      "type": "quality_criterion",
      "domain": "Data Quality",
      "definition": "The ability to repeat a data collection procedure under identical conditions and obtain similar results.",
      "description": "A key criterion in experimental design and sensor-based studies.",
      "properties": {
        "requirements": [
          "controlled conditions",
          "stable instruments"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Reproducibility": {
      "type": "quality_criterion",
      "domain": "Data Quality",
      "definition": "The ability for independent researchers to reproduce results using the same methodology and data.",
      "description": "Requires documentation, transparent procedures, and stable data collection methods.",
      "properties": {
        "dependencies": [
          "documentation",
          "metadata",
          "repeatability"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Stability": {
      "type": "property",
      "domain": "Data Quality",
      "definition": "The extent to which measurement systems remain consistent over time.",
      "description": "Sensitive to environmental changes, sensor degradation, drift, and human behavior.",
      "properties": {
        "related_factors": [
          "drift",
          "non-stationarity",
          "sensor aging"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Margin of Error": {
      "type": "statistical_measure",
      "domain": "Statistics",
      "definition": "An expression of the amount of random sampling error in a survey's results.",
      "description": "Often expressed as a percentage and tied to confidence intervals.",
      "properties": {
        "inputs": [
          "sample size",
          "confidence level"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Confidence Interval": {
      "type": "statistical_concept",
      "domain": "Statistics",
      "definition": "A range of values derived from sample data that is likely to contain the true population value.",
      "description": "Determined using margin of error and Z-scores.",
      "properties": {
        "components": [
          "sample statistic",
          "margin of error"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Z-Score (Statistical)": {
      "type": "statistical_measure",
      "domain": "Statistics",
      "definition": "A normalized value expressing how many standard deviations a measurement is from the mean.",
      "description": "Used to compute confidence intervals and determine significance.",
      "properties": {
        "formula": "z = (x - μ) / σ"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Dropout Rate": {
      "type": "statistical_measure",
      "domain": "Sampling",
      "definition": "The proportion of participants who fail to complete a study or data collection procedure.",
      "description": "Critical for determining effective sample size.",
      "properties": {
        "applications": [
          "study planning",
          "survey design"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Stationary Elements": {
      "type": "concept",
      "domain": "Time Series",
      "definition": "Entities or systems whose properties do not change significantly over time.",
      "description": "Examples include stable mechanical components or controlled environments.",
      "properties": {
        "characteristics": [
          "constant mean",
          "constant variance"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Non-Stationary Elements": {
      "type": "concept",
      "domain": "Time Series",
      "definition": "Entities whose statistical properties change over time.",
      "description": "Typical for environmental data, human behavior, sensor drift, or seasonal changes.",
      "properties": {
        "examples": [
          "humans",
          "ecosystems",
          "aging machinery"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Temporal Drift": {
      "type": "phenomenon",
      "domain": "Time Series",
      "definition": "A gradual change in system behavior or measurement statistics over time.",
      "description": "One of the main challenges when gathering long-term sensor or human data.",
      "properties": {
        "types": [
          "sensor drift",
          "behavioral drift"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Seasonal Variability": {
      "type": "phenomenon",
      "domain": "Time Series",
      "definition": "Regular periodic fluctuations associated with seasons or cyclic processes.",
      "description": "A non-stationary factor influencing environmental and human measurements.",
      "properties": {
        "cycle_length": "annual or periodic"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Time Series Data Collection": {
      "type": "method",
      "domain": "Time Series",
      "definition": "Collecting data at successive time intervals to observe changes or patterns.",
      "description": "Important for avoiding leakage between training and testing windows.",
      "properties": {
        "design_features": [
          "distinct time windows",
          "sampling frequency"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Respect for Persons": {
      "type": "ethical_principle",
      "domain": "Human Research Ethics",
      "definition": "An ethical principle requiring that individuals be treated as autonomous agents.",
      "description": "Includes additional protections for individuals with diminished autonomy.",
      "properties": {
        "components": [
          "autonomy",
          "protection of vulnerable groups"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Beneficence": {
      "type": "ethical_principle",
      "domain": "Human Research Ethics",
      "definition": "An obligation to maximize possible benefits and minimize possible harms to participants.",
      "description": "A foundational principle in ethical human-subject research.",
      "properties": {
        "requirements": [
          "risk-benefit analysis",
          "avoidance of harm"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Justice": {
      "type": "ethical_principle",
      "domain": "Human Research Ethics",
      "definition": "The fair and equitable distribution of the burdens and benefits of research.",
      "description": "Ensures vulnerable or marginalized groups are not exploited.",
      "properties": {
        "considerations": [
          "equity",
          "fair selection of subjects"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Missing Data": {
      "type": "Concept",
      "domain": "Data Science",
      "definition": "Data values that are absent or undefined in a dataset due to various reasons such as measurement errors, data integration issues, or survey non-response.",
      "description": "Missing data are common in real-world datasets and can lead to biased estimates, invalid conclusions, and limited generalizability if not properly handled. Understanding the mechanism of missingness is critical for choosing the right imputation or estimation method.",
      "properties": {
        "Causes": [
          "Equipment malfunction or sensor errors",
          "Manual data entry mistakes",
          "Out-of-range measurements",
          "Survey non-response",
          "Combining datasets with incompatible schemas"
        ],
        "Consequences": [
          "Biased results",
          "Reduced statistical power",
          "Invalid modeling outcomes"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_5_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Missing Completely at Random (MCAR)": {
      "type": "Mechanism",
      "domain": "Statistics",
      "definition": "A missingness mechanism where the probability that data are missing is independent of both observed and unobserved data.",
      "description": "In MCAR, missing values occur purely by chance and are unrelated to any variables in the dataset. Analyses on MCAR data remain unbiased, although statistical power decreases due to data loss.",
      "properties": {
        "Example": "Random equipment failure or random sampling exclusion.",
        "Implication": "Deleting missing data under MCAR does not bias results but reduces dataset size."
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_5_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Missing at Random (MAR)": {
      "type": "Mechanism",
      "domain": "Statistics",
      "definition": "A missingness mechanism where the probability that data are missing depends on observed data but not on the missing data itself.",
      "description": "In MAR, the reason for missingness can be explained by other known variables in the dataset, such as a particular sensor brand failing more often. Many modern imputation and maximum likelihood methods assume MAR.",
      "properties": {
        "Example": "A certain device model frequently fails to record measurements.",
        "Implication": "Bias can be reduced if the imputation model includes the variables related to the missingness mechanism."
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_5_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Missing Not at Random (MNAR)": {
      "type": "Mechanism",
      "domain": "Statistics",
      "definition": "A missingness mechanism where the probability of missing data depends on the missing value itself or on unobserved factors.",
      "description": "MNAR occurs when the reason for missingness is related to the actual missing values, such as patients with severe illness skipping health surveys. MNAR models are complex and often difficult to fit correctly.",
      "properties": {
        "Example": "A sensor fails more often at high temperature readings.",
        "Implication": "Analyses assuming MAR may be biased when data are MNAR."
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_5_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Deletion": {
      "type": "Method",
      "domain": "Data Pre-processing",
      "definition": "A method for handling missing data by removing incomplete records or variables from analysis.",
      "description": "Deletion is the simplest way to deal with missing data but often reduces dataset size and statistical power. It can lead to bias if missingness is not completely random.",
      "properties": {
        "Variants": [
          "Listwise Deletion",
          "Pairwise Deletion"
        ],
        "Pros": [
          "Easy to implement",
          "No imputation needed"
        ],
        "Cons": [
          "Loss of data",
          "Possible bias under MAR or MNAR"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_5_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Imputation": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The process of replacing missing data with substituted values based on statistical or machine learning methods.",
      "description": "Imputation creates complete datasets by estimating missing values from observed data. It helps maintain sample size and reduce bias, provided the missingness mechanism is appropriately modeled.",
      "properties": {
        "Types": [
          "Mean Imputation",
          "Regression Imputation",
          "Stochastic Regression Imputation",
          "Hot Deck Imputation",
          "Cold Deck Imputation",
          "Multiple Imputation",
          "Machine Learning-Based Imputation"
        ],
        "Advantages": [
          "Preserves data volume",
          "Can reduce bias"
        ],
        "Limitations": [
          "Can distort variability",
          "Dependent on assumptions"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_5_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Mean Imputation": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "A simple imputation method where missing numerical values are replaced by the mean of the observed values.",
      "description": "Mean imputation is widely used but can reduce variance, distort correlations, and bias relationships between variables. It is only valid under MCAR assumptions.",
      "properties": {
        "Benefits": [
          "Easy to implement",
          "Restores dataset completeness"
        ],
        "Drawbacks": [
          "Underestimates variability",
          "Biases covariance and regression coefficients"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_5_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Regression Imputation": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "An imputation technique that predicts missing values using regression models based on other observed variables.",
      "description": "Regression imputation can preserve relationships between variables but reduces variance in the dataset. It assumes that missingness can be modeled using observed predictors.",
      "properties": {
        "Assumptions": [
          "Data is MAR",
          "Predictor variables are complete"
        ],
        "Challenges": [
          "May produce perfect fits",
          "Biased estimates if model misspecified"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_5_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Stochastic Regression Imputation": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "An extension of regression imputation that adds random noise to restore natural variability to the imputed values.",
      "description": "This method adds random residuals to regression predictions, maintaining relationships between variables while accounting for uncertainty. It avoids perfectly predicted values and better reflects true data variability.",
      "properties": {
        "Benefits": [
          "Restores variance",
          "Preserves relationships between variables"
        ],
        "Limitations": [
          "Still underestimates uncertainty",
          "Possible impossible values"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_5_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Multiple Imputation": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "A comprehensive imputation framework that replaces missing data with multiple plausible values, generating several complete datasets that are analyzed and combined.",
      "description": "Multiple imputation captures the uncertainty inherent in missing data by creating multiple imputed datasets, analyzing them separately, and pooling results. It provides unbiased parameter estimates and valid standard errors under MAR assumptions.",
      "properties": {
        "Phases": [
          "Imputation phase",
          "Analysis phase",
          "Pooling phase"
        ],
        "Advantages": [
          "Preserves variance",
          "Handles uncertainty",
          "Suitable for classification"
        ],
        "Metrics": [
          "Within-imputation variance (U)",
          "Between-imputation variance (B)",
          "Total variance (T)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_5_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Maximum Likelihood Estimation (MLE)": {
      "type": "Method",
      "domain": "Statistics",
      "definition": "A statistical estimation technique that finds parameter values maximizing the likelihood of observed data, including cases with missing values.",
      "description": "MLE utilizes all available data to estimate model parameters without directly imputing missing values. It works by optimizing a log-likelihood function based on the observed data. Expectation-Maximization (EM) is a common iterative algorithm for this purpose.",
      "properties": {
        "Steps": [
          "Define likelihood function",
          "Compute log-likelihood",
          "Optimize via EM or Newton-Raphson"
        ],
        "Benefits": [
          "Uses all available data",
          "Produces unbiased estimates under MAR"
        ],
        "Limitations": [
          "Underestimates standard errors",
          "Requires numerical data"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_5_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Personal Data": {
      "type": "concept",
      "domain": "Privacy",
      "definition": "Any information relating to an identified or identifiable natural person.",
      "description": "Covers both direct identifiers and quasi-identifiers that can re-identify individuals when combined.",
      "properties": {
        "examples": [
          "names",
          "addresses",
          "location traces",
          "online identifiers"
        ],
        "legal_status": "Protected under GDPR"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Pseudonymisation": {
      "type": "method",
      "domain": "Privacy",
      "definition": "Replacing identifying attributes with artificial identifiers while keeping a mapping for re-identification.",
      "description": "Provides partial privacy but is reversible when the mapping is available.",
      "properties": {
        "characteristics": [
          "mapping retained separately",
          "reversible"
        ],
        "use_cases": [
          "research follow-ups",
          "risk-controlled data release"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "k-Anonymity": {
      "type": "privacy_property",
      "domain": "Privacy",
      "definition": "A property ensuring each record is indistinguishable from at least k-1 others based on quasi-identifiers.",
      "description": "Achieved through generalisation or suppression; vulnerable to attribute disclosure attacks.",
      "properties": {
        "parameter": "k",
        "methods": [
          "suppression",
          "generalisation"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Advanced Data Modification": {
      "type": "method",
      "domain": "Privacy",
      "definition": "Heuristic or algorithmic methods for modifying datasets to reduce privacy risk while retaining analytical value.",
      "description": "Includes perturbation, synthetic data generation, and suppression heuristics.",
      "properties": {
        "approaches": [
          "perturbation",
          "synthetic data",
          "attribute suppression heuristics"
        ]
      },
      "metadata": {
        "source_pdf": true,
        "definition_inferred": true
      }
    },
    "Relational Database": {
      "type": "data_structure",
      "domain": "Databases",
      "definition": "A structured collection of relations (tables) organized according to the relational model.",
      "description": "Implements schemas, supports SQL, and typically enforces ACID properties.",
      "properties": {
        "features": [
          "tables",
          "schemas",
          "ACID transactions"
        ],
        "examples": [
          "PostgreSQL",
          "MySQL",
          "Oracle"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "NoSQL Databases": {
      "type": "category",
      "domain": "Databases",
      "definition": "A broad category of database systems that do not use the relational model.",
      "description": "Includes document stores, key-value stores, columnar stores, and graph databases.",
      "properties": {
        "advantages": [
          "schema flexibility",
          "horizontal scaling"
        ],
        "examples": [
          "MongoDB"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "MongoDB": {
      "type": "software_system",
      "domain": "Databases",
      "definition": "A document-oriented NoSQL database storing data in BSON format.",
      "description": "Allows flexible schema design and uses a hierarchical document model.",
      "properties": {
        "storage_format": "BSON",
        "model": "Document Store"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Entity-Relationship Modeling": {
      "type": "method",
      "domain": "Databases",
      "definition": "A conceptual modeling method describing entities, attributes, and relationships.",
      "description": "Used to design relational schemas and understand domain structure.",
      "properties": {
        "components": [
          "entities",
          "attributes",
          "relationships"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Relational Algebra": {
      "type": "theory",
      "domain": "Databases",
      "definition": "A formal set of operations on relations forming the foundation of SQL.",
      "description": "Core operators include projection, selection, and natural join.",
      "properties": {
        "operators": [
          "projection",
          "selection",
          "join"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "SQL": {
      "type": "language",
      "domain": "Databases",
      "definition": "A declarative language for managing and querying relational databases.",
      "description": "Supports SELECT, JOIN, INSERT, UPDATE, DELETE, and schema definition.",
      "properties": {
        "categories": [
          "DDL",
          "DML",
          "DQL"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "CIA Triad": {
      "type": "security_model",
      "domain": "Security",
      "definition": "A model defining three core principles of information security: Confidentiality, Integrity, Availability.",
      "description": "Used to categorize threats and security controls.",
      "properties": {
        "principles": [
          "Confidentiality",
          "Integrity",
          "Availability"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Open Data Licences": {
      "type": "category",
      "domain": "Licensing",
      "definition": "Licenses governing how datasets may be used, shared, and modified.",
      "description": "Includes CC0, CC BY, and ODbL with differing attribution and sharing requirements.",
      "properties": {
        "examples": [
          "CC0",
          "CC BY",
          "ODbL"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Metadata": {
      "type": "concept",
      "domain": "Data Management",
      "definition": "Structured information describing a dataset’s content, structure, provenance, and semantics.",
      "description": "Supports FAIR principles by enabling discovery and reuse.",
      "properties": {
        "components": [
          "variable descriptions",
          "units",
          "provenance",
          "versioning"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "ActivityMeasurement": {
      "type": "data_entity",
      "domain": "Example Schema",
      "definition": "An example entity representing individual activity measurements.",
      "description": "Used to illustrate relational and document database modeling.",
      "properties": {
        "attributes": [
          "measurementId",
          "personId",
          "monitorId",
          "measurementDate",
          "activeMinutes",
          "steps",
          "calories"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Numerosity Reduction": {
      "type": "Technique",
      "domain": "Data Pre-processing",
      "definition": "A data reduction technique that reduces the volume of data by choosing smaller, more compact representations without significantly sacrificing the integrity of analytical results.",
      "description": "Numerosity reduction aims to improve computational efficiency by replacing the original data with smaller parametric or non-parametric representations. Instead of storing the full dataset, only the model parameters or simplified summaries are kept. This enables faster querying, analysis, and modeling while preserving most essential patterns.",
      "properties": {
        "Goal": "Reduce data volume while maintaining the ability to perform accurate analysis.",
        "Categories": [
          "Parametric methods",
          "Non-parametric methods"
        ],
        "Parametric Methods": [
          "Regression models",
          "Log-linear models",
          "Other statistical modeling approaches where only model parameters are stored"
        ],
        "Non-parametric Methods": [
          "Histograms",
          "Clustering",
          "Sampling"
        ],
        "Advantages": [
          "Reduces storage space",
          "Improves algorithm performance",
          "Enables faster data processing"
        ],
        "Limitations": [
          "Potential loss of detail",
          "Quality depends on model or sampling assumptions"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_7_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Accuracy Paradox": {
      "type": "Concept",
      "domain": "Model Evaluation",
      "definition": "A phenomenon where a model with high accuracy performs poorly on the actual predictive task, especially when the dataset is imbalanced.",
      "description": "Accuracy can give a misleading impression of model performance because it does not account for class imbalance. In problems where one class dominates, models can achieve high accuracy by simply predicting the majority class, despite having little real predictive power. Therefore, additional evaluation metrics such as balanced accuracy, precision, recall, F1 score, or Cohen’s kappa should be used to correctly assess performance.",
      "properties": {
        "Problem": "Accuracy ignores the relative distribution of classes.",
        "Occurs_When": [
          "The dataset is imbalanced",
          "The majority class dominates predictions",
          "Accuracy does not reflect predictive power"
        ],
        "Better_Metrics": [
          "Balanced accuracy",
          "Precision",
          "Recall",
          "F1 score",
          "Cohen's kappa"
        ],
        "Example": "In a dataset with 275 healthy and 25 cancer patients, predicting everyone as healthy yields 91.7% accuracy but 0% predictive power for detecting cancer."
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Validation Set": {
      "type": "Dataset Partition",
      "domain": "Model Evaluation",
      "definition": "A subset of data used to tune model hyperparameters or select among competing models before final evaluation on the test set.",
      "description": "The validation set is used during model selection to ensure that the chosen model is not overfitting the training data. After models are compared based on validation performance, the final selected model is evaluated on a separate test set for unbiased performance estimation. This partitioning improves generalizability and prevents overly optimistic results.",
      "properties": {
        "Purpose": "Model selection, hyperparameter tuning, preventing overfitting.",
        "Process": [
          "Train model on training set",
          "Validate model and compare candidates",
          "Select optimal model before final testing"
        ],
        "When_Used": [
          "Neural network iteration selection",
          "Hyperparameter optimization",
          "Model comparison"
        ],
        "Related_Concepts": [
          "Training Set",
          "Test Set",
          "Cross-validation"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_8_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Artificial Data Generation": {
      "type": "Technique",
      "domain": "Data Pre-processing",
      "definition": "A set of methods used to create additional synthetic samples when insufficient data is available, helping balance datasets or increase training coverage.",
      "description": "Artificial data generation expands a dataset by creating new synthetic samples derived from observed data properties. It includes multiple methods such as noise injection, distribution-based artificial data creation, and SMOTE. These approaches are useful especially when some classes lack sufficient samples or when expanding feature coverage is required.",
      "properties": {
        "Goal": "Increase dataset size, improve class balance, expand training coverage.",
        "Methods": [
          "Artificial data (distribution-based sampling)",
          "Noise injection",
          "SMOTE"
        ],
        "Advantages": [
          "Improves model generalization",
          "Addresses class imbalance",
          "Expands data in a controlled manner"
        ],
        "Limitations": [
          "Can introduce unrealistic samples if assumptions are incorrect",
          "May increase risk of overfitting if not applied carefully"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Artificial Data": {
      "type": "Data Type",
      "domain": "Data Pre-processing",
      "definition": "Synthetic data generated by sampling from a statistical distribution that approximates the original dataset.",
      "description": "Artificial data is created by modeling the original dataset using parameters such as mean (μ) and standard deviation (σ). Each class is modeled separately, and the distribution must be known or estimated. New samples are then drawn from these distributions to form a larger dataset. This method is especially useful when the original dataset is too small for effective model training.",
      "properties": {
        "Goal": "Create a larger dataset based on estimated population characteristics.",
        "Process": [
          "Estimate distribution parameters for each class",
          "Model the population",
          "Draw m synthetic samples where m > N"
        ],
        "Advantages": [
          "Helpful for rare classes",
          "Simple to generate if distribution is known",
          "Supports better model generalization"
        ],
        "Limitations": [
          "Requires known or well-estimated distributions",
          "May oversimplify real-world complexity"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_4_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Noise": {
      "type": "Concept",
      "domain": "Data Quality",
      "definition": "Random or systematic errors in data that obscure the true underlying signal or pattern.",
      "description": "Noise can originate from measurement errors, equipment malfunctions, environmental factors, or data entry mistakes. It can distort models, reduce performance, and create false patterns during analysis, especially in supervised learning.",
      "properties": {
        "Types": [
          "Attribute noise",
          "Label noise"
        ],
        "Sources": [
          "Sensor malfunction",
          "Human error",
          "Data transmission issues"
        ],
        "Impact": [
          "Increased error rate",
          "Overfitting risk",
          "Reduced generalization"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_6_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Robust Learning Algorithms": {
      "type": "Method",
      "domain": "Machine Learning",
      "definition": "Algorithms capable of learning accurate models even when training data contains noise, outliers, or errors.",
      "description": "Robust learners are designed to minimize the impact of corrupted or noisy instances. They produce models similar to those obtained from clean data, often using regularization, pruning, or robust loss functions.",
      "properties": {
        "Examples": [
          "C4.5 Decision Tree with pruning",
          "Support Vector Machines",
          "Random Forests"
        ],
        "Techniques": [
          "Pruning",
          "Regularization",
          "Robust loss functions"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_6_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Polishing": {
      "type": "Technique",
      "domain": "Data Cleaning",
      "definition": "The process of identifying and correcting noisy or corrupted data before building predictive models.",
      "description": "Data polishing focuses on improving data quality by removing or correcting noise manually or through algorithms. It is particularly useful for small datasets where noisy samples can be easily isolated.",
      "properties": {
        "Steps": [
          "Detect noise",
          "Correct corrupted instances",
          "Validate polished data"
        ],
        "Use Cases": [
          "Small datasets",
          "Low noise frequency"
        ],
        "Limitations": [
          "Inefficient for large-scale data",
          "Risk of removing valid samples"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_6_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Noise Filters": {
      "type": "Method",
      "domain": "Signal Processing",
      "definition": "Techniques for identifying and removing noise from data, including specific frequency interference or corrupted samples.",
      "description": "Noise filters aim to isolate and eliminate unwanted variations in the data, such as electrical interference or measurement errors. Over-filtering can remove meaningful information.",
      "properties": {
        "Examples": [
          "Low-pass filter",
          "Notch filter",
          "Spectral filtering"
        ],
        "Sources of Noise": [
          "50 Hz power line",
          "Infrared absorbance errors",
          "Equipment malfunction"
        ],
        "Risks": [
          "Loss of valuable signal components"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_6_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Pollution": {
      "type": "Concept",
      "domain": "Data Quality",
      "definition": "The presence of incorrect, inconsistent, or irrelevant data values that do not conform to intended formats or meanings.",
      "description": "Data pollution occurs when datasets contain invalid entries, such as incorrect category values, formatting errors, or corrupted text fields. These issues often result from system repurposing or manual entry errors.",
      "properties": {
        "Examples": [
          "Gender field containing 'male', 'female', and 'business'",
          "Free text with delimiters",
          "Misplaced decimal points"
        ],
        "Causes": [
          "Software misuse",
          "Copy-paste errors",
          "CSV formatting issues"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_6_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Signal Saturation": {
      "type": "Phenomenon",
      "domain": "Signal Processing",
      "definition": "A condition where sensor readings reach their upper or lower limit and cannot record values beyond that threshold.",
      "description": "Signal saturation leads to repeated minimum or maximum values, reducing variability and masking true signal dynamics. It often occurs when sensors exceed their operational range.",
      "properties": {
        "Symptoms": [
          "Repeated min/max values",
          "Clipped signals",
          "Loss of detail"
        ],
        "Causes": [
          "Sensor range limits",
          "Hardware constraints"
        ],
        "Handling": [
          "Mark corrupted sections",
          "Exclude extreme sequences"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_6_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Outliers": {
      "type": "Concept",
      "domain": "Statistics",
      "definition": "Data points that significantly deviate from the expected pattern or distribution of a dataset.",
      "description": "Outliers can result from noise, measurement errors, or genuine rare events. They may distort statistical summaries and machine learning models. Outlier analysis involves detecting, interpreting, and deciding whether to correct or retain them.",
      "properties": {
        "Types": [
          "Global",
          "Contextual",
          "Collective"
        ],
        "Causes": [
          "Calibration errors",
          "External disruptions",
          "Rare phenomena"
        ],
        "Impact": [
          "Model distortion",
          "Misleading correlations"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_6_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Anomalies": {
      "type": "Concept",
      "domain": "Data Mining",
      "definition": "Rare and valid instances that deviate from normal behavior but carry valuable information rather than being errors.",
      "description": "Anomalies differ from outliers in that they often represent meaningful or significant deviations, such as fraudulent activity, medical conditions, or rare events. Detecting anomalies helps in security, finance, and environmental monitoring.",
      "properties": {
        "Applications": [
          "Intrusion detection",
          "Credit card fraud detection",
          "Medical diagnostics",
          "Climate and environmental monitoring"
        ],
        "Difference from Outliers": "Outliers may be noise or errors; anomalies are often valid and informative."
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_6_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Outlier Detection Methods": {
      "type": "Framework",
      "domain": "Machine Learning",
      "definition": "Algorithms and approaches designed to identify data points that deviate significantly from expected normal patterns.",
      "description": "Outlier detection methods vary based on whether labels exist (supervised), partial labels (semi-supervised), or no labels (unsupervised). They aim to separate normal behavior from abnormal instances using distance, density, or statistical assumptions.",
      "properties": {
        "Categories": [
          "Supervised",
          "Semi-supervised",
          "Unsupervised"
        ],
        "Examples": [
          "k-Nearest Neighbors (distance-based)",
          "Local Outlier Factor (density-based)",
          "Gaussian Distribution (parametric)",
          "Kernel Density Estimation (nonparametric)",
          "K-Means or GMM (clustering-based)",
          "SVM or Random Forest (classification-based)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_6_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Proximity-Based Methods": {
      "type": "Method",
      "domain": "Outlier Detection",
      "definition": "Methods that define outliers as data points distant from their nearest neighbors in feature space.",
      "description": "These methods rely on distance metrics such as Euclidean or Mahalanobis distance to detect isolated instances. Variants include distance-based and density-based approaches.",
      "properties": {
        "Examples": [
          "k-Nearest Neighbors",
          "Local Outlier Factor (LOF)"
        ],
        "Strengths": [
          "Effective for global and local outliers"
        ],
        "Weaknesses": [
          "Sensitive to distance metric choice",
          "Poor scalability on large data"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_6_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Generalizability": {
      "type": "concept",
      "domain": "Model Evaluation",
      "definition": "The degree to which a model's findings or predictions apply beyond the data on which it was trained.",
      "description": "Generalizability reflects how well a model captures patterns that extend to new individuals, settings, or datasets.",
      "properties": {
        "aspects": [
          "population generalizability",
          "dataset generalizability"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Generalization Error": {
      "type": "metric",
      "domain": "Model Evaluation",
      "definition": "The expected error of a model when applied to new, unseen data.",
      "description": "It captures the discrepancy between training performance and real-world prediction accuracy.",
      "properties": {
        "components": [
          "bias",
          "variance"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Generalization Gap": {
      "type": "metric",
      "domain": "Model Evaluation",
      "definition": "The difference between training error and test error for a model.",
      "description": "A large gap indicates overfitting, while a small gap suggests good generalizability.",
      "properties": {
        "formula": "generalization_gap = test_error - training_error"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Dataset Generalization": {
      "type": "concept",
      "domain": "Model Evaluation",
      "definition": "The ability of a model to perform well on datasets collected under conditions different from the training dataset.",
      "description": "Important for real-world deployment, where data collection conditions vary.",
      "properties": {
        "sources_of_variation": [
          "different populations",
          "different sensors",
          "different contexts"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Retrospective Models": {
      "type": "model_type",
      "domain": "Descriptive Modeling",
      "definition": "Models that analyze past data to understand patterns, associations, or structures.",
      "description": "Used for insight generation and explanation rather than future prediction.",
      "properties": {
        "primary_goal": "understanding past phenomena"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Prospective Models": {
      "type": "model_type",
      "domain": "Predictive Modeling",
      "definition": "Models designed to predict future outcomes based on current or historical data.",
      "description": "Used in forecasting, risk assessment, and decision automation.",
      "properties": {
        "primary_goal": "predicting future events"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Theory-Driven Models": {
      "type": "model_type",
      "domain": "Modeling Approaches",
      "definition": "Models constructed primarily from theoretical principles rather than empirical patterns.",
      "description": "Often focuses on causal explanation and interpretable structures.",
      "properties": {
        "advantages": [
          "interpretability",
          "causal validity"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Data-Driven Models": {
      "type": "model_type",
      "domain": "Modeling Approaches",
      "definition": "Models built primarily from observed data, with minimal theoretical assumptions.",
      "description": "Typical in machine learning, where predictive accuracy is prioritized.",
      "properties": {
        "advantages": [
          "high predictive power"
        ],
        "risks": [
          "overfitting"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Explanatory Power": {
      "type": "property",
      "domain": "Model Interpretation",
      "definition": "The ability of a model to explain underlying relationships and mechanisms within data.",
      "description": "Prioritized in scientific modeling where interpretability and causal understanding are necessary.",
      "properties": {
        "factors": [
          "model structure",
          "variable relationships"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Predictive Power": {
      "type": "property",
      "domain": "Model Performance",
      "definition": "The ability of a model to accurately predict new observations.",
      "description": "Often maximized using data-driven machine learning approaches.",
      "properties": {
        "metrics": [
          "RMSE",
          "accuracy",
          "AUC"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Minimum Sample Size for Statistical Inference": {
      "type": "requirement",
      "domain": "Sampling",
      "definition": "The smallest number of observations needed to perform statistically valid inference.",
      "description": "Depends on model complexity, effect sizes, and desired confidence levels.",
      "properties": {
        "influenced_by": [
          "variance",
          "population size",
          "test type"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Sample Size for Predictive Modeling": {
      "type": "requirement",
      "domain": "Sampling",
      "definition": "The amount of data required for building predictive models that generalize well.",
      "description": "Larger sample sizes reduce variance and improve generalization.",
      "properties": {
        "depends_on": [
          "model complexity",
          "noise level",
          "feature dimensionality"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Precision of Parameter Estimation": {
      "type": "metric",
      "domain": "Statistical Inference",
      "definition": "The degree of certainty in estimated model parameters.",
      "description": "Higher sample sizes typically increase precision and reduce standard errors.",
      "properties": {
        "improved_by": [
          "larger datasets"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Population Representativeness": {
      "type": "concept",
      "domain": "Sampling",
      "definition": "The extent to which a sample reflects the target population.",
      "description": "Critical for valid generalization of study results.",
      "properties": {
        "risks_of_failure": [
          "sampling bias",
          "non-response bias"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Access to Data": {
      "type": "constraint",
      "domain": "Data Collection",
      "definition": "The availability of suitable and sufficient data for training and evaluating models.",
      "description": "Practical and ethical challenges may restrict data collection and usage.",
      "properties": {
        "limitations": [
          "privacy",
          "cost",
          "logistics"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Training Set": {
      "type": "data_partition",
      "domain": "Model Evaluation",
      "definition": "The subset of data used to estimate model parameters.",
      "description": "Forms the basis of the model's learned patterns.",
      "properties": {
        "purpose": [
          "parameter estimation"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Test Set": {
      "type": "data_partition",
      "domain": "Model Evaluation",
      "definition": "The subset of data reserved for evaluating a model's predictive performance.",
      "description": "Used only after model training and selection.",
      "properties": {
        "purpose": [
          "generalization assessment"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Hold-Out Method": {
      "type": "method",
      "domain": "Model Evaluation",
      "definition": "A data partitioning approach where the dataset is split into training and testing subsets.",
      "description": "Common simple method for estimating predictive performance.",
      "properties": {
        "common_splits": [
          "80-20",
          "2/3-1/3"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Eighty-Twenty Split": {
      "type": "data_partition",
      "domain": "Model Evaluation",
      "definition": "A partitioning approach where 80% of data is used for training and 20% for testing.",
      "description": "A common default in machine learning experiments.",
      "properties": {
        "ratio": "80-20"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Two-Thirds One-Third Split": {
      "type": "data_partition",
      "domain": "Model Evaluation",
      "definition": "A data partitioning scheme where two-thirds of the data is used for training and one-third for testing.",
      "description": "Alternative to the 80-20 split.",
      "properties": {
        "ratio": "2/3-1/3"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Training-Validation-Test Split": {
      "type": "data_partition",
      "domain": "Model Evaluation",
      "definition": "A three-way data split used to train, tune, and evaluate machine learning models.",
      "description": "Enables both hyperparameter tuning and final unbiased model evaluation.",
      "properties": {
        "components": [
          "training",
          "validation",
          "test"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Participant-Wise Cross-Validation": {
      "type": "method",
      "domain": "Model Evaluation",
      "definition": "A cross-validation technique where data from each participant forms a separate fold.",
      "description": "Used when data from the same participant is highly correlated.",
      "properties": {
        "use_case": [
          "sensor-based human activity recognition"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Leave-One-Out Validation": {
      "type": "method",
      "domain": "Model Evaluation",
      "definition": "A cross-validation technique where each instance is used as its own test set.",
      "description": "Computationally expensive but maximally uses available data.",
      "properties": {
        "folds": "n"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Temporal Dependency in Data": {
      "type": "phenomenon",
      "domain": "Time-Series Data",
      "definition": "The property that observations close in time are correlated rather than independent.",
      "description": "Invalidates random sampling and certain cross-validation methods.",
      "properties": {
        "causes": [
          "sliding windows",
          "sensor signals"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "No Random Sampling Under Temporal Dependence": {
      "type": "constraint",
      "domain": "Time-Series Data",
      "definition": "A restriction that prevents using random train-test splits when data points are temporally correlated.",
      "description": "Random splits cause leakage because training and test sets contain nearly identical patterns.",
      "properties": {
        "effects": [
          "data leakage",
          "inflated accuracy"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Day-Based Partitioning": {
      "type": "method",
      "domain": "Time-Series Data",
      "definition": "A data-splitting strategy where entire days are used as units to avoid temporal leakage.",
      "description": "Ensures that temporally adjacent observations are not split across training and test sets.",
      "properties": {
        "unit": "day"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Temporal Leakage": {
      "type": "error_type",
      "domain": "Model Evaluation",
      "definition": "Inadvertent inclusion of temporally correlated information in both training and test sets.",
      "description": "Causes overestimation of model performance.",
      "properties": {
        "sources": [
          "random splitting",
          "overlapping windows"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Sliding Windows": {
      "type": "method",
      "domain": "Feature Extraction",
      "definition": "A method for extracting features from sequential data by moving a fixed-size window across time.",
      "description": "Used in time-series classification and signal processing.",
      "properties": {
        "parameters": [
          "window size",
          "step size"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Overlapping Windows": {
      "type": "method",
      "domain": "Feature Extraction",
      "definition": "Sliding windows that partially overlap to create highly correlated feature vectors.",
      "description": "Leads to temporal dependency between training instances.",
      "properties": {
        "effect": "induces correlation"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Window-Based Feature Extraction": {
      "type": "method",
      "domain": "Feature Extraction",
      "definition": "Feature extraction performed on short, fixed-length windows of time-series data.",
      "description": "Generates feature vectors for classification tasks such as activity recognition.",
      "properties": {
        "common_features": [
          "mean",
          "variance",
          "energy"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Individual Models": {
      "type": "model_type",
      "domain": "Modeling Approaches",
      "definition": "Models trained on data from a single individual for personalized prediction.",
      "description": "Performs well on the specific individual but generalizes poorly to others.",
      "properties": {
        "strength": "high personalization",
        "weakness": "poor population generalization"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Population Models": {
      "type": "model_type",
      "domain": "Modeling Approaches",
      "definition": "Models trained on data from multiple individuals to generalize across users.",
      "description": "More robust in diverse real-world applications.",
      "properties": {
        "strength": "better generalization"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Activity Recognition": {
      "type": "application",
      "domain": "Machine Learning Applications",
      "definition": "The task of identifying human activities from sensor or contextual data.",
      "description": "Commonly involves using wearable sensor data like accelerometers and temperature sensors.",
      "properties": {
        "typical_inputs": [
          "accelerometer",
          "gyroscope",
          "temperature"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Sensor-Based Activity Detection": {
      "type": "application",
      "domain": "Machine Learning Applications",
      "definition": "Detecting human activities using sensor data collected from wearable or ambient devices.",
      "description": "Depends heavily on feature extraction and handling temporal correlations.",
      "properties": {
        "sensors": [
          "accelerometers",
          "temperature sensors"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Multi-Modal Feature Inputs": {
      "type": "concept",
      "domain": "Feature Engineering",
      "definition": "Combining features from multiple sensor modalities for improved model performance.",
      "description": "For example, integrating accelerometer features with body temperature signals.",
      "properties": {
        "modalities": [
          "accelerometer",
          "temperature"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Model Complexity–Generalization Tradeoff": {
      "type": "concept",
      "domain": "Model Selection",
      "definition": "The tradeoff in which more complex models fit training data better but often generalize worse.",
      "description": "Central to selecting models that balance expressiveness with robustness.",
      "properties": {
        "risk": "overfitting"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Simpler Models Generalize Better": {
      "type": "principle",
      "domain": "Model Selection",
      "definition": "The principle that models with lower complexity often perform better on new data.",
      "description": "Reflects Occam's razor in predictive modeling.",
      "properties": {
        "benefits": [
          "robustness",
          "lower variance"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Overcomplex Models": {
      "type": "model_issue",
      "domain": "Model Selection",
      "definition": "Models that are excessively flexible and overfit training data.",
      "description": "Characterized by small training error and large generalization error.",
      "properties": {
        "symptom": "large generalization gap"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Hyperparameter Selection": {
      "type": "process",
      "domain": "Model Selection",
      "definition": "The process of choosing optimal hyperparameters for a learning algorithm.",
      "description": "Typically done using validation sets or cross-validation.",
      "properties": {
        "examples": [
          "regularization strength",
          "learning rate",
          "tree depth"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Feature Selection": {
      "type": "process",
      "domain": "Model Selection",
      "definition": "The process of identifying the most relevant features for a predictive model.",
      "description": "Reduces overfitting, improves generalization, and simplifies models.",
      "properties": {
        "methods": [
          "filter methods",
          "wrapper methods",
          "embedded methods"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Data Mining": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The process of finding previously unknown and potentially interesting patterns and relationships in large databases through statistical and algorithmic methods.",
      "description": "Data mining automates the extraction of knowledge from data and involves descriptive, predictive, and explanatory modeling. It simplifies and automates the statistical process from data sources to model application, supporting manual and automated methods.",
      "properties": {
        "Goal": "Discover patterns, relationships, and insights from large datasets.",
        "Applications": [
          "Customer segmentation",
          "Fraud detection",
          "Recommendation systems"
        ],
        "Methods": [
          "Classification",
          "Clustering",
          "Regression",
          "Association analysis"
        ],
        "Challenges": [
          "Data quality",
          "Scalability",
          "Interpretability"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_1_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Knowledge Discovery in Databases": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The overall process of discovering useful knowledge from data, including data selection, cleaning, transformation, mining, and interpretation.",
      "description": "KDD integrates data mining as a core step and emphasizes interpretation, prior knowledge, and iterative improvement. It transforms raw data into actionable knowledge through systematic analysis.",
      "properties": {
        "Steps": [
          "Data selection",
          "Data cleaning",
          "Data transformation",
          "Data mining",
          "Pattern evaluation",
          "Knowledge representation"
        ],
        "Applications": [
          "Scientific discovery",
          "Business intelligence",
          "Engineering analytics"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_1_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Pre-processing": {
      "type": "Process",
      "domain": "Data Science",
      "definition": "The step of preparing raw data for mining by cleaning, transforming, integrating, and reducing it to improve quality and usability.",
      "description": "Data pre-processing deals with incomplete, noisy, or inconsistent data, improving accuracy and ensuring that models are built correctly. It converts raw data into suitable forms for mining and analysis.",
      "properties": {
        "Tasks": [
          "Data cleaning",
          "Data integration",
          "Data transformation",
          "Data reduction",
          "Data discretization"
        ],
        "Benefits": [
          "Improved data quality",
          "Reduced noise",
          "Higher model accuracy"
        ],
        "Problems addressed": [
          "Missing values",
          "Outliers",
          "Inconsistencies",
          "Bias"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_1_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Quality": {
      "type": "Concept",
      "domain": "Data Management",
      "definition": "A measure of the condition of data based on factors such as accuracy, completeness, reliability, and relevance.",
      "description": "High-quality data ensures valid and actionable analysis. Characteristics include accuracy, precision, completeness, timeliness, consistency, and reliability.",
      "properties": {
        "Characteristics": [
          "Accuracy and precision",
          "Completeness",
          "Availability and accessibility",
          "Timeliness and relevance",
          "Granularity",
          "Reliability and consistency"
        ],
        "Risks": [
          "Inaccurate models",
          "Faulty decision-making",
          "Unreliable analytics"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_1_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Cleaning": {
      "type": "Task",
      "domain": "Data Science",
      "definition": "The process of detecting and correcting inaccurate or corrupt data to improve data quality.",
      "description": "Data cleaning involves handling missing values, removing noise, identifying outliers, and correcting inconsistencies using statistical or algorithmic methods.",
      "properties": {
        "Methods": [
          "Imputation",
          "Binning",
          "Clustering-based outlier removal",
          "Regression smoothing"
        ],
        "Goals": [
          "Ensure data accuracy",
          "Reduce noise",
          "Improve model performance"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_1_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Big Data": {
      "type": "Concept",
      "domain": "Data Science",
      "definition": "Extremely large data sets that can be analyzed computationally to reveal patterns, trends, and associations, especially relating to human behavior and interactions.",
      "description": "Big Data is characterized by the 5Vs: Volume, Velocity, Variety, Veracity, and Value. It requires specialized tools and techniques for storage, processing, and analysis.",
      "properties": {
        "Characteristics": [
          "Volume",
          "Velocity",
          "Variety",
          "Veracity",
          "Value"
        ],
        "Challenges": [
          "Scalability",
          "Quality assurance",
          "Privacy"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_1_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Literacy": {
      "type": "Skill",
      "domain": "Education",
      "definition": "The ability to read, understand, create, and communicate data as information.",
      "description": "Data literacy enables individuals and organizations to use data effectively, supporting evidence-based decision making and fostering a data-driven culture.",
      "properties": {
        "Components": [
          "Reading and interpreting data",
          "Communicating insights",
          "Applying statistical reasoning",
          "Understanding data ethics"
        ],
        "Relevance": [
          "Data science",
          "Business analytics",
          "Policy making"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_1_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Transformation": {
      "type": "Task",
      "domain": "Data Science",
      "definition": "The process of converting data into a suitable format for analysis or mining.",
      "description": "Transformation includes normalization, aggregation, generalization, and feature extraction, making the data compatible with mining algorithms.",
      "properties": {
        "Techniques": [
          "Normalization",
          "Standardization",
          "Feature extraction",
          "Aggregation"
        ],
        "Goals": [
          "Ensure consistency",
          "Enhance interpretability",
          "Prepare for modeling"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_1_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Data Reduction": {
      "type": "Task",
      "domain": "Data Science",
      "definition": "The process of reducing the volume of data while maintaining its analytical value.",
      "description": "Data reduction techniques simplify data without losing critical information, enabling efficient analysis and storage.",
      "properties": {
        "Methods": [
          "Sampling",
          "Aggregation",
          "Principal Component Analysis",
          "Clustering"
        ],
        "Purpose": [
          "Reduce computational cost",
          "Improve scalability"
        ],
        "Techniques": [
          "Attribute reduction (feature selection)",
          "Data aggregation and generalization",
          "Variable construction",
          "Numerosity reduction",
          "Principal Component Analysis (PCA)",
          "Discrete Wavelet Transform (DWT)"
        ],
        "Goals": [
          "Efficiency",
          "Simplicity",
          "Improved interpretability"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_1_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Normalization": {
      "type": "Process",
      "domain": "Data Pre-processing",
      "definition": "The process of adjusting values measured on different scales to a notionally common scale without distorting differences in the ranges of values.",
      "description": "Normalization helps learning algorithms converge faster and ensures that attributes with large numerical ranges do not dominate those with smaller ranges. It is a core data preprocessing step required by many distance-based or gradient-based learning models.",
      "properties": {
        "Reasons": [
          "Improves model convergence and training stability",
          "Prevents attributes with large ranges from dominating smaller ones",
          "Required for certain algorithms (e.g., neural networks, k-NN, PCA)"
        ],
        "Considerations": [
          "Store scaling parameters for later use (e.g., for inference data)",
          "Choose normalization technique based on variable characteristics"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_7_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Min-Max Normalization": {
      "type": "Method",
      "domain": "Data Pre-processing",
      "definition": "A normalization method that scales data to a fixed range, typically [0, 1], based on the minimum and maximum values of each variable.",
      "description": "This linear transformation maps the minimum value to 0 and the maximum to 1 (or another defined interval [a, b]). It preserves relationships between data points but is sensitive to outliers.",
      "properties": {
        "Formula": "v' = (v - v_min) / (v_max - v_min)",
        "Range": "[0, 1] or [a, b]",
        "Advantages": [
          "Simple to compute",
          "Preserves shape of distribution"
        ],
        "Disadvantages": [
          "Sensitive to outliers"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_7_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Z-score Standardization": {
      "type": "Method",
      "domain": "Data Pre-processing",
      "definition": "A normalization technique that rescales data to have a mean of 0 and a standard deviation of 1.",
      "description": "Z-score standardization centers data by removing the mean and scales it by dividing by the standard deviation. It is especially useful when min and max values are unknown or when data contains outliers.",
      "properties": {
        "Formula": "v' = (v - mean(v)) / sd(v)",
        "Advantages": [
          "Handles outliers better than min-max scaling",
          "Useful when ranges are unknown"
        ],
        "Use Cases": [
          "Regression",
          "PCA",
          "Outlier detection under Gaussian assumption"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_7_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Decimal Scaling": {
      "type": "Method",
      "domain": "Data Pre-processing",
      "definition": "A normalization method that scales data by moving the decimal point of values based on the maximum absolute value of the variable.",
      "description": "Decimal scaling divides each value by a power of 10 such that all transformed values fall within the range [-1, 1]. The power is determined by the largest absolute value in the dataset.",
      "properties": {
        "Formula": "v' = v / 10^j where j = smallest integer such that max(|v'|) < 1",
        "Advantages": [
          "Simple to compute",
          "Effective when values vary by powers of ten"
        ],
        "Disadvantages": [
          "Less flexible for distributions with small variance"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_7_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Discretization": {
      "type": "Process",
      "domain": "Data Transformation",
      "definition": "The process of converting continuous variables into categorical ones by dividing their range into intervals or groups.",
      "description": "Discretization is used when models or analyses require categorical data, or when researchers want to study group-level differences instead of individual variations. However, it often reduces information and statistical power.",
      "properties": {
        "Methods": [
          "Median split",
          "Equal width binning",
          "Equal frequency binning",
          "Domain-based grouping"
        ],
        "Advantages": [
          "Simplifies analysis",
          "Enables categorical modeling"
        ],
        "Disadvantages": [
          "Loss of information",
          "Reduced precision",
          "Incompatibility across studies due to arbitrary cutoffs"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_7_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Dummy Coding": {
      "type": "Technique",
      "domain": "Data Transformation",
      "definition": "A method for encoding categorical variables as binary numeric features for use in regression and machine learning models.",
      "description": "Dummy coding represents K categories with K-1 binary variables. It allows categorical data to be incorporated into models that require numerical inputs.",
      "properties": {
        "Example": {
          "Variable": [
            "Red",
            "Blue",
            "Green",
            "Yellow"
          ],
          "Encoded": [
            "Dummy_Red",
            "Dummy_Blue",
            "Dummy_Green"
          ]
        },
        "Advantages": [
          "Enables inclusion of categorical variables in regression models"
        ],
        "Limitations": [
          "Increases dimensionality",
          "Risk of multicollinearity if not handled properly"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_7_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Principal Component Analysis (PCA)": {
      "type": "Method",
      "domain": "Dimensionality Reduction",
      "definition": "A statistical technique that transforms correlated variables into a smaller number of uncorrelated variables called principal components.",
      "description": "PCA reduces the dimensionality of data by finding orthogonal directions that capture the most variance. It is commonly used for visualization, compression, and to handle multicollinearity.",
      "properties": {
        "Steps": [
          "Compute covariance matrix",
          "Find eigenvalues and eigenvectors",
          "Project data onto principal components"
        ],
        "Advantages": [
          "Removes multicollinearity",
          "Improves computation efficiency"
        ],
        "Limitations": [
          "Loss of interpretability",
          "Assumes linearity"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_7_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Discrete Wavelet Transform (DWT)": {
      "type": "Method",
      "domain": "Signal Processing",
      "definition": "A linear signal processing technique used to decompose data into different frequency components and analyze each component with a resolution matched to its scale.",
      "description": "DWT is effective for dimensionality reduction in high-dimensional data, especially when the number of variables exceeds the number of samples. It preserves both frequency and location information, unlike Fourier transform.",
      "properties": {
        "Applications": [
          "Image compression",
          "Feature extraction",
          "Time-series analysis"
        ],
        "Advantages": [
          "Handles non-stationary data",
          "Reduces dimensionality efficiently"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_7_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Box-Cox Transformation": {
      "type": "Method",
      "domain": "Statistical Transformation",
      "definition": "A family of power transformations that aim to stabilize variance and make data more normally distributed.",
      "description": "Box-Cox transformation improves normality assumptions for statistical models. It only applies to positive data but can handle skewed distributions effectively.",
      "properties": {
        "Formula": "T(Y) = (Y^λ - 1) / λ for λ ≠ 0; T(Y) = ln(Y) for λ = 0",
        "Limitations": [
          "Only works with positive values",
          "No guarantee of perfect normality"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "towards_data_mining_lecture_7_2024.pdf",
        "created_at": "2025-11-06",
        "version": "1.0"
      }
    },
    "Scaling to Arbitrary Range": {
      "type": "method",
      "domain": "Data Transformation",
      "definition": "A normalization technique that linearly maps values to any target interval [a, b].",
      "description": "Generalization of min–max normalization where the target range is not limited to [0, 1], enabling flexible rescaling.",
      "properties": {
        "formula": "x' = (x - min) / (max - min) * (b - a) + a",
        "requirements": [
          "known min",
          "known max"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Storing Normalization Parameters": {
      "type": "process",
      "domain": "Data Transformation",
      "definition": "The practice of saving transformation parameters such as means, standard deviations, and min–max values for use in future data preprocessing.",
      "description": "Essential to avoid data leakage and ensure consistent scaling between training and test sets.",
      "properties": {
        "parameters": [
          "mean",
          "standard deviation",
          "min",
          "max"
        ],
        "purpose": [
          "reproducibility",
          "consistency"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Extreme Z-Score Threshold Rule": {
      "type": "heuristic",
      "domain": "Outlier Detection",
      "definition": "A method for identifying extreme values using high absolute Z-score cutoffs, typically 6–7, instead of the conventional 3.",
      "description": "Used to avoid misclassifying valid but rare signals as outliers, particularly in highly variable datasets.",
      "properties": {
        "thresholds": [
          "|z| > 6",
          "|z| > 7"
        ],
        "purpose": "avoid over-removal of valid extreme values"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Measurement Error Visualization": {
      "type": "method",
      "domain": "Data Quality",
      "definition": "Visualization techniques used to detect measurement errors through plots and distribution analysis.",
      "description": "Includes visual detection of extreme Z-scores, artifacts, or sudden jumps in sensor data.",
      "properties": {
        "tools": [
          "histograms",
          "box plots",
          "time-series plots"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Median Split": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A discretization method that divides a continuous variable into two groups using the median value as the threshold.",
      "description": "Commonly used for creating binary categories but may lead to information loss.",
      "properties": {
        "output": "two categories",
        "risk": [
          "loss of variance",
          "misclassification"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Distinctive Grouping": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A discretization method that forms groups based on meaningful or domain-specific distinctions rather than uniform intervals.",
      "description": "Useful when natural categories exist, such as age groups or educational stages.",
      "properties": {
        "basis": [
          "domain knowledge",
          "semantic categories"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Equal-Length Binning": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A discretization method dividing the range of a variable into intervals of equal width.",
      "description": "Bin boundaries are uniformly spaced, which may result in uneven sample counts per bin in skewed distributions.",
      "properties": {
        "bin_type": "fixed-width",
        "risks": [
          "unbalanced bins"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Equal-Size Binning": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A discretization method dividing data into bins containing approximately equal numbers of observations.",
      "description": "Also known as equal-frequency binning; adjusts bin boundaries based on data distribution.",
      "properties": {
        "bin_type": "equal-frequency",
        "risks": [
          "variable bin widths"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Natural Cut Points": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A discretization approach that chooses cut points based on evident gaps or changes in the data distribution.",
      "description": "Often identified visually or via distribution-based heuristics.",
      "properties": {
        "identification": [
          "visual inspection",
          "distribution jumps"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Concept Hierarchies": {
      "type": "structure",
      "domain": "Discretization",
      "definition": "Hierarchical structures that represent data from low-level granular categories to increasingly general abstractions.",
      "description": "Used for discretization and data generalization, enabling multi-level analysis.",
      "properties": {
        "levels": [
          "fine-grained",
          "intermediate",
          "coarse"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Dichotomization": {
      "type": "method",
      "domain": "Discretization",
      "definition": "A method that converts a continuous variable into two groups based on a threshold.",
      "description": "Simplifies analysis but leads to substantial information loss.",
      "properties": {
        "risks": [
          "information loss",
          "increased misclassification"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Problems of Discretization": {
      "type": "concept",
      "domain": "Discretization",
      "definition": "The set of issues created when transforming continuous variables into discrete categories.",
      "description": "Includes information loss, artificial boundaries, and increased risk of misclassification.",
      "properties": {
        "effects": [
          "reduced statistical power",
          "loss of granularity"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Information Loss Through Dichotomization": {
      "type": "phenomenon",
      "domain": "Discretization",
      "definition": "The reduction of statistical information caused by collapsing continuous variables into binary categories.",
      "description": "Leads to loss of variability and weaker associations.",
      "properties": {
        "causes": [
          "thresholding",
          "binary splitting"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Misclassification Risk at Cut Points": {
      "type": "phenomenon",
      "domain": "Discretization",
      "definition": "The risk of mislabeling cases that fall near bin boundaries when discretizing continuous variables.",
      "description": "Boundary decisions may not reflect underlying data continuity.",
      "properties": {
        "causes": [
          "abrupt thresholds"
        ],
        "effects": [
          "noisy categorical labels"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Factor Variables": {
      "type": "datatype",
      "domain": "Categorical Data",
      "definition": "Variables that represent categories encoded as discrete levels rather than numeric values.",
      "description": "Used in statistical modeling to encode nominal or ordinal categories.",
      "properties": {
        "levels": "categorical labels"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Frequency Tables": {
      "type": "method",
      "domain": "Categorical Data",
      "definition": "Tabular summaries of categorical variables showing counts per category.",
      "description": "Useful for summarizing distributions of factor variables.",
      "properties": {
        "outputs": [
          "counts",
          "proportions"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Correspondence Tests": {
      "type": "method",
      "domain": "Categorical Data",
      "definition": "Statistical tests evaluating the association between two categorical variables.",
      "description": "Includes chi-square–type methods applicable to contingency tables.",
      "properties": {
        "inputs": [
          "contingency table"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Categorical vs Continuous Visualization": {
      "type": "method",
      "domain": "Categorical Data",
      "definition": "Visualization techniques that compare categorical groups against continuous measurements.",
      "description": "Examples include grouped boxplots for continuous outcomes across category levels.",
      "properties": {
        "tools": [
          "boxplots",
          "violin plots"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Data Aggregation": {
      "type": "method",
      "domain": "Data Reduction",
      "definition": "A data reduction method that summarizes groups of data points into aggregate values.",
      "description": "Common aggregates include mean, sum, and group-level statistics.",
      "properties": {
        "aggregates": [
          "mean",
          "sum",
          "count"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Data Generalization": {
      "type": "method",
      "domain": "Data Reduction",
      "definition": "A reduction method where data is replaced by more general forms using hierarchical abstraction.",
      "description": "Often implemented using concept hierarchies to reduce granularity.",
      "properties": {
        "approach": [
          "hierarchical abstraction"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Variable Construction": {
      "type": "process",
      "domain": "Data Reduction",
      "definition": "Constructing new variables from existing ones to improve predictive performance or simplify data representation.",
      "description": "Includes building composite indicators or derived attributes.",
      "properties": {
        "examples": [
          "interaction terms",
          "index scores"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Nonparametric Data Reduction Methods": {
      "type": "method",
      "domain": "Data Reduction",
      "definition": "Reduction methods that do not assume specific parametric forms and rely on data-driven transformations.",
      "description": "Useful when normality or linearity assumptions do not hold.",
      "properties": {
        "examples": [
          "binning",
          "wavelet transforms"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Transformation to Normality": {
      "type": "method",
      "domain": "Data Transformation",
      "definition": "Techniques aimed at making a variable's distribution more Gaussian.",
      "description": "Used to satisfy model assumptions; includes Box–Cox and other power transformations.",
      "properties": {
        "goal": "approximate normality"
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Normality Tests": {
      "type": "method",
      "domain": "Statistical Diagnostics",
      "definition": "Statistical methods used to evaluate whether data follow a normal distribution.",
      "description": "Although not named explicitly, includes families of tests such as Shapiro–Wilk and KS tests.",
      "properties": {
        "outputs": [
          "p-value",
          "decision on normality"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    },
    "Solutions for Non-Normality": {
      "type": "strategy",
      "domain": "Statistical Diagnostics",
      "definition": "Approaches to address non-normal data, including transformations or use of nonparametric models.",
      "description": "Ensures suitability of statistical models when assumptions fail.",
      "properties": {
        "strategies": [
          "transform variables",
          "apply nonparametric models"
        ]
      },
      "metadata": {
        "source_pdf": true
      }
    }
  },
  "edges": [
    {
      "source": "Attribute Noise",
      "type": "related_to",
      "target": "Noise"
    },
    {
      "source": "Attribute Noise",
      "type": "affects",
      "target": "Supervised Learning"
    },
    {
      "source": "Label Noise",
      "type": "part_of",
      "target": "Noisy Data"
    },
    {
      "source": "Label Noise",
      "type": "affects",
      "target": "Supervised Learning"
    },
    {
      "source": "Noise Frequency Filtering",
      "type": "instance_of",
      "target": "Noise Filters"
    },
    {
      "source": "Equipment Error",
      "type": "produces",
      "target": "Attribute Noise"
    },
    {
      "source": "Garbage Values",
      "type": "part_of",
      "target": "Data Pollution"
    },
    {
      "source": "Delimiter Contamination",
      "type": "related_to",
      "target": "Data Pollution"
    },
    {
      "source": "Free-Text Field Contamination",
      "type": "related_to",
      "target": "Data Pollution"
    },
    {
      "source": "Thousand-Separator Contamination",
      "type": "related_to",
      "target": "Data Format Differences"
    },
    {
      "source": "Domain Drift",
      "type": "part_of",
      "target": "Data Pollution"
    },
    {
      "source": "Global Outliers",
      "type": "instance_of",
      "target": "Outliers"
    },
    {
      "source": "Contextual Outliers",
      "type": "contrast_with",
      "target": "Global Outliers"
    },
    {
      "source": "Collective Outliers",
      "type": "related_to",
      "target": "Contextual Outliers"
    },
    {
      "source": "Outlier Score",
      "type": "used_in",
      "target": "Unsupervised Outlier Detection"
    },
    {
      "source": "Binary Outlier Classification",
      "type": "derived_from",
      "target": "Outlier Score"
    },
    {
      "source": "Subjective Outlier Thresholding",
      "type": "related_to",
      "target": "Binary Outlier Classification"
    },
    {
      "source": "Supervised Outlier Detection",
      "type": "contrast_with",
      "target": "Unsupervised Outlier Detection"
    },
    {
      "source": "Semi-Supervised Outlier Detection",
      "type": "contrast_with",
      "target": "Supervised Outlier Detection"
    },
    {
      "source": "Unsupervised Outlier Detection",
      "type": "contrast_with",
      "target": "Supervised Outlier Detection"
    },
    {
      "source": "Parametric Outlier Detection",
      "type": "instance_of",
      "target": "Statistical Outlier Detection"
    },
    {
      "source": "Nonparametric Outlier Detection",
      "type": "instance_of",
      "target": "Statistical Outlier Detection"
    },
    {
      "source": "Gaussian Outlier Modeling",
      "type": "part_of",
      "target": "Parametric Outlier Detection"
    },
    {
      "source": "Kernel Density Estimation for Outlier Detection",
      "type": "part_of",
      "target": "Nonparametric Outlier Detection"
    },
    {
      "source": "Distance-Based Outlier Detection",
      "type": "related_to",
      "target": "Proximity-Based Methods"
    },
    {
      "source": "Density-Based Outlier Detection",
      "type": "related_to",
      "target": "Proximity-Based Methods"
    },
    {
      "source": "Clustering-Based Outlier Detection",
      "type": "related_to",
      "target": "Clustering Algorithms"
    },
    {
      "source": "Sparse-Cluster Outliers",
      "type": "instance_of",
      "target": "Clustering-Based Outlier Detection"
    },
    {
      "source": "Gaussian Mixture Model Outlier Detection",
      "type": "related_to",
      "target": "Gaussian Outlier Modeling"
    },
    {
      "source": "One-Class Classification",
      "type": "used_in",
      "target": "Supervised Outlier Detection"
    },
    {
      "source": "Outlier Detection via SVM",
      "type": "instance_of",
      "target": "One-Class Classification"
    },
    {
      "source": "Outlier Detection via Random Forest",
      "type": "related_to",
      "target": "Supervised Outlier Detection"
    },
    {
      "source": "Predictive Outlier Detection",
      "type": "used_in",
      "target": "Streaming Data Analysis"
    },
    {
      "source": "Streaming Outlier Detection",
      "type": "related_to",
      "target": "Predictive Outlier Detection"
    },
    {
      "source": "Unusual-Shape Outlier Detection",
      "type": "related_to",
      "target": "Streaming Outlier Detection"
    },
    {
      "source": "Multidimensional Streaming Outlier Detection",
      "type": "related_to",
      "target": "Streaming Outlier Detection"
    },
    {
      "source": "Change-Point Outlier Detection",
      "type": "instance_of",
      "target": "Time-Series Outlier Detection"
    },
    {
      "source": "Rare-Class Detection in Time Series",
      "type": "related_to",
      "target": "Multidimensional Streaming Outlier Detection"
    },
    {
      "source": "Data Ethics",
      "type": "related_to",
      "target": "Information Privacy"
    },
    {
      "source": "Data Ethics",
      "type": "governed_by",
      "target": "Data Protection Laws"
    },
    {
      "source": "Data Ethics",
      "type": "component_of",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Ethical Data Mining",
      "type": "requires",
      "target": "Informed Consent"
    },
    {
      "source": "Ethical Data Mining",
      "type": "related_to",
      "target": "Privacy Preservation"
    },
    {
      "source": "Ethical Data Mining",
      "type": "regulated_by",
      "target": "GDPR"
    },
    {
      "source": "Informed Consent",
      "type": "required_by",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Informed Consent",
      "type": "related_to",
      "target": "Privacy Preservation"
    },
    {
      "source": "Informed Consent",
      "type": "regulated_by",
      "target": "GDPR"
    },
    {
      "source": "General Data Protection Regulation (GDPR)",
      "type": "governs",
      "target": "Personal Data"
    },
    {
      "source": "General Data Protection Regulation (GDPR)",
      "type": "related_to",
      "target": "Data Ethics"
    },
    {
      "source": "General Data Protection Regulation (GDPR)",
      "type": "enforces",
      "target": "Informed Consent"
    },
    {
      "source": "Information Security",
      "type": "related_to",
      "target": "Information Privacy"
    },
    {
      "source": "Information Security",
      "type": "supports",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Information Security",
      "type": "includes",
      "target": "Confidentiality"
    },
    {
      "source": "Information Privacy",
      "type": "protected_by",
      "target": "Information Security"
    },
    {
      "source": "Information Privacy",
      "type": "threatened_by",
      "target": "Data Mining"
    },
    {
      "source": "Information Privacy",
      "type": "regulated_by",
      "target": "GDPR"
    },
    {
      "source": "Anonymisation",
      "type": "used_in",
      "target": "Privacy Preservation"
    },
    {
      "source": "Anonymisation",
      "type": "required_by",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Anonymisation",
      "type": "related_to",
      "target": "Pseudonymisation"
    },
    {
      "source": "Open Data",
      "type": "supports",
      "target": "Scientific Research"
    },
    {
      "source": "Open Data",
      "type": "regulated_by",
      "target": "Open Knowledge Foundation"
    },
    {
      "source": "Open Data",
      "type": "guided_by",
      "target": "FAIR Principles"
    },
    {
      "source": "FAIR Principles",
      "type": "applies_to",
      "target": "Open Data"
    },
    {
      "source": "FAIR Principles",
      "type": "promoted_by",
      "target": "GO FAIR Initiative"
    },
    {
      "source": "FAIR Principles",
      "type": "used_in",
      "target": "Research Data Management"
    },
    {
      "source": "Data Management",
      "type": "includes",
      "target": "Database Management"
    },
    {
      "source": "Data Management",
      "type": "related_to",
      "target": "Open Data"
    },
    {
      "source": "Data Management",
      "type": "supports",
      "target": "Data Mining"
    },
    {
      "source": "Machine Learning",
      "type": "related_to",
      "target": "Predictive Modeling"
    },
    {
      "source": "Machine Learning",
      "type": "parent_of",
      "target": "Machine Learning Models"
    },
    {
      "source": "Machine Learning Models",
      "type": "instance_of",
      "target": "Machine Learning"
    },
    {
      "source": "Machine Learning Models",
      "type": "related_to",
      "target": "Model Selection"
    },
    {
      "source": "Supervised Learning",
      "type": "related_to",
      "target": "Predictive Modeling"
    },
    {
      "source": "Supervised Learning",
      "type": "related_to",
      "target": "Label Noise"
    },
    {
      "source": "Avoiding Overfitting",
      "type": "mitigates",
      "target": "Overfitting"
    },
    {
      "source": "Avoiding Overfitting",
      "type": "related_to",
      "target": "Hyperparameter Selection"
    },
    {
      "source": "Bias-Variance Tradeoff",
      "type": "related_to",
      "target": "Model Complexity–Generalization Tradeoff"
    },
    {
      "source": "Bias-Variance Tradeoff",
      "type": "affects",
      "target": "Generalization Error"
    },
    {
      "source": "Regularization",
      "type": "mitigates",
      "target": "Overfitting"
    },
    {
      "source": "Regularization",
      "type": "related_to",
      "target": "Hyperparameter Selection"
    },
    {
      "source": "Model Bias",
      "type": "related_to",
      "target": "Bias"
    },
    {
      "source": "Model Bias",
      "type": "affects",
      "target": "Model Evaluation"
    },
    {
      "source": "Accuracy",
      "type": "alternative_to",
      "target": "Balanced Accuracy"
    },
    {
      "source": "Accuracy",
      "type": "related_to",
      "target": "Performance Metrics"
    },
    {
      "source": "Training Error",
      "type": "contrast_with",
      "target": "Generalization Error"
    },
    {
      "source": "Training Error",
      "type": "component_of",
      "target": "Generalization Gap"
    },
    {
      "source": "Statistical Methods",
      "type": "includes",
      "target": "Maximum Likelihood Estimation (MLE)"
    },
    {
      "source": "Statistical Methods",
      "type": "related_to",
      "target": "Statistical Power"
    },
    {
      "source": "Specialized Statistical Modeling",
      "type": "related_to",
      "target": "Parameterized Models"
    },
    {
      "source": "Specialized Statistical Modeling",
      "type": "related_to",
      "target": "Time Series Analysis"
    },
    {
      "source": "Regression Modeling",
      "type": "related_to",
      "target": "Regression Imputation"
    },
    {
      "source": "Regression Modeling",
      "type": "related_to",
      "target": "Statistical Methods"
    },
    {
      "source": "Causal Hypothesis",
      "type": "related_to",
      "target": "Explanatory Modeling"
    },
    {
      "source": "Causal Hypothesis",
      "type": "related_to",
      "target": "Randomized Controlled Trial (RCT)"
    },
    {
      "source": "Statistical Power",
      "type": "related_to",
      "target": "Minimum Sample Size for Statistical Inference"
    },
    {
      "source": "Statistical Significance",
      "type": "related_to",
      "target": "Statistical Methods"
    },
    {
      "source": "Statistical Significance",
      "type": "related_to",
      "target": "Statistical Power"
    },
    {
      "source": "Nonlinear PCA",
      "type": "related_to",
      "target": "Principal Component Analysis (PCA)"
    },
    {
      "source": "Statistical Outlier Detection",
      "type": "parent_of",
      "target": "Parametric Outlier Detection"
    },
    {
      "source": "Statistical Outlier Detection",
      "type": "parent_of",
      "target": "Nonparametric Outlier Detection"
    },
    {
      "source": "Experimental Design",
      "type": "related_to",
      "target": "Randomized Controlled Trial (RCT)"
    },
    {
      "source": "Experimental Design",
      "type": "related_to",
      "target": "Sample Size"
    },
    {
      "source": "Poor Experimental Design",
      "type": "related_to",
      "target": "Human Data Collection"
    },
    {
      "source": "Poor Experimental Design",
      "type": "contrasts_with",
      "target": "Experimental Design"
    },
    {
      "source": "Sampling Rate Adjustment",
      "type": "related_to",
      "target": "Sampling Rate Mismatch"
    },
    {
      "source": "Sampling Rate Adjustment",
      "type": "related_to",
      "target": "Timestamp Synchronization"
    },
    {
      "source": "Data Augmentation",
      "type": "related_to",
      "target": "Synthetic Minority Oversampling Technique (SMOTE)"
    },
    {
      "source": "Data Augmentation",
      "type": "mitigates",
      "target": "Overfitting"
    },
    {
      "source": "Research Data Management",
      "type": "related_to",
      "target": "Metadata"
    },
    {
      "source": "Research Data Management",
      "type": "related_to",
      "target": "FAIR Principles"
    },
    {
      "source": "Documentation",
      "type": "related_to",
      "target": "Data Documentation"
    },
    {
      "source": "Documentation",
      "type": "supports",
      "target": "Reproducibility"
    },
    {
      "source": "Design Relational Database Schemas",
      "type": "related_to",
      "target": "Entity-Relationship Modeling"
    },
    {
      "source": "Design Relational Database Schemas",
      "type": "supports",
      "target": "Relational Database"
    },
    {
      "source": "Tabular Data",
      "type": "related_to",
      "target": "Relational Database"
    },
    {
      "source": "Tabular Data",
      "type": "used_in",
      "target": "Predictive Modeling"
    },
    {
      "source": "Data Protection Laws",
      "type": "includes",
      "target": "General Data Protection Regulation (GDPR)"
    },
    {
      "source": "Data Protection Laws",
      "type": "related_to",
      "target": "Pseudonymisation"
    },
    {
      "source": "GO FAIR Initiative",
      "type": "related_to",
      "target": "FAIR Principles"
    },
    {
      "source": "GO FAIR Initiative",
      "type": "related_to",
      "target": "Open Data"
    },
    {
      "source": "Human Research Ethics",
      "type": "related_to",
      "target": "Respect for Persons"
    },
    {
      "source": "Human Research Ethics",
      "type": "supports",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Ethical Approval",
      "type": "required_for",
      "target": "Human Data Collection"
    },
    {
      "source": "Ethical Approval",
      "type": "related_to",
      "target": "Informed Consent"
    },
    {
      "source": "Privacy Preservation",
      "type": "related_to",
      "target": "k-Anonymity"
    },
    {
      "source": "Privacy Preservation",
      "type": "related_to",
      "target": "Pseudonymisation"
    },
    {
      "source": "Time Series Analysis",
      "type": "related_to",
      "target": "Time Series Data Collection"
    },
    {
      "source": "Time Series Analysis",
      "type": "related_to",
      "target": "Temporal Drift"
    },
    {
      "source": "Streaming Data Analysis",
      "type": "related_to",
      "target": "Streaming Outlier Detection"
    },
    {
      "source": "Streaming Data Analysis",
      "type": "related_to",
      "target": "Multidimensional Streaming Outlier Detection"
    },
    {
      "source": "Time-Series Missing Data Handling",
      "type": "related_to",
      "target": "Last Observation Carried Forward"
    },
    {
      "source": "Time-Series Missing Data Handling",
      "type": "related_to",
      "target": "Baseline Observation Carried Forward"
    },
    {
      "source": "Temporal Alignment",
      "type": "related_to",
      "target": "Timestamp Synchronization"
    },
    {
      "source": "Temporal Alignment",
      "type": "related_to",
      "target": "Timestamp Mismatch"
    },
    {
      "source": "Data Completeness",
      "type": "related_to",
      "target": "Missing Data"
    },
    {
      "source": "Data Completeness",
      "type": "affects",
      "target": "Reproducibility"
    },
    {
      "source": "Scaling",
      "type": "related_to",
      "target": "Min-Max Normalization"
    },
    {
      "source": "Scaling",
      "type": "related_to",
      "target": "Z-score Standardization"
    },
    {
      "source": "Sensor Calibration",
      "type": "related_to",
      "target": "Calibration Differences"
    },
    {
      "source": "Sensor Calibration",
      "type": "mitigates",
      "target": "Equipment Error"
    },
    {
      "source": "Signal Quality",
      "type": "related_to",
      "target": "Noise"
    },
    {
      "source": "Signal Quality",
      "type": "related_to",
      "target": "Signal Saturation"
    },
    {
      "source": "Available-Case Analysis",
      "type": "contrast_with",
      "target": "Complete-Case Analysis"
    },
    {
      "source": "Available-Case Analysis",
      "type": "related_to",
      "target": "Deletion"
    },
    {
      "source": "Complete-Case Analysis",
      "type": "contrast_with",
      "target": "Available-Case Analysis"
    },
    {
      "source": "Complete-Case Analysis",
      "type": "related_to",
      "target": "Deletion"
    },
    {
      "source": "Hot Deck Imputation",
      "type": "contrast_with",
      "target": "Cold Deck Imputation"
    },
    {
      "source": "Hot Deck Imputation",
      "type": "instance_of",
      "target": "Imputation"
    },
    {
      "source": "Cold Deck Imputation",
      "type": "contrast_with",
      "target": "Hot Deck Imputation"
    },
    {
      "source": "Cold Deck Imputation",
      "type": "instance_of",
      "target": "Imputation"
    },
    {
      "source": "Last Observation Carried Forward",
      "type": "related_to",
      "target": "Baseline Observation Carried Forward"
    },
    {
      "source": "Last Observation Carried Forward",
      "type": "part_of",
      "target": "Time-Series Missing Data Handling"
    },
    {
      "source": "Baseline Observation Carried Forward",
      "type": "related_to",
      "target": "Last Observation Carried Forward"
    },
    {
      "source": "Full Information Maximum Likelihood",
      "type": "uses",
      "target": "Expectation-Maximization Algorithm"
    },
    {
      "source": "Full Information Maximum Likelihood",
      "type": "instance_of",
      "target": "Maximum Likelihood Estimation"
    },
    {
      "source": "Expectation-Maximization Algorithm",
      "type": "computes",
      "target": "Full Information Maximum Likelihood"
    },
    {
      "source": "Expectation-Maximization Algorithm",
      "type": "related_to",
      "target": "Maximum Likelihood Estimation"
    },
    {
      "source": "Imputation Pile-Up",
      "type": "caused_by",
      "target": "Mean Imputation"
    },
    {
      "source": "Loss of Variability in Imputed Data",
      "type": "related_to",
      "target": "Imputation Pile-Up"
    },
    {
      "source": "Impossible Imputation Values",
      "type": "related_to",
      "target": "Regression Imputation"
    },
    {
      "source": "Within-Imputation Variance",
      "type": "part_of",
      "target": "Total Variance in Multiple Imputation"
    },
    {
      "source": "Between-Imputation Variance",
      "type": "part_of",
      "target": "Total Variance in Multiple Imputation"
    },
    {
      "source": "Total Variance in Multiple Imputation",
      "type": "includes",
      "target": "Within-Imputation Variance"
    },
    {
      "source": "Total Variance in Multiple Imputation",
      "type": "includes",
      "target": "Between-Imputation Variance"
    },
    {
      "source": "Data Merging",
      "type": "requires",
      "target": "Sampling Synchronization"
    },
    {
      "source": "Data Merging",
      "type": "affected_by",
      "target": "Sensor Calibration"
    },
    {
      "source": "Data Merging",
      "type": "supports",
      "target": "Data Integration"
    },
    {
      "source": "Sampling Synchronization",
      "type": "used_in",
      "target": "Data Merging"
    },
    {
      "source": "Sampling Synchronization",
      "type": "requires",
      "target": "Sampling Rate Adjustment"
    },
    {
      "source": "Sampling Synchronization",
      "type": "related_to",
      "target": "Temporal Alignment"
    },
    {
      "source": "Downsampling",
      "type": "complementary_to",
      "target": "Oversampling"
    },
    {
      "source": "Downsampling",
      "type": "used_in",
      "target": "Sampling Synchronization"
    },
    {
      "source": "Oversampling",
      "type": "complementary_to",
      "target": "Downsampling"
    },
    {
      "source": "Oversampling",
      "type": "used_in",
      "target": "Sampling Synchronization"
    },
    {
      "source": "Sampling Methods",
      "type": "includes",
      "target": "Simple Random Sampling With Replacement (SRSWR)"
    },
    {
      "source": "Sampling Methods",
      "type": "includes",
      "target": "Simple Random Sampling Without Replacement (SRSWOR)"
    },
    {
      "source": "Sampling Methods",
      "type": "includes",
      "target": "Balanced Sampling"
    },
    {
      "source": "Simple Random Sampling Without Replacement (SRSWOR)",
      "type": "subtype_of",
      "target": "Sampling Methods"
    },
    {
      "source": "Simple Random Sampling Without Replacement (SRSWOR)",
      "type": "alternative_to",
      "target": "Simple Random Sampling With Replacement (SRSWR)"
    },
    {
      "source": "Simple Random Sampling With Replacement (SRSWR)",
      "type": "subtype_of",
      "target": "Sampling Methods"
    },
    {
      "source": "Simple Random Sampling With Replacement (SRSWR)",
      "type": "alternative_to",
      "target": "Simple Random Sampling Without Replacement (SRSWOR)"
    },
    {
      "source": "Simple Random Sampling With Replacement (SRSWR)",
      "type": "used_in",
      "target": "Artificial Data Generation"
    },
    {
      "source": "Balanced Sampling",
      "type": "used_in",
      "target": "Imbalanced Data Handling"
    },
    {
      "source": "Balanced Sampling",
      "type": "requires",
      "target": "Sampling Methods"
    },
    {
      "source": "Synthetic Minority Oversampling Technique (SMOTE)",
      "type": "used_in",
      "target": "Imbalanced Data Handling"
    },
    {
      "source": "Synthetic Minority Oversampling Technique (SMOTE)",
      "type": "related_to",
      "target": "Balanced Sampling"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "Accuracy"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "Precision"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "Recall"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "F1-score"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "Cohen’s Kappa"
    },
    {
      "source": "Performance Metrics",
      "type": "includes",
      "target": "Balanced Accuracy"
    },
    {
      "source": "Data Collection",
      "type": "part_of",
      "target": "Knowledge Discovery in Databases"
    },
    {
      "source": "Data Collection",
      "type": "requires",
      "target": "Data Planning"
    },
    {
      "source": "Data Collection",
      "type": "related_to",
      "target": "Data Quality"
    },
    {
      "source": "Data Planning",
      "type": "enables",
      "target": "Data Collection"
    },
    {
      "source": "Data Planning",
      "type": "supports",
      "target": "Experimental Design"
    },
    {
      "source": "Data Planning",
      "type": "improves",
      "target": "Data Quality"
    },
    {
      "source": "Sampling",
      "type": "used_in",
      "target": "Data Collection"
    },
    {
      "source": "Sampling",
      "type": "related_to",
      "target": "Statistical Significance"
    },
    {
      "source": "Stationary Data",
      "type": "contrasts_with",
      "target": "Non-Stationary Data"
    },
    {
      "source": "Stationary Data",
      "type": "used_in",
      "target": "Time Series Analysis"
    },
    {
      "source": "Non-Stationary Data",
      "type": "contrasts_with",
      "target": "Stationary Data"
    },
    {
      "source": "Non-Stationary Data",
      "type": "used_in",
      "target": "Human-Centered Data Collection"
    },
    {
      "source": "Human Data Collection",
      "type": "requires",
      "target": "Informed Consent"
    },
    {
      "source": "Human Data Collection",
      "type": "related_to",
      "target": "Ethical Data Mining"
    },
    {
      "source": "Human Data Collection",
      "type": "example_of",
      "target": "Non-Stationary Data"
    },
    {
      "source": "Bias",
      "type": "mitigated_by",
      "target": "Randomized Controlled Trial (RCT)"
    },
    {
      "source": "Bias",
      "type": "caused_by",
      "target": "Poor Experimental Design"
    },
    {
      "source": "Bias",
      "type": "related_to",
      "target": "Data Quality"
    },
    {
      "source": "Randomized Controlled Trial (RCT)",
      "type": "reduces",
      "target": "Bias"
    },
    {
      "source": "Randomized Controlled Trial (RCT)",
      "type": "used_in",
      "target": "Human Data Collection"
    },
    {
      "source": "Randomized Controlled Trial (RCT)",
      "type": "requires",
      "target": "Ethical Approval"
    },
    {
      "source": "Data Documentation",
      "type": "required_by",
      "target": "Data Collection"
    },
    {
      "source": "Data Documentation",
      "type": "supports",
      "target": "Open Science"
    },
    {
      "source": "Data Documentation",
      "type": "related_to",
      "target": "FAIR Principles"
    },
    {
      "source": "Data Format Differences",
      "type": "affects",
      "target": "Data Merging"
    },
    {
      "source": "Data Format Differences",
      "type": "related_to",
      "target": "Data Quality"
    },
    {
      "source": "Cultural Formatting Differences",
      "type": "related_to",
      "target": "Data Format Differences"
    },
    {
      "source": "Unit Mismatch",
      "type": "affects",
      "target": "Data Merging"
    },
    {
      "source": "Timestamp Mismatch",
      "type": "requires",
      "target": "Timestamp Synchronization"
    },
    {
      "source": "Timestamp Synchronization",
      "type": "supports",
      "target": "Data Merging"
    },
    {
      "source": "Label Inconsistency",
      "type": "affects",
      "target": "Supervised Learning"
    },
    {
      "source": "Sensor Placement Variability",
      "type": "related_to",
      "target": "Calibration Differences"
    },
    {
      "source": "Environmental Condition Variability",
      "type": "affects",
      "target": "Data Merging"
    },
    {
      "source": "Calibration Differences",
      "type": "affects",
      "target": "Sensor Placement Variability"
    },
    {
      "source": "Calibration Differences",
      "type": "related_to",
      "target": "Data Merging"
    },
    {
      "source": "Incompatible Data Collection Protocols",
      "type": "prevents",
      "target": "Data Merging"
    },
    {
      "source": "Sampling Rate Mismatch",
      "type": "requires",
      "target": "GCD-Based Downsampling"
    },
    {
      "source": "Sampling Rate Mismatch",
      "type": "requires",
      "target": "LCM-Based Oversampling"
    },
    {
      "source": "Battery–Sampling Trade-Off",
      "type": "related_to",
      "target": "Sampling Rate Mismatch"
    },
    {
      "source": "GCD-Based Downsampling",
      "type": "addresses",
      "target": "Sampling Rate Mismatch"
    },
    {
      "source": "LCM-Based Oversampling",
      "type": "addresses",
      "target": "Sampling Rate Mismatch"
    },
    {
      "source": "Confusion Matrix",
      "type": "foundation_of",
      "target": "Precision"
    },
    {
      "source": "Confusion Matrix",
      "type": "foundation_of",
      "target": "Recall"
    },
    {
      "source": "Confusion Matrix",
      "type": "foundation_of",
      "target": "F1 Score"
    },
    {
      "source": "Balanced Accuracy",
      "type": "improves",
      "target": "Accuracy"
    },
    {
      "source": "Balanced Accuracy",
      "type": "related_to",
      "target": "Imbalanced Data"
    },
    {
      "source": "Precision",
      "type": "related_to",
      "target": "Recall"
    },
    {
      "source": "Precision",
      "type": "component_of",
      "target": "F1 Score"
    },
    {
      "source": "Recall",
      "type": "related_to",
      "target": "Precision"
    },
    {
      "source": "Recall",
      "type": "component_of",
      "target": "F1 Score"
    },
    {
      "source": "Specificity",
      "type": "related_to",
      "target": "Recall"
    },
    {
      "source": "F1 Score",
      "type": "depends_on",
      "target": "Precision"
    },
    {
      "source": "F1 Score",
      "type": "depends_on",
      "target": "Recall"
    },
    {
      "source": "Cohen’s Kappa",
      "type": "alternative_to",
      "target": "Accuracy"
    },
    {
      "source": "Model Generalization",
      "type": "affected_by",
      "target": "Sample Size"
    },
    {
      "source": "Model Generalization",
      "type": "affected_by",
      "target": "Model Complexity"
    },
    {
      "source": "Model Generalization",
      "type": "ensured_by",
      "target": "Cross-Validation"
    },
    {
      "source": "Model Generalization",
      "type": "related_to",
      "target": "Overfitting"
    },
    {
      "source": "Model Generalization",
      "type": "related_to",
      "target": "Underfitting"
    },
    {
      "source": "Descriptive Modeling",
      "type": "contrasts_with",
      "target": "Predictive Modeling"
    },
    {
      "source": "Descriptive Modeling",
      "type": "contrasts_with",
      "target": "Explanatory Modeling"
    },
    {
      "source": "Descriptive Modeling",
      "type": "used_in",
      "target": "Exploratory Data Analysis"
    },
    {
      "source": "Explanatory Modeling",
      "type": "contrasts_with",
      "target": "Predictive Modeling"
    },
    {
      "source": "Explanatory Modeling",
      "type": "related_to",
      "target": "Statistical Inference"
    },
    {
      "source": "Explanatory Modeling",
      "type": "requires",
      "target": "Causal Hypothesis"
    },
    {
      "source": "Predictive Modeling",
      "type": "contrasts_with",
      "target": "Explanatory Modeling"
    },
    {
      "source": "Predictive Modeling",
      "type": "related_to",
      "target": "Model Generalization"
    },
    {
      "source": "Predictive Modeling",
      "type": "evaluated_by",
      "target": "Cross-Validation"
    },
    {
      "source": "Predictive Modeling",
      "type": "related_to",
      "target": "Overfitting"
    },
    {
      "source": "Overfitting",
      "type": "opposite_of",
      "target": "Underfitting"
    },
    {
      "source": "Overfitting",
      "type": "reduced_by",
      "target": "Cross-Validation"
    },
    {
      "source": "Overfitting",
      "type": "reduced_by",
      "target": "Regularization"
    },
    {
      "source": "Overfitting",
      "type": "related_to",
      "target": "Model Generalization"
    },
    {
      "source": "Underfitting",
      "type": "opposite_of",
      "target": "Overfitting"
    },
    {
      "source": "Underfitting",
      "type": "related_to",
      "target": "Model Bias"
    },
    {
      "source": "Underfitting",
      "type": "affects",
      "target": "Model Generalization"
    },
    {
      "source": "Sample Size",
      "type": "affects",
      "target": "Model Generalization"
    },
    {
      "source": "Sample Size",
      "type": "affects",
      "target": "Statistical Power"
    },
    {
      "source": "Sample Size",
      "type": "used_in",
      "target": "Training and Testing"
    },
    {
      "source": "Cross-Validation",
      "type": "used_in",
      "target": "Model Evaluation"
    },
    {
      "source": "Cross-Validation",
      "type": "reduces",
      "target": "Overfitting"
    },
    {
      "source": "Cross-Validation",
      "type": "requires",
      "target": "Data Partitioning"
    },
    {
      "source": "Model Selection",
      "type": "used_in",
      "target": "Model Generalization"
    },
    {
      "source": "Model Selection",
      "type": "related_to",
      "target": "Bias-Variance Tradeoff"
    },
    {
      "source": "Model Selection",
      "type": "requires",
      "target": "Validation Set"
    },
    {
      "source": "Data Collection Procedure",
      "type": "supports",
      "target": "Data Collection"
    },
    {
      "source": "Data Collection Procedure",
      "type": "related_to",
      "target": "Repeatability"
    },
    {
      "source": "Data Collection Failure Modes",
      "type": "affects",
      "target": "Data Quality"
    },
    {
      "source": "Signal Filtering Errors",
      "type": "instance_of",
      "target": "Data Collection Failure Modes"
    },
    {
      "source": "Sensor Placement Error",
      "type": "related_to",
      "target": "Human Data Collection"
    },
    {
      "source": "Battery Failure",
      "type": "affects",
      "target": "Data Completeness"
    },
    {
      "source": "Repeatability",
      "type": "supports",
      "target": "Reproducibility"
    },
    {
      "source": "Reproducibility",
      "type": "requires",
      "target": "Documentation"
    },
    {
      "source": "Stability",
      "type": "related_to",
      "target": "Non-Stationary Elements"
    },
    {
      "source": "Margin of Error",
      "type": "part_of",
      "target": "Confidence Interval"
    },
    {
      "source": "Confidence Interval",
      "type": "requires",
      "target": "Z-Score"
    },
    {
      "source": "Z-Score (Statistical)",
      "type": "used_in",
      "target": "Confidence Interval"
    },
    {
      "source": "Dropout Rate",
      "type": "affects",
      "target": "Sample Size"
    },
    {
      "source": "Stationary Elements",
      "type": "contrasts_with",
      "target": "Non-Stationary Elements"
    },
    {
      "source": "Non-Stationary Elements",
      "type": "causes",
      "target": "Temporal Drift"
    },
    {
      "source": "Temporal Drift",
      "type": "derived_from",
      "target": "Non-Stationary Elements"
    },
    {
      "source": "Seasonal Variability",
      "type": "instance_of",
      "target": "Non-Stationary Elements"
    },
    {
      "source": "Time Series Data Collection",
      "type": "used_in",
      "target": "Model Generalization"
    },
    {
      "source": "Respect for Persons",
      "type": "part_of",
      "target": "Human Research Ethics"
    },
    {
      "source": "Beneficence",
      "type": "part_of",
      "target": "Human Research Ethics"
    },
    {
      "source": "Justice",
      "type": "part_of",
      "target": "Human Research Ethics"
    },
    {
      "source": "Missing Data",
      "type": "has_subtype",
      "target": "Missing Completely at Random (MCAR)"
    },
    {
      "source": "Missing Data",
      "type": "has_subtype",
      "target": "Missing at Random"
    },
    {
      "source": "Missing Data",
      "type": "has_subtype",
      "target": "Missing Not at Random"
    },
    {
      "source": "Missing Data",
      "type": "handled_by",
      "target": "Imputation"
    },
    {
      "source": "Missing Data",
      "type": "handled_by",
      "target": "Deletion"
    },
    {
      "source": "Missing Data",
      "type": "handled_by",
      "target": "Maximum Likelihood Estimation"
    },
    {
      "source": "Missing Completely at Random (MCAR)",
      "type": "subtype_of",
      "target": "Missing Data"
    },
    {
      "source": "Missing Completely at Random (MCAR)",
      "type": "related_to",
      "target": "Complete-Case Analysis"
    },
    {
      "source": "Missing at Random (MAR)",
      "type": "subtype_of",
      "target": "Missing Data"
    },
    {
      "source": "Missing at Random (MAR)",
      "type": "used_in",
      "target": "Multiple Imputation"
    },
    {
      "source": "Missing at Random (MAR)",
      "type": "used_in",
      "target": "Maximum Likelihood Estimation"
    },
    {
      "source": "Missing Not at Random (MNAR)",
      "type": "subtype_of",
      "target": "Missing Data"
    },
    {
      "source": "Missing Not at Random (MNAR)",
      "type": "requires",
      "target": "Specialized Statistical Modeling"
    },
    {
      "source": "Deletion",
      "type": "used_in",
      "target": "Data Cleaning"
    },
    {
      "source": "Deletion",
      "type": "alternative_to",
      "target": "Imputation"
    },
    {
      "source": "Deletion",
      "type": "influences",
      "target": "Bias"
    },
    {
      "source": "Imputation",
      "type": "applied_to",
      "target": "Missing Data"
    },
    {
      "source": "Imputation",
      "type": "implemented_by",
      "target": "Regression Imputation"
    },
    {
      "source": "Imputation",
      "type": "implemented_by",
      "target": "Multiple Imputation"
    },
    {
      "source": "Imputation",
      "type": "implemented_by",
      "target": "Mean Imputation"
    },
    {
      "source": "Mean Imputation",
      "type": "subtype_of",
      "target": "Imputation"
    },
    {
      "source": "Mean Imputation",
      "type": "related_to",
      "target": "Regression Imputation"
    },
    {
      "source": "Regression Imputation",
      "type": "subtype_of",
      "target": "Imputation"
    },
    {
      "source": "Regression Imputation",
      "type": "improved_by",
      "target": "Stochastic Regression Imputation"
    },
    {
      "source": "Stochastic Regression Imputation",
      "type": "extends",
      "target": "Regression Imputation"
    },
    {
      "source": "Stochastic Regression Imputation",
      "type": "related_to",
      "target": "Multiple Imputation"
    },
    {
      "source": "Multiple Imputation",
      "type": "subtype_of",
      "target": "Imputation"
    },
    {
      "source": "Multiple Imputation",
      "type": "requires",
      "target": "MAR Assumption"
    },
    {
      "source": "Multiple Imputation",
      "type": "related_to",
      "target": "Maximum Likelihood Estimation"
    },
    {
      "source": "Maximum Likelihood Estimation (MLE)",
      "type": "implemented_by",
      "target": "Expectation-Maximization Algorithm"
    },
    {
      "source": "Maximum Likelihood Estimation (MLE)",
      "type": "alternative_to",
      "target": "Multiple Imputation"
    },
    {
      "source": "Maximum Likelihood Estimation (MLE)",
      "type": "applied_to",
      "target": "Missing Data"
    },
    {
      "source": "Personal Data",
      "type": "protected_by",
      "target": "General Data Protection Regulation (GDPR)"
    },
    {
      "source": "Pseudonymisation",
      "type": "related_to",
      "target": "Personal Data"
    },
    {
      "source": "Pseudonymisation",
      "type": "contrasts_with",
      "target": "Anonymisation"
    },
    {
      "source": "k-Anonymity",
      "type": "applied_to",
      "target": "Tabular Data"
    },
    {
      "source": "k-Anonymity",
      "type": "supports",
      "target": "Anonymisation"
    },
    {
      "source": "Advanced Data Modification",
      "type": "derived_from",
      "target": "Anonymisation"
    },
    {
      "source": "Relational Database",
      "type": "supports",
      "target": "SQL"
    },
    {
      "source": "NoSQL Databases",
      "type": "contrasts_with",
      "target": "Relational Database"
    },
    {
      "source": "MongoDB",
      "type": "instance_of",
      "target": "NoSQL Databases"
    },
    {
      "source": "Entity-Relationship Modeling",
      "type": "used_to",
      "target": "Design Relational Database schemas"
    },
    {
      "source": "Relational Algebra",
      "type": "foundation_of",
      "target": "SQL"
    },
    {
      "source": "SQL",
      "type": "used_in",
      "target": "Relational Database"
    },
    {
      "source": "CIA Triad",
      "type": "applies_to",
      "target": "Information Security"
    },
    {
      "source": "Open Data Licences",
      "type": "applies_to",
      "target": "Open Data"
    },
    {
      "source": "Metadata",
      "type": "supports",
      "target": "FAIR Principles"
    },
    {
      "source": "ActivityMeasurement",
      "type": "part_of",
      "target": "Activity Monitoring Schema"
    },
    {
      "source": "Numerosity Reduction",
      "type": "part_of",
      "target": "Data Reduction"
    },
    {
      "source": "Numerosity Reduction",
      "type": "related_to",
      "target": "Sampling"
    },
    {
      "source": "Numerosity Reduction",
      "type": "related_to",
      "target": "Clustering"
    },
    {
      "source": "Numerosity Reduction",
      "type": "related_to",
      "target": "Histograms"
    },
    {
      "source": "Numerosity Reduction",
      "type": "contrasts_with",
      "target": "Dimensionality Reduction"
    },
    {
      "source": "Accuracy Paradox",
      "type": "contrasts_with",
      "target": "Balanced Accuracy"
    },
    {
      "source": "Accuracy Paradox",
      "type": "related_to",
      "target": "Imbalanced Data"
    },
    {
      "source": "Accuracy Paradox",
      "type": "related_to",
      "target": "Model Evaluation Metrics"
    },
    {
      "source": "Validation Set",
      "type": "part_of",
      "target": "Data Partitioning"
    },
    {
      "source": "Validation Set",
      "type": "related_to",
      "target": "Model Selection"
    },
    {
      "source": "Validation Set",
      "type": "used_in",
      "target": "Avoiding Overfitting"
    },
    {
      "source": "Artificial Data Generation",
      "type": "related_to",
      "target": "Artificial Data"
    },
    {
      "source": "Artificial Data Generation",
      "type": "related_to",
      "target": "SMOTE"
    },
    {
      "source": "Artificial Data Generation",
      "type": "related_to",
      "target": "Imbalanced Data"
    },
    {
      "source": "Artificial Data Generation",
      "type": "used_in",
      "target": "Data Augmentation"
    },
    {
      "source": "Artificial Data",
      "type": "instance_of",
      "target": "Artificial Data Generation"
    },
    {
      "source": "Artificial Data",
      "type": "related_to",
      "target": "Noise Injection"
    },
    {
      "source": "Artificial Data",
      "type": "related_to",
      "target": "SMOTE"
    },
    {
      "source": "Noise",
      "type": "related_to",
      "target": "Outliers"
    },
    {
      "source": "Noise",
      "type": "handled_by",
      "target": "Noise Filters"
    },
    {
      "source": "Noise",
      "type": "handled_by",
      "target": "Data Polishing"
    },
    {
      "source": "Noise",
      "type": "mitigated_by",
      "target": "Robust Learning Algorithms"
    },
    {
      "source": "Robust Learning Algorithms",
      "type": "used_in",
      "target": "Supervised Learning"
    },
    {
      "source": "Robust Learning Algorithms",
      "type": "mitigates",
      "target": "Noise"
    },
    {
      "source": "Robust Learning Algorithms",
      "type": "alternative_to",
      "target": "Data Polishing"
    },
    {
      "source": "Data Polishing",
      "type": "alternative_to",
      "target": "Robust Learning Algorithms"
    },
    {
      "source": "Data Polishing",
      "type": "related_to",
      "target": "Noise Filters"
    },
    {
      "source": "Data Polishing",
      "type": "used_in",
      "target": "Data Cleaning"
    },
    {
      "source": "Noise Filters",
      "type": "used_in",
      "target": "Data Preprocessing"
    },
    {
      "source": "Noise Filters",
      "type": "mitigates",
      "target": "Noise"
    },
    {
      "source": "Noise Filters",
      "type": "related_to",
      "target": "Data Polishing"
    },
    {
      "source": "Data Pollution",
      "type": "related_to",
      "target": "Noise"
    },
    {
      "source": "Data Pollution",
      "type": "handled_by",
      "target": "Data Cleaning"
    },
    {
      "source": "Data Pollution",
      "type": "influences",
      "target": "Data Quality"
    },
    {
      "source": "Signal Saturation",
      "type": "related_to",
      "target": "Noise"
    },
    {
      "source": "Signal Saturation",
      "type": "handled_by",
      "target": "Data Cleaning"
    },
    {
      "source": "Signal Saturation",
      "type": "affects",
      "target": "Signal Quality"
    },
    {
      "source": "Outliers",
      "type": "contrasts_with",
      "target": "Anomalies"
    },
    {
      "source": "Outliers",
      "type": "handled_by",
      "target": "Outlier Detection Methods"
    },
    {
      "source": "Outliers",
      "type": "related_to",
      "target": "Noise"
    },
    {
      "source": "Anomalies",
      "type": "contrasts_with",
      "target": "Outliers"
    },
    {
      "source": "Anomalies",
      "type": "detected_by",
      "target": "Anomaly Detection Methods"
    },
    {
      "source": "Anomalies",
      "type": "related_to",
      "target": "Novelty Detection"
    },
    {
      "source": "Outlier Detection Methods",
      "type": "used_in",
      "target": "Anomaly Detection"
    },
    {
      "source": "Outlier Detection Methods",
      "type": "applies_to",
      "target": "Outliers"
    },
    {
      "source": "Outlier Detection Methods",
      "type": "includes",
      "target": "Proximity-Based Methods"
    },
    {
      "source": "Outlier Detection Methods",
      "type": "includes",
      "target": "Statistical Methods"
    },
    {
      "source": "Outlier Detection Methods",
      "type": "includes",
      "target": "Clustering Methods"
    },
    {
      "source": "Proximity-Based Methods",
      "type": "subtype_of",
      "target": "Outlier Detection Methods"
    },
    {
      "source": "Proximity-Based Methods",
      "type": "related_to",
      "target": "Distance Measures"
    },
    {
      "source": "Proximity-Based Methods",
      "type": "used_in",
      "target": "Unsupervised Outlier Detection"
    },
    {
      "source": "Generalizability",
      "type": "related_to",
      "target": "Model Generalization"
    },
    {
      "source": "Generalizability",
      "type": "part_of",
      "target": "Model Evaluation"
    },
    {
      "source": "Generalization Error",
      "type": "related_to",
      "target": "Generalizability"
    },
    {
      "source": "Generalization Error",
      "type": "contrast_with",
      "target": "Training Error"
    },
    {
      "source": "Generalization Gap",
      "type": "related_to",
      "target": "Overfitting"
    },
    {
      "source": "Dataset Generalization",
      "type": "related_to",
      "target": "Generalizability"
    },
    {
      "source": "Retrospective Models",
      "type": "instance_of",
      "target": "Descriptive Modeling"
    },
    {
      "source": "Prospective Models",
      "type": "instance_of",
      "target": "Predictive Modeling"
    },
    {
      "source": "Theory-Driven Models",
      "type": "contrast_with",
      "target": "Data-Driven Models"
    },
    {
      "source": "Data-Driven Models",
      "type": "contrast_with",
      "target": "Theory-Driven Models"
    },
    {
      "source": "Explanatory Power",
      "type": "often_increases",
      "target": "Model Simplicity"
    },
    {
      "source": "Predictive Power",
      "type": "related_to",
      "target": "Predictive Modeling"
    },
    {
      "source": "Minimum Sample Size for Statistical Inference",
      "type": "related_to",
      "target": "Sample Size"
    },
    {
      "source": "Sample Size for Predictive Modeling",
      "type": "related_to",
      "target": "Sample Size"
    },
    {
      "source": "Precision of Parameter Estimation",
      "type": "related_to",
      "target": "Sample Size"
    },
    {
      "source": "Population Representativeness",
      "type": "supports",
      "target": "Generalizability"
    },
    {
      "source": "Access to Data",
      "type": "affects",
      "target": "Sample Size"
    },
    {
      "source": "Training Set",
      "type": "part_of",
      "target": "Data Partitioning"
    },
    {
      "source": "Test Set",
      "type": "part_of",
      "target": "Data Partitioning"
    },
    {
      "source": "Hold-Out Method",
      "type": "instance_of",
      "target": "Data Partitioning"
    },
    {
      "source": "Eighty-Twenty Split",
      "type": "instance_of",
      "target": "Hold-Out Method"
    },
    {
      "source": "Two-Thirds One-Third Split",
      "type": "instance_of",
      "target": "Hold-Out Method"
    },
    {
      "source": "Training-Validation-Test Split",
      "type": "related_to",
      "target": "Cross-Validation"
    },
    {
      "source": "Participant-Wise Cross-Validation",
      "type": "related_to",
      "target": "Cross-Validation"
    },
    {
      "source": "Leave-One-Out Validation",
      "type": "part_of",
      "target": "Cross-Validation"
    },
    {
      "source": "Temporal Dependency in Data",
      "type": "affects",
      "target": "Data Partitioning"
    },
    {
      "source": "No Random Sampling Under Temporal Dependence",
      "type": "related_to",
      "target": "Temporal Dependency in Data"
    },
    {
      "source": "Day-Based Partitioning",
      "type": "addresses",
      "target": "Temporal Leakage"
    },
    {
      "source": "Temporal Leakage",
      "type": "caused_by",
      "target": "Temporal Dependency in Data"
    },
    {
      "source": "Sliding Windows",
      "type": "related_to",
      "target": "Window-Based Feature Extraction"
    },
    {
      "source": "Overlapping Windows",
      "type": "causes",
      "target": "Temporal Dependency in Data"
    },
    {
      "source": "Window-Based Feature Extraction",
      "type": "uses",
      "target": "Sliding Windows"
    },
    {
      "source": "Individual Models",
      "type": "contrast_with",
      "target": "Population Models"
    },
    {
      "source": "Population Models",
      "type": "contrast_with",
      "target": "Individual Models"
    },
    {
      "source": "Activity Recognition",
      "type": "uses",
      "target": "Window-Based Feature Extraction"
    },
    {
      "source": "Sensor-Based Activity Detection",
      "type": "related_to",
      "target": "Activity Recognition"
    },
    {
      "source": "Multi-Modal Feature Inputs",
      "type": "used_in",
      "target": "Activity Recognition"
    },
    {
      "source": "Model Complexity–Generalization Tradeoff",
      "type": "related_to",
      "target": "Model Selection"
    },
    {
      "source": "Simpler Models Generalize Better",
      "type": "supports",
      "target": "Model Complexity–Generalization Tradeoff"
    },
    {
      "source": "Overcomplex Models",
      "type": "related_to",
      "target": "Overfitting"
    },
    {
      "source": "Hyperparameter Selection",
      "type": "related_to",
      "target": "Model Selection"
    },
    {
      "source": "Hyperparameter Selection",
      "type": "related_to",
      "target": "Cross-Validation"
    },
    {
      "source": "Feature Selection",
      "type": "part_of",
      "target": "Model Selection"
    },
    {
      "source": "Data Mining",
      "type": "part_of",
      "target": "Knowledge Discovery in Databases"
    },
    {
      "source": "Data Mining",
      "type": "requires",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Mining",
      "type": "related_to",
      "target": "Machine Learning"
    },
    {
      "source": "Knowledge Discovery in Databases",
      "type": "includes",
      "target": "Data Mining"
    },
    {
      "source": "Knowledge Discovery in Databases",
      "type": "depends_on",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Pre-processing",
      "type": "enables",
      "target": "Data Mining"
    },
    {
      "source": "Data Pre-processing",
      "type": "requires",
      "target": "Raw Data"
    },
    {
      "source": "Data Quality",
      "type": "affects",
      "target": "Data Mining"
    },
    {
      "source": "Data Quality",
      "type": "improved_by",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Cleaning",
      "type": "part_of",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Cleaning",
      "type": "enables",
      "target": "Data Mining"
    },
    {
      "source": "Big Data",
      "type": "context_for",
      "target": "Data Mining"
    },
    {
      "source": "Big Data",
      "type": "related_to",
      "target": "Machine Learning"
    },
    {
      "source": "Data Literacy",
      "type": "required_for",
      "target": "Data Mining"
    },
    {
      "source": "Data Literacy",
      "type": "enhances",
      "target": "Decision Making"
    },
    {
      "source": "Data Transformation",
      "type": "part_of",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Transformation",
      "type": "used_by",
      "target": "Machine Learning Models"
    },
    {
      "source": "Data Reduction",
      "type": "part_of",
      "target": "Data Pre-processing"
    },
    {
      "source": "Data Reduction",
      "type": "supports",
      "target": "Data Mining"
    },
    {
      "source": "Normalization",
      "type": "has_subtype",
      "target": "Min-Max Normalization"
    },
    {
      "source": "Normalization",
      "type": "has_subtype",
      "target": "Z-score Standardization"
    },
    {
      "source": "Normalization",
      "type": "has_subtype",
      "target": "Decimal Scaling"
    },
    {
      "source": "Normalization",
      "type": "related_to",
      "target": "Data Transformation"
    },
    {
      "source": "Normalization",
      "type": "used_in",
      "target": "Machine Learning"
    },
    {
      "source": "Min-Max Normalization",
      "type": "subtype_of",
      "target": "Normalization"
    },
    {
      "source": "Min-Max Normalization",
      "type": "related_to",
      "target": "Scaling"
    },
    {
      "source": "Z-score Standardization",
      "type": "subtype_of",
      "target": "Normalization"
    },
    {
      "source": "Z-score Standardization",
      "type": "used_in",
      "target": "Outlier Detection"
    },
    {
      "source": "Z-score Standardization",
      "type": "related_to",
      "target": "Standard Deviation"
    },
    {
      "source": "Decimal Scaling",
      "type": "subtype_of",
      "target": "Normalization"
    },
    {
      "source": "Decimal Scaling",
      "type": "related_to",
      "target": "Scaling"
    },
    {
      "source": "Discretization",
      "type": "used_in",
      "target": "Data Classification"
    },
    {
      "source": "Discretization",
      "type": "alternative_to",
      "target": "Continuous Variable Modeling"
    },
    {
      "source": "Discretization",
      "type": "related_to",
      "target": "Concept Hierarchy"
    },
    {
      "source": "Dummy Coding",
      "type": "used_in",
      "target": "Regression Modeling"
    },
    {
      "source": "Dummy Coding",
      "type": "related_to",
      "target": "Categorical Variables"
    },
    {
      "source": "Data Reduction",
      "type": "related_to",
      "target": "Feature Engineering"
    },
    {
      "source": "Data Reduction",
      "type": "includes",
      "target": "Principal Component Analysis (PCA)"
    },
    {
      "source": "Data Reduction",
      "type": "includes",
      "target": "Discrete Wavelet Transform (DWT)"
    },
    {
      "source": "Principal Component Analysis (PCA)",
      "type": "subtype_of",
      "target": "Data Reduction"
    },
    {
      "source": "Principal Component Analysis (PCA)",
      "type": "alternative_to",
      "target": "Nonlinear PCA"
    },
    {
      "source": "Principal Component Analysis (PCA)",
      "type": "related_to",
      "target": "Feature Extraction"
    },
    {
      "source": "Discrete Wavelet Transform (DWT)",
      "type": "subtype_of",
      "target": "Data Reduction"
    },
    {
      "source": "Discrete Wavelet Transform (DWT)",
      "type": "related_to",
      "target": "Principal Component Analysis (PCA)"
    },
    {
      "source": "Box-Cox Transformation",
      "type": "used_in",
      "target": "Transformation to Normality"
    },
    {
      "source": "Box-Cox Transformation",
      "type": "related_to",
      "target": "Normalization"
    },
    {
      "source": "Scaling to Arbitrary Range",
      "type": "related_to",
      "target": "Min-Max Normalization"
    },
    {
      "source": "Scaling to Arbitrary Range",
      "type": "part_of",
      "target": "Data Transformation"
    },
    {
      "source": "Storing Normalization Parameters",
      "type": "supports",
      "target": "Model Generalization"
    },
    {
      "source": "Storing Normalization Parameters",
      "type": "required_by",
      "target": "Min-Max Normalization"
    },
    {
      "source": "Extreme Z-Score Threshold Rule",
      "type": "related_to",
      "target": "Z-score Standardization"
    },
    {
      "source": "Extreme Z-Score Threshold Rule",
      "type": "instance_of",
      "target": "Outlier Detection Methods"
    },
    {
      "source": "Measurement Error Visualization",
      "type": "supports",
      "target": "Outlier Detection Methods"
    },
    {
      "source": "Measurement Error Visualization",
      "type": "related_to",
      "target": "Data Quality"
    },
    {
      "source": "Median Split",
      "type": "instance_of",
      "target": "Discretization"
    },
    {
      "source": "Distinctive Grouping",
      "type": "part_of",
      "target": "Discretization"
    },
    {
      "source": "Equal-Length Binning",
      "type": "contrasts_with",
      "target": "Equal-Size Binning"
    },
    {
      "source": "Equal-Size Binning",
      "type": "contrasts_with",
      "target": "Equal-Length Binning"
    },
    {
      "source": "Natural Cut Points",
      "type": "related_to",
      "target": "Discretization"
    },
    {
      "source": "Concept Hierarchies",
      "type": "used_in",
      "target": "Data Generalization"
    },
    {
      "source": "Concept Hierarchies",
      "type": "part_of",
      "target": "Discretization"
    },
    {
      "source": "Dichotomization",
      "type": "related_to",
      "target": "Median Split"
    },
    {
      "source": "Problems of Discretization",
      "type": "related_to",
      "target": "Discretization"
    },
    {
      "source": "Information Loss Through Dichotomization",
      "type": "instance_of",
      "target": "Problems of Discretization"
    },
    {
      "source": "Misclassification Risk at Cut Points",
      "type": "related_to",
      "target": "Discretization"
    },
    {
      "source": "Factor Variables",
      "type": "used_in",
      "target": "Dummy Coding"
    },
    {
      "source": "Frequency Tables",
      "type": "related_to",
      "target": "Categorical Data"
    },
    {
      "source": "Correspondence Tests",
      "type": "related_to",
      "target": "Frequency Tables"
    },
    {
      "source": "Categorical vs Continuous Visualization",
      "type": "supports",
      "target": "Factor Variables"
    },
    {
      "source": "Data Aggregation",
      "type": "part_of",
      "target": "Data Reduction"
    },
    {
      "source": "Data Generalization",
      "type": "uses",
      "target": "Concept Hierarchies"
    },
    {
      "source": "Variable Construction",
      "type": "related_to",
      "target": "Data Reduction"
    },
    {
      "source": "Nonparametric Data Reduction Methods",
      "type": "related_to",
      "target": "Data Reduction"
    },
    {
      "source": "Transformation to Normality",
      "type": "related_to",
      "target": "Box-Cox Transformation"
    },
    {
      "source": "Normality Tests",
      "type": "supports",
      "target": "Transformation to Normality"
    },
    {
      "source": "Solutions for Non-Normality",
      "type": "uses",
      "target": "Normality Tests"
    },
    {
      "source": "Solutions for Non-Normality",
      "type": "related_to",
      "target": "Transformation to Normality"
    }
  ],
  "metadata": {
    "last_built": "2025-11-14 10:12:07",
    "node_count": 290,
    "edge_count": 517
  }
}