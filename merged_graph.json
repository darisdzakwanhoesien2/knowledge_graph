{
  "nodes": {
    "Singular Value Decomposition": {
      "type": "Method",
      "domain": "Linear Algebra",
      "definition": "The Singular Value Decomposition (SVD) of a matrix A ∈ ℂ^{m×n} is a factorization A = U Σ V^*, where U and V are unitary matrices with orthonormal columns, and Σ is a diagonal matrix with non-negative singular values σ₁ ≥ σ₂ ≥ ⋯ ≥ σ_n ≥ 0 on the diagonal.",
      "description": "SVD provides the best low-rank approximation of a matrix in both spectral and Frobenius norms, enabling optimal matrix compression, dimensionality reduction, and solving least squares problems. It decomposes any matrix into orthogonal bases that capture the principal directions of variation.",
      "properties": {
        "Goal": "Provide orthogonal factorization and low-rank approximations.",
        "Applications": [
          "Data compression",
          "Principal component analysis",
          "Pseudo-inverse"
        ],
        "Methods": [
          "Golub-Reinsch algorithm",
          "Divide-and-conquer",
          "Iterative methods"
        ],
        "Examples": [
          "A ≈ U_k Σ_k V_k^* for rank-k approximation"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Singular Value": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The singular values of a matrix A are the non-negative square roots of the eigenvalues of A^*A (or AA^*), ordered as σ₁ ≥ σ₂ ≥ ⋯ ≥ σ_r > 0, where r is the rank of A.",
      "description": "Singular values measure the 'importance' or 'strength' of each principal direction in the matrix. The largest singular value σ₁ equals the operator norm ||A||, and they uniquely determine the best low-rank approximations.",
      "properties": {
        "Goal": "Quantify the magnitude of principal components in matrix decomposition.",
        "Applications": [
          "Rank determination",
          "Condition number computation (σ₁/σ_r)",
          "Numerical stability analysis"
        ],
        "Methods": [
          "Eigenvalue decomposition of A^*A",
          "Square root of eigenvalues"
        ],
        "Examples": [
          "σ₁ = ||A|| = max_{||x||=1} ||Ax|| represents maximum stretch"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Unitary Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A unitary matrix Q ∈ ℂ^{n×n} satisfies Q^*Q = I, meaning its columns (and rows) form an orthonormal basis that preserves the Euclidean norm: ||Qx|| = ||x|| for all x ∈ ℂ^n.",
      "description": "Unitary matrices represent rotations and reflections in complex space. They appear in QR decomposition, SVD, and eigenvalue problems, preserving distances and angles during transformations.",
      "properties": {
        "Goal": "Preserve norms and inner products in linear transformations.",
        "Applications": [
          "QR decomposition",
          "Singular Value Decomposition",
          "Numerical linear algebra algorithms",
          "Quantum computing gates"
        ],
        "Methods": [
          "Gram-Schmidt process",
          "Householder reflections"
        ],
        "Examples": [
          "Q = [q₁ ⋯ qₙ] where {qⱼ} are orthonormal vectors"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Low Rank Approximation": {
      "type": "Method",
      "domain": "Linear Algebra",
      "definition": "The low-rank approximation problem finds a rank-k matrix F_k that minimizes ||A - F_k||_F among all rank-k matrices, solved uniquely by SVD: F_k = ∑_{j=1}^k σⱼ uⱼ vⱼ^*.",
      "description": "SVD provides the optimal rank-k approximation in Frobenius norm, equivalent to keeping the k largest singular values. This compresses data while minimizing reconstruction error.",
      "properties": {
        "Goal": "Approximate high-dimensional matrix with lower-rank version minimizing ||A - F_k||_F.",
        "Applications": [
          "Data compression",
          "Dimensionality reduction",
          "Image denoising",
          "Recommendation systems"
        ],
        "Methods": [
          "Truncated SVD",
          "Eckart-Young theorem"
        ],
        "Examples": [
          "F_k = U_k Σ_k V_k^* where error = ∑_{j=k+1}^r σⱼ²"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Frobenius Norm": {
      "type": "Metric",
      "domain": "Linear Algebra",
      "definition": "The Frobenius norm of A ∈ ℂ^{m×n} is ||A||_F = √(∑_{j=1}^m ∑_{k=1}^n |a_{jk}|²) = √trace(A^*A), measuring the Euclidean norm of the matrix treated as a vector in ℂ^{mn}.",
      "description": "Widely used in matrix approximation problems due to computational convenience and equivalence to SVD error. It's unitarily invariant: ||Q₁AQ₂||_F = ||A||_F for unitary Q₁, Q₂.",
      "properties": {
        "Goal": "Measure matrix 'size' as vector in high-dimensional space.",
        "Applications": [
          "Low-rank approximation error",
          "Matrix compression",
          "Least squares problems"
        ],
        "Methods": [
          "√(sum of squared entries)",
          "√trace(A^*A)"
        ],
        "Examples": [
          "||A||_F² = ∑ σⱼ² where σⱼ are singular values"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Operator Norm": {
      "type": "Metric",
      "domain": "Linear Algebra",
      "definition": "The operator norm ||A|| = max_{||x||=1} ||Ax|| measures the maximum stretch factor of the linear transformation A: ℂ^n → ℂ^m.",
      "description": "For SVD, ||A|| = σ₁, the largest singular value. It quantifies how much A can amplify unit vectors and determines numerical stability.",
      "properties": {
        "Goal": "Measure maximum amplification by linear transformation.",
        "Applications": [
          "Condition number (||A|| ⋅ ||A⁻¹||)",
          "Stability analysis",
          "Spectral radius bounds"
        ],
        "Methods": [
          "Largest singular value σ₁",
          "max_{||x||=1} ||Ax||"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "03_the_singular_value_decomposition.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Matrix Function": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "A matrix function f(A) extends scalar functions to matrices A, defined via Jordan form, power series, or other representations, enabling operations like exponentials or square roots on matrices.",
      "description": "Matrix functions are computed iteratively for large matrices, used in differential equations, control theory, and eigenvalue problems.",
      "properties": {
        "Goal": "Extend scalar functions to matrices while preserving algebraic structure.",
        "Applications": [
          "Matrix exponential in ODEs",
          "Matrix logarithm in geometry",
          "Sign function in control"
        ],
        "Methods": [
          "Schur-Parlett",
          "Scaling-and-squaring",
          "Padé approximation",
          "Contour integral"
        ],
        "Examples": [
          "exp(A)",
          "A^{1/2}",
          "sign(A)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Matrix Square Root": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "The matrix square root X satisfies X^2 = A for a matrix A, with the principal square root being positive definite if A is.",
      "description": "It is computed iteratively, e.g., via Newton's method, for applications in statistics, control, and geometry.",
      "properties": {
        "Goal": "Find X such that X^2 = A.",
        "Applications": [
          "Covariance matrices",
          "Riemannian metrics",
          "Polar decomposition"
        ],
        "Methods": [
          "Newton iteration",
          "Denman-Beavers",
          "Schur method"
        ],
        "Examples": [
          "X = sqrt(A) for SPD A"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Newton's Iteration for Square Root": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Newton's iteration for the matrix square root updates X_{k+1} = (X_k + A X_k^{-1}) / 2, starting from X_0 = I or other, converging quadratically to sqrt(A).",
      "description": "It is stable for positive definite A, with safeguards for convergence, used in large-scale computations via iterative solvers.",
      "properties": {
        "Goal": "Compute sqrt(A) iteratively.",
        "Applications": [
          "Matrix sign function",
          "Algebraic Riccati equations"
        ],
        "Methods": [
          "Matrix inversion at each step",
          "Quadratic convergence"
        ],
        "Examples": [
          "X_{k+1} = (X_k + A X_k^{-1}) / 2"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Eigenvalue Problem": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "The eigenvalue problem seeks scalars λ and vectors x ≠ 0 such that A x = λ x for matrix A.",
      "description": "Iterative methods like power iteration or Lanczos are used for large sparse matrices, shifting spectrum for better conditioning.",
      "properties": {
        "Goal": "Compute spectrum and invariant subspaces for analysis and transformation of linear operators.",
        "Applications": [
          "Vibration analysis",
          "Stability of dynamical systems",
          "Google PageRank",
          "Quantum mechanics"
        ],
        "Methods": [
          "Power iteration",
          "QR algorithm",
          "Arnoldi iteration",
          "Jacobi-Davidson"
        ],
        "Examples": [
          "Ax = λx",
          "det(A−λI)=0 (characteristic equation)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Spectral Shift": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Spectral shift transforms the eigenvalue problem to (A - μ I) x = (λ - μ) x, shifting eigenvalues by μ to target specific parts of the spectrum or improve conditioning.",
      "description": "Choosing μ near a target eigenvalue accelerates convergence in iterative methods; for real μ, it can make the matrix positive definite.",
      "properties": {
        "Goal": "Adjust spectrum for better numerical properties.",
        "Applications": [
          "Interior eigenvalues",
          "Deflation",
          "Preconditioning"
        ],
        "Methods": [
          "μ close to target λ",
          "μ = (λ_min + λ_max)/2"
        ],
        "Examples": [
          "Λ(A - μ I) = Λ(A) - μ"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Positive Definite Shift": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Positive definite shift chooses μ such that A - μ I is positive definite, e.g., μ < λ_min(A) for symmetric A, enabling use of CG or other SPD methods.",
      "description": "It stabilizes iterations and bounds condition number; for estimated ˆμ ≈ λ_min, adjust with a > 0 to ensure positivity.",
      "properties": {
        "Goal": "Make shifted matrix SPD for efficient solving.",
        "Applications": [
          "Shift-and-invert",
          "Preconditioned eigensolvers"
        ],
        "Methods": [
          "μ = ˆμ - a, a > 0",
          "Rayleigh quotient estimate"
        ],
        "Examples": [
          "A - μ I with μ = ˆμ - a"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Spectrum Λ(A)": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The spectrum Λ(A) is the set of all eigenvalues of matrix A.",
      "description": "Shifting modifies the spectrum as Λ(A - μ I) = Λ(A) - μ, used to isolate eigenvalues or improve numerical properties.",
      "properties": {
        "Goal": "Characterize matrix via its eigenvalues.",
        "Applications": [
          "Spectral radius",
          "Conditioning",
          "Stability"
        ],
        "Methods": [
          "Eigen decomposition",
          "Characteristic polynomial"
        ],
        "Examples": [
          "Λ(A) subset C for A in C^{n x n}"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "11_Iterative methods_for_eigenvalue_problems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Matrix Computations": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "The study and development of algorithms for performing operations on matrices, including addition, multiplication, inversion, decomposition, and solving linear systems.",
      "description": "Matrix computations are central to numerical analysis, scientific computing, and computational science. They underpin applications in physics simulations, data analysis, machine learning, and computer graphics. The computational complexity of classical algorithms for matrix multiplication is O(n³), while storage is typically O(n²).",
      "properties": {
        "Goal": "Solve matrix-related problems efficiently in numerical contexts.",
        "Applications": [
          "Scientific computing",
          "Data analysis",
          "PDE solving",
          "Linear algebra problems"
        ],
        "Methods": [
          "Factorizations (LU, SVD)",
          "Eigenvalue computations",
          "Iterative solvers",
          "Subspace approximations"
        ],
        "Examples": [
          "Solving Ax = b",
          "Approximating large matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01_introduction.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Inner Product": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A binary operation on two vectors in ℂⁿ that produces a scalar, defined as ⟨x, y⟩ = yᵀx̄ = ∑ⱼ xⱼȳⱼ, where x, y ∈ ℂⁿ.",
      "description": "The inner product (also known as dot product in real vector spaces) measures similarity between vectors and forms the foundation of orthogonality, norms, and projections. In complex spaces, it involves conjugation to ensure positive definiteness of the induced norm.",
      "properties": {
        "Goal": "Quantify angle and similarity between vectors; enable orthogonal decomposition",
        "Applications": [
          "Gram-Schmidt orthogonalization",
          "Least squares",
          "Signal processing",
          "Quantum mechanics"
        ],
        "Methods": [
          "N/A"
        ],
        "Examples": [
          "⟨x, y⟩ = ∑ xⱼȳⱼ over j=1 to n"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01_introduction.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Computational Complexity (Matrix Multiplication)": {
      "type": "Metric",
      "domain": "Algorithm Analysis",
      "definition": "The asymptotic resource requirement for matrix multiplication and related operations, classically O(n³) time and O(n²) space for n×n matrices.",
      "description": "Standard matrix multiplication of two n×n matrices requires O(n³) arithmetic operations using the naive algorithm. While theoretical improvements exist (e.g., Strassen’s O(n².⁸⁰⁷)), practical methods remain close to O(n³). Storage scales quadratically as O(n²).",
      "properties": {
        "Goal": "Assess efficiency and scalability of matrix algorithms",
        "Applications": [
          "Performance prediction",
          "Algorithm selection",
          "Hardware design"
        ],
        "Methods": [
          "Big-O notation",
          "Arithmetic circuit complexity"
        ],
        "Examples": [
          "O(n³) time",
          "O(n²) space"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "01_introduction.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "LU Factorization": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "LU factorization decomposes a matrix A into a lower triangular matrix L and an upper triangular matrix U such that A = LU, or with pivoting PA = LU, enabling efficient solution of linear systems.",
      "description": "It is a fundamental algorithm with O(n^3) complexity, used for solving Ax = b by forward and backward substitution, and approximated for large matrices using subspace products.",
      "properties": {
        "Goal": "Decompose matrices for efficient linear system solving.",
        "Applications": [
          "Numerical linear algebra",
          "PDE discretization",
          "Data processing"
        ],
        "Methods": [
          "Gaussian elimination",
          "Pivoting for stability",
          "Partial pivoting"
        ],
        "Examples": [
          "A = LU for square matrices",
          "PA = LU with permutation P"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02_product_of_matrix_subspaces_in_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Product of Matrix Subspaces": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "The product of matrix subspaces V1 V2 is defined as {V1 V2 : V1 ∈ V1, V2 ∈ V2}, providing a framework for approximating matrices with fewer parameters than full rank.",
      "description": "It allows factoring matrices into low-complexity forms like sum of outer products, useful for large n with small k, achieving 2nk parameters and potential for lower computational costs.",
      "properties": {
        "Goal": "Approximate matrices efficiently using subspace products.",
        "Applications": [
          "Large matrix factorization",
          "PDE discretization",
          "Data storage"
        ],
        "Methods": [
          "Subspace multiplication",
          "Rank-k approximation",
          "Norm minimization"
        ],
        "Examples": [
          "A ≈ sum_{j=1}^k u_j v_j^*",
          "I + V1 V2 inversion"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02_product_of_matrix_subspaces_in_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Gram-Schmidt Orthogonalization": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Gram-Schmidt orthogonalization transforms a set of linearly independent vectors into an orthonormal set using projections and normalization.",
      "description": "It is used in QR factorization and subspace computations, with classical and modified variants for numerical stability.",
      "properties": {
        "Goal": "Produce orthonormal bases from vector sets.",
        "Applications": [
          "QR decomposition",
          "Subspace orthogonalization",
          "Least squares"
        ],
        "Methods": [
          "Classical Gram-Schmidt",
          "Modified Gram-Schmidt",
          "Householder reflections"
        ],
        "Examples": [
          "Orthogonalizing columns of A"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02_product_of_matrix_subspaces_in_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Low-Rank Approximation": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Low-rank approximation finds a matrix Fk of rank k that minimizes ||A - Fk|| for a given norm, often using SVD or subspace products for efficiency.",
      "description": "It reduces storage and computation for large matrices, with subspace products offering 2nk parameters for rank-k approximations.",
      "properties": {
        "Goal": "Approximate high-dimensional matrices with lower rank.",
        "Applications": [
          "Data compression",
          "Noise reduction",
          "Machine learning"
        ],
        "Methods": [
          "SVD truncation",
          "Subspace product",
          "Randomized algorithms"
        ],
        "Examples": [
          "Fk = sum u_j v_j^*",
          "min_{rank(F)=k} ||A - F||"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02_product_of_matrix_subspaces_in_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Hermitian Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A Hermitian matrix M satisfies M^* = M, where * denotes conjugate transpose, with real eigenvalues and orthogonal eigenvectors.",
      "description": "It requires n^2 real parameters for storage, or fewer in subspace approximations, used in quantum mechanics and signal processing.",
      "properties": {
        "Goal": "Model self-adjoint operators in complex spaces.",
        "Applications": [
          "Quantum computing",
          "Covariance matrices",
          "Spectral analysis"
        ],
        "Methods": [
          "Eigen decomposition",
          "Cholesky factorization (positive definite)"
        ],
        "Examples": [
          "M with real diagonal and conjugate symmetric off-diagonals"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "02_product_of_matrix_subspaces_in_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Matrix Subspace": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A subset V ⊆ ℂ^{n×n} of matrices closed under addition and scalar multiplication, forming a vector space over ℂ.",
      "description": "Matrix subspaces are used to model families of matrices sharing structural properties (e.g., invertibility, symmetry, triangularity). They enable low-rank approximations, invariant subspaces in eigenvalue problems, and structured matrix factorizations such as LU or SVD within constrained sets.",
      "properties": {
        "Goal": "Group matrices with common algebraic or analytic traits to simplify computations and preserve structure in factorizations.",
        "Applications": [
          "Structured matrix factorization",
          "Low-rank approximation",
          "Krylov methods",
          "Invariant subspace computation"
        ],
        "Methods": [
          "Span construction",
          "Closure under operations",
          "Equivalence via similarity"
        ],
        "Examples": [
          "span{I, A}",
          "span{A, B}",
          "upper triangular matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Nonsingular Matrix Subspace": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A matrix subspace V ⊆ ℂ^{n×n} that contains at least one invertible matrix (det V ≠ 0 for some V ∈ V).",
      "description": "Nonsingular subspaces guarantee the existence of invertible elements, enabling the definition of the set of inverses Inv(V) = {V⁻¹ : V ∈ V, det V ≠ 0}. Such subspaces are crucial for ensuring well-defined inverse-based factorizations (e.g., LU within V).",
      "properties": {
        "Goal": "Ensure invertibility within a structured family of matrices to support factorization algorithms.",
        "Applications": [
          "LU factorization",
          "Generalized inverse",
          "Structured preconditioning"
        ],
        "Methods": [
          "Perturbation arguments",
          "Open-dense property in finite dimensions"
        ],
        "Examples": [
          "V = span{I, A} for nonsingular A",
          "Upper triangular matrices with nonzero diagonals"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Inv(V)": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The set of inverses of all invertible matrices in a matrix subspace V: Inv(V) = {W : W = V⁻¹, V ∈ V, det V ≠ 0}.",
      "description": "For nonsingular matrix subspaces, Inv(V) is itself a matrix subspace under mild conditions. This enables dual-space factorizations and ensures closure under inversion within structured sets, vital for algorithmic stability in LU-type methods.",
      "properties": {
        "Goal": "Construct a subspace of inverses to maintain structure across factorization steps.",
        "Applications": [
          "LU within subspace",
          "Preconditioner design",
          "Group-inverse problems"
        ],
        "Methods": [
          "Similarity transformation",
          "Closure proof via nonsingularity"
        ],
        "Examples": [
          "Inv(upper triangular) = lower triangular",
          "Inv(Hermitian) = Hermitian"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "LU Factorization within Subspace": {
      "type": "Method Gaussian elimination",
      "domain": "Numerical Linear Algebra",
      "definition": "Decomposition of a matrix A ∈ V into A = LU where L is lower triangular and U is upper triangular, both belonging to predefined matrix subspaces derived from V.",
      "description": "By identifying nonsingular matrix subspaces V and W = Inv(V), LU factorization can be confined within V × W, ensuring all intermediate matrices remain structured. This supports specialized algorithms for banded, symmetric positive-definite, or approximate low-rank problems.",
      "properties": {
        "Goal": "Perform Gaussian elimination while preserving membership in designated matrix subspaces.",
        "Applications": [
          "Banded LU",
          "SPD factorization",
          "Low-rank updates",
          "Krylov-based solvers"
        ],
        "Methods": [
          "Schur complement",
          "Pivot-free elimination",
          "Subspace projection"
        ],
        "Examples": [
          "A ∈ upper triangular → L = I, U = A",
          "A ∈ V, L ∈ V, U ∈ Inv(V)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Krylov Subspace": {
      "type": "Concept",
      "domain": "Numerical Linear Algebra",
      "definition": "The matrix subspace K_j(A; I) = span{I, A, A², ..., A^{j-1}} generated by powers of a matrix A starting from the identity.",
      "description": "Krylov subspaces lie at the core of iterative methods for large-scale eigenvalue and linear system problems (GMRES, Lanczos, Arnoldi). Their dimension grows linearly until saturation at the degree of the minimal polynomial of A.",
      "properties": {
        "Goal": "Generate subspaces for iterative numerical methods.",
        "Applications": [
          "Eigenvalue computation",
          "Linear solvers",
          "Matrix exponentiation"
        ],
        "Methods": [
          "Arnoldi iteration",
          "Lanczos algorithm",
          "GMRES"
        ],
        "Examples": [
          "K_1(A; I) = span{I}",
          "K_2(A; I) = span{I, A}"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Matrix Polynomials": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "Functions p(z) = ∑_{k=0}^m c_k z^k evaluated at matrices: p(A) = ∑_{k=0}^m c_k A^k for A ∈ ℂ^{n×n}.",
      "description": "Matrix polynomials map matrix subspaces to themselves if V is closed under powers of its members. They define minimal and characteristic polynomials, enable Cayley-Hamilton applications, and support function-based iterative methods.",
      "properties": {
        "Goal": "Extend scalar polynomial theory to matrices for spectral analysis and function approximation.",
        "Applications": [
          "Cayley-Hamilton theorem",
          "Matrix exponential",
          " preconditioning",
          "Spectral projectors"
        ],
        "Methods": [
          "Horner scheme",
          "Paterson-Stockmeyer",
          "Jordan form evaluation"
        ],
        "Examples": [
          "p(z) = z²−2z+1 → p(A) = A²−2A+I = 0",
          "q(z) = det(A−zI) → q(A)=0"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Projection Operator": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A linear operator P on ℂ^n such that P² = P, projecting vectors onto a subspace R(P) with kernel R(I−P).",
      "description": "Projections decompose spaces into direct sums and are building blocks for oblique and orthogonal projections in iterative solvers. Orthogonal projections minimize least-squares error and appear in GMRES and conjugate gradients.",
      "properties": {
        "Goal": "Project vectors onto subspaces.",
        "Applications": [
          "Least squares",
          "Subspace methods",
          "Decomposition"
        ],
        "Methods": [
          "Orthogonal projection",
          "Oblique projection"
        ],
        "Examples": [
          "P with P^2 = P",
          "I - P as complement projection"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "04_matrix_subspaces_for_factoring_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Sparsity Structure": {
      "type": "Concept",
      "domain": "Matrix Computations",
      "definition": "The sparsity structure of a matrix or matrix subspace specifies the pattern of entries that may be nonzero, independent of the actual numerical values.",
      "description": "Sparsity structures capture the combinatorial pattern of allowed nonzeros in matrices, enabling algorithms that exploit memory efficiency and reduce computational cost. They are essential in large-scale problems such as PDE discretizations, graph Laplacians, and structured matrix factorizations.",
      "properties": {
        "Goal": "Identify and exploit zero-patterns for efficient computation.",
        "Applications": [
          "Sparse LU factorization",
          "Finite difference discretizations",
          "Graph-based matrix algorithms"
        ],
        "Methods": [
          "Pattern analysis",
          "Fill-in minimization",
          "Graph reordering"
        ],
        "Examples": [
          "Tridiagonal pattern",
          "Band matrix structure"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_Section2.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Standard Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Computations",
      "definition": "A standard matrix subspace is a subspace that admits a basis consisting of standard matrices, each with exactly one entry equal to 1 and all other entries equal to 0.",
      "description": "Standard matrix subspaces represent matrix sets defined purely by sparsity patterns, without structural constraints such as symmetry. Orthogonal projection onto these subspaces is straightforward—simply zero out entries outside the allowed pattern.",
      "properties": {
        "Goal": "Model matrix sets defined solely by sparsity constraints.",
        "Applications": [
          "Sparse matrix approximations",
          "Projection methods",
          "Structured LU factorization"
        ],
        "Methods": [
          "Entrywise projection",
          "Basis construction from standard matrices"
        ],
        "Examples": [
          "Diagonal matrices",
          "Strictly lower triangular matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_Section2.3.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Orthogonal Projector (Matrix Subspace)": {
      "type": "Operator",
      "domain": "Numerical Analysis",
      "definition": "An orthogonal projector onto a matrix subspace V is a linear operator P such that P² = P and the range of P is orthogonal to the nullspace of P.",
      "description": "Orthogonal projectors provide optimal approximations in least-squares and matrix approximation problems. Projection onto matrix subspaces is fundamental in algorithmic factoring, dimension reduction, and solving underdetermined systems.",
      "properties": {
        "Goal": "Project matrices onto a subspace with minimal error under the Frobenius inner product.",
        "Applications": [
          "Approximate factoring",
          "Subspace splitting",
          "Sylvester-type equations"
        ],
        "Methods": [
          "Construction via orthonormal basis",
          "Symmetrization operators"
        ],
        "Examples": [
          "P(A) = (A + A*)/2 for Hermitian matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_ProjectionTheory.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Invertible Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Theory",
      "definition": "A matrix subspace V is invertible if the set of inverses of its nonsingular elements forms another matrix subspace V⁻¹.",
      "description": "Invertible matrix subspaces allow algorithmic factorization because their inverses preserve linear structure. Classical examples include triangular matrices and Hermitian matrices, which maintain structure under inversion.",
      "properties": {
        "Goal": "Support structured factorization where both A and its factors belong to linear matrix families.",
        "Applications": [
          "LU factorization",
          "Symmetric factorizations",
          "Matrix subspace factoring"
        ],
        "Methods": [
          "Closure under inversion",
          "Polynomial relations"
        ],
        "Examples": [
          "Lower triangular matrices",
          "Hermitian matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_MatrixSubspaces.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Singular Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Theory",
      "definition": "A matrix subspace is singular if every matrix within it is singular, i.e., no element has a nonzero determinant.",
      "description": "Singular matrix subspaces arise in low-rank approximations, special matrix pencils, and structured operator families. They often encode degenerate behavior and limit the applicability of classical factorizations.",
      "properties": {
        "Goal": "Characterize spaces where invertibility is impossible.",
        "Applications": [
          "Rank-k matrix sets",
          "SVD truncation analysis",
          "Generalized eigenvalue pencils"
        ],
        "Methods": [
          "Nullspace analysis",
          "Subspace closure"
        ],
        "Examples": [
          "Rank-k matrices for k < n"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_Section2.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Polynomially Closed Matrix Subspace": {
      "type": "Concept",
      "domain": "Matrix Theory",
      "definition": "A matrix subspace V is polynomially closed if p(A) ∈ V for every A ∈ V and every polynomial p with scalar coefficients.",
      "description": "Polynomial closure ensures that structural properties of matrices are preserved under algebraic operations such as exponentiation, inversion, or functional calculus. This property is essential when applying iterative or polynomial-based algorithms within the subspace.",
      "properties": {
        "Goal": "Guarantee closure under matrix polynomial transformations.",
        "Applications": [
          "Iterative methods",
          "Matrix functions",
          "Structured inversion"
        ],
        "Methods": [
          "Polynomial evaluation",
          "Minimal polynomial arguments"
        ],
        "Examples": [
          "Hermitian matrices",
          "Complex symmetric matrices"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_MinimalPolynomials.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Closure of V₁V₂": {
      "type": "Concept",
      "domain": "Matrix Subspaces",
      "definition": "The closure of the product set V₁V₂ consists of all matrices that can be approximated arbitrarily well by products of matrices from subspaces V₁ and V₂.",
      "description": "The closure of V₁V₂ determines whether approximate factorizations exist even when exact ones do not. This concept plays a central role in understanding numerical stability, perturbation behavior, and feasibility of approximate matrix factorizations.",
      "properties": {
        "Goal": "Characterize attainable approximate factorizations.",
        "Applications": [
          "Approximate LU",
          "Low-rank approximations",
          "Structured matrix decompositions"
        ],
        "Methods": [
          "Topology of matrix spaces",
          "Perturbation analysis"
        ],
        "Examples": [
          "Every matrix is arbitrarily close to one with an LU factorization"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "LectureNotes_Factoring.pdf",
        "created_at": "2025-11-14",
        "version": "1.0"
      }
    },
    "Preconditioning": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "Preconditioning is the transformation of a linear system Ax = b into an equivalent system M^{-1}Ax = M^{-1}b (left preconditioning) or AM^{-1}y = b with x = M^{-1}y (right), where M ≈ A is a nonsingular matrix that is inexpensive to invert or solve with, designed to improve the convergence rate of iterative methods.",
      "description": "The goal is to cluster the eigenvalues of the preconditioned matrix away from zero, making Krylov subspace methods like CG, GMRES, or MINRES converge in significantly fewer iterations. Effective preconditioners balance accuracy (M close to A) with computational cost (O(n) or O(n log n) per application).",
      "properties": {
        "Goal": "Accelerate iterative solver convergence.",
        "Applications": [
          "Large sparse systems",
          "PDE solvers"
        ],
        "Methods": [
          "Incomplete LU",
          "Jacobi",
          "Multigrid",
          "Domain decomposition"
        ],
        "Examples": [
          "Left preconditioning: solve M y = c then A x = M y"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Jacobi Preconditioner": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "The Jacobi preconditioner is M = diag(A) + ωI, where diag(A) contains the diagonal entries of A and ω ∈ ℂ is an optional damping parameter (often ω = 0). It is the simplest splitting M = D, N = A - D.",
      "description": "Extremely cheap to apply (O(n) scaling) and parallelizable. Effective when A is diagonally dominant. Corresponds to the classical Jacobi iterative method. Often used as a baseline or building block in more sophisticated preconditioners.",
      "properties": {
        "Goal": "Damp high-frequency error components using only diagonal information.",
        "Applications": [
          "Smooth initial guess for multigrid",
          "Baseline for preconditioner comparison",
          "Diagonally dominant systems"
        ],
        "Methods": [
          "Extract diagonal",
          "Optional damping ω",
          "Inverse is element-wise division"
        ],
        "Examples": [
          "M_i = diag(A) + ωI with ω = 0 for standard Jacobi"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Incomplete LU Factorization": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "Incomplete LU (ILU) computes factors L̂ and Û such that L̂Û ≈ A with the same sparsity pattern as A (or a prescribed superset), dropping fill-in during Gaussian elimination to control memory and cost.",
      "description": "Widely used for general sparse matrices. Variants include ILU(0) (no fill), ILU(k) (level-k fill), and ILUT (threshold dropping). Provides a trade-off between robustness and efficiency. Often combined with reordering (e.g., RCM, nested dissection).",
      "properties": {
        "Goal": "Approximate LU factorization while preserving sparsity for use as preconditioner.",
        "Applications": [
          "Finite element systems",
          "CFD problems",
          "Circuit simulation"
        ],
        "Methods": [
          "Modified Gaussian elimination with drop tolerance",
          "Dual-threshold ILUT",
          "Level-of-fill ILU(k)"
        ],
        "Examples": [
          "ILU(0): drop all fill-in outside original nonzero pattern"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Sparse Approximate Inverse": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "A sparse approximate inverse (SAI) preconditioner computes a sparse matrix M such that ||I - MA||_F or ||I - AW||_F is minimized over all matrices W with a prescribed sparsity pattern, effectively approximating A^{-1} directly.",
      "description": "Application cost is O(nnz(M)) matrix-vector products. Excellent parallel performance due to explicit form. Often constructed via Frobenius norm minimization on independent columns or using QR factorizations of local submatrices.",
      "properties": {
        "Goal": "Construct explicit sparse approximation to A^{-1} for fast matrix-vector products",
        "Applications": [
          "Highly parallel architectures (GPU, many-core)",
          "Unstructured grids",
          "High-performance computing"
        ],
        "Methods": [
          "Frobenius norm minimization per column",
          "SPAID (sparse approximate inverse by distance)",
          "FSAI (factorized sparse approximate inverse)"
        ],
        "Examples": [
          "min_W ||AW - I||_F with W constrained to sparsity pattern of A^k"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Left Preconditioning": {
      "type": "Technique",
      "domain": "Numerical Linear Algebra",
      "definition": "Left preconditioning transforms Ax = b into M^{-1}Ax = M^{-1}b, preserving the solution x but altering the residual norm to ||M^{-1}(b - Ax)||.",
      "description": "Common in practice because it directly improves the convergence diagnostics used by Krylov methods (residual-based stopping criteria). Does not change the right-hand side in a way that affects eigenvalue clustering as strongly as right preconditioning.",
      "properties": {
        "Goal": "Improve convergence while keeping solution unchanged and monitoring preconditioned residuals.",
        "Applications": [
          "Standard choice in most libraries (PETSc, Trilinos)",
          "GMRES with ILU"
        ],
        "Methods": [
          "Apply M^{-1} to system matrix and RHS"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Right Preconditioning": {
      "type": "Technique",
      "domain": "Numerical Linear Algebra",
      "definition": "Right preconditioning transforms Ax = b into AM^{-1}y = b with x = M^{-1}y, preserving the residual norm ||b - Ax|| but solving for an intermediate variable y.",
      "description": "Often preferred when the preconditioner naturally approximates the inverse action from the right. Common in domain decomposition and multigrid methods. Requires extra step to recover x.",
      "properties": {
        "Goal": "Cluster eigenvalues of AM^{-1} while monitoring true residuals.",
        "Applications": [
          "Additive Schwarz",
          "Algebraic multigrid (AMG)",
          "When M approximates A from the right"
        ],
        "Methods": [
          "Solve AM^{-1}y = b, then x = M^{-1}y"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Splitting Methods": {
      "type": "Framework",
      "domain": "Iterative Methods",
      "definition": "A matrix splitting decomposes A = M - N where M is nonsingular and easy to invert. The iteration x^{k+1} = M^{-1}Nx^k + M^{-1}b converges if ρ(M^{-1}N) < 1.",
      "description": "Foundation of classical iterative methods (Jacobi, Gauss-Seidel, SOR) and modern preconditioning. The spectral radius of the iteration matrix B = M^{-1}N determines convergence rate.",
      "properties": {
        "Goal": "Construct fixed-point iterations via A = M - N with ρ(M^{-1}N) < 1.",
        "Applications": [
          "Stationary iterative methods",
          "Preconditioner design",
          "Convergence theory"
        ],
        "Methods": [
          "M = D (Jacobi)",
          "M = D+L (Gauss-Seidel)",
          "M = (D+ωL)/ω (SOR)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "09_preconditioning.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Cholesky Factorization": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "A decomposition of a Hermitian positive definite matrix A into A = R*R, where R is an upper triangular matrix with positive diagonal entries.",
      "description": "Cholesky factorization exploits symmetry and positive definiteness to reduce computational cost from O(n³) for general LU to approximately n³/3 flops while requiring only n²/2 storage. It is widely used in optimization, Monte Carlo simulations, and solving normal equations.",
      "properties": {
        "Goal": "Efficiently factorize Hermitian positive definite matrices with half the storage and one-third the operations of LU.",
        "Applications": [
          "Quadratic programming",
          "Kalman filtering",
          "Covariance decomposition",
          "Monte Carlo methods"
        ],
        "Methods": [
          "Block elimination",
          "Outer product form",
          "Inner product form"
        ],
        "Examples": [
          "A = R*R with R upper triangular and diag(R) > 0"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Positive Definite Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A Hermitian matrix A ∈ ℂ^{n×n} such that (Ax, x) > 0 for all nonzero x ∈ ℂ^n.",
      "description": "Positive definite matrices arise in energy minimization, covariance modeling, and elliptic PDEs. They guarantee unique Cholesky factors, stable inverses, and real positive eigenvalues. The property is preserved under congruence: M*A*M is positive definite if M is invertible.",
      "properties": {
        "Goal": "Model strictly convex quadratic forms and ensure numerical stability in factorizations.",
        "Applications": [
          "Optimization",
          "Statistics",
          "Physics simulations",
          "Control theory"
        ],
        "Methods": [
          "Sylvester's criterion",
          "Cholesky test",
          "Eigenvalue analysis"
        ],
        "Examples": [
          "Covariance matrices",
          "Hessians of convex functions",
          "A = R*R"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Sylvester Equation": {
      "type": "Concept",
      "domain": "Control Theory",
      "definition": "A matrix equation of the form AX − XB = C, where A, B, C are given matrices and X is unknown.",
      "description": "The Sylvester equation models linear system interconnections and appears in control design, model reduction, and eigenvalue assignment. When σ(A) ∩ σ(B) = ∅, it has a unique solution solvable in O(n³) via Schur triangulation or vectorization (kronecker form).",
      "properties": {
        "Goal": "Solve for coupling matrix X in interconnected linear systems or compute Lyapunov functions.",
        "Applications": [
          "Stability analysis",
          "Model order reduction",
          "Riccati equations",
          "Pole placement"
        ],
        "Methods": [
          "Schur method",
          "Hessenberg-Schur algorithm",
          "Bartels-Stewart",
          "Vectorization"
        ],
        "Examples": [
          "AX − XA* = −BB* (Lyapunov)",
          "AX − XB = C with diagonal A, B"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Discrete Fourier Transform (DFT)": {
      "type": "Method",
      "domain": "Signal Processing",
      "definition": "A linear transformation mapping a sequence x₀, ..., x_{n-1} to coefficients cⱼ = ∑ₖ xₖ ω^{jk}, where ω = e^{-2πi/n}, represented by the Vandermonde matrix Fₙ.",
      "description": "The DFT diagonalizes circulant matrices and enables fast convolution, filtering, and spectral analysis. The Fast Fourier Transform (FFT) computes it in O(n log n) using divide-and-conquer on power-of-two sizes.",
      "properties": {
        "Goal": "Decompose signals into frequency components and accelerate convolution/correlation",
        "Applications": [
          "Audio processing",
          "Image compression",
          "PDE solvers",
          "Polynomial multiplication"
        ],
        "Methods": [
          "Cooley-Tukey FFT",
          "Radix-2",
          "Split-radix",
          "Bluestein"
        ],
        "Examples": [
          "F₄ = [[1,1,1,1], [1,-1,1,-1], ...]",
          "FFT of length 2^l"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Fast Fourier Transform (FFT)": {
      "type": "Algorithm",
      "domain": "Numerical Algorithms",
      "definition": "An efficient algorithm for computing the DFT in O(n log n) operations by recursively splitting into even/odd indices when n is a power of two.",
      "description": "The Cooley-Tukey FFT reduces DFT complexity from O(n²) to O(n log n) using butterfly operations and twiddle factors. It is foundational in digital signal processing and enables real-time spectral analysis.",
      "properties": {
        "Goal": "Compute DFT with minimal arithmetic operations using recursive decomposition",
        "Applications": [
          "Spectral methods",
          "FFT-based convolution",
          "MRI reconstruction",
          "Audio synthesis"
        ],
        "Methods": [
          "Decimation-in-time",
          "Decimation-in-frequency",
          "Bit-reversal",
          "In-place computation"
        ],
        "Examples": [
          "Radix-2 butterfly: yⱼ = x_{2j} + ω^j x_{2j+1}"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Schur Triangulation": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "A similarity transformation A = Q T Q* where T is upper triangular and Q is unitary, revealing eigenvalues on the diagonal of T.",
      "description": "Schur form is the foundation for robust eigenvalue computation and solving Sylvester equations. The QR-based Francis algorithm computes it in O(n³) with high backward stability. Real Schur form handles complex conjugate pairs.",
      "properties": {
        "Goal": "Reduce matrix to triangular form under unitary similarity to expose eigenvalues and enable block algorithms.",
        "Applications": [
          "Eigenvalue problems",
          "Sylvester solvers",
          "Matrix functions",
          "Control theory"
        ],
        "Methods": [
          "Francis QR algorithm",
          "Hessenberg reduction",
          "Double shift"
        ],
        "Examples": [
          "A = Q T Q* with T upper triangular, diag(T) = eigenvalues"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "07_using_the_structure_in_computations_Cholesky_factorization_Sylvester_equation_and_FFT.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Iterative Methods": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Iterative methods are algorithms that generate a sequence of approximations x_j to the solution x of a linear system Ax = b, starting from an initial guess and refining it until convergence.",
      "description": "They are preferred for large sparse systems where direct methods like Gaussian elimination are O(n^3) and computationally expensive, with per-iteration costs often O(n^2) or less, such as O(n) for sparse or O(n log n) with structured matrices.",
      "properties": {
        "Goal": "Solve large linear systems efficiently without full factorization.",
        "Applications": [
          "PDE discretizations",
          "Optimization",
          "Eigenproblems"
        ],
        "Methods": [
          "Krylov subspace methods",
          "Conjugate gradient",
          "GMRES",
          "Preconditioned iterations"
        ],
        "Examples": [
          "x_{j+1} = x_j + correction",
          "Convergence when ||r_j|| small"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Linear System": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "A linear system is an equation of the form Ax = b where A is an n x n matrix, x is the unknown vector, and b is the right-hand side vector.",
      "description": "For large n (e.g., 10^4 to 10^8), iterative methods are used due to high O(n^3) cost of direct solvers, especially when A is sparse or structured.",
      "properties": {
        "Goal": "Find x such that Ax = b.",
        "Applications": [
          "Scientific simulations",
          "Machine learning",
          "Engineering"
        ],
        "Methods": [
          "Direct (LU, QR)",
          "Iterative (CG, GMRES)"
        ],
        "Examples": [
          "A in C^{n x n}, b in C^n"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Arnoldi Process": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "The Arnoldi process builds an orthonormal basis Qⱼ for the Krylov subspace Kⱼ(A; b) and a Hessenberg matrix Hⱼ such that A Qⱼ = Qⱼ₊₁ Ĥⱼ.",
      "description": "It uses Gram–Schmidt-like orthogonalisation to compute basis vectors qₖ recursively, enabling reduced-order projections for solving systems or eigenvalues.",
      "properties": {
        "Goal": "Orthogonalise Krylov basis for stable computations.",
        "Applications": [
          "GMRES",
          "Eigenvalue solvers",
          "Matrix functions"
        ],
        "Methods": [
          "Recursive computation: hₖ,ₖ₋₁ qₖ = A qₖ₋₁ − Σ hₗ,ₖ₋₁ qₗ"
        ],
        "Examples": [
          "Qⱼ₊₁ Ĥⱼ = A Qⱼ"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "GMRES": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Generalised Minimal Residual (GMRES) is an iterative method that finds xⱼ in x₀ + Kⱼ(A; r₀) minimising ‖b − A xⱼ‖₂ for nonsymmetric systems.",
      "description": "It uses Arnoldi to build the basis and solves a least-squares problem with the Hessenberg matrix at each step, restarting when j is large.",
      "properties": {
        "Goal": "Minimise residual norm over Krylov subspace.",
        "Applications": [
          "Nonsymmetric linear systems",
          "PDE solvers"
        ],
        "Methods": [
          "Arnoldi orthogonalisation",
          "Least squares on Ĥⱼ y − α e₁"
        ],
        "Examples": [
          "xⱼ = Qⱼ yⱼ where yⱼ minimises ‖Ĥⱼ y − α e₁‖"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Conjugate Gradient Method": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "The Conjugate Gradient (CG) method solves symmetric positive definite systems Ax = b by minimising the A-norm error ‖x − xⱼ‖ₐ over the Krylov subspace.",
      "description": "It generates A-conjugate search directions implicitly via a three-term recurrence, equivalent to Lanczos for symmetric matrices, with optimal polynomial approximation properties.",
      "properties": {
        "Goal": "Minimise quadratic form (x, A x)/2 − (b, x).",
        "Applications": [
          "SPD linear systems",
          "Optimisation (Newton-CG)"
        ],
        "Methods": [
          "Conjugate directions",
          "Residual orthogonalisation"
        ],
        "Examples": [
          "xⱼ₊₁ = xⱼ + αⱼ pⱼ with pⱼ₊₁ = rⱼ₊₁ + βⱼ pⱼ"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Residual Norm": {
      "type": "Metric",
      "domain": "Numerical Analysis",
      "definition": "The residual norm ‖rⱼ‖ = ‖b − A xⱼ‖ measures how well the approximate solution xⱼ satisfies the system Ax = b.",
      "description": "In iterative methods, residuals decrease monotonically in GMRES, and are used to test convergence.",
      "properties": {
        "Goal": "Quantify approximation error in equation satisfaction.",
        "Applications": [
          "Convergence testing",
          "Stopping criteria"
        ],
        "Methods": [
          "Euclidean norm",
          "Relative residual"
        ],
        "Examples": [
          "‖rⱼ₊₁‖ ≤ ‖rⱼ‖ in GMRES"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "A-Norm Error": {
      "type": "Metric",
      "domain": "Numerical Analysis",
      "definition": "The A-norm error ‖x − xⱼ‖ₐ = ( (x − xⱼ), A (x − xⱼ) )^{1/2} measures the error in the energy norm for SPD A.",
      "description": "CG minimises this norm over the Krylov subspace, relating to the quadratic form minimised in the system.",
      "properties": {
        "Goal": "Quantify solution error in energy sense.",
        "Applications": [
          "CG convergence analysis",
          "Variational problems"
        ],
        "Methods": [
          "Defined via inner product (x, A y)"
        ],
        "Examples": [
          "Min ‖x − xⱼ‖ₐ in CG"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Polynomial Approximation": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "Polynomial approximation in iterative methods views xⱼ = pⱼ₋₁(A) b as a polynomial in A applied to b, minimising residuals or errors via min-max problems over polynomials.",
      "description": "Convergence bounds use Chebyshev or other polynomials to estimate rates based on eigenvalue distribution.",
      "properties": {
        "Goal": "Approximate A⁻¹ b via polynomials in A.",
        "Applications": [
          "Convergence analysis",
          "Accelerated methods"
        ],
        "Methods": [
          "Min-max over deg ≤ j-1",
          "Chebyshev acceleration"
        ],
        "Examples": [
          "min_{deg p ≤ j-1, p(0)=1} max_λ |p(λ)|"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Hessenberg Matrix": {
      "type": "Concept",
      "domain": "Numerical Analysis",
      "definition": "A Hessenberg matrix H is upper triangular except for the subdiagonal, arising in Arnoldi as the projection of A onto the Krylov basis.",
      "description": "It simplifies least-squares solves in GMRES and eigenvalue computations.",
      "properties": {
        "Goal": "Reduce matrix for efficient projections.",
        "Applications": [
          "GMRES minimisation",
          "QR algorithm"
        ],
        "Methods": [
          "From Arnoldi: Ĥⱼ with subdiagonal"
        ],
        "Examples": [
          "Hⱼ tridiagonal in Lanczos"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "08_iterative_methods_for_linear_systems.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Orthogonal Complement": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The orthogonal complement of a subspace V is the set of vectors perpendicular to all elements in V, denoted V^⊥.",
      "description": "For projections, if P is orthogonal, then R(P) ⊥ R(I - P), ensuring the decomposition is orthogonal.",
      "properties": {
        "Goal": "Decompose space into perpendicular subspaces.",
        "Applications": [
          "Gram-Schmidt",
          "Least squares",
          "Spectral methods"
        ],
        "Methods": [
          "Dot product zero condition",
          "Null space of transpose"
        ],
        "Examples": [
          "R(I - P) as complement of R(P)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Gershgorin Circle Theorem": {
      "type": "Theorem",
      "domain": "Matrix Analysis",
      "definition": "Every eigenvalue of A ∈ ℂ^{n×n} lies in at least one disk Dⱼ = {z : |z − aⱼⱼ| ≤ Rⱼ}, where Rⱼ = Σ_{l≠j} |aⱼₗ|}.",
      "description": "Provides eigenvalue localization without computation. Disks are centered at diagonal entries with radii equal to off-diagonal row sums. Useful for bounding spectral radius and detecting diagonal dominance.",
      "properties": {
        "Goal": "Locate eigenvalues in complex plane using only matrix entries.",
        "Applications": [
          "Convergence analysis",
          "Error bounding",
          "Preconditioning design"
        ],
        "Methods": [
          "Row-sum computation",
          "Union of disks",
          "Refinement via similarity"
        ],
        "Examples": [
          "Strictly diagonally dominant → eigenvalues in disjoint disks"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Power Iteration": {
      "type": "Method",
      "domain": "Eigenvalue Algorithms",
      "definition": "An iterative method that computes the dominant eigenvalue and eigenvector by repeatedly applying A to a vector and normalizing: q^{(k)} = A q^{(k-1)} / ||A q^{(k-1)}||, λ^{(k)} = (A q^{(k)}, q^{(k)}).",
      "description": "Converges linearly to the eigenvector corresponding to the largest-magnitude eigenvalue if |λ₁| > |λ₂| ≥ ... ≥ |λₙ|. Convergence rate is |λ₂/λ₁|. Inverse iteration targets smallest or shifted eigenvalues.",
      "properties": {
        "Goal": "Find dominant eigenpair with minimal storage and simple operations.",
        "Applications": [
          "PageRank",
          "Principal Component Analysis",
          "Vibration modes"
        ],
        "Methods": [
          "Rayleigh quotient",
          "Normalization",
          "Deflation",
          "Shift-and-invert"
        ],
        "Examples": [
          "q^{(k)} ≈ x₁ + O((λ₂/λ₁)^k)"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Householder Reflection": {
      "type": "Transformation",
      "domain": "Numerical Linear Algebra",
      "definition": "A unitary matrix H = I − 2vv*/(v*v) that reflects a vector x across the hyperplane perpendicular to v, mapping x to σe₁.",
      "description": "Used in QR factorization and Hessenberg reduction to introduce zeros below the subdiagonal. Numerically stable and requires O(n) operations per reflection. Essential for implicit QR algorithm.",
      "properties": {
        "Goal": "Zero out selected entries via unitary similarity while preserving eigenvalues.",
        "Applications": [
          "QR decomposition",
          "Hessenberg form",
          "Tridiagonalization"
        ],
        "Methods": [
          "Sign choice for stability",
          "WY representation",
          "Blocked Householder"
        ],
        "Examples": [
          "H x = −sign(x₁) ||x|| e₁"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "QR Algorithm": {
      "type": "Algorithm",
      "domain": "Eigenvalue Computation",
      "definition": "An iterative method that computes the Schur form via repeated QR decompositions: A_{k+1} = R_k Q_k, with A_0 = A. With shifts, converges cubically to upper triangular form.",
      "description": "The de facto standard for dense eigenvalue problems. Implicit version uses Householder/Bulge chasing to reduce cost from O(n³) per iteration to O(n). Francis shift accelerates convergence.",
      "properties": {
        "Goal": "Compute all eigenvalues (and optionally eigenvectors) via unitary similarity to triangular form.",
        "Applications": [
          "MATLAB eig",
          "LAPACK",
          "Control theory",
          "PDE solvers"
        ],
        "Methods": [
          "Hessenberg reduction",
          "Francis double shift",
          "Deflation",
          "Balancing"
        ],
        "Examples": [
          "A_{k+1} = Q_k* A_k Q_k"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Generalized Eigenvalue Problem": {
      "type": "Problem",
      "domain": "Numerical Linear Algebra",
      "definition": "Find λ ∈ ℂ and nonzero x ∈ ℂⁿ such that Ax = λBx, with A, B ∈ ℂ^{n×n}.",
      "description": "Arises in structural dynamics, control, and Markov chains. Reduced to standard form if B invertible (M = B⁻¹A). QZ algorithm generalizes QR using unitary transformations to triangularize both matrices.",
      "properties": {
        "Goal": "Solve coupled systems or weighted eigenvalue problems.",
        "Applications": [
          "Vibration with constraints",
          "Markov chain stationary distribution",
          "Optimal control"
        ],
        "Methods": [
          "QZ algorithm",
          "Cholesky + standard EVP",
          "Shift-and-invert"
        ],
        "Examples": [
          "Ax = λBx with B positive definite"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Field of Values": {
      "type": "Concept",
      "domain": "Matrix Analysis",
      "definition": "The set F(A) = {x*Ax : x ∈ ℂⁿ, ||x||=1}, also known as the numerical range.",
      "description": "Convex, compact set containing all eigenvalues. Bounds spectral radius and condition number. For normal matrices, F(A) is the convex hull of eigenvalues.",
      "properties": {
        "Goal": "Characterize operator behavior beyond eigenvalues.",
        "Applications": [
          "Stability analysis",
          "Convergence of iterations",
          "Pseudospectrum approximation"
        ],
        "Methods": [
          "Rayleigh quotient",
          "Hausdorff-Toeplitz theorem",
          "Discretization"
        ],
        "Examples": [
          "F(A) = conv(σ(A)) if A normal"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "10_eigenvalue_problems_and_functions_of_matrices.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "LU Factorization with Partial Pivoting": {
      "type": "Method",
      "domain": "Numerical Linear Algebra",
      "definition": "LU factorization with partial pivoting decomposes a matrix A ∈ ℂ^{n×n} into PA = LU, where P is a permutation matrix, L is unit lower triangular with |l_{ij}| ≤ 1 for i > j, and U is upper triangular. Partial pivoting selects the largest absolute entry in the current column as the pivot to minimize numerical instability.",
      "description": "This algorithm enhances the stability of Gaussian elimination by row permutations to avoid small pivots. It is the standard method for solving linear systems Ax = b in practice, balancing computational cost (O(n³)) with robustness against round-off errors in floating-point arithmetic.",
      "properties": {
        "Goal": "Stable triangular factorization of a matrix for solving linear systems and computing inverses.",
        "Applications": [
          "Solving Ax = b",
          "Matrix inversion",
          "Determinant computation",
          "Condition number estimation"
        ],
        "Methods": [
          "Gaussian elimination with row pivoting",
          "In-place storage using single array",
          "Compact WY representation for L"
        ],
        "Examples": [
          "4×4 matrix example with pivots 8, 17/4, -6/7, 2 showing growth control"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Partial Pivoting": {
      "type": "Strategy",
      "domain": "Numerical Linear Algebra",
      "definition": "A pivoting strategy in Gaussian elimination that, at step k, permutes rows so that the entry of largest magnitude in column k (from row k to n) becomes the pivot, ensuring |pivot| = max_{i≥k} |a_{ik}|.",
      "description": "Partial pivoting prevents division by small pivots, reducing amplification of round-off errors. It guarantees that all subdiagonal entries in L satisfy |l_{ij}| ≤ 1, bounding the growth factor ρ ≤ 2^{n-1} in theory, though typically much smaller in practice.",
      "properties": {
        "Goal": "Minimize numerical error propagation during elimination by choosing largest available pivot.",
        "Applications": [
          "LU factorization",
          "Linear system solving",
          "Matrix decomposition in finite precision"
        ],
        "Methods": [
          "Row interchange before elimination step",
          "Column scanning for max absolute value"
        ],
        "Examples": [
          "P1 swaps rows to bring largest entry to diagonal position"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Permutation Matrix": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A permutation matrix P is a square matrix with exactly one 1 in each row and column and 0s elsewhere. Multiplying PA permutes the rows of A; AP permutes columns.",
      "description": "In LU with partial pivoting, P represents the cumulative row interchanges. Since P^{-1} = P^T = P^*, it preserves norms: ||Px|| = ||x||. The final factorization is PA = LU.",
      "properties": {
        "Goal": "Represent row or column reordering in matrix factorizations.",
        "Applications": [
          "Pivoting in LU",
          "Reordering for sparsity",
          "Graph relabeling"
        ],
        "Methods": [
          "Identity with swapped rows",
          "Product of elementary permutation matrices"
        ],
        "Examples": [
          "P = [0 1; 1 0] swaps rows 1 and 2"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Growth Factor": {
      "type": "Metric",
      "domain": "Numerical Linear Algebra",
      "definition": "The growth factor rho in LU factorization with partial pivoting is defined as rho = max_{i,j} |u_{ij}| / max_{i,j} |a_{ij}|, measuring the largest entry in U relative to the original matrix A.",
      "description": "Controls backward stability: computed factors satisfy L_hat U_hat = P A + delta A with ||delta A|| / ||A|| = O(rho epsilon_machine). Partial pivoting keeps rho moderate in practice, though worst-case rho = 2^{n-1} is possible.",
      "properties": {
        "Goal": "Quantify element growth during Gaussian elimination to assess numerical stability.",
        "Applications": [
          "Backward error analysis",
          "Condition estimation",
          "Pivoting strategy evaluation"
        ],
        "Methods": [
          "Ratio of max |u_{ij}| to max |a_{ij}|",
          "Monitored during factorization"
        ],
        "Examples": [
          "Wilkinson's matrix gives rho approx 2^{n-1}"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Condition Number": {
      "type": "Metric",
      "domain": "Numerical Linear Algebra",
      "definition": "The condition number κ(W) of a nonsingular matrix W is κ(W) = ||W|| ⋅ ||W^{-1}||, with respect to any consistent matrix norm. For the 1-norm or ∞-norm, κ₁(W) = σ₁/σₙ where σ are singular values.",
      "description": "Measures sensitivity of linear system solution to perturbations. Large κ implies ill-conditioned system: small changes in input cause large output changes. In LU context, related to pivot size and growth.",
      "properties": {
        "Goal": "Quantify sensitivity of Ax = b to perturbations in A or b.",
        "Applications": [
          "Error bounds in linear solvers",
          "Preconditioning design",
          "Numerical stability analysis"
        ],
        "Methods": [
          "SVD-based: κ₂ = σ_max / σ_min",
          "1-norm estimation via Hager's method"
        ],
        "Examples": [
          "κ(A) ≈ 10^k ⇒ lose k digits of accuracy"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Gaussian Elimination": {
      "type": "Method",
      "domain": "Linear Algebra",
      "definition": "Gaussian elimination transforms a linear system Ax = b into upper triangular form Ux = c via row operations: adding multiples of one row to another. With pivoting, it forms the basis of LU factorization.",
      "description": "Core algorithm for solving linear systems. Without pivoting, unstable for small pivots. With partial pivoting, becomes robust standard method. Can be expressed as sequence of rank-1 updates or multiplier storage in L.",
      "properties": {
        "Goal": "Reduce system to triangular form for back substitution.",
        "Applications": [
          "Linear system solving",
          "Matrix factorization",
          "Determinant via product of diagonals"
        ],
        "Methods": [
          "Forward elimination",
          "Back substitution",
          "Pivoting variants"
        ],
        "Examples": [
          "4×4 system reduced step-by-step with P, L, U shown"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "06_computing_the_LU_factorization_with_partial_pivoting.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Matrix Product V1 V2": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The matrix product V1 V2 represents the set of all matrices formed by multiplying elements from subspaces V1 and V2, i.e., {V1 V2 : V1 ∈ V1, V2 ∈ V2}.",
      "description": "This construct is used to approximate or factor large matrices A by finding subspaces such that A lies in V1 V2, enabling low-parameter representations.",
      "properties": {
        "Goal": "Represent matrices as products of subspace elements for factorization.",
        "Applications": [
          "Matrix approximation",
          "Low-rank factoring",
          "Compression"
        ],
        "Methods": [
          "Subspace identification",
          "Optimization over subspaces"
        ],
        "Examples": [
          "A ≈ V1 V2 for V1, V2 low-dimensional"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Range of Projection": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "The range R(P) of a projection P is the set {y : y = Px for some x}, representing the subspace onto which P projects.",
      "description": "For orthogonal projections, R(P) is perpendicular to R(I - P), forming an orthogonal decomposition of the space.",
      "properties": {
        "Goal": "Define the projected subspace.",
        "Applications": [
          "Subspace identification",
          "Dimensionality reduction"
        ],
        "Methods": [
          "Column span of P",
          "Fixed points of P"
        ],
        "Examples": [
          "R(P) = span{columns of P}"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Algorithmic Factoring": {
      "type": "Method",
      "domain": "Numerical Analysis",
      "definition": "Algorithmic factoring refers to computational methods for decomposing matrices into structured forms like products of subspaces or triangular factors.",
      "description": "It leverages Krylov subspaces and projections to factor matrices efficiently, especially for large-scale problems.",
      "properties": {
        "Goal": "Factor matrices using algorithmic techniques.",
        "Applications": [
          "Large linear systems",
          "Eigenproblems",
          "Approximations"
        ],
        "Methods": [
          "Subspace iteration",
          "Projection-based factoring"
        ],
        "Examples": [
          "A = V1 W^{-1} using subspaces"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Invariant Subspace": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "An invariant subspace W for matrix A satisfies A W ⊆ W, meaning A maps W into itself.",
      "description": "Invariant subspaces are key in spectral theory and factoring, often identified via Krylov methods or projections.",
      "properties": {
        "Goal": "Find subspaces stable under matrix action.",
        "Applications": [
          "Eigen decomposition",
          "Schur form",
          "Model reduction"
        ],
        "Methods": [
          "Subspace iteration",
          "Arnoldi process"
        ],
        "Examples": [
          "Eigenvector spans 1D invariant subspace"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    },
    "Square Matrix A": {
      "type": "Concept",
      "domain": "Linear Algebra",
      "definition": "A square matrix A ∈ ℂ^{n×n} is a matrix with equal rows and columns, central to linear transformations and eigenvalue problems.",
      "description": "In factoring contexts, A is decomposed using subspaces, projections, or iterations for computational efficiency.",
      "properties": {
        "Goal": "Represent linear operators on finite-dimensional spaces.",
        "Applications": [
          "Systems of equations",
          "Transformations",
          "Spectral analysis"
        ],
        "Methods": [
          "Factorization",
          "Iteration",
          "Diagonalization"
        ],
        "Examples": [
          "A with complex entries"
        ]
      },
      "metadata": {
        "created_by": "system",
        "source": "05_factoring_algorithmically.pdf",
        "created_at": "2025-11-10",
        "version": "1.0"
      }
    }
  },
  "edges": [
    {
      "source": "Singular Value Decomposition",
      "type": "uses",
      "target": "Unitary Matrix"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "uses",
      "target": "Singular Value"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "enables",
      "target": "Low Rank Approximation"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "related_to",
      "target": "Principal Component Analysis"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "solves",
      "target": "Matrix Approximation Problem"
    },
    {
      "source": "Singular Value",
      "type": "part_of",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Singular Value",
      "type": "computed_from",
      "target": "Eigenvalue"
    },
    {
      "source": "Singular Value",
      "type": "determines",
      "target": "Matrix Rank"
    },
    {
      "source": "Singular Value",
      "type": "equals",
      "target": "Operator Norm"
    },
    {
      "source": "Unitary Matrix",
      "type": "used_in",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Unitary Matrix",
      "type": "preserves",
      "target": "Euclidean Norm"
    },
    {
      "source": "Unitary Matrix",
      "type": "part_of",
      "target": "QR Decomposition"
    },
    {
      "source": "Unitary Matrix",
      "type": "generalizes",
      "target": "Orthogonal Matrix"
    },
    {
      "source": "Low Rank Approximation",
      "type": "solved_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Low Rank Approximation",
      "type": "uses",
      "target": "Frobenius Norm"
    },
    {
      "source": "Low Rank Approximation",
      "type": "enables",
      "target": "Principal Component Analysis"
    },
    {
      "source": "Low Rank Approximation",
      "type": "minimizes",
      "target": "Reconstruction Error"
    },
    {
      "source": "Frobenius Norm",
      "type": "used_in",
      "target": "Low Rank Approximation"
    },
    {
      "source": "Frobenius Norm",
      "type": "equivalent_to",
      "target": "Euclidean Norm"
    },
    {
      "source": "Frobenius Norm",
      "type": "sum_of",
      "target": "Singular Value Squared"
    },
    {
      "source": "Frobenius Norm",
      "type": "invariant_under",
      "target": "Unitary Transformation"
    },
    {
      "source": "Operator Norm",
      "type": "equals",
      "target": "Largest Singular Value"
    },
    {
      "source": "Operator Norm",
      "type": "part_of",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Operator Norm",
      "type": "measures",
      "target": "Linear Transformation Stretch"
    },
    {
      "source": "Matrix Function",
      "type": "computed_via",
      "target": "Newton's Iteration"
    },
    {
      "source": "Matrix Function",
      "type": "applied_to",
      "target": "Square Matrix A"
    },
    {
      "source": "Matrix Function",
      "type": "used_in",
      "target": "Eigenvalue Problems"
    },
    {
      "source": "Matrix Function",
      "type": "related_to",
      "target": "Matrix Square Root"
    },
    {
      "source": "Matrix Square Root",
      "type": "instance_of",
      "target": "Matrix Function"
    },
    {
      "source": "Matrix Square Root",
      "type": "computed_by",
      "target": "Newton's Iteration for Square Root"
    },
    {
      "source": "Matrix Square Root",
      "type": "requires",
      "target": "Positive Definite A (for principal)"
    },
    {
      "source": "Matrix Square Root",
      "type": "related_to",
      "target": "Eigenvalue Decomposition"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "computes",
      "target": "Matrix Square Root"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "variant_of",
      "target": "Newton's Method"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "requires",
      "target": "Matrix Inversion"
    },
    {
      "source": "Newton's Iteration for Square Root",
      "type": "converges_to",
      "target": "sqrt(A)"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "solved_by",
      "target": "Iterative Methods"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "shifted_as",
      "target": "A - μ I"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "spectrum",
      "target": "Λ(A)"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "related_to",
      "target": "Singular Value Problem"
    },
    {
      "source": "Spectral Shift",
      "type": "transforms",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Spectral Shift",
      "type": "enables",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Spectral Shift",
      "type": "used_in",
      "target": "Inverse Iteration"
    },
    {
      "source": "Spectral Shift",
      "type": "estimates",
      "target": "Approximate μ"
    },
    {
      "source": "Positive Definite Shift",
      "type": "variant_of",
      "target": "Spectral Shift"
    },
    {
      "source": "Positive Definite Shift",
      "type": "ensures",
      "target": "Positive Definiteness"
    },
    {
      "source": "Positive Definite Shift",
      "type": "used_for",
      "target": "Conjugate Gradient"
    },
    {
      "source": "Positive Definite Shift",
      "type": "based_on",
      "target": "Eigenvalue Estimate ˆμ"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "of",
      "target": "Square Matrix A"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "shifted_by",
      "target": "Spectral Shift"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "estimated_via",
      "target": "Iterative Methods"
    },
    {
      "source": "Spectrum Λ(A)",
      "type": "related_to",
      "target": "Jordan Form"
    },
    {
      "source": "Matrix Computations",
      "type": "part_of",
      "target": "Numerical Analysis"
    },
    {
      "source": "Matrix Computations",
      "type": "used_in",
      "target": "Scientific Computing"
    },
    {
      "source": "Matrix Computations",
      "type": "depends_on",
      "target": "Linear Algebra"
    },
    {
      "source": "Matrix Computations",
      "type": "related_to",
      "target": "Partial Differential Equations"
    },
    {
      "source": "Matrix Computations",
      "type": "related_to",
      "target": "Matrix Factorization"
    },
    {
      "source": "Inner Product",
      "type": "generalization_of",
      "target": "Dot Product"
    },
    {
      "source": "Inner Product",
      "type": "used_in",
      "target": "Gram-Schmidt Process"
    },
    {
      "source": "Inner Product",
      "type": "foundation_for",
      "target": "Norm"
    },
    {
      "source": "Inner Product",
      "type": "related_to",
      "target": "Orthogonality"
    },
    {
      "source": "Computational Complexity (Matrix Multiplication)",
      "type": "applies_to",
      "target": "Matrix Multiplication"
    },
    {
      "source": "Computational Complexity (Matrix Multiplication)",
      "type": "contrasts_with",
      "target": "Subcubic Algorithms"
    },
    {
      "source": "Computational Complexity (Matrix Multiplication)",
      "type": "related_to",
      "target": "Strassen Algorithm"
    },
    {
      "source": "Matrix Computations",
      "type": "includes",
      "target": "LU Factorization"
    },
    {
      "source": "Matrix Computations",
      "type": "includes",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Matrix Computations",
      "type": "foundation_for",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Matrix Computations",
      "type": "related_to",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "LU Factorization",
      "type": "part_of",
      "target": "Matrix Factorization"
    },
    {
      "source": "LU Factorization",
      "type": "used_in",
      "target": "Linear System Solving"
    },
    {
      "source": "LU Factorization",
      "type": "approximated_by",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "LU Factorization",
      "type": "related_to",
      "target": "Gram-Schmidt Orthogonalization"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "part_of",
      "target": "Matrix Factorization"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "alternative_to",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "related_to",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Singular Value Decomposition",
      "type": "used_for",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "solved_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "related_to",
      "target": "LU Factorization"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "approximated_by",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "used_in",
      "target": "Partial Differential Equations"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "alternative_to",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "used_for",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "related_to",
      "target": "Gram-Schmidt Orthogonalization"
    },
    {
      "source": "Product of Matrix Subspaces",
      "type": "applied_to",
      "target": "Triangular Matrices"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "used_in",
      "target": "QR Factorization"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "related_to",
      "target": "LU Factorization"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "applied_in",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Gram-Schmidt Orthogonalization",
      "type": "foundation_for",
      "target": "Orthogonal Matrices"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "achieved_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "alternative_via",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "used_in",
      "target": "Matrix Factorization"
    },
    {
      "source": "Low-Rank Approximation",
      "type": "measures",
      "target": "Frobenius Norm"
    },
    {
      "source": "Hermitian Matrix",
      "type": "special_case_of",
      "target": "Symmetric Matrix (real case)"
    },
    {
      "source": "Hermitian Matrix",
      "type": "approximated_by",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Hermitian Matrix",
      "type": "related_to",
      "target": "Unitary Matrix"
    },
    {
      "source": "Hermitian Matrix",
      "type": "decomposed_by",
      "target": "Singular Value Decomposition"
    },
    {
      "source": "Matrix Subspace",
      "type": "generalization_of",
      "target": "Vector Subspace"
    },
    {
      "source": "Matrix Subspace",
      "type": "used_in",
      "target": "Krylov Subspace"
    },
    {
      "source": "Matrix Subspace",
      "type": "foundation_for",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "Matrix Subspace",
      "type": "related_to",
      "target": "Invariant Subspace"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "subtype_of",
      "target": "Matrix Subspace"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "enables",
      "target": "LU Factorization"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "defines",
      "target": "Inv(V)"
    },
    {
      "source": "Nonsingular Matrix Subspace",
      "type": "related_to",
      "target": "GL(n,ℂ)"
    },
    {
      "source": "Inv(V)",
      "type": "derived_from",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "Inv(V)",
      "type": "is_a",
      "target": "Matrix Subspace"
    },
    {
      "source": "Inv(V)",
      "type": "preserves_under",
      "target": "Similarity Transformation"
    },
    {
      "source": "Inv(V)",
      "type": "used_in",
      "target": "LU Factorization"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "requires",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "uses",
      "target": "Inv(V)"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "extends",
      "target": "LU Factorization"
    },
    {
      "source": "LU Factorization within Subspace",
      "type": "related_to",
      "target": "Schur Complement"
    },
    {
      "source": "Krylov Subspace",
      "type": "subtype_of",
      "target": "Matrix Subspace"
    },
    {
      "source": "Krylov Subspace",
      "type": " invariant_under",
      "target": "Power Iteration"
    },
    {
      "source": "Krylov Subspace",
      "type": "foundation_for",
      "target": "GMRES"
    },
    {
      "source": "Krylov Subspace",
      "type": "bounded_by",
      "target": "Minimal Polynomial"
    },
    {
      "source": "Matrix Polynomials",
      "type": "generalizes",
      "target": "Scalar Polynomial"
    },
    {
      "source": "Matrix Polynomials",
      "type": "satisfies",
      "target": "Cayley-Hamilton Theorem"
    },
    {
      "source": "Matrix Polynomials",
      "type": "defines",
      "target": "Minimal Polynomial"
    },
    {
      "source": "Matrix Polynomials",
      "type": "preserves",
      "target": "Matrix Subspace"
    },
    {
      "source": "Projection Operator",
      "type": "special_case_of",
      "target": "Idempotent Operator"
    },
    {
      "source": "Projection Operator",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Projection Operator",
      "type": "orthogonal_to",
      "target": "I−P"
    },
    {
      "source": "Projection Operator",
      "type": "related_to",
      "target": "Oblique Projection"
    },
    {
      "source": "Sparsity Structure",
      "type": "defines",
      "target": "Standard Matrix Subspace"
    },
    {
      "source": "Sparsity Structure",
      "type": "used_in",
      "target": "Incomplete LU Factorization"
    },
    {
      "source": "Sparsity Structure",
      "type": "related_to",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Standard Matrix Subspace",
      "type": "based_on",
      "target": "Sparsity Structure"
    },
    {
      "source": "Standard Matrix Subspace",
      "type": "used_in",
      "target": "LU Factorization within Subspace"
    },
    {
      "source": "Standard Matrix Subspace",
      "type": "related_to",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Orthogonal Projector (Matrix Subspace)",
      "type": "projects_onto",
      "target": "Matrix Subspace"
    },
    {
      "source": "Orthogonal Projector (Matrix Subspace)",
      "type": "used_in",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Orthogonal Projector (Matrix Subspace)",
      "type": "related_to",
      "target": "Projection Operator"
    },
    {
      "source": "Invertible Matrix Subspace",
      "type": "opposes",
      "target": "Singular Matrix Subspace"
    },
    {
      "source": "Invertible Matrix Subspace",
      "type": "related_to",
      "target": "Nonsingular Matrix Subspace"
    },
    {
      "source": "Invertible Matrix Subspace",
      "type": "enables",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Singular Matrix Subspace",
      "type": "opposes",
      "target": "Invertible Matrix Subspace"
    },
    {
      "source": "Singular Matrix Subspace",
      "type": "subset_of",
      "target": "Matrix Subspace"
    },
    {
      "source": "Singular Matrix Subspace",
      "type": "related_to",
      "target": "Low Rank Approximation"
    },
    {
      "source": "Polynomially Closed Matrix Subspace",
      "type": "supports",
      "target": "Invertible Matrix Subspace"
    },
    {
      "source": "Polynomially Closed Matrix Subspace",
      "type": "related_to",
      "target": "Matrix Polynomials"
    },
    {
      "source": "Polynomially Closed Matrix Subspace",
      "type": "enables",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Closure of V₁V₂",
      "type": "extends",
      "target": "Product of Matrix Subspaces"
    },
    {
      "source": "Closure of V₁V₂",
      "type": "used_in",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Closure of V₁V₂",
      "type": "related_to",
      "target": "Matrix Approximation Problem"
    },
    {
      "source": "Preconditioning",
      "type": "improves",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "Preconditioning",
      "type": "improves",
      "target": "GMRES"
    },
    {
      "source": "Preconditioning",
      "type": "requires",
      "target": "Approximate Inverse"
    },
    {
      "source": "Preconditioning",
      "type": "reduces",
      "target": "Spectral Radius"
    },
    {
      "source": "Preconditioning",
      "type": "clusters",
      "target": "Eigenvalues"
    },
    {
      "source": "Preconditioning",
      "type": "used_with",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "is_a",
      "target": "Diagonal Preconditioner"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "part_of",
      "target": "Splitting Methods"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "used_in",
      "target": "Jacobi Iteration"
    },
    {
      "source": "Jacobi Preconditioner",
      "type": "simpler_than",
      "target": "Gauss-Seidel Preconditioner"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "approximates",
      "target": "LU Factorization"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "used_as",
      "target": "Preconditioner"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "extends",
      "target": "Gaussian Elimination"
    },
    {
      "source": "Incomplete LU Factorization",
      "type": "controls",
      "target": "Fill-in"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "approximates",
      "target": "Matrix Inverse"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "avoids",
      "target": "Triangular Solves"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "competes_with",
      "target": "Incomplete LU"
    },
    {
      "source": "Sparse Approximate Inverse",
      "type": "enables",
      "target": "High Parallelism"
    },
    {
      "source": "Left Preconditioning",
      "type": "variant_of",
      "target": "Preconditioning"
    },
    {
      "source": "Left Preconditioning",
      "type": "preserves",
      "target": "Solution Vector"
    },
    {
      "source": "Left Preconditioning",
      "type": "changes",
      "target": "Residual Norm"
    },
    {
      "source": "Left Preconditioning",
      "type": "contrasts",
      "target": "Right Preconditioning"
    },
    {
      "source": "Right Preconditioning",
      "type": "variant_of",
      "target": "Preconditioning"
    },
    {
      "source": "Right Preconditioning",
      "type": "preserves",
      "target": "Residual Norm"
    },
    {
      "source": "Right Preconditioning",
      "type": "requires",
      "target": "Extra Solve"
    },
    {
      "source": "Right Preconditioning",
      "type": "used_in",
      "target": "Domain Decomposition"
    },
    {
      "source": "Splitting Methods",
      "type": "underlies",
      "target": "Jacobi Method"
    },
    {
      "source": "Splitting Methods",
      "type": "underlies",
      "target": "Gauss-Seidel Method"
    },
    {
      "source": "Splitting Methods",
      "type": "generalizes",
      "target": "Richardson Iteration"
    },
    {
      "source": "Splitting Methods",
      "type": "foundation_of",
      "target": "Preconditioning"
    },
    {
      "source": "Cholesky Factorization",
      "type": "specializes",
      "target": "LU Factorization"
    },
    {
      "source": "Cholesky Factorization",
      "type": "requires",
      "target": "Positive Definite Matrix"
    },
    {
      "source": "Cholesky Factorization",
      "type": "outputs",
      "target": "Upper Triangular Factor"
    },
    {
      "source": "Cholesky Factorization",
      "type": "related_to",
      "target": "LDL* Decomposition"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "generalization_of",
      "target": "Positive Semidefinite"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "enables",
      "target": "Cholesky Factorization"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "preserved_under",
      "target": "Congruence Transformation"
    },
    {
      "source": "Positive Definite Matrix",
      "type": "related_to",
      "target": "Elliptic PDE"
    },
    {
      "source": "Sylvester Equation",
      "type": "generalizes",
      "target": "Lyapunov Equation"
    },
    {
      "source": "Sylvester Equation",
      "type": "solved_via",
      "target": "Schur Triangulation"
    },
    {
      "source": "Sylvester Equation",
      "type": "used_in",
      "target": "Control Synthesis"
    },
    {
      "source": "Sylvester Equation",
      "type": "related_to",
      "target": "Kronecker Product"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "computed_by",
      "target": "Fast Fourier Transform"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "diagonalizes",
      "target": "Circulant Matrix"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "unitary_up_to_scaling",
      "target": "Fₙ*/√n"
    },
    {
      "source": "Discrete Fourier Transform (DFT)",
      "type": "related_to",
      "target": "Vandermonde Matrix"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "implements",
      "target": "Discrete Fourier Transform"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "complexity_reduction",
      "target": "O(n²) to O(n log n)"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "uses",
      "target": "Twiddle Factors"
    },
    {
      "source": "Fast Fourier Transform (FFT)",
      "type": "related_to",
      "target": "Butterfly Diagram"
    },
    {
      "source": "Schur Triangulation",
      "type": "generalizes",
      "target": "Hessenberg Form"
    },
    {
      "source": "Schur Triangulation",
      "type": "enables",
      "target": "Sylvester Equation Solution"
    },
    {
      "source": "Schur Triangulation",
      "type": "computed_by",
      "target": "QR Algorithm"
    },
    {
      "source": "Schur Triangulation",
      "type": "related_to",
      "target": "Unitary Similarity"
    },
    {
      "source": "Iterative Methods",
      "type": "alternative_to",
      "target": "Direct Methods"
    },
    {
      "source": "Iterative Methods",
      "type": "uses",
      "target": "Krylov Subspace"
    },
    {
      "source": "Iterative Methods",
      "type": "improved_by",
      "target": "Preconditioning"
    },
    {
      "source": "Iterative Methods",
      "type": "measures_convergence",
      "target": "Residual Norm"
    },
    {
      "source": "Linear System",
      "type": "solved_by",
      "target": "Iterative Methods"
    },
    {
      "source": "Linear System",
      "type": "preconditioned_as",
      "target": "M^{-1} A x = M^{-1} b"
    },
    {
      "source": "Linear System",
      "type": "residual",
      "target": "r = b - A x"
    },
    {
      "source": "Linear System",
      "type": "convergence_bound",
      "target": "Polynomial Minimization"
    },
    {
      "source": "Krylov Subspace",
      "type": "generated_by",
      "target": "Matrix Powers"
    },
    {
      "source": "Krylov Subspace",
      "type": "orthogonalized_by",
      "target": "Arnoldi Process"
    },
    {
      "source": "Krylov Subspace",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Krylov Subspace",
      "type": "invariant_under",
      "target": "A"
    },
    {
      "source": "Arnoldi Process",
      "type": "orthogonalises",
      "target": "Krylov Subspace"
    },
    {
      "source": "Arnoldi Process",
      "type": "produces",
      "target": "Hessenberg Matrix"
    },
    {
      "source": "Arnoldi Process",
      "type": "used_in",
      "target": "GMRES"
    },
    {
      "source": "Arnoldi Process",
      "type": "similar_to",
      "target": "Lanczos Algorithm"
    },
    {
      "source": "GMRES",
      "type": "based_on",
      "target": "Arnoldi Process"
    },
    {
      "source": "GMRES",
      "type": "minimises",
      "target": "Residual Norm"
    },
    {
      "source": "GMRES",
      "type": "for",
      "target": "Nonsymmetric Matrices"
    },
    {
      "source": "GMRES",
      "type": "variant_of",
      "target": "Krylov Subspace Methods"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "equivalent_to",
      "target": "Lanczos Algorithm (for SPD)"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "minimises",
      "target": "A-Norm Error"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "for",
      "target": "Symmetric Positive Definite Matrices"
    },
    {
      "source": "Conjugate Gradient Method",
      "type": "improved_by",
      "target": "Preconditioning"
    },
    {
      "source": "Preconditioning",
      "type": "improves",
      "target": "Iterative Methods"
    },
    {
      "source": "Preconditioning",
      "type": "approximates",
      "target": "A⁻¹"
    },
    {
      "source": "Preconditioning",
      "type": "used_in",
      "target": "PCG"
    },
    {
      "source": "Preconditioning",
      "type": "related_to",
      "target": "Condition Number"
    },
    {
      "source": "Residual Norm",
      "type": "minimised_by",
      "target": "GMRES"
    },
    {
      "source": "Residual Norm",
      "type": "orthogonal_to",
      "target": "Search Directions (in CG)"
    },
    {
      "source": "Residual Norm",
      "type": "computed_in",
      "target": "Iterative Methods"
    },
    {
      "source": "Residual Norm",
      "type": "bounds_convergence",
      "target": "Polynomial Approximation"
    },
    {
      "source": "A-Norm Error",
      "type": "minimised_by",
      "target": "Conjugate Gradient Method"
    },
    {
      "source": "A-Norm Error",
      "type": "related_to",
      "target": "Quadratic Form"
    },
    {
      "source": "A-Norm Error",
      "type": "for",
      "target": "SPD Matrices"
    },
    {
      "source": "A-Norm Error",
      "type": "bounds_via",
      "target": "Chebyshev Polynomials"
    },
    {
      "source": "Polynomial Approximation",
      "type": "underlies",
      "target": "Krylov Methods Convergence"
    },
    {
      "source": "Polynomial Approximation",
      "type": "uses",
      "target": "Eigenvalue Spectrum"
    },
    {
      "source": "Polynomial Approximation",
      "type": "improved_by",
      "target": "Preconditioning (clustering eigenvalues)"
    },
    {
      "source": "Polynomial Approximation",
      "type": "related_to",
      "target": "Condition Number"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "produced_by",
      "target": "Arnoldi Process"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "used_in",
      "target": "GMRES Least Squares"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "similar_to",
      "target": "Tridiagonal Matrix (symmetric case)"
    },
    {
      "source": "Hessenberg Matrix",
      "type": "decomposed_by",
      "target": "QR Factorisation"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "generalizes_to",
      "target": "Generalized Eigenvalue Problem"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "solved_by",
      "target": "QR Algorithm"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "foundation_for",
      "target": "Matrix Functions"
    },
    {
      "source": "Eigenvalue Problem",
      "type": "related_to",
      "target": "Jordan Canonical Form"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "bounds",
      "target": "Spectrum"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "refined_by",
      "target": "Brauer Cassini Ovals"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "used_in",
      "target": "Initial Eigenvalue Estimation"
    },
    {
      "source": "Gershgorin Circle Theorem",
      "type": "related_to",
      "target": "Spectral Radius"
    },
    {
      "source": "Power Iteration",
      "type": "special_case_of",
      "target": "Subspace Iteration"
    },
    {
      "source": "Power Iteration",
      "type": "enhanced_by",
      "target": "Rayleigh Quotient Iteration"
    },
    {
      "source": "Power Iteration",
      "type": "requires",
      "target": "Spectral Gap"
    },
    {
      "source": "Power Iteration",
      "type": "related_to",
      "target": "Inverse Iteration"
    },
    {
      "source": "Householder Reflection",
      "type": "building_block_of",
      "target": "QR Algorithm"
    },
    {
      "source": "Householder Reflection",
      "type": "unitary",
      "target": "Reflection"
    },
    {
      "source": "Householder Reflection",
      "type": "used_in",
      "target": "Hessenberg Reduction"
    },
    {
      "source": "Householder Reflection",
      "type": "related_to",
      "target": "Givens Rotation"
    },
    {
      "source": "QR Algorithm",
      "type": "computes",
      "target": "Schur Form"
    },
    {
      "source": "QR Algorithm",
      "type": "uses",
      "target": "Householder Reflection"
    },
    {
      "source": "QR Algorithm",
      "type": "accelerated_by",
      "target": "Francis Shift"
    },
    {
      "source": "QR Algorithm",
      "type": "related_to",
      "target": "Hessenberg Form"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "solved_by",
      "target": "QZ Algorithm"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "reduces_to",
      "target": "Standard Eigenvalue Problem"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "generalizes",
      "target": "Eigenvalue Problem"
    },
    {
      "source": "Generalized Eigenvalue Problem",
      "type": "related_to",
      "target": "Matrix Pencil"
    },
    {
      "source": "Matrix Function",
      "type": "computed_via",
      "target": "Schur Decomposition"
    },
    {
      "source": "Matrix Function",
      "type": "preserves",
      "target": "Spectral Mapping Theorem"
    },
    {
      "source": "Matrix Function",
      "type": "used_in",
      "target": "Exponential Integrators"
    },
    {
      "source": "Matrix Function",
      "type": "related_to",
      "target": "Cauchy Integral Formula"
    },
    {
      "source": "Field of Values",
      "type": "contains",
      "target": "Spectrum"
    },
    {
      "source": "Field of Values",
      "type": "equals_for",
      "target": "Normal Matrix"
    },
    {
      "source": "Field of Values",
      "type": "bounds",
      "target": "Numerical Radius"
    },
    {
      "source": "Field of Values",
      "type": "related_to",
      "target": "Pseudospectrum"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "improves",
      "target": "Gaussian Elimination"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "uses",
      "target": "Permutation Matrix"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "produces",
      "target": "Lower Triangular Matrix"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "produces",
      "target": "Upper Triangular Matrix"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "controls",
      "target": "Growth Factor"
    },
    {
      "source": "LU Factorization with Partial Pivoting",
      "type": "enables",
      "target": "Backward Stable Solver"
    },
    {
      "source": "Partial Pivoting",
      "type": "part_of",
      "target": "LU Factorization with Partial Pivoting"
    },
    {
      "source": "Partial Pivoting",
      "type": "reduces",
      "target": "Round-off Error"
    },
    {
      "source": "Partial Pivoting",
      "type": "bounds",
      "target": "Growth Factor"
    },
    {
      "source": "Partial Pivoting",
      "type": "contrasts",
      "target": "Complete Pivoting"
    },
    {
      "source": "Permutation Matrix",
      "type": "used_in",
      "target": "LU Factorization with Partial Pivoting"
    },
    {
      "source": "Permutation Matrix",
      "type": "preserves",
      "target": "Euclidean Norm"
    },
    {
      "source": "Permutation Matrix",
      "type": "inverts_to",
      "target": "Itself"
    },
    {
      "source": "Permutation Matrix",
      "type": "represents",
      "target": "Row Permutation"
    },
    {
      "source": "Growth Factor",
      "type": "bounded_by",
      "target": "Partial Pivoting"
    },
    {
      "source": "Growth Factor",
      "type": "affects",
      "target": "Backward Stability"
    },
    {
      "source": "Growth Factor",
      "type": "measured_in",
      "target": "Upper Triangular Factor"
    },
    {
      "source": "Condition Number",
      "type": "estimated_via",
      "target": "LU Factorization"
    },
    {
      "source": "Condition Number",
      "type": "equals",
      "target": "Ratio of Extreme Singular Values"
    },
    {
      "source": "Condition Number",
      "type": "impacts",
      "target": "Solution Accuracy"
    },
    {
      "source": "Gaussian Elimination",
      "type": "foundation_of",
      "target": "LU Factorization"
    },
    {
      "source": "Gaussian Elimination",
      "type": "enhanced_by",
      "target": "Partial Pivoting"
    },
    {
      "source": "Gaussian Elimination",
      "type": "performs",
      "target": "Row Reduction"
    },
    {
      "source": "Krylov Subspace",
      "type": "generated_from",
      "target": "Matrix Powers"
    },
    {
      "source": "Krylov Subspace",
      "type": "used_in",
      "target": "Iterative Methods"
    },
    {
      "source": "Krylov Subspace",
      "type": "related_to",
      "target": "Invariant Subspace"
    },
    {
      "source": "Krylov Subspace",
      "type": "dimension",
      "target": "j"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "approximates",
      "target": "Square Matrix A"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "related_to",
      "target": "Low-Rank Approximation"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "uses",
      "target": "Subspaces V1 V2"
    },
    {
      "source": "Matrix Product V1 V2",
      "type": "enables",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Projection Operator",
      "type": "satisfies",
      "target": "Idempotency P^2 = P"
    },
    {
      "source": "Projection Operator",
      "type": "defines",
      "target": "Range R(P)"
    },
    {
      "source": "Projection Operator",
      "type": "complements_with",
      "target": "I - P"
    },
    {
      "source": "Projection Operator",
      "type": "used_in",
      "target": "Subspace Decomposition"
    },
    {
      "source": "Range of Projection",
      "type": "defined_by",
      "target": "Projection Operator"
    },
    {
      "source": "Range of Projection",
      "type": "orthogonal_to",
      "target": "R(I - P)"
    },
    {
      "source": "Range of Projection",
      "type": "part_of",
      "target": "Direct Sum Decomposition"
    },
    {
      "source": "Range of Projection",
      "type": "related_to",
      "target": "Kernel of I - P"
    },
    {
      "source": "Orthogonal Complement",
      "type": "property_of",
      "target": "Orthogonal Projection"
    },
    {
      "source": "Orthogonal Complement",
      "type": "ensures",
      "target": "Direct Sum"
    },
    {
      "source": "Orthogonal Complement",
      "type": "related_to",
      "target": "Inner Product Space"
    },
    {
      "source": "Orthogonal Complement",
      "type": "used_in",
      "target": "Subspace Factoring"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "uses",
      "target": "Krylov Subspace"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "incorporates",
      "target": "Projection Operator"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "alternative_to",
      "target": "Direct Factorization"
    },
    {
      "source": "Algorithmic Factoring",
      "type": "applied_to",
      "target": "Square Matrix A"
    },
    {
      "source": "Invariant Subspace",
      "type": "related_to",
      "target": "Krylov Subspace"
    },
    {
      "source": "Invariant Subspace",
      "type": "projected_onto",
      "target": "Projection Operator"
    },
    {
      "source": "Invariant Subspace",
      "type": "used_in",
      "target": "Matrix Factoring"
    },
    {
      "source": "Invariant Subspace",
      "type": "contains",
      "target": "Eigenvectors"
    },
    {
      "source": "Square Matrix A",
      "type": "factored_by",
      "target": "Algorithmic Factoring"
    },
    {
      "source": "Square Matrix A",
      "type": "generates",
      "target": "Krylov Subspace"
    },
    {
      "source": "Square Matrix A",
      "type": "acts_on",
      "target": "Invariant Subspace"
    },
    {
      "source": "Square Matrix A",
      "type": "projected_by",
      "target": "Projection Operator"
    }
  ],
  "metadata": {
    "last_built": "2025-11-14 08:31:53",
    "node_count": 75,
    "edge_count": 330
  }
}